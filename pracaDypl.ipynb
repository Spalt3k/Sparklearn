{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"C:\\spark\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pracaDypl, master=local[*]) created by __init__ at <ipython-input-2-95e70462ced8>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-95e70462ced8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pracaDypl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 299\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pracaDypl, master=local[*]) created by __init__ at <ipython-input-2-95e70462ced8>:1 "
     ]
    }
   ],
   "source": [
    "sc=SparkContext(appName=\"pracaDypl\", master=\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"C:\\\\spark\\README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524219795100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Import danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dane, na których będziemy pracować to dump forum Stack Exchange. \n",
    "# Dane są dostępne pod adresem \"https://archive.org/details/stackexchange\".\n",
    "# Poszczególne działy forum są udostępnione w postaci plików XML. \n",
    "\n",
    "# Do dalszej analizy wybrano dział: Datascience, plik: Posts. W pliku znjdują się szczegółowe informacje odnośnie wpisów\n",
    "# na forum, poczynając od id, daty, powiązań z innymi wpisami, na treści wpisu kończąc. Celem jest wyciągnięcie samej treści \n",
    "# pytań."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bezpośrednio wczytujemy plik XML do Sparkowego RDD \n",
    "lines = sc.textFile(\"file:///Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "# # Czy ścieżkę dostępu da się zwinąć do zmiennej? a może sam url? jest jakiś problem ze znakami z XMLa, zamienia na papkę "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"utf-8\"?>',\n",
       " '<posts>',\n",
       " '  <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2014-05-13T23:58:30.457\" Score=\"8\" ViewCount=\"412\" Body=\"&lt;p&gt;I\\'ve always been interested in machine learning, but I can\\'t figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn\\'t just use relative motion, because the obstacles move around, but I don\\'t want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\" OwnerUserId=\"5\" LastActivityDate=\"2014-05-14T00:36:31.077\" Title=\"How can I do simple machine learning without hard-coding behavior?\" Tags=\"&lt;machine-learning&gt;\" AnswerCount=\"1\" CommentCount=\"1\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T14:40:25.950\" />',\n",
       " '  <row Id=\"7\" PostTypeId=\"1\" AcceptedAnswerId=\"10\" CreationDate=\"2014-05-14T00:11:06.457\" Score=\"4\" ViewCount=\"363\" Body=\"&lt;p&gt;As a researcher and instructor, I\\'m looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I\\'m especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\" OwnerUserId=\"36\" LastEditorUserId=\"97\" LastEditDate=\"2014-05-16T13:45:00.237\" LastActivityDate=\"2014-05-16T13:45:00.237\" Title=\"What open-source books (or other materials) provide a relatively thorough overview of data science?\" Tags=\"&lt;education&gt;&lt;open-source&gt;\" AnswerCount=\"3\" CommentCount=\"4\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T08:40:54.950\" />']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Poza pierwszymi dwoma wierszami, jeden wiersz to komplet informacji na temat jednego pytania lub odpowiedzi (dlaczego \n",
    "# mamy tylko nieparzyste row Id?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Jak usunąć dwa pierwsze wiersze z RDD? filter, ale może da się łatwiej? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "posts = lines.filter(lambda x: \"row Id\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2014-05-13T23:58:30.457\" Score=\"8\" ViewCount=\"412\" Body=\"&lt;p&gt;I\\'ve always been interested in machine learning, but I can\\'t figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn\\'t just use relative motion, because the obstacles move around, but I don\\'t want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\" OwnerUserId=\"5\" LastActivityDate=\"2014-05-14T00:36:31.077\" Title=\"How can I do simple machine learning without hard-coding behavior?\" Tags=\"&lt;machine-learning&gt;\" AnswerCount=\"1\" CommentCount=\"1\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T14:40:25.950\" />',\n",
       " '  <row Id=\"7\" PostTypeId=\"1\" AcceptedAnswerId=\"10\" CreationDate=\"2014-05-14T00:11:06.457\" Score=\"4\" ViewCount=\"363\" Body=\"&lt;p&gt;As a researcher and instructor, I\\'m looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I\\'m especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\" OwnerUserId=\"36\" LastEditorUserId=\"97\" LastEditDate=\"2014-05-16T13:45:00.237\" LastActivityDate=\"2014-05-16T13:45:00.237\" Title=\"What open-source books (or other materials) provide a relatively thorough overview of data science?\" Tags=\"&lt;education&gt;&lt;open-source&gt;\" AnswerCount=\"3\" CommentCount=\"4\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T08:40:54.950\" />',\n",
       " '  <row Id=\"9\" PostTypeId=\"2\" ParentId=\"5\" CreationDate=\"2014-05-14T00:36:31.077\" Score=\"5\" Body=\"&lt;p&gt;Not sure if this fits the scope of this SE, but here\\'s a stab at an answer anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all AI approaches you have to decide what it is you\\'re modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It\\'s all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;&#xA;\" OwnerUserId=\"51\" LastActivityDate=\"2014-05-14T00:36:31.077\" CommentCount=\"0\" />']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zniknęły trzy wiersze, a nie tylko pierwsze dwa... jeszcze pewnie na końcu coś wypadło\n",
    "questions.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Czyli mamy w dziale DataScience 19213 pytań i dopowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Niestety nie potrafię z takiegoe RDD wyciąć kawałka, który sobie założyłem na początku..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda Druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drugi pomysł, to użycie pakietu w Pysparku do importu xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%AddDeps` not found.\n"
     ]
    }
   ],
   "source": [
    "%AddDeps com.databricks spark-csv_2.10 1.2.0 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.xml. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:546)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:302)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:530)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6e86155bcfbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowTag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"file:///Users/Spal/Desktop/Praca dyplomowa/XML testing/Users1.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.xml. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:546)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:302)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:530)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='user'). \\\n",
    "load(\"file:///Users/Spal/Desktop/Praca dyplomowa/XML testing/Users1.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version # która wersja sparka, Scala jest w wersji 2.11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sypie błędami i nie umiem tego przeskoczyć."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda trzecia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Kolejny pomysł, to użycie pakietu spark xml bezpośrednio w scali. W notatniku wpisuję treść poleceń, które należy przekleić\n",
    "# do cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Należy uruchomić spark shell w trybie xml. Cyferki na końcu to wersja scali, która ma odpowiadać zainstalowanej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark-shell --packages com.databricks:spark-xml_2.11:0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Następnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Zaimportować sql context\n",
    "import org.apache.spark.sql.SQLContext\n",
    "# Dodac sql context\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "# Wczytanie pliku, utworzenie data framu, jest problem z prawidłowym podaniem ścieżki pliku\n",
    "val df = sqlContext.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"user\").load(\"file:///Users/Spal/Desktop/Praca dyplomowa/Dane/Users.xml\")\n",
    "val df4 = sqlContext.read.format(\"com.databricks.spark.xml\").schema(customSchema).load(\"C://Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "val df5 = sqlContext.read.format(\"com.databricks.spark.xml\").load(\"C://Users/Spal/Desktop/Praca dyplomowa/XML testing/Users.xml\")\n",
    "#podgląd \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# wychodzi pusta data frame bez informacji o błedzie... nie wiem co jest nie tak... format pliku xml trochę inny niż wymagany?\n",
    "# brak zamknięcia </row> ? \n",
    "# Problem wynika z tego, że w dokumenatcji mamy \"At the moment, rows containing self closing xml tags are not supported\"...\n",
    "# czyli XML z danymi jest akurat w takiej postaci, której nie rozumie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Znalazłem obejście - przekształcenie pliku XML do postaci zjadliwej dla spark xml\n",
    "# trzeba utworzyć plik xsl, w którym definiujemy nową strukturę xml \n",
    "# w pliku xml trzeba zapisać odniesienie do xls\n",
    "# po otwarciu za pomocą word zapisuje jako oddzielny plik xml\n",
    "# trzeba go pozniej otworzyc w notatniku i wyciać odniesienei do xsla \n",
    "# i to już jest plik, który spark łyknie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Metoda czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bezpośrednio w Scali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# piszemy\n",
    "import xml._\n",
    "XML.loadFile(\"C://Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "# i cały plik się wyświetli w konsoli, co mi z tego? nie mam pojęcia\n",
    "# wyciąganie zawartości\n",
    "res1 \\ \"row\"\n",
    "res2 \\\\\"@Body\" # to działa, ale czy to jest BIG DATA czy tylko programowanie w Scali?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Analiza zbioru. Proste transformacje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda pierwsza cd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Potrzebna metoda na wyfiltrowanie konkretnego fragmentu informacji z wiersza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Jak dobrać się do konkretnej porcji danych? Pierwszy pomysł: map, split i rozdzielenie spacja? źle...\n",
    "fields = posts.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"5\"',\n",
       "  'PostTypeId=\"1\"',\n",
       "  'CreationDate=\"2014-05-13T23:58:30.457\"',\n",
       "  'Score=\"8\"',\n",
       "  'ViewCount=\"412\"',\n",
       "  'Body=\"&lt;p&gt;I\\'ve',\n",
       "  'always',\n",
       "  'been',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'machine',\n",
       "  'learning,',\n",
       "  'but',\n",
       "  'I',\n",
       "  \"can't\",\n",
       "  'figure',\n",
       "  'out',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'about',\n",
       "  'starting',\n",
       "  'out',\n",
       "  'with',\n",
       "  'a',\n",
       "  'simple',\n",
       "  '&quot;Hello',\n",
       "  'World&quot;',\n",
       "  'example',\n",
       "  '-',\n",
       "  'how',\n",
       "  'can',\n",
       "  'I',\n",
       "  'avoid',\n",
       "  'hard-coding',\n",
       "  'behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For',\n",
       "  'example,',\n",
       "  'if',\n",
       "  'I',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  '&quot;teach&quot;',\n",
       "  'a',\n",
       "  'bot',\n",
       "  'how',\n",
       "  'to',\n",
       "  'avoid',\n",
       "  'randomly',\n",
       "  'placed',\n",
       "  'obstacles,',\n",
       "  'I',\n",
       "  \"couldn't\",\n",
       "  'just',\n",
       "  'use',\n",
       "  'relative',\n",
       "  'motion,',\n",
       "  'because',\n",
       "  'the',\n",
       "  'obstacles',\n",
       "  'move',\n",
       "  'around,',\n",
       "  'but',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'hard',\n",
       "  'code,',\n",
       "  'say,',\n",
       "  'distance,',\n",
       "  'because',\n",
       "  'that',\n",
       "  'ruins',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'point',\n",
       "  'of',\n",
       "  'machine',\n",
       "  'learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously,',\n",
       "  'randomly',\n",
       "  'generating',\n",
       "  'code',\n",
       "  'would',\n",
       "  'be',\n",
       "  'impractical,',\n",
       "  'so',\n",
       "  'how',\n",
       "  'could',\n",
       "  'I',\n",
       "  'do',\n",
       "  'this?&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"5\"',\n",
       "  'LastActivityDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'Title=\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'do',\n",
       "  'simple',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'without',\n",
       "  'hard-coding',\n",
       "  'behavior?\"',\n",
       "  'Tags=\"&lt;machine-learning&gt;\"',\n",
       "  'AnswerCount=\"1\"',\n",
       "  'CommentCount=\"1\"',\n",
       "  'FavoriteCount=\"1\"',\n",
       "  'ClosedDate=\"2014-05-14T14:40:25.950\"',\n",
       "  '/>'],\n",
       " ['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"7\"',\n",
       "  'PostTypeId=\"1\"',\n",
       "  'AcceptedAnswerId=\"10\"',\n",
       "  'CreationDate=\"2014-05-14T00:11:06.457\"',\n",
       "  'Score=\"4\"',\n",
       "  'ViewCount=\"363\"',\n",
       "  'Body=\"&lt;p&gt;As',\n",
       "  'a',\n",
       "  'researcher',\n",
       "  'and',\n",
       "  'instructor,',\n",
       "  \"I'm\",\n",
       "  'looking',\n",
       "  'for',\n",
       "  'open-source',\n",
       "  'books',\n",
       "  '(or',\n",
       "  'similar',\n",
       "  'materials)',\n",
       "  'that',\n",
       "  'provide',\n",
       "  'a',\n",
       "  'relatively',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'of',\n",
       "  'data',\n",
       "  'science',\n",
       "  'from',\n",
       "  'an',\n",
       "  'applied',\n",
       "  'perspective.',\n",
       "  'To',\n",
       "  'be',\n",
       "  'clear,',\n",
       "  \"I'm\",\n",
       "  'especially',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'a',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'that',\n",
       "  'provides',\n",
       "  'material',\n",
       "  'suitable',\n",
       "  'for',\n",
       "  'a',\n",
       "  'college-level',\n",
       "  'course,',\n",
       "  'not',\n",
       "  'particular',\n",
       "  'pieces',\n",
       "  'or',\n",
       "  'papers.&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"36\"',\n",
       "  'LastEditorUserId=\"97\"',\n",
       "  'LastEditDate=\"2014-05-16T13:45:00.237\"',\n",
       "  'LastActivityDate=\"2014-05-16T13:45:00.237\"',\n",
       "  'Title=\"What',\n",
       "  'open-source',\n",
       "  'books',\n",
       "  '(or',\n",
       "  'other',\n",
       "  'materials)',\n",
       "  'provide',\n",
       "  'a',\n",
       "  'relatively',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'of',\n",
       "  'data',\n",
       "  'science?\"',\n",
       "  'Tags=\"&lt;education&gt;&lt;open-source&gt;\"',\n",
       "  'AnswerCount=\"3\"',\n",
       "  'CommentCount=\"4\"',\n",
       "  'FavoriteCount=\"1\"',\n",
       "  'ClosedDate=\"2014-05-14T08:40:54.950\"',\n",
       "  '/>'],\n",
       " ['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"9\"',\n",
       "  'PostTypeId=\"2\"',\n",
       "  'ParentId=\"5\"',\n",
       "  'CreationDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'Score=\"5\"',\n",
       "  'Body=\"&lt;p&gt;Not',\n",
       "  'sure',\n",
       "  'if',\n",
       "  'this',\n",
       "  'fits',\n",
       "  'the',\n",
       "  'scope',\n",
       "  'of',\n",
       "  'this',\n",
       "  'SE,',\n",
       "  'but',\n",
       "  \"here's\",\n",
       "  'a',\n",
       "  'stab',\n",
       "  'at',\n",
       "  'an',\n",
       "  'answer',\n",
       "  'anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With',\n",
       "  'all',\n",
       "  'AI',\n",
       "  'approaches',\n",
       "  'you',\n",
       "  'have',\n",
       "  'to',\n",
       "  'decide',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  \"you're\",\n",
       "  'modelling',\n",
       "  'and',\n",
       "  'what',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'uncertainty',\n",
       "  'there',\n",
       "  'is.',\n",
       "  'Once',\n",
       "  'you',\n",
       "  'pick',\n",
       "  'a',\n",
       "  'framework',\n",
       "  'that',\n",
       "  'allows',\n",
       "  'modelling',\n",
       "  'of',\n",
       "  'your',\n",
       "  'situation,',\n",
       "  'you',\n",
       "  'then',\n",
       "  'see',\n",
       "  'which',\n",
       "  'elements',\n",
       "  'are',\n",
       "  '&quot;fixed&quot;',\n",
       "  'and',\n",
       "  'which',\n",
       "  'are',\n",
       "  'flexible.',\n",
       "  'For',\n",
       "  'example,',\n",
       "  'the',\n",
       "  'model',\n",
       "  'may',\n",
       "  'allow',\n",
       "  'you',\n",
       "  'to',\n",
       "  'define',\n",
       "  'your',\n",
       "  'own',\n",
       "  'network',\n",
       "  'structure',\n",
       "  '(or',\n",
       "  'even',\n",
       "  'learn',\n",
       "  'it)',\n",
       "  'with',\n",
       "  'certain',\n",
       "  'constraints.',\n",
       "  'You',\n",
       "  'have',\n",
       "  'to',\n",
       "  'decide',\n",
       "  'whether',\n",
       "  'this',\n",
       "  'flexibility',\n",
       "  'is',\n",
       "  'sufficient',\n",
       "  'for',\n",
       "  'your',\n",
       "  'purposes.',\n",
       "  'Then',\n",
       "  'within',\n",
       "  'a',\n",
       "  'particular',\n",
       "  'network',\n",
       "  'structure,',\n",
       "  'you',\n",
       "  'can',\n",
       "  'learn',\n",
       "  'parameters',\n",
       "  'given',\n",
       "  'a',\n",
       "  'specific',\n",
       "  'training',\n",
       "  'dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You',\n",
       "  'rarely',\n",
       "  'hard-code',\n",
       "  'behavior',\n",
       "  'in',\n",
       "  'AI/ML',\n",
       "  'solutions.',\n",
       "  \"It's\",\n",
       "  'all',\n",
       "  'about',\n",
       "  'modelling',\n",
       "  'the',\n",
       "  'underlying',\n",
       "  'situation',\n",
       "  'and',\n",
       "  'accommodating',\n",
       "  'different',\n",
       "  'situations',\n",
       "  'by',\n",
       "  'tweaking',\n",
       "  'elements',\n",
       "  'of',\n",
       "  'the',\n",
       "  'model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In',\n",
       "  'your',\n",
       "  'example,',\n",
       "  'perhaps',\n",
       "  'you',\n",
       "  'might',\n",
       "  'have',\n",
       "  'the',\n",
       "  'robot',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'to',\n",
       "  'detect',\n",
       "  'obstacles',\n",
       "  '(by',\n",
       "  'analyzing',\n",
       "  'elements',\n",
       "  'in',\n",
       "  'the',\n",
       "  'environment),',\n",
       "  'or',\n",
       "  'you',\n",
       "  'might',\n",
       "  'have',\n",
       "  'it',\n",
       "  'keep',\n",
       "  'track',\n",
       "  'of',\n",
       "  'where',\n",
       "  'the',\n",
       "  'obstacles',\n",
       "  'were',\n",
       "  'and',\n",
       "  'which',\n",
       "  'way',\n",
       "  'they',\n",
       "  'were',\n",
       "  'moving.&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"51\"',\n",
       "  'LastActivityDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'CommentCount=\"0\"',\n",
       "  '/>']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Porcja danych ma zawsze postać XX=\"....\". Dla danego XX chciałbym wydobyć fragment znajdujący się pomiędzy \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# czyli jakoś tak, pożniej odliczyć na którym miejscu jest interesujące nas pole i ognia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = posts.map(lambda x: x.split('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['  <row Id=',\n",
       "  '5',\n",
       "  ' PostTypeId=',\n",
       "  '1',\n",
       "  ' CreationDate=',\n",
       "  '2014-05-13T23:58:30.457',\n",
       "  ' Score=',\n",
       "  '8',\n",
       "  ' ViewCount=',\n",
       "  '412',\n",
       "  ' Body=',\n",
       "  \"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",\n",
       "  ' OwnerUserId=',\n",
       "  '5',\n",
       "  ' LastActivityDate=',\n",
       "  '2014-05-14T00:36:31.077',\n",
       "  ' Title=',\n",
       "  'How can I do simple machine learning without hard-coding behavior?',\n",
       "  ' Tags=',\n",
       "  '&lt;machine-learning&gt;',\n",
       "  ' AnswerCount=',\n",
       "  '1',\n",
       "  ' CommentCount=',\n",
       "  '1',\n",
       "  ' FavoriteCount=',\n",
       "  '1',\n",
       "  ' ClosedDate=',\n",
       "  '2014-05-14T14:40:25.950',\n",
       "  ' />']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
