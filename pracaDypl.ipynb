{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"C:\\spark\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc=SparkContext(appName=\"pracaDypl\", master=\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## sprawdzenie czy działa spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"C:\\\\spark\\README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1525002273277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Import danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dane, na których będziemy pracować to dump forum Stack Exchange. \n",
    "# Dane są dostępne pod adresem \"https://archive.org/details/stackexchange\".\n",
    "# Poszczególne działy forum są udostępnione w postaci plików XML. \n",
    "\n",
    "# Do dalszej analizy wybrano dział: Datascience, plik: Posts. W pliku znjdują się szczegółowe informacje odnośnie wpisów\n",
    "# na forum, poczynając od id, daty, powiązań z innymi wpisami, na treści wpisu kończąc. Celem jest wyciągnięcie samej treści \n",
    "# pytań."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bezpośrednio wczytujemy plik XML do Sparkowego RDD \n",
    "lines = sc.textFile(\"file:///Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "# # Czy ścieżkę dostępu da się zwinąć do zmiennej? a może sam url? jest jakiś problem ze znakami z XMLa, zamienia na papkę "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"utf-8\"?>',\n",
       " '<posts>',\n",
       " '  <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2014-05-13T23:58:30.457\" Score=\"8\" ViewCount=\"412\" Body=\"&lt;p&gt;I\\'ve always been interested in machine learning, but I can\\'t figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn\\'t just use relative motion, because the obstacles move around, but I don\\'t want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\" OwnerUserId=\"5\" LastActivityDate=\"2014-05-14T00:36:31.077\" Title=\"How can I do simple machine learning without hard-coding behavior?\" Tags=\"&lt;machine-learning&gt;\" AnswerCount=\"1\" CommentCount=\"1\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T14:40:25.950\" />',\n",
       " '  <row Id=\"7\" PostTypeId=\"1\" AcceptedAnswerId=\"10\" CreationDate=\"2014-05-14T00:11:06.457\" Score=\"4\" ViewCount=\"363\" Body=\"&lt;p&gt;As a researcher and instructor, I\\'m looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I\\'m especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\" OwnerUserId=\"36\" LastEditorUserId=\"97\" LastEditDate=\"2014-05-16T13:45:00.237\" LastActivityDate=\"2014-05-16T13:45:00.237\" Title=\"What open-source books (or other materials) provide a relatively thorough overview of data science?\" Tags=\"&lt;education&gt;&lt;open-source&gt;\" AnswerCount=\"3\" CommentCount=\"4\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T08:40:54.950\" />',\n",
       " '  <row Id=\"9\" PostTypeId=\"2\" ParentId=\"5\" CreationDate=\"2014-05-14T00:36:31.077\" Score=\"5\" Body=\"&lt;p&gt;Not sure if this fits the scope of this SE, but here\\'s a stab at an answer anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all AI approaches you have to decide what it is you\\'re modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It\\'s all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;&#xA;\" OwnerUserId=\"51\" LastActivityDate=\"2014-05-14T00:36:31.077\" CommentCount=\"0\" />',\n",
       " '  <row Id=\"10\" PostTypeId=\"2\" ParentId=\"7\" CreationDate=\"2014-05-14T00:53:43.273\" Score=\"12\" Body=\"&lt;p&gt;One book that\\'s freely available is &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (published by Springer): &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot;&gt;see Tibshirani\\'s website&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another fantastic source, although it isn\\'t a book, is Andrew Ng\\'s Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.&lt;/p&gt;&#xA;\" OwnerUserId=\"22\" LastActivityDate=\"2014-05-14T00:53:43.273\" CommentCount=\"1\" />']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Poza pierwszymi dwoma wierszami i ostatnim, jeden wiersz to komplet informacji na temat jednego pytania lub odpowiedzi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Jak usunąć dwa pierwsze wiersze z RDD? filter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "posts = lines.filter(lambda x: \"row Id\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2014-05-13T23:58:30.457\" Score=\"8\" ViewCount=\"412\" Body=\"&lt;p&gt;I\\'ve always been interested in machine learning, but I can\\'t figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn\\'t just use relative motion, because the obstacles move around, but I don\\'t want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\" OwnerUserId=\"5\" LastActivityDate=\"2014-05-14T00:36:31.077\" Title=\"How can I do simple machine learning without hard-coding behavior?\" Tags=\"&lt;machine-learning&gt;\" AnswerCount=\"1\" CommentCount=\"1\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T14:40:25.950\" />',\n",
       " '  <row Id=\"7\" PostTypeId=\"1\" AcceptedAnswerId=\"10\" CreationDate=\"2014-05-14T00:11:06.457\" Score=\"4\" ViewCount=\"363\" Body=\"&lt;p&gt;As a researcher and instructor, I\\'m looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I\\'m especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\" OwnerUserId=\"36\" LastEditorUserId=\"97\" LastEditDate=\"2014-05-16T13:45:00.237\" LastActivityDate=\"2014-05-16T13:45:00.237\" Title=\"What open-source books (or other materials) provide a relatively thorough overview of data science?\" Tags=\"&lt;education&gt;&lt;open-source&gt;\" AnswerCount=\"3\" CommentCount=\"4\" FavoriteCount=\"1\" ClosedDate=\"2014-05-14T08:40:54.950\" />',\n",
       " '  <row Id=\"9\" PostTypeId=\"2\" ParentId=\"5\" CreationDate=\"2014-05-14T00:36:31.077\" Score=\"5\" Body=\"&lt;p&gt;Not sure if this fits the scope of this SE, but here\\'s a stab at an answer anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all AI approaches you have to decide what it is you\\'re modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It\\'s all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;&#xA;\" OwnerUserId=\"51\" LastActivityDate=\"2014-05-14T00:36:31.077\" CommentCount=\"0\" />']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zniknęły trzy wiersze, a nie tylko pierwsze dwa... jeszcze pewnie na końcu coś wypadło\n",
    "posts.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Czyli mamy w dziale DataScience 19213 pytań i dopowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Z tego miejsca skaczemy do Tomkowego pomysłu na dzielenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda Druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drugi pomysł, to użycie pakietu w Pysparku do importu xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o173.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.xml. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:546)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:302)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:530)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6e86155bcfbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowTag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"file:///Users/Spal/Desktop/Praca dyplomowa/XML testing/Users1.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o173.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.xml. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:546)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:302)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:530)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='user'). \\\n",
    "load(\"file:///Users/Spal/Desktop/Praca dyplomowa/XML testing/Users1.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version # która wersja sparka, Scala jest w wersji 2.11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sypie błędami i nie umiem tego przeskoczyć."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda trzecia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Kolejny pomysł, to użycie pakietu spark xml bezpośrednio w scali. W notatniku wpisuję treść poleceń, które należy przekleić\n",
    "# do cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Należy uruchomić spark shell w trybie xml. Cyferki na końcu to wersja scali, która ma odpowiadać zainstalowanej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark-shell --packages com.databricks:spark-xml_2.11:0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Następnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Zaimportować sql context\n",
    "import org.apache.spark.sql.SQLContext\n",
    "# Dodac sql context\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "# Wczytanie pliku, utworzenie data framu, jest problem z prawidłowym podaniem ścieżki pliku\n",
    "val df = sqlContext.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"user\").load(\"file:///Users/Spal/Desktop/Praca dyplomowa/Dane/Users.xml\")\n",
    "val df4 = sqlContext.read.format(\"com.databricks.spark.xml\").schema(customSchema).load(\"C://Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "val df5 = sqlContext.read.format(\"com.databricks.spark.xml\").load(\"C://Users/Spal/Desktop/Praca dyplomowa/XML testing/Users.xml\")\n",
    "#podgląd \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# wychodzi pusta data frame bez informacji o błedzie... nie wiem co jest nie tak... format pliku xml trochę inny niż wymagany?\n",
    "# brak zamknięcia </row> ? \n",
    "# Problem wynika z tego, że w dokumenatcji mamy \"At the moment, rows containing self closing xml tags are not supported\"...\n",
    "# czyli XML z danymi jest akurat w takiej postaci, której nie rozumie spark xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Znalazłem obejście - przekształcenie pliku XML do postaci zjadliwej dla spark xml\n",
    "# trzeba utworzyć plik xsl, w którym definiujemy nową strukturę xml \n",
    "# w pliku xml trzeba zapisać odniesienie do xls\n",
    "# po otwarciu za pomocą word zapisuje jako oddzielny plik xml\n",
    "# trzeba go pozniej otworzyc w notatniku i wyciać odniesienei do xsla \n",
    "# i to już jest plik, który spark łyknie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Metoda czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bezpośrednio w Scali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# piszemy\n",
    "import xml._\n",
    "XML.loadFile(\"C://Users/Spal/Desktop/Praca dyplomowa/Dane/Posts.xml\")\n",
    "# i cały plik się wyświetli w konsoli, co mi z tego? nie mam pojęcia\n",
    "# wyciąganie zawartości\n",
    "res1 \\ \"row\"\n",
    "res2 \\\\\"@Body\" # to działa, ale czy to jest BIG DATA czy tylko programowanie w Scali?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Analiza zbioru. Proste transformacje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metoda pierwsza cd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Potrzebna metoda na wyfiltrowanie konkretnego fragmentu informacji z wiersza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Jak dobrać się do konkretnej porcji danych? Pierwszy pomysł: map, split i rozdzielenie spacja? źle...\n",
    "fields = posts.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"5\"',\n",
       "  'PostTypeId=\"1\"',\n",
       "  'CreationDate=\"2014-05-13T23:58:30.457\"',\n",
       "  'Score=\"8\"',\n",
       "  'ViewCount=\"412\"',\n",
       "  'Body=\"&lt;p&gt;I\\'ve',\n",
       "  'always',\n",
       "  'been',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'machine',\n",
       "  'learning,',\n",
       "  'but',\n",
       "  'I',\n",
       "  \"can't\",\n",
       "  'figure',\n",
       "  'out',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'about',\n",
       "  'starting',\n",
       "  'out',\n",
       "  'with',\n",
       "  'a',\n",
       "  'simple',\n",
       "  '&quot;Hello',\n",
       "  'World&quot;',\n",
       "  'example',\n",
       "  '-',\n",
       "  'how',\n",
       "  'can',\n",
       "  'I',\n",
       "  'avoid',\n",
       "  'hard-coding',\n",
       "  'behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For',\n",
       "  'example,',\n",
       "  'if',\n",
       "  'I',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  '&quot;teach&quot;',\n",
       "  'a',\n",
       "  'bot',\n",
       "  'how',\n",
       "  'to',\n",
       "  'avoid',\n",
       "  'randomly',\n",
       "  'placed',\n",
       "  'obstacles,',\n",
       "  'I',\n",
       "  \"couldn't\",\n",
       "  'just',\n",
       "  'use',\n",
       "  'relative',\n",
       "  'motion,',\n",
       "  'because',\n",
       "  'the',\n",
       "  'obstacles',\n",
       "  'move',\n",
       "  'around,',\n",
       "  'but',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'hard',\n",
       "  'code,',\n",
       "  'say,',\n",
       "  'distance,',\n",
       "  'because',\n",
       "  'that',\n",
       "  'ruins',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'point',\n",
       "  'of',\n",
       "  'machine',\n",
       "  'learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously,',\n",
       "  'randomly',\n",
       "  'generating',\n",
       "  'code',\n",
       "  'would',\n",
       "  'be',\n",
       "  'impractical,',\n",
       "  'so',\n",
       "  'how',\n",
       "  'could',\n",
       "  'I',\n",
       "  'do',\n",
       "  'this?&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"5\"',\n",
       "  'LastActivityDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'Title=\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'do',\n",
       "  'simple',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'without',\n",
       "  'hard-coding',\n",
       "  'behavior?\"',\n",
       "  'Tags=\"&lt;machine-learning&gt;\"',\n",
       "  'AnswerCount=\"1\"',\n",
       "  'CommentCount=\"1\"',\n",
       "  'FavoriteCount=\"1\"',\n",
       "  'ClosedDate=\"2014-05-14T14:40:25.950\"',\n",
       "  '/>'],\n",
       " ['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"7\"',\n",
       "  'PostTypeId=\"1\"',\n",
       "  'AcceptedAnswerId=\"10\"',\n",
       "  'CreationDate=\"2014-05-14T00:11:06.457\"',\n",
       "  'Score=\"4\"',\n",
       "  'ViewCount=\"363\"',\n",
       "  'Body=\"&lt;p&gt;As',\n",
       "  'a',\n",
       "  'researcher',\n",
       "  'and',\n",
       "  'instructor,',\n",
       "  \"I'm\",\n",
       "  'looking',\n",
       "  'for',\n",
       "  'open-source',\n",
       "  'books',\n",
       "  '(or',\n",
       "  'similar',\n",
       "  'materials)',\n",
       "  'that',\n",
       "  'provide',\n",
       "  'a',\n",
       "  'relatively',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'of',\n",
       "  'data',\n",
       "  'science',\n",
       "  'from',\n",
       "  'an',\n",
       "  'applied',\n",
       "  'perspective.',\n",
       "  'To',\n",
       "  'be',\n",
       "  'clear,',\n",
       "  \"I'm\",\n",
       "  'especially',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'a',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'that',\n",
       "  'provides',\n",
       "  'material',\n",
       "  'suitable',\n",
       "  'for',\n",
       "  'a',\n",
       "  'college-level',\n",
       "  'course,',\n",
       "  'not',\n",
       "  'particular',\n",
       "  'pieces',\n",
       "  'or',\n",
       "  'papers.&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"36\"',\n",
       "  'LastEditorUserId=\"97\"',\n",
       "  'LastEditDate=\"2014-05-16T13:45:00.237\"',\n",
       "  'LastActivityDate=\"2014-05-16T13:45:00.237\"',\n",
       "  'Title=\"What',\n",
       "  'open-source',\n",
       "  'books',\n",
       "  '(or',\n",
       "  'other',\n",
       "  'materials)',\n",
       "  'provide',\n",
       "  'a',\n",
       "  'relatively',\n",
       "  'thorough',\n",
       "  'overview',\n",
       "  'of',\n",
       "  'data',\n",
       "  'science?\"',\n",
       "  'Tags=\"&lt;education&gt;&lt;open-source&gt;\"',\n",
       "  'AnswerCount=\"3\"',\n",
       "  'CommentCount=\"4\"',\n",
       "  'FavoriteCount=\"1\"',\n",
       "  'ClosedDate=\"2014-05-14T08:40:54.950\"',\n",
       "  '/>'],\n",
       " ['',\n",
       "  '',\n",
       "  '<row',\n",
       "  'Id=\"9\"',\n",
       "  'PostTypeId=\"2\"',\n",
       "  'ParentId=\"5\"',\n",
       "  'CreationDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'Score=\"5\"',\n",
       "  'Body=\"&lt;p&gt;Not',\n",
       "  'sure',\n",
       "  'if',\n",
       "  'this',\n",
       "  'fits',\n",
       "  'the',\n",
       "  'scope',\n",
       "  'of',\n",
       "  'this',\n",
       "  'SE,',\n",
       "  'but',\n",
       "  \"here's\",\n",
       "  'a',\n",
       "  'stab',\n",
       "  'at',\n",
       "  'an',\n",
       "  'answer',\n",
       "  'anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With',\n",
       "  'all',\n",
       "  'AI',\n",
       "  'approaches',\n",
       "  'you',\n",
       "  'have',\n",
       "  'to',\n",
       "  'decide',\n",
       "  'what',\n",
       "  'it',\n",
       "  'is',\n",
       "  \"you're\",\n",
       "  'modelling',\n",
       "  'and',\n",
       "  'what',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'uncertainty',\n",
       "  'there',\n",
       "  'is.',\n",
       "  'Once',\n",
       "  'you',\n",
       "  'pick',\n",
       "  'a',\n",
       "  'framework',\n",
       "  'that',\n",
       "  'allows',\n",
       "  'modelling',\n",
       "  'of',\n",
       "  'your',\n",
       "  'situation,',\n",
       "  'you',\n",
       "  'then',\n",
       "  'see',\n",
       "  'which',\n",
       "  'elements',\n",
       "  'are',\n",
       "  '&quot;fixed&quot;',\n",
       "  'and',\n",
       "  'which',\n",
       "  'are',\n",
       "  'flexible.',\n",
       "  'For',\n",
       "  'example,',\n",
       "  'the',\n",
       "  'model',\n",
       "  'may',\n",
       "  'allow',\n",
       "  'you',\n",
       "  'to',\n",
       "  'define',\n",
       "  'your',\n",
       "  'own',\n",
       "  'network',\n",
       "  'structure',\n",
       "  '(or',\n",
       "  'even',\n",
       "  'learn',\n",
       "  'it)',\n",
       "  'with',\n",
       "  'certain',\n",
       "  'constraints.',\n",
       "  'You',\n",
       "  'have',\n",
       "  'to',\n",
       "  'decide',\n",
       "  'whether',\n",
       "  'this',\n",
       "  'flexibility',\n",
       "  'is',\n",
       "  'sufficient',\n",
       "  'for',\n",
       "  'your',\n",
       "  'purposes.',\n",
       "  'Then',\n",
       "  'within',\n",
       "  'a',\n",
       "  'particular',\n",
       "  'network',\n",
       "  'structure,',\n",
       "  'you',\n",
       "  'can',\n",
       "  'learn',\n",
       "  'parameters',\n",
       "  'given',\n",
       "  'a',\n",
       "  'specific',\n",
       "  'training',\n",
       "  'dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You',\n",
       "  'rarely',\n",
       "  'hard-code',\n",
       "  'behavior',\n",
       "  'in',\n",
       "  'AI/ML',\n",
       "  'solutions.',\n",
       "  \"It's\",\n",
       "  'all',\n",
       "  'about',\n",
       "  'modelling',\n",
       "  'the',\n",
       "  'underlying',\n",
       "  'situation',\n",
       "  'and',\n",
       "  'accommodating',\n",
       "  'different',\n",
       "  'situations',\n",
       "  'by',\n",
       "  'tweaking',\n",
       "  'elements',\n",
       "  'of',\n",
       "  'the',\n",
       "  'model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In',\n",
       "  'your',\n",
       "  'example,',\n",
       "  'perhaps',\n",
       "  'you',\n",
       "  'might',\n",
       "  'have',\n",
       "  'the',\n",
       "  'robot',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'to',\n",
       "  'detect',\n",
       "  'obstacles',\n",
       "  '(by',\n",
       "  'analyzing',\n",
       "  'elements',\n",
       "  'in',\n",
       "  'the',\n",
       "  'environment),',\n",
       "  'or',\n",
       "  'you',\n",
       "  'might',\n",
       "  'have',\n",
       "  'it',\n",
       "  'keep',\n",
       "  'track',\n",
       "  'of',\n",
       "  'where',\n",
       "  'the',\n",
       "  'obstacles',\n",
       "  'were',\n",
       "  'and',\n",
       "  'which',\n",
       "  'way',\n",
       "  'they',\n",
       "  'were',\n",
       "  'moving.&lt;/p&gt;&#xA;\"',\n",
       "  'OwnerUserId=\"51\"',\n",
       "  'LastActivityDate=\"2014-05-14T00:36:31.077\"',\n",
       "  'CommentCount=\"0\"',\n",
       "  '/>']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Porcja danych ma zawsze postać XX=\"....\". Dla danego XX chciałbym wydobyć fragment znajdujący się pomiędzy \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# czyli jakoś tak, pożniej odliczyć na którym miejscu jest interesujące nas pole i ognia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fields = posts.map(lambda x: x.split('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['  <row Id=',\n",
       "  '5',\n",
       "  ' PostTypeId=',\n",
       "  '1',\n",
       "  ' CreationDate=',\n",
       "  '2014-05-13T23:58:30.457',\n",
       "  ' Score=',\n",
       "  '8',\n",
       "  ' ViewCount=',\n",
       "  '412',\n",
       "  ' Body=',\n",
       "  \"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",\n",
       "  ' OwnerUserId=',\n",
       "  '5',\n",
       "  ' LastActivityDate=',\n",
       "  '2014-05-14T00:36:31.077',\n",
       "  ' Title=',\n",
       "  'How can I do simple machine learning without hard-coding behavior?',\n",
       "  ' Tags=',\n",
       "  '&lt;machine-learning&gt;',\n",
       "  ' AnswerCount=',\n",
       "  '1',\n",
       "  ' CommentCount=',\n",
       "  '1',\n",
       "  ' FavoriteCount=',\n",
       "  '1',\n",
       "  ' ClosedDate=',\n",
       "  '2014-05-14T14:40:25.950',\n",
       "  ' />']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tu na szczęście przyszła odsiecz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cleaningRDD(RDDIn):\n",
    "    def cleaning(listIn):\n",
    "        listOut=[]\n",
    "        for x in listIn:\n",
    "            if listIn.index(x)%2==0:\n",
    "                listOut.append(x.replace(' ','').replace('<','').replace('=',''))\n",
    "            else:\n",
    "                listOut.append(x)\n",
    "        return {listOut[i]:listOut[i+1] for i in range(0,len(listOut)-1,2)}\n",
    "    return RDDIn.filter(lambda x: x.startswith('  <row Id=')).\\\n",
    "map(lambda x: x.split('\"')).map(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cleared=cleaningRDD(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AnswerCount': '1',\n",
       "  'Body': \"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",\n",
       "  'ClosedDate': '2014-05-14T14:40:25.950',\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2014-05-13T23:58:30.457',\n",
       "  'FavoriteCount': '1',\n",
       "  'LastActivityDate': '2014-05-14T00:36:31.077',\n",
       "  'OwnerUserId': '5',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '8',\n",
       "  'Tags': '&lt;machine-learning&gt;',\n",
       "  'Title': 'How can I do simple machine learning without hard-coding behavior?',\n",
       "  'ViewCount': '412',\n",
       "  'rowId': '5'},\n",
       " {'AcceptedAnswerId': '10',\n",
       "  'AnswerCount': '3',\n",
       "  'Body': \"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\",\n",
       "  'ClosedDate': '2014-05-14T08:40:54.950',\n",
       "  'CommentCount': '4',\n",
       "  'CreationDate': '2014-05-14T00:11:06.457',\n",
       "  'FavoriteCount': '1',\n",
       "  'LastActivityDate': '2014-05-16T13:45:00.237',\n",
       "  'LastEditDate': '2014-05-16T13:45:00.237',\n",
       "  'LastEditorUserId': '97',\n",
       "  'OwnerUserId': '36',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '4',\n",
       "  'Tags': '&lt;education&gt;&lt;open-source&gt;',\n",
       "  'Title': 'What open-source books (or other materials) provide a relatively thorough overview of data science?',\n",
       "  'ViewCount': '363',\n",
       "  'rowId': '7'},\n",
       " {'Body': \"&lt;p&gt;Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;&#xA;\",\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2014-05-14T00:36:31.077',\n",
       "  'LastActivityDate': '2014-05-14T00:36:31.077',\n",
       "  'OwnerUserId': '51',\n",
       "  'ParentId': '5',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '5',\n",
       "  'rowId': '9'},\n",
       " {'Body': \"&lt;p&gt;One book that's freely available is &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (published by Springer): &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot;&gt;see Tibshirani's website&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.&lt;/p&gt;&#xA;\",\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2014-05-14T00:53:43.273',\n",
       "  'LastActivityDate': '2014-05-14T00:53:43.273',\n",
       "  'OwnerUserId': '22',\n",
       "  'ParentId': '7',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '12',\n",
       "  'rowId': '10'},\n",
       " {'AcceptedAnswerId': '29',\n",
       "  'AnswerCount': '4',\n",
       "  'Body': '&lt;p&gt;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?&lt;/p&gt;&#xA;',\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2014-05-14T01:25:59.677',\n",
       "  'FavoriteCount': '4',\n",
       "  'LastActivityDate': '2014-06-20T17:36:05.023',\n",
       "  'LastEditDate': '2014-06-17T16:17:20.473',\n",
       "  'LastEditorUserId': '322',\n",
       "  'OwnerUserId': '66',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '19',\n",
       "  'Tags': '&lt;data-mining&gt;&lt;definitions&gt;',\n",
       "  'Title': 'Is Data Science the Same as Data Mining?',\n",
       "  'ViewCount': '999',\n",
       "  'rowId': '14'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",\n",
       " \"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\",\n",
       " \"&lt;p&gt;Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;&#xA;\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared.map(lambda x: x['Body']).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kolejna zagadka, to wybranie wszystkich komentarzy, które są pytaniem. Czyli 'PostTypeId': '1',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "lambda cannot contain assignment (<ipython-input-17-7e0d3340baa2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-7e0d3340baa2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cleared.filter(lambda x: x=\"'PostTypeId': '1'\").take(2)\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m lambda cannot contain assignment\n"
     ]
    }
   ],
   "source": [
    "cleared.filter(lambda x: x==\"'PostTypeId': '1'\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rowId': '5', 'ViewCount': '412', 'Title': 'How can I do simple machine learning without hard-coding behavior?', 'AnswerCount': '1', 'ClosedDate': '2014-05-14T14:40:25.950', 'CreationDate': '2014-05-13T23:58:30.457', 'FavoriteCount': '1', 'CommentCount': '1', 'Body': \"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\", 'LastActivityDate': '2014-05-14T00:36:31.077', 'Score': '8', 'OwnerUserId': '5', 'PostTypeId': '1', 'Tags': '&lt;machine-learning&gt;'}\n",
      "{'rowId': '7', 'ViewCount': '363', 'Title': 'What open-source books (or other materials) provide a relatively thorough overview of data science?', 'AnswerCount': '3', 'ClosedDate': '2014-05-14T08:40:54.950', 'CreationDate': '2014-05-14T00:11:06.457', 'LastEditDate': '2014-05-16T13:45:00.237', 'AcceptedAnswerId': '10', 'FavoriteCount': '1', 'CommentCount': '4', 'LastEditorUserId': '97', 'Tags': '&lt;education&gt;&lt;open-source&gt;', 'LastActivityDate': '2014-05-16T13:45:00.237', 'Score': '4', 'OwnerUserId': '36', 'PostTypeId': '1', 'Body': \"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\"}\n"
     ]
    }
   ],
   "source": [
    "for i in cleared.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-66d1ae2dc963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleared\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'PipelinedRDD' object does not support indexing"
     ]
    }
   ],
   "source": [
    "cleared[2] # nie można "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "zebrane = cleared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnswerCount': '0',\n",
       " 'Body': '&lt;p&gt;In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?&lt;/p&gt;&#xA;',\n",
       " 'ClosedDate': '2014-05-14T07:41:49.437',\n",
       " 'CommentCount': '1',\n",
       " 'CreationDate': '2014-05-14T01:41:23.110',\n",
       " 'LastActivityDate': '2014-05-14T01:41:23.110',\n",
       " 'OwnerUserId': '64',\n",
       " 'PostTypeId': '1',\n",
       " 'Score': '2',\n",
       " 'Tags': '&lt;databases&gt;',\n",
       " 'Title': 'What are the advantages and disadvantages of SQL versus NoSQL in data science?',\n",
       " 'ViewCount': '494',\n",
       " 'rowId': '15'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zebrane[5] # po collect już tak, ale collect ściąga rdd na driver, to raczej bez sensu ruch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zebrane[2][\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnswerCount': '1',\n",
       " 'Body': \"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",\n",
       " 'ClosedDate': '2014-05-14T14:40:25.950',\n",
       " 'CommentCount': '1',\n",
       " 'CreationDate': '2014-05-13T23:58:30.457',\n",
       " 'FavoriteCount': '1',\n",
       " 'LastActivityDate': '2014-05-14T00:36:31.077',\n",
       " 'OwnerUserId': '5',\n",
       " 'PostTypeId': '1',\n",
       " 'Score': '8',\n",
       " 'Tags': '&lt;machine-learning&gt;',\n",
       " 'Title': 'How can I do simple machine learning without hard-coding behavior?',\n",
       " 'ViewCount': '412',\n",
       " 'rowId': '5'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zebrane[\"Score\"==5] # tak nie działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utworzenie data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark/python\\pyspark\\sql\\session.py:356: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "df=cleared.toDF(['AnswerCount','Body','ClosedDate','CommentCount','CreationDate','FavoriteCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame_1') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark/python\\pyspark\\sql\\session.py:356: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(cleared) ##czyli że co?, płacze ale działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-aea77fc92f24>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-aea77fc92f24>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print cleared.first(1)\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print cleared.first(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "|AnswerCount|                Body|          ClosedDate|CommentCount|        CreationDate|FavoriteCount|    LastActivityDate|OwnerUserId|PostTypeId|Score|                Tags|               Title|ViewCount|rowId|\n",
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "|          1|&lt;p&gt;I've alw...|2014-05-14T14:40:...|           1|2014-05-13T23:58:...|            1|2014-05-14T00:36:...|          5|         1|    8|&lt;machine-learn...|How can I do simp...|      412|    5|\n",
      "|          3|&lt;p&gt;As a res...|2014-05-14T08:40:...|           4|2014-05-14T00:11:...|            1|2014-05-16T13:45:...|         36|         1|    4|&lt;education&gt;...|What open-source ...|      363|    7|\n",
      "|       null|&lt;p&gt;Not sure...|                null|           0|2014-05-14T00:36:...|         null|2014-05-14T00:36:...|         51|         2|    5|                null|                null|     null|    9|\n",
      "|       null|&lt;p&gt;One book...|                null|           1|2014-05-14T00:53:...|         null|2014-05-14T00:53:...|         22|         2|   12|                null|                null|     null|   10|\n",
      "|          4|&lt;p&gt;I am sur...|                null|           1|2014-05-14T01:25:...|            4|2014-06-20T17:36:...|         66|         1|   19|&lt;data-mining&g...|Is Data Science t...|      999|   14|\n",
      "|          0|&lt;p&gt;In which...|2014-05-14T07:41:...|           1|2014-05-14T01:41:...|         null|2014-05-14T01:41:...|         64|         1|    2|   &lt;databases&gt;|What are the adva...|      494|   15|\n",
      "|          2|&lt;p&gt;I use &l...|                null|           0|2014-05-14T01:57:...|         null|2014-05-17T16:24:...|         63|         1|   17|&lt;machine-learn...|Use liblinear on ...|      302|   16|\n",
      "|       null|&lt;p&gt;&lt;a hr...|                null|           0|2014-05-14T02:49:...|         null|2014-05-16T13:44:...|         63|         5|    0|                null|                null|     null|   17|\n",
      "|       null|                    |                null|           0|2014-05-14T02:49:...|         null|2014-05-14T02:49:...|         -1|         4|    0|                null|                null|     null|   18|\n",
      "|         12|&lt;p&gt;Lots of ...|                null|           5|2014-05-14T03:56:...|           16|2016-11-25T22:34:...|         84|         1|   67|&lt;bigdata&gt;&l...|How big is big data?|     6292|   19|\n",
      "|          5|&lt;p&gt;we creat...|                null|           1|2014-05-14T05:37:...|            1|2017-08-29T11:26:...|         96|         1|   16|&lt;nosql&gt;&lt;...|the data on our r...|      304|   20|\n",
      "|       null|&lt;p&gt;As you r...|                null|           0|2014-05-14T05:44:...|         null|2014-05-14T05:44:...|         14|         2|   29|                null|                null|     null|   21|\n",
      "|          7|&lt;p&gt;My data ...|                null|           2|2014-05-14T05:58:...|           66|2017-10-25T02:17:...|         97|         1|   81|&lt;data-mining&g...|K-Means clusterin...|    83843|   22|\n",
      "|       null|&lt;p&gt;Data Sci...|                null|           0|2014-05-14T06:06:...|         null|2014-05-14T06:06:...|         97|         2|    8|                null|                null|     null|   23|\n",
      "|       null|&lt;p&gt;The stan...|                null|           9|2014-05-14T06:26:...|         null|2016-11-29T20:06:...|         14|         2|   70|                null|                null|     null|   24|\n",
      "|       null|&lt;p&gt;Big Data...|                null|           0|2014-05-14T07:26:...|         null|2014-05-14T07:26:...|        104|         2|    8|                null|                null|     null|   25|\n",
      "|       null|&lt;p&gt;A few gi...|                null|           0|2014-05-14T07:38:...|         null|2014-05-14T11:03:...|        115|         2|   14|                null|                null|     null|   26|\n",
      "|       null|&lt;p&gt;To answe...|                null|           0|2014-05-14T07:53:...|         null|2014-05-14T08:03:...|        108|         2|   10|                null|                null|     null|   27|\n",
      "|       null|&lt;p&gt;There is...|                null|           0|2014-05-14T07:55:...|         null|2014-05-14T07:55:...|        118|         2|    6|                null|                null|     null|   28|\n",
      "|       null|&lt;p&gt;&lt;a hr...|                null|           0|2014-05-14T07:56:...|         null|2014-05-14T07:56:...|         53|         2|   24|                null|                null|     null|   29|\n",
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AnswerCount', 'string'),\n",
       " ('Body', 'string'),\n",
       " ('ClosedDate', 'string'),\n",
       " ('CommentCount', 'string'),\n",
       " ('CreationDate', 'string'),\n",
       " ('FavoriteCount', 'string'),\n",
       " ('LastActivityDate', 'string'),\n",
       " ('OwnerUserId', 'string'),\n",
       " ('PostTypeId', 'string'),\n",
       " ('Score', 'string'),\n",
       " ('Tags', 'string'),\n",
       " ('Title', 'string'),\n",
       " ('ViewCount', 'string'),\n",
       " ('rowId', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "|AnswerCount|                Body|          ClosedDate|CommentCount|        CreationDate|FavoriteCount|    LastActivityDate|OwnerUserId|PostTypeId|Score|                Tags|               Title|ViewCount|rowId|\n",
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "|          1|&lt;p&gt;I've alw...|2014-05-14T14:40:...|           1|2014-05-13T23:58:...|            1|2014-05-14T00:36:...|          5|         1|    8|&lt;machine-learn...|How can I do simp...|      412|    5|\n",
      "|          3|&lt;p&gt;As a res...|2014-05-14T08:40:...|           4|2014-05-14T00:11:...|            1|2014-05-16T13:45:...|         36|         1|    4|&lt;education&gt;...|What open-source ...|      363|    7|\n",
      "|       null|&lt;p&gt;Not sure...|                null|           0|2014-05-14T00:36:...|         null|2014-05-14T00:36:...|         51|         2|    5|                null|                null|     null|    9|\n",
      "|       null|&lt;p&gt;One book...|                null|           1|2014-05-14T00:53:...|         null|2014-05-14T00:53:...|         22|         2|   12|                null|                null|     null|   10|\n",
      "|          4|&lt;p&gt;I am sur...|                null|           1|2014-05-14T01:25:...|            4|2014-06-20T17:36:...|         66|         1|   19|&lt;data-mining&g...|Is Data Science t...|      999|   14|\n",
      "+-----------+--------------------+--------------------+------------+--------------------+-------------+--------------------+-----------+----------+-----+--------------------+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Body|\n",
      "+--------------------+\n",
      "|&lt;p&gt;I've alw...|\n",
      "|&lt;p&gt;As a res...|\n",
      "|&lt;p&gt;I am sur...|\n",
      "|&lt;p&gt;In which...|\n",
      "|&lt;p&gt;I use &l...|\n",
      "|&lt;p&gt;Lots of ...|\n",
      "|&lt;p&gt;we creat...|\n",
      "|&lt;p&gt;My data ...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;In worki...|\n",
      "|&lt;p&gt;I heard ...|\n",
      "|&lt;p&gt;R has ma...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;From my ...|\n",
      "|&lt;p&gt;In revie...|\n",
      "|&lt;p&gt;Logic of...|\n",
      "|&lt;p&gt;First, t...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;If small...|\n",
      "|&lt;p&gt;&lt;em&g...|\n",
      "|&lt;p&gt;&lt;stro...|\n",
      "|&lt;p&gt;What is(...|\n",
      "|&lt;p&gt;Given we...|\n",
      "|&lt;p&gt;For exam...|\n",
      "|&lt;p&gt;While bu...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;Assume t...|\n",
      "|&lt;p&gt;Consider...|\n",
      "|&lt;p&gt;When a r...|\n",
      "|&lt;p&gt;If I hav...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I want l...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;From wik...|\n",
      "|&lt;p&gt;In our c...|\n",
      "|&lt;p&gt;Any smal...|\n",
      "|&lt;p&gt;As we al...|\n",
      "|&lt;p&gt;One of t...|\n",
      "|&lt;p&gt;I see a ...|\n",
      "|&lt;p&gt;Assume a...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I have d...|\n",
      "|&lt;p&gt;I am see...|\n",
      "|&lt;p&gt;The deta...|\n",
      "|&lt;p&gt;I'm lear...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;Can some...|\n",
      "|&lt;p&gt;The most...|\n",
      "|&lt;p&gt;So we ha...|\n",
      "|&lt;p&gt;LDA has ...|\n",
      "|&lt;p&gt;Working ...|\n",
      "|&lt;p&gt;A recomm...|\n",
      "|&lt;p&gt;I'm new ...|\n",
      "|&lt;p&gt;I'm buil...|\n",
      "|&lt;p&gt;So, I ha...|\n",
      "|&lt;p&gt;Having a...|\n",
      "|&lt;p&gt;The outp...|\n",
      "|&lt;p&gt;Can some...|\n",
      "|&lt;p&gt;Going th...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;As Yann ...|\n",
      "|&lt;p&gt;Data vis...|\n",
      "|&lt;p&gt;There se...|\n",
      "|&lt;p&gt;An aspir...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;Being ne...|\n",
      "|&lt;p&gt;I am dev...|\n",
      "|&lt;p&gt;Yann LeC...|\n",
      "|&lt;p&gt;There is...|\n",
      "|&lt;p&gt;I have r...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I've bui...|\n",
      "|&lt;p&gt;The setu...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I'm just...|\n",
      "|&lt;p&gt;I've now...|\n",
      "|&lt;p&gt;Could yo...|\n",
      "|&lt;p&gt;In some ...|\n",
      "|&lt;p&gt;as I am ...|\n",
      "|&lt;p&gt;I attack...|\n",
      "|&lt;p&gt;Logic of...|\n",
      "|&lt;p&gt;What kin...|\n",
      "|&lt;p&gt;I'd like...|\n",
      "|&lt;p&gt;I'm curi...|\n",
      "|&lt;p&gt;The majo...|\n",
      "|&lt;p&gt;I'm deve...|\n",
      "|&lt;p&gt;I've cam...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I once h...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;&lt;img ...|\n",
      "|&lt;p&gt;If I hav...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;It seems...|\n",
      "|&lt;h1&gt;Motivat...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;Does any...|\n",
      "|&lt;p&gt;As an ex...|\n",
      "|&lt;p&gt;I'm plan...|\n",
      "|&lt;p&gt;I recent...|\n",
      "|&lt;p&gt;Is anyon...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I think ...|\n",
      "|&lt;p&gt;With Had...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm deve...|\n",
      "|&lt;p&gt;I unders...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;Which fr...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;There's ...|\n",
      "|&lt;p&gt;I tried ...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;The usua...|\n",
      "|&lt;p&gt;In netwo...|\n",
      "|&lt;p&gt;There's ...|\n",
      "|&lt;p&gt;I though...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;My data ...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;I wonder...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;Please, ...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;There is...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;More oft...|\n",
      "|&lt;p&gt;Data Sci...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;I often ...|\n",
      "|&lt;p&gt;I am a C...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I've wri...|\n",
      "|&lt;p&gt;As I inc...|\n",
      "|&lt;p&gt;Many tim...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I'm new ...|\n",
      "|&lt;p&gt;There ar...|\n",
      "|&lt;p&gt;While do...|\n",
      "|&lt;p&gt;I'm new ...|\n",
      "|&lt;p&gt;There ar...|\n",
      "|&lt;p&gt;I am wor...|\n",
      "|&lt;p&gt;I'm buil...|\n",
      "|&lt;p&gt;I am bra...|\n",
      "|&lt;p&gt;I have j...|\n",
      "|&lt;p&gt;I unders...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;I am new...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;For a re...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;Suppose ...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I m new ...|\n",
      "|&lt;p&gt;I downlo...|\n",
      "|&lt;p&gt;Note tha...|\n",
      "|&lt;p&gt;R base f...|\n",
      "|&lt;p&gt;I was bu...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;When I s...|\n",
      "|&lt;p&gt;I made a...|\n",
      "|&lt;p&gt;I have 2...|\n",
      "|&lt;p&gt;I am ver...|\n",
      "|&lt;p&gt;I'm usin...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I have g...|\n",
      "|&lt;p&gt;In my un...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;This que...|\n",
      "|&lt;p&gt;I have i...|\n",
      "|&lt;p&gt;I know t...|\n",
      "|&lt;p&gt;I'm usin...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;As far a...|\n",
      "|&lt;p&gt;When I s...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I am an ...|\n",
      "|&lt;p&gt;It looks...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I asked ...|\n",
      "|&lt;p&gt;I am lea...|\n",
      "|&lt;p&gt;I am usi...|\n",
      "|&lt;p&gt;I am wor...|\n",
      "|&lt;p&gt;I am giv...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;t-SNE, a...|\n",
      "|&lt;p&gt;New to t...|\n",
      "|&lt;p&gt;I read i...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I am a r...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;How can ...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;Basicall...|\n",
      "|&lt;p&gt;I was st...|\n",
      "|&lt;p&gt;i want t...|\n",
      "|&lt;p&gt;My 'mach...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;Apologie...|\n",
      "|&lt;p&gt;There wa...|\n",
      "|&lt;p&gt;Most veh...|\n",
      "|&lt;p&gt;While ru...|\n",
      "|&lt;p&gt;Have you...|\n",
      "|&lt;p&gt;My data ...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;While ru...|\n",
      "|&lt;p&gt;I've bee...|\n",
      "|&lt;p&gt;I don't ...|\n",
      "|&lt;p&gt;so I'm u...|\n",
      "|&lt;h2&gt;Image S...|\n",
      "|&lt;p&gt;I have i...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I need t...|\n",
      "|&lt;p&gt;I am cur...|\n",
      "|&lt;p&gt;Does any...|\n",
      "|&lt;p&gt;So, I'm ...|\n",
      "|&lt;p&gt;I am att...|\n",
      "|&lt;p&gt;Well thi...|\n",
      "|&lt;p&gt;I am wor...|\n",
      "|&lt;p&gt;I am cur...|\n",
      "|&lt;p&gt;I am bui...|\n",
      "|&lt;p&gt;I assume...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;Are ther...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I need t...|\n",
      "|&lt;p&gt;Is there...|\n",
      "|&lt;p&gt;I am att...|\n",
      "|&lt;p&gt;Data set...|\n",
      "|&lt;p&gt;There is...|\n",
      "|&lt;p&gt;I am fit...|\n",
      "|&lt;p&gt;I have  ...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;I have t...|\n",
      "|&lt;p&gt;I am att...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;We are s...|\n",
      "|&lt;p&gt;We have ...|\n",
      "|&lt;p&gt;Cross po...|\n",
      "|&lt;p&gt;I have f...|\n",
      "|&lt;p&gt;I am fac...|\n",
      "|&lt;p&gt;I need t...|\n",
      "|&lt;p&gt;does any...|\n",
      "|&lt;p&gt;I know t...|\n",
      "|&lt;p&gt;&lt;img ...|\n",
      "|&lt;p&gt;I know t...|\n",
      "|&lt;p&gt;For expe...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I am usi...|\n",
      "|&lt;p&gt;While fi...|\n",
      "|&lt;p&gt;I like t...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;Let me s...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I have n...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;The prob...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I've bee...|\n",
      "|&lt;p&gt;Where ca...|\n",
      "|&lt;p&gt;Caveat: ...|\n",
      "|&lt;p&gt;I recent...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I've jus...|\n",
      "|&lt;p&gt;I have i...|\n",
      "|&lt;p&gt;I'm stud...|\n",
      "|&lt;p&gt;I am hop...|\n",
      "|&lt;p&gt;I have t...|\n",
      "|&lt;p&gt;I've fit...|\n",
      "|&lt;p&gt;I have b...|\n",
      "|&lt;p&gt;I have b...|\n",
      "|&lt;p&gt;Which on...|\n",
      "|&lt;p&gt;I am exp...|\n",
      "|&lt;p&gt;I am exp...|\n",
      "|&lt;p&gt;I have c...|\n",
      "|&lt;p&gt;I am see...|\n",
      "|&lt;p&gt;As what ...|\n",
      "|&lt;p&gt;Have any...|\n",
      "|&lt;p&gt;&lt;stro...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I was wo...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I am int...|\n",
      "|&lt;p&gt;How is t...|\n",
      "|&lt;p&gt;I am loo...|\n",
      "|&lt;p&gt;In SVMs ...|\n",
      "|&lt;p&gt;Backgrou...|\n",
      "|&lt;p&gt;I'm goin...|\n",
      "|&lt;p&gt;I'm sear...|\n",
      "|&lt;p&gt;I was cu...|\n",
      "|&lt;p&gt;We have ...|\n",
      "|&lt;p&gt;It may b...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;Are ther...|\n",
      "|&lt;p&gt;&lt;stro...|\n",
      "|&lt;p&gt;The prob...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;The Core...|\n",
      "|&lt;p&gt;I have b...|\n",
      "|&lt;p&gt;I am cur...|\n",
      "|&lt;p&gt;I am hav...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;As menti...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;Suppose ...|\n",
      "|&lt;p&gt;I am wor...|\n",
      "|&lt;p&gt;I am int...|\n",
      "|&lt;p&gt;I am loo...|\n",
      "|&lt;p&gt;I am a r...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I have t...|\n",
      "|&lt;p&gt;I am loo...|\n",
      "|&lt;p&gt;Recently...|\n",
      "|&lt;p&gt;One of t...|\n",
      "|&lt;p&gt;I am wor...|\n",
      "|&lt;p&gt;I'm a Ja...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I couldn...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm goin...|\n",
      "|&lt;p&gt;Matlab i...|\n",
      "|&lt;p&gt;Are ther...|\n",
      "|&lt;p&gt;Not sure...|\n",
      "|&lt;p&gt;I have p...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I'm inte...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I am pla...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;Below is...|\n",
      "|&lt;p&gt;Statemen...|\n",
      "|&lt;p&gt;The data...|\n",
      "|&lt;p&gt;I have s...|\n",
      "|&lt;p&gt;I'm codi...|\n",
      "|&lt;p&gt;Many dis...|\n",
      "|&lt;p&gt;I've mad...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;Next wee...|\n",
      "|&lt;p&gt;Okay, he...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I am a h...|\n",
      "|&lt;p&gt;I am a 3...|\n",
      "|&lt;p&gt;I'm curi...|\n",
      "|&lt;p&gt;I have p...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;Lets say...|\n",
      "|&lt;p&gt;how to g...|\n",
      "|&lt;p&gt;I know t...|\n",
      "|&lt;p&gt;I do mov...|\n",
      "|&lt;p&gt;Suppose,...|\n",
      "|&lt;p&gt;When ML ...|\n",
      "|&lt;p&gt;let's as...|\n",
      "|&lt;p&gt;I hope y...|\n",
      "|&lt;p&gt;I need s...|\n",
      "|&lt;p&gt;I have s...|\n",
      "|&lt;p&gt;First of...|\n",
      "|&lt;p&gt;I was wo...|\n",
      "|&lt;p&gt;I am new...|\n",
      "|&lt;p&gt;I am loo...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;How woul...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;Recently...|\n",
      "|&lt;p&gt;I am kin...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I've bui...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;Is there...|\n",
      "|&lt;p&gt;Most Tre...|\n",
      "|&lt;p&gt;I am loo...|\n",
      "|&lt;p&gt;Where is...|\n",
      "|&lt;p&gt;A Random...|\n",
      "|&lt;p&gt;Where is...|\n",
      "|&lt;p&gt;I am dra...|\n",
      "|&lt;p&gt;Are ther...|\n",
      "|&lt;p&gt;Can anyo...|\n",
      "|&lt;p&gt;There're...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I am not...|\n",
      "|&lt;p&gt;I need s...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I work i...|\n",
      "|&lt;p&gt;I have T...|\n",
      "|&lt;p&gt;For my C...|\n",
      "|&lt;p&gt;I have d...|\n",
      "|&lt;p&gt;I cant s...|\n",
      "|&lt;p&gt;Sorry, i...|\n",
      "|&lt;p&gt;Our syst...|\n",
      "|&lt;p&gt;I'm trai...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;The most...|\n",
      "|&lt;p&gt;I'm doin...|\n",
      "|&lt;p&gt;I'm exam...|\n",
      "|&lt;p&gt;I'm plan...|\n",
      "|&lt;p&gt;I am a n...|\n",
      "|&lt;p&gt;I tried ...|\n",
      "|&lt;p&gt;In most ...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'd like...|\n",
      "|&lt;p&gt;I had a ...|\n",
      "|&lt;h1&gt;My Back...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I'm just...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;The mean...|\n",
      "|&lt;p&gt;By &quot...|\n",
      "|&lt;p&gt;I was go...|\n",
      "|&lt;p&gt;I need t...|\n",
      "|&lt;p&gt;n this p...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I need t...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;We have ...|\n",
      "|&lt;p&gt;For prob...|\n",
      "|&lt;p&gt;We're cu...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I have m...|\n",
      "|&lt;p&gt;I did sm...|\n",
      "|&lt;p&gt;I have t...|\n",
      "|&lt;p&gt;I am dea...|\n",
      "|&lt;p&gt;I think ...|\n",
      "|&lt;p&gt;I am new...|\n",
      "|&lt;p&gt;I was wo...|\n",
      "|&lt;p&gt;this is ...|\n",
      "|&lt;p&gt;The ques...|\n",
      "|&lt;p&gt;Data sam...|\n",
      "|&lt;p&gt;Currentl...|\n",
      "|&lt;p&gt;I have 1...|\n",
      "|&lt;p&gt;I am run...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I do at ...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I'm tryi...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;From &lt...|\n",
      "|&lt;p&gt;To all:&...|\n",
      "|&lt;p&gt;I browse...|\n",
      "|&lt;p&gt;Common m...|\n",
      "|&lt;p&gt;I am hav...|\n",
      "|&lt;p&gt;Hi this ...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;GBMs, li...|\n",
      "|&lt;p&gt;I have h...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;I'm from...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I am doi...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;A short ...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I have s...|\n",
      "|&lt;p&gt;In this ...|\n",
      "|&lt;p&gt;I'm look...|\n",
      "|&lt;p&gt;I have s...|\n",
      "|&lt;p&gt;Which of...|\n",
      "|&lt;p&gt;I am try...|\n",
      "|&lt;p&gt;I am cur...|\n",
      "|&lt;p&gt;I'm very...|\n",
      "|&lt;p&gt;Is there...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I've bee...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;I am int...|\n",
      "|&lt;p&gt;I came a...|\n",
      "|&lt;p&gt;My data ...|\n",
      "|&lt;p&gt;Can anyo...|\n",
      "|&lt;p&gt;Although...|\n",
      "|&lt;p&gt;Few thin...|\n",
      "+--------------------+\n",
      "only showing top 500 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.\\\n",
    "select('Body').\\\n",
    "where(df.PostTypeId.like('%1%')).\\\n",
    "show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "samTekst=df.\\\n",
    "select('Body').\\\n",
    "where(df.PostTypeId.like('%1%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body=\"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;Libsvm&lt;/a&gt; to train data and predict classification on &lt;strong&gt;semantic analysis&lt;/strong&gt; problem. But it has a &lt;strong&gt;performance&lt;/strong&gt; issue on large-scale data, because semantic analysis concerns &lt;strong&gt;&lt;em&gt;n-dimension&lt;/em&gt;&lt;/strong&gt; problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last year, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;Liblinear&lt;/a&gt; was release, and it can solve performance bottleneck.&#xA;But it cost too much &lt;strong&gt;memory&lt;/strong&gt;. Is &lt;strong&gt;MapReduce&lt;/strong&gt; the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on &lt;strong&gt;Liblinear&lt;/strong&gt;?&lt;/p&gt;&#xA;')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samTekst.take(5) ### lo matko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AnswerCount: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- ClosedDate: string (nullable = true)\n",
      " |-- CommentCount: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- FavoriteCount: string (nullable = true)\n",
      " |-- LastActivityDate: string (nullable = true)\n",
      " |-- OwnerUserId: string (nullable = true)\n",
      " |-- PostTypeId: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      " |-- rowId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# zeby robić kwerendy sql trzeba stworzyć temporary view\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Body|\n",
      "+--------------------+\n",
      "|&lt;p&gt;I've alw...|\n",
      "|&lt;p&gt;As a res...|\n",
      "|&lt;p&gt;I am sur...|\n",
      "|&lt;p&gt;In which...|\n",
      "|&lt;p&gt;I use &l...|\n",
      "|&lt;p&gt;Lots of ...|\n",
      "|&lt;p&gt;we creat...|\n",
      "|&lt;p&gt;My data ...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;In worki...|\n",
      "|&lt;p&gt;I heard ...|\n",
      "|&lt;p&gt;R has ma...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;From my ...|\n",
      "|&lt;p&gt;In revie...|\n",
      "|&lt;p&gt;Logic of...|\n",
      "|&lt;p&gt;First, t...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;If small...|\n",
      "|&lt;p&gt;&lt;em&g...|\n",
      "|&lt;p&gt;&lt;stro...|\n",
      "|&lt;p&gt;What is(...|\n",
      "|&lt;p&gt;Given we...|\n",
      "|&lt;p&gt;For exam...|\n",
      "|&lt;p&gt;While bu...|\n",
      "|&lt;p&gt;What is ...|\n",
      "|&lt;p&gt;Assume t...|\n",
      "|&lt;p&gt;Consider...|\n",
      "|&lt;p&gt;When a r...|\n",
      "|&lt;p&gt;If I hav...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I want l...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;&lt;a hr...|\n",
      "|&lt;p&gt;From wik...|\n",
      "|&lt;p&gt;In our c...|\n",
      "|&lt;p&gt;Any smal...|\n",
      "|&lt;p&gt;As we al...|\n",
      "|&lt;p&gt;One of t...|\n",
      "|&lt;p&gt;I see a ...|\n",
      "|&lt;p&gt;Assume a...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I have d...|\n",
      "|&lt;p&gt;I am see...|\n",
      "|&lt;p&gt;The deta...|\n",
      "|&lt;p&gt;I'm lear...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;Can some...|\n",
      "|&lt;p&gt;The most...|\n",
      "|&lt;p&gt;So we ha...|\n",
      "|&lt;p&gt;LDA has ...|\n",
      "|&lt;p&gt;Working ...|\n",
      "|&lt;p&gt;A recomm...|\n",
      "|&lt;p&gt;I'm new ...|\n",
      "|&lt;p&gt;I'm buil...|\n",
      "|&lt;p&gt;So, I ha...|\n",
      "|&lt;p&gt;Having a...|\n",
      "|&lt;p&gt;The outp...|\n",
      "|&lt;p&gt;Can some...|\n",
      "|&lt;p&gt;Going th...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;As Yann ...|\n",
      "|&lt;p&gt;Data vis...|\n",
      "|&lt;p&gt;There se...|\n",
      "|&lt;p&gt;An aspir...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;Being ne...|\n",
      "|&lt;p&gt;I am dev...|\n",
      "|&lt;p&gt;Yann LeC...|\n",
      "|&lt;p&gt;There is...|\n",
      "|&lt;p&gt;I have r...|\n",
      "|&lt;p&gt;I'm work...|\n",
      "|&lt;p&gt;What are...|\n",
      "|&lt;p&gt;I've bui...|\n",
      "|&lt;p&gt;The setu...|\n",
      "|&lt;p&gt;I want t...|\n",
      "|&lt;p&gt;I'm just...|\n",
      "|&lt;p&gt;I've now...|\n",
      "|&lt;p&gt;Could yo...|\n",
      "|&lt;p&gt;In some ...|\n",
      "|&lt;p&gt;as I am ...|\n",
      "|&lt;p&gt;I attack...|\n",
      "|&lt;p&gt;Logic of...|\n",
      "|&lt;p&gt;What kin...|\n",
      "|&lt;p&gt;I'd like...|\n",
      "|&lt;p&gt;I'm curi...|\n",
      "|&lt;p&gt;The majo...|\n",
      "|&lt;p&gt;I'm deve...|\n",
      "|&lt;p&gt;I've cam...|\n",
      "|&lt;p&gt;I have a...|\n",
      "|&lt;p&gt;I once h...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;&lt;img ...|\n",
      "|&lt;p&gt;If I hav...|\n",
      "|&lt;p&gt;I'm curr...|\n",
      "|&lt;p&gt;It seems...|\n",
      "|&lt;h1&gt;Motivat...|\n",
      "|&lt;p&gt;I would ...|\n",
      "|&lt;p&gt;Does any...|\n",
      "+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Body\").filter(\"PostTypeId=1\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Body: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samTekst.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Body|\n",
      "+--------------------+\n",
      "|&lt;p&gt;I've alw...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samTekst.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(samTekst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(samTekst.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body=\"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;Libsvm&lt;/a&gt; to train data and predict classification on &lt;strong&gt;semantic analysis&lt;/strong&gt; problem. But it has a &lt;strong&gt;performance&lt;/strong&gt; issue on large-scale data, because semantic analysis concerns &lt;strong&gt;&lt;em&gt;n-dimension&lt;/em&gt;&lt;/strong&gt; problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last year, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;Liblinear&lt;/a&gt; was release, and it can solve performance bottleneck.&#xA;But it cost too much &lt;strong&gt;memory&lt;/strong&gt;. Is &lt;strong&gt;MapReduce&lt;/strong&gt; the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on &lt;strong&gt;Liblinear&lt;/strong&gt;?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Lots of people use the term &lt;em&gt;big data&lt;/em&gt; in a rather &lt;em&gt;commercial&lt;/em&gt; way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, &lt;em&gt;big data&lt;/em&gt; always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a &lt;em&gt;big data&lt;/em&gt; problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled &lt;em&gt;big data&lt;/em&gt; if the dataset was &lt;em&gt;big enough&lt;/em&gt;? Also, how &lt;em&gt;big&lt;/em&gt; is &lt;em&gt;big enough&lt;/em&gt; (if this is possible to define)?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it's a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;My data set contains a number of numeric attributes and one categorical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say, &lt;code&gt;NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr&lt;/code&gt;, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;CategoricalAttr&lt;/code&gt; takes one of three possible values: &lt;code&gt;CategoricalAttrValue1&lt;/code&gt;, &lt;code&gt;CategoricalAttrValue2&lt;/code&gt; or &lt;code&gt;CategoricalAttrValue3&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using default k-means clustering algorithm implementation for Octave &lt;a href=&quot;https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/&quot;&gt;https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/&lt;/a&gt;.&#xA;It works with numeric data only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question: is it correct to split the categorical attribute &lt;code&gt;CategoricalAttr&lt;/code&gt; into three numeric (binary) variables, like &lt;code&gt;IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3&lt;/code&gt; ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a bunch of customer profiles stored in a &lt;a href=&quot;/questions/tagged/elasticsearch&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;elasticsearch&amp;#39;&quot; rel=&quot;tag&quot;&gt;elasticsearch&lt;/a&gt; cluster. These profiles are now used for creation of target groups for our email subscriptions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could I search for interesting groups &lt;strong&gt;automatically&lt;/strong&gt; - using data science, machine learning, clustering or something else?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;/questions/tagged/r&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;r&amp;#39;&quot; rel=&quot;tag&quot;&gt;r&lt;/a&gt; programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can the algorithm development itself be sped up and made more scalable?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some specific challenges: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can algorithm parameters be tuned faster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I heard about many tools / frameworks for helping people to process their data (big data environment). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are they complementary?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, &quot;Machine Learning with R&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've seen a guideline of 5TB for a dataset to be considered as Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: Is R suitable for the amount of Data typically seen in Big Data problems? &#xA;Are there strategies to be employed when using R with this size of dataset?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I'm on Windows, but I could easily put this script on my Linux machine if that would simplify the process. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;In reviewing “&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1461468485&quot; rel=&quot;nofollow&quot;&gt;Applied Predictive Modeling&lt;/a&gt;&quot; a &lt;a href=&quot;http://www.information-management.com/blogs/applied-predictive-modeling-10024771-1.html&quot; rel=&quot;nofollow&quot;&gt;reviewer states&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;One critique I have of statistical learning (SL) pedagogy is the&#xA;  absence of computation performance considerations in the evaluation of&#xA;  different modeling techniques. With its emphases on bootstrapping and&#xA;  cross-validation to tune/test models, SL is quite compute-intensive.&#xA;  Add to that the re-sampling that's embedded in techniques like bagging&#xA;  and boosting, and you have the specter of computation hell for&#xA;  supervised learning of large data sets. &lt;strong&gt;In fact, R's memory&#xA;  constraints impose pretty severe limits on the size of models that can&#xA;  be fit by top-performing methods like random forests.&lt;/strong&gt; Though SL does a&#xA;  good job calibrating model performance against small data sets, it'd&#xA;  sure be nice to understand performance versus computational cost for&#xA;  larger data.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What are R's memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_forest&quot; rel=&quot;nofollow&quot;&gt;random forests&lt;/a&gt;?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Related:&lt;/strong&gt; Followup to the question above, &quot;&lt;a href=&quot;https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted&quot;&gt;When is a Model Underfitted?&lt;/a&gt;&quot;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;First, think it's worth me stating what I mean by replication &amp;amp; reproducibility:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.&lt;/li&gt;&#xA;&lt;li&gt;Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closet comparison I'm able to think of is &lt;a href=&quot;http://en.wikipedia.org/wiki/Documentation_generator&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation generators&lt;/a&gt; that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the &quot;reproducibility documentation generators&quot; the documentation should match.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, whole intent of this is to understand if it's possible to &quot;bake-in&quot; reproducibility documentation as you build out a stack, not after the stack is built.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;UPDATE:&lt;/strong&gt; Please note that this is the second draft of this question and that &lt;a href=&quot;https://datascience.stackexchange.com/users/178/christopher-louden&quot;&gt;Christopher Louden&lt;/a&gt; was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!&lt;/em&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;&lt;em&gt;(Note: Pulled this question from the &lt;a href=&quot;http://area51.stackexchange.com/proposals/55053/data-science/57398#57398&quot;&gt;list of questions in Area51&lt;/a&gt;, but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.)&lt;/em&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; Following is from the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1449356265&quot;&gt;Graph Databases&lt;/a&gt;, which covers a performance test mentioned in the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1617290769&quot;&gt;Neo4j in Action&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Relationships in a graph naturally form paths. Querying, or&#xA;  traversing, the graph involves following paths. Because of the&#xA;  fundamentally path-oriented nature of the datamodel, the majority of&#xA;  path-based graph database operations are highly aligned with the way&#xA;  in which the data is laid out, making them extremely efficient. In&#xA;  their book Neo4j in Action, Partner and Vukotic perform an experiment&#xA;  using a relational store and Neo4j.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The comparison shows that the graph database is substantially quicker&#xA;  for connected data than a relational store.Partner and Vukotic’s&#xA;  experiment seeks to find friends-of-friends in a social network, to a&#xA;  maximum depth of five. Given any two persons chosen at random, is&#xA;  there a path that connects them which is at most five relationships&#xA;  long? For a social network containing 1,000,000 people, each with&#xA;  approximately 50 friends, the results strongly suggest that graph&#xA;  databases are the best choice for connected data, as we see in Table&#xA;  2-1.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4j&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Depth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned&#xA;2       0.016                       0.01                         ~2500    &#xA;3       30.267                      0.168                        ~110,000 &#xA;4       1543.505                    1.359                        ~600,000 &#xA;5       Unfinished                  2.132                        ~800,000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, it’s clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4j’s response time remains relatively flat: just a fraction of a second to perform the query—definitely quick enough for an online system.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;At depth four the relational database exhibits crippling latency,&#xA;  making it practically useless for an online system. Neo4j’s timings&#xA;  have deteriorated a little too, but the latency here is at the&#xA;  periphery of being acceptable for a responsive online system. Finally,&#xA;  at depth five, the relational database simply takes too long to&#xA;  complete the query. Neo4j, in contrast, returns a result in around two&#xA;  seconds. At depth five, it transpires almost the entire network is our&#xA;  friend: for many real-world use cases, we’d likely trim the results,&#xA;  and the timings.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions are:&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is this a reasonable test to emulate what one might except to find in a social network? &lt;em&gt;(Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/The_rich_get_richer_%28statistics%29&quot;&gt;rich get richer&lt;/a&gt;&quot; model would be more natural for social networks, though might be wrong.)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible? &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is &lt;em&gt;parallel AND distributed&lt;/em&gt;?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Given website access data in the form &lt;code&gt;session_id, ip, user_agent&lt;/code&gt;, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;session_id&lt;/code&gt;: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;IP&lt;/code&gt; can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;User_agent&lt;/code&gt; is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data might look as the fiddle here:&#xA;&lt;a href=&quot;http://sqlfiddle.com/#!2/c4de40/1&quot;&gt;http://sqlfiddle.com/#!2/c4de40/1&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id&#xA;from &#xA;    (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr&#xA;    from&#xA;        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a&#xA;    join&#xA;        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b&#xA;        order by 1&#xA;    )d&#xA;inner join&#xA;    (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;    union all select 6 union all select 7 union all select 8 union all select 9 )e&#xA;    on d.nr&amp;gt;=e.nr&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;For example, when searching something in Google, results return nigh-instantly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does MapReduce help solve this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Assume that we have a set of elements &lt;em&gt;E&lt;/em&gt; and a similarity (&lt;strong&gt;not distance&lt;/strong&gt;) function &lt;em&gt;sim(ei, ej)&lt;/em&gt; between two elements &lt;em&gt;ei,ej ∈ E&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could we (efficiently) cluster the elements of &lt;em&gt;E&lt;/em&gt;, using &lt;em&gt;sim&lt;/em&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;k&lt;/em&gt;-means, for example, requires a given &lt;em&gt;k&lt;/em&gt;, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note, that &lt;em&gt;sim&lt;/em&gt; is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of &lt;em&gt;E&lt;/em&gt;).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Consider a stream containing &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Tuple&quot; rel=&quot;nofollow&quot;&gt;tuples&lt;/a&gt; &lt;code&gt;(user, new_score)&lt;/code&gt; representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to have some standing queries like: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Which players posted more than x scores in a sliding window of one hour&lt;/li&gt;&#xA;&lt;li&gt;Which players gained x% score in a sliding window of one hour&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My question is which open source tools can I employ to jumpstart this project? I am considering &lt;a href=&quot;http://esper.codehaus.org/&quot; rel=&quot;nofollow&quot;&gt;Esper&lt;/a&gt; at the moment. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I have just completed reading &quot;Mining Data Streams&quot; (chapter 4 of &lt;a href=&quot;http://infolab.stanford.edu/~ullman/mmds.html&quot; rel=&quot;nofollow&quot;&gt;Mining of Massive Datasets&lt;/a&gt;) and I am quite new to mining data streams.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;When a relational database, like MySQL, has better performance than a no relational, like MongoDB?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw a question on Quora other day, about why Quora still uses MySQL as their backend, and that their performance is still good.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper names are like &quot;Assessment of Utility in Web Mining for the Domain of Public Health&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three crucial characteristics of my database:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;there's many more features than samples.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I mainly use Python, so Python-specific hints would be greatly appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I'm asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Also, your opinions on NoSQL databases, and any recommendations for learning about this topic are welcome.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process&quot;&gt;Hierarchical Dirichlet Process (HDP)&lt;/a&gt; are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn't. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm&quot;&gt;This question&lt;/a&gt; asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? &lt;strong&gt;How are generative and discriminative models used in NLP?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;From wikipedia, &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;dimensionality reduction or dimension reduction is the process of&#xA;  reducing the number of random variables under consideration, and&#xA;  can be divided into feature selection and feature extraction.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is the difference between feature selection and feature extraction?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is an example of dimensionality reduction in a Natural Language Processing task?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB's map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, people tend to reach out for C/C++/low-level languages. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course such set of benefits would not come without a cost: writing the code, and sometimes even &lt;em&gt;reinventing the wheel&lt;/em&gt;, can be quite expensive/tiresome. Although there are lots of libraries available, people are inclined to write the code by themselves whenever they need to &lt;em&gt;grant&lt;/em&gt; performance. What &lt;em&gt;disables&lt;/em&gt; performance assertions from using libraries while processing large databases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider an entreprise that continuously crawls webpages and parses the data collected. For each sliding-window, different data mining algorithms are run upon the data extracted. Why would the developers ditch off using available libraries/frameworks (be it for crawling, text processing, and data mining)? Using stuff already implemented would not only ease the burden of coding the whole process, but also would save a lot of time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In a single shot&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;what makes writing the code by oneself a &lt;em&gt;guarantee&lt;/em&gt; of performance?&lt;/li&gt;&#xA;&lt;li&gt;why is it &lt;em&gt;risky&lt;/em&gt; to rely on a frameworks/libraries when you must &lt;strong&gt;assure&lt;/strong&gt; high performance?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metrics from various sources for making a higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somewhat repetitive work. For example analyzing tweets, facebook posts, Wikipedia articles etc. is a part of a lot of big data problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of these data sets are accessible using public APIs provided by the provider site, but usually, some valuable information or metrics are missing from these APIs and everyone has to do the same analyses again and again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API nor available publicly in independent data sets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other big data problems? I mean something like GitHub (or a group of sites/public datasets or at least a comprehensive listing) for the data science. If not, what are the reasons for not having such a platform for data science? The commercial value of data, need to frequently update data sets, ...? Can we not have an open-source model for sharing data sets devised for data scientists?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I see a lot of courses in Data Science emerging in the last 2 years. Even big universities like Stanford and Columbia offers MS specifically in Data Science. But as long as I see, it looks like data science is just a mix of computer science and statistics techniques.&#xA;So I always think about this. If it is just a trend and if in 10 years from now, someone will still mention Data Science as an entire field or just a subject/topic inside CS or stats.&#xA;What do you think?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. &quot;nationality&quot; vs &quot;bornIn&quot;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to find some &quot;important&quot; attributes that somehow &quot;define&quot; the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the attribute &quot;lastName&quot; is more discriminative than the attribute &quot;nationality&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How could I (statistically) find such attributes that are more important than others?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A naive solution would be to take the average IDF of the values of each attribute and make this the &quot;importance&quot; factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have seen the term feature, or attribute selection in machine learning, but I don't want to discard the remaining attributes, I just want to put higher weights to the most important ones.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a modeling and scoring program that makes heavy use of the &lt;code&gt;DataFrame.isin&lt;/code&gt; function of pandas, searching through lists of facebook &quot;like&quot; records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have data coming from a source system that is pipe delimited. Pipe was selected over comma since it was believed no pipes appeared in field, while it was known that commas do occur. After ingesting this data into Hive however it has been discovered that rarely a field does in fact contain a pipe character.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to a constraint we are unable to regenerate from source to escape the delimiter or change delimiters in the usual way. However we have the metadata used to create the Hive table. Could we use knowledge of the fields around the problem field to reprocess the file on our side to escape it or to change the file delimiter prior to reloading the data into Hive?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the existing solutions is &lt;a href=&quot;http://www.stanford.edu/group/sonia/&quot;&gt;SoNIA: Social Network Image Animator&lt;/a&gt;. It let's you make movies like &lt;a href=&quot;https://www.youtube.com/watch?v=yGSNCED6mDc&quot;&gt;this one&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right after posting this question I'll dig into &lt;a href=&quot;http://sigmajs.org/&quot;&gt;sigma.js&lt;/a&gt;, so please consider this library covered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, my input data would be something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time_elapsed; node1; node2&#xA;1; A; B&#xA;2; A; C&#xA;3; B; C&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The details of the Google Prediction API are on this &lt;a href=&quot;https://developers.google.com/prediction/&quot;&gt;page&lt;/a&gt;, but I am not able to find any details about the prediction algorithms running behind the API. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have gathered that they let you provide your preprocessing steps in PMML format.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm learning &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;Support Vector Machines&lt;/a&gt;, and I'm unable to understand how a class label is chosen for a data point in a binary classifier. Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently using &lt;a href=&quot;http://en.wikipedia.org/wiki/General_Algebraic_Modeling_System&quot; rel=&quot;nofollow&quot;&gt;General Algebraic Modeling System&lt;/a&gt; (GAMS), and more specifically CPLEX within GAMS, to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores), and it finds an optimal solution in a relatively short amount of time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I'm open to suggestions of all kinds, and other users may be interested in different solutions.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Can someone explain me, how to classify a data like MNIST with MLBP-Neural network if I make more than one output (e.g 8), I mean if I just use one output I can easily classify the data, but if I use more than one, which output should I choose ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;The most popular use case seem to be recommender systems of different kinds (such as recommending shopping items, users in social networks etc.).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what are other typical data science applications, which may be used in a different verticals?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: customer churn prediction with machine learning, evaluating customer lifetime value, sales forecasting.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a &quot;bucket&quot; that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Item A { 4-door, small, steel } =&amp;gt; { sedan }&#xA;Item B { 2-door, big,   steel } =&amp;gt; { truck }&#xA;Item C { 2-door, small, steel } =&amp;gt; { coupe }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I just want the rules that say &quot;if it's big and a 2-door, it's a truck,&quot; not the rules that say &quot;if it's a 4-door it's also small.&quot; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;LDA has two hyperparameters, tuning them changes the induced topics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does the alpha and beta hyperparameters contribute to LDA? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does the topic change if one or the other hyperparameters increase or decrease? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why are they hyperparamters and not just parameters?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Working on what could often be called &quot;medium data&quot; projects, I've been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I'm looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bandwidth and RAM are non-trivial concerns in my projects, but it's easy to spot when those are the bottlenecks and readjust. It's much harder, I'd imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I've just failed to google this one properly, feel free to point me to the right answer somewhere else!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It's like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;user_id item_id result&#xA;1       4       1&#xA;1       7       -1&#xA;5       19      1&#xA;5       80      1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 1&lt;/strong&gt;&#xA;Let us think about it in the context of a matrix factorization approach. If we treat -1 and 1 as ratings, there will be some problem. For example, user 1 likes movie A which scores high in one factor (e.g. having glorious background music) in the latent factor space. The system recommends movie B which also scores high in &quot;glorious background music&quot;, but for some reason user 1 is too busy to look into the recommendation, and we have a -1 rating movie B. If we just treat 1 or -1 equally, then the system might be discouraged to recommend movie with glorious BGM to user 1 while user 1 still loves movie with glorious BGM. I think this situation is to be avoided.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm new to this community and hopefully my question will well fit in here.&#xA;As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I'm concern this topic relates to Machine Learning and Support Vector Machines. I'm not well familiar with this technologies yet so I will need some help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have decided to follow this project idea &lt;a href=&quot;http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html&quot;&gt;http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html&lt;/a&gt; (first project on the top)&#xA;The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject's waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the data set is given in one folder with some description and feature labels. The data is divided for 'test' and 'train' files in which data is represented in this format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And that's only a very small sample of what the file contain. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use? &#xA;Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any hints/tips would be much appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm building a workflow for creating machine learning models (in my case, using Python's &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;sklearn&lt;/code&gt; packages) from data pulled from a very large database (here, Vertica by way of SQL and &lt;code&gt;pyodbc&lt;/code&gt;), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I'm curious where best to locate this step in a multi-platform workflow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's simple enough to do this in Python, either with the &lt;a href=&quot;http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values&quot;&gt;&lt;code&gt;sklearn.preprocessing.Imputer&lt;/code&gt;&lt;/a&gt; class, using the &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.fillna.html&quot;&gt;&lt;code&gt;pandas.DataFrame.fillna&lt;/code&gt;&lt;/a&gt; method, or by hand (depending upon the complexity of the imputation method used). But since I'm going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there's a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building &quot;complete&quot; versions of tables, so we don't need to fill in a new set of missing values from scratch every time we want to run a model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't been able to find much guidance about this, but I imagine that we could:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column&lt;/li&gt;&#xA;&lt;li&gt;join the substitute value table with the original table to assign a substitute value for each row and incomplete column&lt;/li&gt;&#xA;&lt;li&gt;use a series of case statements to take the original value if available and the substitute value otherwise&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;So, I have a dataset with 39.949 variables and 180 rows. dataset is successfully &#xA;saved in DataFrame but when I try to find cov() it result an error.&#xA;here is the code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  import pandas as pd&#xA;  cov_data=pd.DataFrame(dataset).cov()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;/home/syahdeini/Desktop/FP/pca_2.py&quot;, line 44, in find_eagen&#xA;cov_data=pd.DataFrame(data_mat).cov()&#xA;File &quot;/usr/lib/python2.7/dist-packages/pandas/core/frame.py&quot;, line 3716, in cov&#xA;baseCov = np.cov(mat.T)&#xA;File &quot;/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py&quot;, line 1766, in cov&#xA;return (dot(X, X.T.conj()) / fact).squeeze()&#xA;ValueError: array is too big.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I saw the company's manager last day.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;Person name=&quot;John&quot;&amp;gt;I&amp;lt;/Person&amp;gt; saw the &amp;lt;Organization name=&quot;ACME&quot;&amp;gt;company&amp;lt;/Organization&amp;gt;'s&#xA;manager &amp;lt;Time value=&quot;2014-5-29&quot;&amp;gt;last day&amp;lt;/Time&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The output of my word alignment file looks as such:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it . In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen .   0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28&#xA;It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it .    Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen .   0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I produce the phrase tables that are used by MOSES from this output?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Going through the presentation and material of Summingbird by Twitter, one of the reasons that is mentioned for using Storm and Hadoop clusters together in Summingbird is that processing through Storm results in cascading of error. In order to avoid this cascading of error and accumulation of it, Hadoop cluster is used to batch process the data and discard the Storm results after the same data is processed by Hadoop. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the reasons for generation of this accumulation of error? and why is it not present in Hadoop? Since I have not worked with Storm, I do not know the reasons for it. Is it because Storm uses some approximate algorithm to process the data in order to process them in real time? or is the cause something else?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I want to test the accuracy of a methodology. I ran it ~400 times, and I got a different classification for each run. I also have the ground truth, i.e., the real classification to test against.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;May I sum all confusion matrices in order to obtain the overall one?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;As Yann LeCun &lt;a href=&quot;http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/chisdw1&quot; rel=&quot;noreferrer&quot;&gt;mentioned&lt;/a&gt;, a number of PhD programs in data science will be popping up in the next few years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://datascience.nyu.edu/academics/programs/&quot; rel=&quot;noreferrer&quot;&gt;NYU&lt;/a&gt; already have one, where Prof.LeCun is at right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A statistics or cs PhD in machine learning is probably more rigorous than a data science one.  Is data science PhD for the less mathy people like myself? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are these cash cow programs?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a huge industry demand for big data, but what is the academic value of these programs, as you probably can't be a professor or publish any paper.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Data visualization is an important sub-field in data science and python programmers would need to have available toolkits for them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there a Python API to Tableau?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there any Python-based data visualization toolkits?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;There seem to be at least 2 ways to connect to HBase from external application, with language other then Java (i.e. Python):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;HBase Thrift API&lt;/li&gt;&#xA;&lt;li&gt;HBase Stargate (REST API) &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Does anyone know which one should be used in which circumstances?&#xA;I.e. what are their main differences, and pros/cons?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;An aspiring data scientist here. I don't know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back &lt;em&gt;theoretically should&lt;/em&gt; have a tendency to run the ball in that situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Scoring might be more difficult to predict, but the winning team might be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Being new to machine-learning in general, I'd like to start playing around and see what the possibilities are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am developing a system that is intended to capture the &quot;context&quot; of user activity within an application; it is a framework that web applications can use to tag user activity based on requests made to the system.  It is hoped that this data can then power ML features such as context aware information retrieval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm having trouble deciding on what features to select in addition to these user tags - the URL being requested, approximate time spent with any given resource, estimating the current &quot;activity&quot; within the system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested to know if there are good examples of this kind of technology or any prior research on the subject - a cursory search of the ACM DL revealed some related papers but nothing really spot-on.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Yann LeCun mentioned in his &lt;a href=&quot;http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/&quot; rel=&quot;nofollow&quot;&gt;AMA&lt;/a&gt; that he considers having a PhD very important in order to get a job at a top company.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a masters in statistics and my undergrad was in economics and applied math, but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;There is plenty of hype surrounding Hadoop and its eco-system.  However, in practice, where many data sets are in the terabyte range, is it not more reasonable to use &lt;a href=&quot;http://aws.amazon.com/redshift/&quot;&gt;Amazon RedShift&lt;/a&gt; for querying large data sets, rather than spending time and effort building a Hadoop cluster? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have read lot of blogs\\\\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What kinda data these companies used. What was the size of the data&lt;/li&gt;&#xA;&lt;li&gt;What kinda of tools technologies they used to process the data&lt;/li&gt;&#xA;&lt;li&gt;What was the problem they were facing and how the insight they got the data helped them to resolve the issue.&lt;/li&gt;&#xA;&lt;li&gt;How they selected the tool\\\\technology to suit their need.&lt;/li&gt;&#xA;&lt;li&gt;What kinda pattern they identified from the data &amp;amp; what kind of patterns they were looking from the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be great if someone share how finance industry is making use of Big Data Analytic.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are about 1,150 known &quot;positives&quot;, against a background of about 13 million protein sequences (&quot;Unknown/poorly annotated background&quot;), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly &quot;negative&quot; way). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My previous implementation looked at this as a binary classification problem: &#xA;Positive set = Proteins marked as Neuropeptides.&#xA;Negative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it's at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts were to:&#xA;1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set.&#xA; (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) . &#xA;2) One class learning - Would be nice, but as I understand it, it's meant just for anomaly detection, and has poorer performance than discriminatory approaches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*) I've heard of P-U learning, which sounds neat, but I'm a programming N00b, and I don't know of any existing implementations for it. (In Python/sci-kit learn).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the &quot;negative&quot; proteins, but they're all very very different from each other, so I don't know how well the classifier would handle them as one big , unbalanced mix).&#xA;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;What are the books about the science and mathematics behind data science? It feels like so many &quot;data science&quot; books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples: Agresti's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470463635&quot;&gt;Categorical Data Analysis&lt;/a&gt;, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441902996&quot;&gt;Linear Mixed Models for Longitudinal Data&lt;/a&gt;, etc... etc...&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've implemented gradient checking, double checked everything etc and I'm pretty certain it's working correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've run it a few times and it reaches 'Optimization terminated successfully' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be interested to understand what's going on here, or if I've implemented neural net incorrectly?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;The setup is simple: binary classification using a simple decision tree, each node of the tree has a single threshold applied on a single feature. In general, building a ROC curve requires moving a decision threshold over different values and computing the effect of that change on the true positive rate and the false positives rate of predictions. What's that decision threshold in the case of a simple fixed decision tree? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to extract news about a company from online news by using RODBC package in R. And I want to use the extracted data for sentiment analysis. I want to accomplish this in such a way that the positive news is assigned a value of +1, the negative news is assigned a value of -1 and the neutral news is assigned a value of 0.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm just starting to develop a &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; application for academic purposes. I'm currently using &lt;strong&gt;R&lt;/strong&gt; and training myself in it. However, in a lot of places, I saw people using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are people using in academia and industry, and what is the recommendation?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've now seen two data science certification programs - the &lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage&quot;&gt;John Hopkins one available at Coursera&lt;/a&gt; and the &lt;a href=&quot;http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html&quot;&gt;Cloudera one&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure there are others out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;R Programming&lt;/li&gt;&#xA;&lt;li&gt;cleaning and obtaining data&lt;/li&gt;&#xA;&lt;li&gt;Data Analysis&lt;/li&gt;&#xA;&lt;li&gt;Reproducible Research&lt;/li&gt;&#xA;&lt;li&gt;Statistical Inference&lt;/li&gt;&#xA;&lt;li&gt;Regression Models&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning&lt;/li&gt;&#xA;&lt;li&gt;Developing Data Products&lt;/li&gt;&#xA;&lt;li&gt;And what looks to be a Project based completion task similar to Cloudera's Data Science Challenge&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The Cloudera program looks thin on the surface, but looks to answer the two important questions - &quot;Do you know the tools&quot;, &quot;Can you apply the tools in the real world&quot;.  Their program consists of:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Introduction to Data Science&lt;/li&gt;&#xA;&lt;li&gt;Data Science Essentials Exam&lt;/li&gt;&#xA;&lt;li&gt;Data Science Challenge (a real world data science project scenario)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am not looking for a recommendation on a program or a quality comparison.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: These are all great answers.  I'm choosing the correct answer by votes.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Could you give some examples of typical tasks that a data scientist does in his daily job, and the must-know minimum for each of the levels (like junior, senior, etc. if there are any)? If possible, something like a &lt;a href=&quot;http://www.starling-software.com/employment/programmer-competency-matrix.html&quot; rel=&quot;nofollow&quot;&gt;Programmer competency matrix&lt;/a&gt;.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;In some cases, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20975147&quot;&gt;it may be impossible&lt;/a&gt; to draw Euler diagrams with overlapping circles to represent all the overlapping subsets in the correct proportions. This type of data then requires using polygons or other figures to represent each set. When dealing with data that describes overlapping subsets, how can I figure out whether a simple Euler diagram is possible?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;as I am very interested in programming and statistics, Data Science seems like a great career path to me - I like both fields and would like to combine them. Unfortunately, I have studied political science with a non-statistical sounding Master. I focused on statistics in this Master, visiting optional courses and writing a statistical thesis on a rather large dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since almost all job adds are requiring  a degree in informatics, physics or some other techy-field, I am wondering if there is a chance to become a data scientist or if I should drop that idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am lacking knowledge in machine learning, sql and hadoop, while having a rather strong informatics and statistics background. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So can somebody tell me how feasible my goal of becoming a data scientist is? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I attack this problem frequently with inefficiency because it's always pretty low on the priority list and my clients are resistant to change until things break.  I would like some input on how to speed things up.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have multiple datasets of information in a SQL database.  The database is vendor-designed, so I have little control over the structure.  It's a sql representation of a class-based structure.  It looks a little bit like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Main-class table&#xA; -sub-class table 1&#xA; -sub-class table 2&#xA;  -sub-sub-class table&#xA; ...&#xA; -sub-class table n&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Each table contains fields for each attribute of the class.  A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class' table, joined by a unique identifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are hundreds of classes. which means thousands of views and tens of thousands of columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Beyond that, there are multiple datasets, indicated by a field value in the Main-class table.  There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again.  The rules are in place because we might trust one source of data more than another for a particular value of a particular class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row.  The common changes are limited to a handful of fields in each row, but since anything can change we compare each value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours.  We resolve that problem generally, by throwing more hardware at it, but this isn't a hadoop environment currently so there's a pretty finite limit to what can be done in that regard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you go about scaling a solution to this problem such that there were no limitations?  And how would you go about accomplishing the most efficient data-merge?  (currently it is field by field comparisons... painfully slow).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Logic often states that by underfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is a followup to my question, &quot;&lt;a href=&quot;https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/&quot;&gt;Why Is Overfitting Bad?&lt;/a&gt;&quot;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What kind of error measures do RMSE and nDCG give while evaluating a recommender system, and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'd like to explore 'data science'. The term seems a little vague to me, but I expect it to require:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;machine learning (rather than traditional statistics);&lt;/li&gt;&#xA;&lt;li&gt;a large enough dataset that you have to run analyses on clusters.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What are some good datasets and problems, accessible to a statistician with some programming background, that I can use to explore the field of data science?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm curious about natural language querying.  Stanford has what looks to be a strong set of &lt;a href=&quot;http://nlp.stanford.edu/software/index.shtml&quot; rel=&quot;noreferrer&quot;&gt;software for processing natural language&lt;/a&gt;.  I've also seen the &lt;a href=&quot;http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html&quot; rel=&quot;noreferrer&quot;&gt;Apache OpenNLP library&lt;/a&gt;, and the &lt;a href=&quot;http://gate.ac.uk/science.html&quot; rel=&quot;noreferrer&quot;&gt;General Architecture for Text Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first rectangle on my flow chart is a bit of a mystery.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/wJPx9.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I might want to know:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;How many books were sold last month?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I'd want that translated into&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Select count(*) &#xA;  from sales &#xA;  where &#xA;   item_type='book' and &#xA;   sales_date &amp;gt;= '5/1/2014' and &#xA;   sales_date &amp;lt;= '5/31/2014'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The majority of people use S3. However, Google Drive seems a promising alternative solution for storing large amounts of data. Are there specific reasons why one is better than the other?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we're able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), and depending on what is being evaluated, it may raise &lt;em&gt;false/unfair&lt;/em&gt; comparisons. For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, how can one honestly compare different experiments to point out efficiency gains? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've came across the following problem, that I recon is rather typical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some large data, say, a few million rows. I run some non-trivial analysis on it, e.g. an SQL query consisting of several sub-queries. I get some result, stating, for example, that property X is increasing over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, there are two possible things that could lead to that:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;X is indeed increasing over time&lt;/li&gt;&#xA;&lt;li&gt;I have a bug in my analysis&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;How can I test that the first happened, rather than the second? A step-wise debugger, even if one exists, won't help, since intermediate results can still consist of millions of lines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only thing I could think of was to somehow generate a small, synthetic data set with the property that I want to test and run the analysis on it as a unit test. Are there tools to do this? Particularly, but not limited to, SQL.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a binary classification problem:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Approximately 1000 samples in training set&lt;/li&gt;&#xA;&lt;li&gt;10 attributes, including binary, numeric and categorical&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which algorithm is the best choice for this type of problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default I'm going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it'd become a burden to continuously validate the &lt;em&gt;current state&lt;/em&gt; of each spammer blocked, checking if the site/domain still disseminate spam data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: if possible, any example of strategy, even if just the intuition behind it, would be very welcome along with the answer.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In view of this I face the following dilemma: Should I learn Python to be able to use its extensive scientific libraries (Pandas, Numpy, Scipy, ...), or should I just dive into similar packages of a language I'm already acquainted with (Racket, or to a lesser extent Scala)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Ideally I would learn Python in parallel with using statistical libraries in Racket, but I'm not sure I'll have time for both)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not looking for an answer to this dilemma, but rather for feedback on my different considerations:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current position is as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In favour of Python:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Extensively used libraries&lt;/li&gt;&#xA;&lt;li&gt;Widely used (may be decisive in case of collaboration with others)&lt;/li&gt;&#xA;&lt;li&gt;A lot of online material to start learning it&lt;/li&gt;&#xA;&lt;li&gt;Conferences that are specifically dedicated to Scientific Computing with Python&lt;/li&gt;&#xA;&lt;li&gt;Learning Python won't be a waste of time anyway&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In favour of a language I already know:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It's a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)&lt;/li&gt;&#xA;&lt;li&gt;It is feasible. Both Racket and Scala have good mathematics and statistics libraries&lt;/li&gt;&#xA;&lt;li&gt;I can start right away with learning what I need to know rather than first having to learn the basics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Two concrete questions:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What am I forgetting?&lt;/li&gt;&#xA;&lt;li&gt;How big of a nuisance could the Python 2 vs 3 issue be?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4Ih6o.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to do Logistic Regression using SAS Enterprise Miner. &#xA;My Independent variables are &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CPR/Inc (Categorical 1 to 7)&#xA;OD/Inc (Categorical 1 to 4)&#xA;Insurance (Binary 0 or 1)&#xA;Income Loss (Binary 0 or 1)&#xA;Living Arrangement (Categorical 1 to 7)&#xA;Employment Status (categorical 1 to 8)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My Dependent Variable is Default (Binary 0 or 1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following is the output from running Regression Model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Analysis of Maximum Likelihood Estimates&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                                  Standard          Wald&#xA;Parameter       DF    Estimate       Error    Chi-Square    Pr &amp;gt; ChiSq    Exp(Est)&#xA;&#xA;Intercept        1     -0.4148      0.0645         41.30        &amp;lt;.0001       0.660&#xA;CPR___Inc  1     1     -0.8022      0.1051         58.26        &amp;lt;.0001       0.448&#xA;CPR___Inc  2     1     -0.4380      0.0966         20.57        &amp;lt;.0001       0.645&#xA;CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363&#xA;CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997&#xA;CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142&#xA;CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185&#xA;Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795&#xA;Emp_Status 2     1      0.4061      0.0940         18.66        &amp;lt;.0001       1.501&#xA;Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809&#xA;Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116&#xA;Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796&#xA;Emp_Status 6     1      0.3761      0.0943         15.91        &amp;lt;.0001       1.457&#xA;Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716&#xA;Inc_Loss   0     1     -0.1996      0.0449         19.76        &amp;lt;.0001       0.819&#xA;Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134&#xA;Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893&#xA;Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294&#xA;Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024&#xA;Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100&#xA;Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952&#xA;Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689&#xA;OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808&#xA;OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973&#xA;OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I used this Model to Score a new set of data. An example row of my new data is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CPR - 7&#xA;OD - 4&#xA;Living Arrangement - 4&#xA;Employment Status - 4&#xA;Insurance - 0&#xA;Income Loss - 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For this sample row, the model predicted output (Probability of default = 1) as 0.7335 &#xA;To check this manually, I added the estimates&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0&#xA;-0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Odds ratio = Exponential(-0.0839) = 0.9195&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am unable to understand why there is such a mismatch between the Model's predicted probability and theoretical probability. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be much appreciated .&#xA;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked into machine learning algorithms, but I'm not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN (with what I think are sensible parameters and distance function).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently working on implementing Stochastic Gradient Descent, &lt;code&gt;SGD&lt;/code&gt;, for neural nets using back-propagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?&lt;/li&gt;&#xA;&lt;li&gt;If so, how do you use this information to inform your decision about a value?&lt;/li&gt;&#xA;&lt;li&gt;If it's not what sort of values should I choose, and how should I choose them?&lt;/li&gt;&#xA;&lt;li&gt;It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?&lt;/li&gt;&#xA;&lt;li&gt;Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In short: How do I choose the learning rate for SGD?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;It seems as though most languages have some number of scientific computing libraries available. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Python has &lt;code&gt;Scipy&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Rust&lt;/code&gt; has &lt;code&gt;SciRust&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;C++&lt;/code&gt; has several including &lt;code&gt;ViennaCL&lt;/code&gt; and &lt;code&gt;Armadillo&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Java&lt;/code&gt; has &lt;code&gt;Java Numerics&lt;/code&gt; and &lt;code&gt;Colt&lt;/code&gt; as well as several other&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Not to mention languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Julia&lt;/code&gt; designed explicitly for scientific computing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. And will anything ever outperform &lt;code&gt;Fortran&lt;/code&gt;? Additionally compiled languages tend to have GPU acceleration, while interpreted languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; don't. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I've missed?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;h1&gt;Motivation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Anonymizing Confidential Data&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What Doesn't Work&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &amp;lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &amp;lt;= 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the only method I am familiar with that does this is &lt;a href=&quot;http://www.techrepublic.com/blog/it-security/cryptographys-running-gag-rot13/&quot; rel=&quot;noreferrer&quot;&gt;ROT13&lt;/a&gt; (more generally, any &lt;a href=&quot;https://en.wikipedia.org/wiki/Caesar_cipher&quot; rel=&quot;noreferrer&quot;&gt;shift cipher&lt;/a&gt;), which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;Promise you won't flip the paper over?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another &lt;strong&gt;bad&lt;/strong&gt; solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+----+-------------------+-----------+--------+&#xA;| Row | ID | Name              | WordChars | Origin |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp;amp; 3, or about the distance between rows 2 &amp;amp; 4 (i.e., how close they are to matching).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found a couple papers that might be relevant, but they're a bit over my head:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.merl.com/publications/docs/TR2010-109.pdf&quot; rel=&quot;noreferrer&quot;&gt;Privacy Preserving String Comparisons Based on Levenshtein Distance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf&quot; rel=&quot;noreferrer&quot;&gt;An Empirical Comparison of Approaches to Approximate String &#xA;Matching in Private Record Linkage&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Does anyone know some good tutorials on online machine learning technics?&#xA;I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD: Thank you everyone for answers, by &quot;online&quot; I mean methods which can be trained in a real-time mode, based on a new inputs one by one.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;As an extension to our great list of &lt;a href=&quot;https://datascience.stackexchange.com/questions/155/publicly-available-datasets&quot;&gt;publicly available datasets&lt;/a&gt;, I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the name of the social network;&lt;/li&gt;&#xA;&lt;li&gt;what kind of user information it provides (posts, profile, friendship network, ...);&lt;/li&gt;&#xA;&lt;li&gt;whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);&lt;/li&gt;&#xA;&lt;li&gt;whether it simply provides a snapshot of the whole dataset.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any suggestions and further characteristics to be added are very welcome.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm planning to run experiments with large datasets on distributed system in order to evaluate efficiency gains in comparison with previous proposals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have limited number of machines nearly ten machines having 200 GB of free space on hard disk on each. On the contrary, I wished to perform experiments on more than available nodes in order to measure scalability, &lt;strong&gt;more precisely&lt;/strong&gt;. Since I don't have any, I thought about using a commodity cluster. However, I'm not sure about the policies of usage, and I need to reliably measure execution times. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there commodity services which will grant me that only my application would be running at a given time?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I recently saw a cool feature that &lt;a href=&quot;https://support.google.com/docs/answer/3543688?hl=en&quot;&gt;was once available&lt;/a&gt; in Google Sheets: you start by writing a few related keywords in consecutive cells, say: &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, and it automatically generates similar keywords (in this case, other colors). See more examples in &lt;a href=&quot;http://youtu.be/dlslNhfrQmw&quot;&gt;this YouTube video&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to reproduce this in my own program. I'm thinking of using Freebase, and it would work like this intuitively: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Retrieve the list of given words in Freebase;&lt;/li&gt;&#xA;&lt;li&gt;Find their &quot;common denominator(s)&quot; and construct a distance metric based on this;&lt;/li&gt;&#xA;&lt;li&gt;Rank other concepts based on their &quot;distance&quot; to the original keywords;&lt;/li&gt;&#xA;&lt;li&gt;Display the next closest concepts.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;As I'm not familiar with this area, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is there a better way to do this?&lt;/li&gt;&#xA;&lt;li&gt;What tools are available for each step?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is anyone using &lt;code&gt;Julia&lt;/code&gt; (&lt;a href=&quot;http://julialang.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://julialang.org/&lt;/a&gt;) for professional jobs?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or using it instead of R, Matlab, or Mathematica? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it a good language?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have to predict next 5-10 years: Do you think it grow up enough to became such a standard in data science like R or similar?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to understand how all the &quot;big data&quot; components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can't run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use &lt;a href=&quot;http://en.wikipedia.org/wiki/Inductive_logic_programming&quot;&gt;Inductive Logic Programming&lt;/a&gt; (ILP), but due to special circumstances I can't do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other way to tackle this would be just to try to aggregate the values when I have a foreign relation. However, I have thousands of important and distinct rows for some nominal attributes (e.g.: A patient with a relation to several distinct drug prescriptions). So, I just can't do that without creating a new attribute for each distinct row of that nominal attribute, and furthermore most of the new columns would have NULL values if I do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any non-ILP algorithm that allows me to data mine relational databases without resorting to techniques like pivoting, which would create thousands of new columns?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I think that Bootstrap can be useful in my work, where we have a lot a variables that we don't know the distribution of it. So, simulations could help.&#xA;What are good sources to learn about Bootstrap/other useful simulation methods?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I've read through the introduction documentation for Spark, but I'm curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a large number of samples which represent Manchester encoded bit streams as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have manually decoded these streams, but I was wondering if I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? &lt;a href=&quot;http://en.wikipedia.org/wiki/Independent_component_analysis&quot; rel=&quot;nofollow&quot;&gt;Independent Component Analysis&lt;/a&gt; (ICA) seems like could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm developing a distributed application, and as it's been designed, there'll be a great load of communication during the processing. Since the communication is already as much &lt;em&gt;spread&lt;/em&gt; along the entire process as possible, I'm wondering if there any standard solutions to improve the performance of the message passing layer of my application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What changes/improvements could I apply to my code to reduce the time spent sending messages? For what it's worth, I'm communicating up to 10GB between 9 computing nodes, and the framework I'm using is implemented with OpenMPI.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I understand that compression methods may be split into two main sets: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;global&lt;/li&gt;&#xA;&lt;li&gt;local&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing on any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting information that usually improves the compression rate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While reading about some of these methods, I noticed that &lt;a href=&quot;http://en.wikipedia.org/wiki/Universal_code_%28data_compression%29#Universal_and_non-universal_codes&quot; rel=&quot;nofollow noreferrer&quot;&gt;the unary method is not universal&lt;/a&gt;, which surprised me since I thought &quot;globality&quot; and &quot;universality&quot; referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My primary questions:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is the difference between universal and global methods? &lt;/li&gt;&#xA;&lt;li&gt;Aren't these classifications synonyms?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it's difficult to create training datasets that contain enough examples of the positive class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Which freely available datasets can I use to train a text classifier?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are trying to enhance our users engagement by recommending the most related content for him, so we thought If we classified our content based on a predefined bag of words we can recommend to him engaging content by getting his feedback on random number of posts already classified before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can use this info to recommend for him pulses labeled with those classes. But we found If we used a predefined bag of words not related to our content the feature vector will be full of zeros, also categories may be not relevant to our content. so for those reasons we tried another solution that will be clustering our content not classifying it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks :)&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;K-means&lt;/a&gt; is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;There's this side project I'm working on where I need to structure a solution to the following problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two groups of people (clients). Group &lt;code&gt;A&lt;/code&gt; intends to buy and group &lt;code&gt;B&lt;/code&gt; intends to sell a determined product &lt;code&gt;X&lt;/code&gt;. The product has a series of attributes &lt;code&gt;x_i&lt;/code&gt;, and my objective is to facilitate the transaction between &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; by matching their preferences. The main idea is to point out to each member of &lt;code&gt;A&lt;/code&gt; a corresponding in &lt;code&gt;B&lt;/code&gt; whose product better suits his needs, and vice versa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some complicating aspects of the problem:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Attributes might be continuous, binary, or non-quantifiable (ex: price, functionality, design);&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any suggestion on how to approach this problem and solve it in an automated way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also appreciate some references to other similar problems if possible.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Great suggestions! Many similarities in to the way I'm thinking of approaching the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Let’s take an example of a car. The product “car” has lots and lots of attributes that range from its performance, mechanical structure, price etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I just want a cheap car, or an electric car. Ok, that's easy to map because they represent main features of this product. But let’s say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn't ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them. Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I'm thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next step would be a “refined search”. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot “guess” what the buyer needs. But might get me close.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The weighting criteria suggested is great. It allows me to quantify the level to which the seller matches the buyer’s needs. The scaling part might be a problem though, because the importance of each attribute varies from client to client. I'm thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I can't find the reason. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not an expert so I would like to ask you what I can improve and what I'm doing wrong. This is the complete description: &lt;a href=&quot;https://github.com/denadai2/Gas-consumption-outliers&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/denadai2/Gas-consumption-outliers&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The neural network is a FeedFoward Network with Back Propagation. As described &lt;a href=&quot;http://nbviewer.ipython.org/github/denadai2/Gas-consumption-outliers/blob/master/3-%20Regression_NN.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; I splitted the dataset in a &quot;small&quot; dataset of 41'000 rows, 9 features and I tried to add more features. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I trained the networks but the results have 14.14 RMSE, so it can't predict so well the gas consumptions, consecutively I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I improve? What am I doing wrong? Can you have a look of my description?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working on a project and need resources to get me up to speed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The dataset is around 35000 observations on 30 or so variables.  About half the variables are categorical with some having many different possible values, i.e. if you split the categorical variables into dummy variables you would have a lot more than 30 variables.  But still probably on the order of a couple of hundred max.  (n&gt;p).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The response we want to predict is ordinal with 5 levels (1,2,3,4,5).  Predictors are a mix of continuous and categorical, about half of each.  These are my thoughts/plans so far:&#xA;1.  Treat the response as continuous and run vanilla linear regression.&#xA;2.  Run nominal and ordinal logistic and probit regression&#xA;3.  Use MARS and/or another flavor of non-linear regression&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm familiar with linear regression.  MARS is well enough described by Hastie and Tibshirani.  But I'm at a loss when it comes to ordinal logit/probit, especially with so many variables and a big data set.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The r package &lt;a href=&quot;http://cran.r-project.org/web/packages/glmnetcr/index.html&quot;&gt;glmnetcr&lt;/a&gt; seems to be my best bet so far, but the documentation hardly suffices to get me where I need to be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where can I go to learn more?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The usual definition of regression (as far as I am aware) is &lt;em&gt;predicting a continuous output variable from a given set of input variables&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Logistic regression is a binary classification algorithm, so it produces a categorical output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it really a regression algorithm? If so, why?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;In network structure, what is the difference between &lt;strong&gt;k-cliques&lt;/strong&gt; and &lt;strong&gt;p-cliques&lt;/strong&gt;, can anyone give a brief explaination with examples? Thanks in advanced!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;============================&#xA;&lt;br&gt;EDIT:&#xA;I found an online &lt;a href=&quot;http://open.umich.edu/sites/default/files/SI508-F08-Week7-Lab6.ppt&quot; rel=&quot;nofollow&quot;&gt;ppt&lt;/a&gt; while I am googling, please take a look on &lt;strong&gt;p.37&lt;/strong&gt; and &lt;strong&gt;p.39&lt;/strong&gt;, can you comment on them?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;There's this side project I'm working on where I need to structure a solution to the following problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two groups of people (clients). Group &quot;A&quot; intends to buy and group &quot;B&quot; intends to sell a determined product &quot;X&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The product has a series of attributes x_i and my objective is to facilitate the transaction between &quot;A&quot; e &quot;B&quot; by matching their preferences. The main idea is to point out to each member of &quot;A&quot; a corresponding in &quot;B&quot; who’s product better suits his needs, and vice versa. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some complicating aspects of the problem:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any suggestion on how to approach this problem and solve it in an automated way?&lt;br&gt;&#xA;The idea is to really think out of the box here so feel free to &quot;go wild&quot; on your suggestions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also appreciate some references to other similar problems if possible. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;My data set is formatted like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;User-id | Threat_score&#xA;aaa       45&#xA;bbb       32&#xA;ccc       20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The list contains the top 100 users with the highest threat scores. I generate such a list monthly and store each month's list in its own file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three things I would like to get from this data:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;1. Users who are consistently showing up in this list&lt;/ul&gt;&#xA;&#xA;&lt;ul&gt;2. Users who are consistently showing up in this list with high threat scores&lt;/ul&gt;&#xA;&#xA;&lt;ul&gt;3. Users whose threat scores are increasing very quickly&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am thinking a visual summary would be nice; each month (somehow) decide which users I want to plot on a graph of historic threat scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any known visualization techniques that deal with similar requirements?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I be transforming my current data to achieve what I am looking for?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example scenario:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business's daily walk in client's following a specific event?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am attempting to determine the impact that a particular marketing agency has on their client's website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently working with a dataset with a wide range of document lengths -- anywhere from a single word to a full page of text.  In addition, the grammatical structure and use of punctuation varies wildly from document to document.  The goal is to classify those documents into one of about 10-15 categories.  I'm currently using ridge regression and logistic regression for the task, and CV for the alpha values of ridge.  The feature vectors are tf-idf ngrams.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Recently I've noticed that longer documents are much less likely to be categorized. Why might this be the case, and how can one &quot;normalize&quot; for this kind of variation?  As a more general question, how does one typically deal with diverse data sets?  Should documents be grouped based off of metrics like document length, use of punctuation, grammatical rigor, etc. and then fed through different classifiers?  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Classification algorithms used: random forest &amp;amp; logistic regression.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working on multiclass logistic regression model with a large number of features (numFeatures &gt; 100). Using a Maximum Likelihood Estimation based on the cost function and gradient, the fmincg algorithm solves the problem quickly. However, I'm also experimenting with a different cost function and do not have a gradient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good way to speed up the calculation process? E.g., is there a different algorithm or fmincg setting that I can use?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Please, could someone recommend a paper or blog post that describes the online k-means algorithm.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm&quot; rel=&quot;nofollow noreferrer&quot;&gt;Smith-Waterman Algorithm&lt;/a&gt; to compare the similarity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I've drawn a picture of how I'm thinking about representing the data - &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/VC4em.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Image that this matrix represents a key type of &quot;things&quot; I then need to add the &quot;things&quot; similarity score into a vector of 0 or 1. Thats ok.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;There is a general recommendation that algorithms in ensemble learning combinations should be different in nature. Is there a classification table, a scale or some rules that allow to evaluate how far away are the algorithms from each other? What are the best combinations? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a dataset with following specifications:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Training dataset with 193176 samples with 2821 positives&lt;/li&gt;&#xA;&lt;li&gt;Test Dataset with 82887 samples with 673 positives&lt;/li&gt;&#xA;&lt;li&gt;There are 10 features.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mean square error : 0.00804710026904&#xA;Confusion matrix : [[82214   667]&#xA;                   [    0     6]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Different algorithms like RandomForest, DecisionTree, SVM&lt;/li&gt;&#xA;&lt;li&gt;Changing parameters value to call the function&lt;/li&gt;&#xA;&lt;li&gt;Some intuition based feature engineering to include compounded features&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Now, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What can I do to improve the number of positive hits ? &lt;/li&gt;&#xA;&lt;li&gt;How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )&lt;/li&gt;&#xA;&lt;li&gt;At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )&lt;/li&gt;&#xA;&lt;li&gt;Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?&lt;/li&gt;&#xA;&lt;li&gt;Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am using the scikit-learn library with Python and all implementations are library functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the results with a few other algorithms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Random Forest Classifier(n_estimators=100)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[82211   667]&#xA;[    3     6]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Decision Trees:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[78611   635]&#xA;[ 3603    38]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;More often than not, data I am working with is not 100% clean. Even if it is reasonably clean, still there are portions that need to be fixed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When a fraction of data needs it, I write a script and incorporate it in data processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;,  such as that in CSV files or a relational database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The practical problems I encountered:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).&lt;/li&gt;&#xA;&lt;li&gt;Copying and modifying data may make a mess, as:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generating it again will destroy all fixes.&lt;/li&gt;&#xA;&lt;li&gt;When there are more errors of different kinds, too many copies of the same file result, and it is hard to keep track of them all.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, &lt;em&gt;seems&lt;/em&gt; to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What are the best practices in such a case as this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: The question is on the workflow, not whether to use it or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(In my particular case I don't want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Data Science and Machine Learning include a lot of different topics and it´s hard to stay up-to-date about all the news about papers, researches or new tutorials and tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What sources do you use to get all the information?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use mostly Reddit as my first source and the subreddits Machine Learning and R&#xA;&lt;a href=&quot;http://www.reddit.com/r/MachineLearning&quot; rel=&quot;nofollow&quot;&gt;http://www.reddit.com/r/MachineLearning&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.reddit.com/r/rstats&quot; rel=&quot;nofollow&quot;&gt;http://www.reddit.com/r/rstats&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But also datatau.com and of course the great KDNuggets page &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;What is the very first thing you do when you get your hands on a new data set (assuming it is cleaned and well structured)? Please share sample code snippets as I am sure this would be extremely helpful for both beginners and experienced. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;ADD:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As mentioned in a response below, I also considered (but forgot to add here) using Dynamic Time Warping (DTW) and then hierarchical clustering on the resulting distance matrix - creating some number of clusters and then using the cluster membership as a feature. Scoring test data would likely have to follow a process where the DTW was done on new cases and the cluster centroids - matching the new data series to their closest centroids... &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Practices to make sure the dataset is clean?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Practices to make sure the dataset doesn't have wrong values or so?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The task is the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there's a cat present so that I may go chase it off using my super soaker. For simplicity's sake, say that I only care about a cat with black and white coloring.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/cNqus.jpg&quot; alt=&quot;Sample garden image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xA;a,b,.2&#xA;a,c,.3&#xA;a,d,.4&#xA;a,e,.1&#xA;e,b,.3&#xA;e,c,.5&#xA;e,d,.8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've looked at &lt;a href=&quot;https://github.com/mbostock/d3/wiki/Gallery&quot; rel=&quot;nofollow noreferrer&quot;&gt;some graphs&lt;/a&gt;, but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business that for these users the recommendation system is useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;@Steve Kallestad do you mean something like this : &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4zyQR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;As I increase the number of trees in &lt;a href=&quot;http://scikit-learn.org/stable/&quot; rel=&quot;nofollow&quot;&gt;scikit learn&lt;/a&gt;'s &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features, most of which are binary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the parameters that I was tuning were:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the number of trees/iterations;&lt;/li&gt;&#xA;&lt;li&gt;learning depth;&lt;/li&gt;&#xA;&lt;li&gt;and learning rate.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code is something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;X_train, X_test, y_train, y_test = train_test_split(X, y)&#xA;&#xA;reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = 'ls', learning_rate = .01)&#xA;&#xA;reg.fit(X_train, y_train)&#xA;&#xA;ypred = reg.predict(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Many times &lt;a href=&quot;http://en.wikipedia.org/wiki/Named-entity_recognition&quot;&gt;Named Entity Recognition&lt;/a&gt; (NER) doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider the following input:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Barack Obama is a great person.&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And the output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]),&#xA;    ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where as for the input: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;'Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he &quot;was honored&quot; to be compared to Darth Vader while in office.'&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;the output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'),&#xA;    Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'),&#xA;    ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]),&#xA;    ('that', 'IN'), ('he', 'PRP'), ('``', '``'), ('was', 'VBD'), ('honored', 'VBN'),&#xA;    (&quot;''&quot;, &quot;''&quot;), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'),&#xA;    Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'),&#xA;    ('office', 'NN'), ('.', '.')])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here &lt;code&gt;Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP)&lt;/code&gt; is correctly extracted. So, I think if &lt;code&gt;nltk.ne_chunk&lt;/code&gt; is used first, and then if two consecutive trees are NNP, there are higher chances that both refer to one entity. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been playing with NLTK toolkit, and I came across this problem a lot, but couldn't find a satisfying answer. Any suggestion will be really appreciated. I'm looking for flaws in my approach.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to rank some percentages. I have numerators and denominators for each ratio. To give a concrete example, consider ratio as &lt;code&gt;total graduates / total students&lt;/code&gt; in a school.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the issue is that &lt;code&gt;total students&lt;/code&gt; vary over a long range (1000-20000). Smaller schools seem to have higher percentage of students graduating, but I want to standardize it, and not let the size of the school affect the ranking. Is there a way to do it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm new to the world of text mining and have been reading up on annotators at places like the &lt;a href=&quot;http://uima.apache.org/&quot; rel=&quot;nofollow&quot;&gt;UIMA website&lt;/a&gt;. I'm encountering many new terms like named entity recognition, tokenizer, lemmatizer, gazetteer, etc. Coming from a layman background, this is all very confusing so can anyone tell me or link to resources that can explain what the main categories of annotators are and what they do?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they're restricted to US stocks. Yahoo Finance provides this data online but there's no option to download it ( or none I am aware of ). &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?&lt;/li&gt;&#xA;&lt;li&gt;Is there some way to download it via Yahoo Finance or Google Finance ?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I need data for the last decade or so and hence need some script or API which would do this.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;While doing a Google image search, the page displays some figured out categories for the images of the topic being searched for. I'm interested in learning how this works, and how it chooses and creates categories.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, I couldn't find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this, and what basis these categories are created from?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I search for &quot;animals&quot; I get the categories:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;cute&quot;, &quot;baby&quot;, &quot;wild&quot;, &quot;farm&quot;, &quot;zoo&quot;, &quot;clipart&quot;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If I go into &quot;wild&quot;, I then have subcategories:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;forest&quot;, &quot;baby&quot;, &quot;africa&quot;, &quot;clipart&quot;, &quot;rainforest&quot;, &quot;domestic&quot;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm new to machine learning, but I have an interesting problem. I have a large sample of people and visited sites. Some people have indicated gender, age, and other parameters. Now I want to restore these parameters to each user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which way do I look for? Which algorithm is suitable to solve this problem? I'm familiar with Neural Networks (supervised learning), but it seems they don't fit.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;There are several classic datasets for machine learning classification/regression tasks. The most popular are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html&quot;&gt;Iris Flower Data Set&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.kaggle.com/c/titanic-gettingStarted&quot;&gt;Titanic Data Set&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&quot;&gt;Motor Trend Cars&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;But does anyone know similar datasets for networks analysis / graph theory? More concrete - I'm looking for &lt;strong&gt;Gold standard&lt;/strong&gt; datasets for comparing/evaluating/learning:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;centrality measures;&lt;/li&gt;&#xA;&lt;li&gt;network clustering algorithms.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I don't need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's quite difficult to provide exact features for &quot;gold standard dataset&quot;, but here are some thoughts. I think, real classic dataset should satisfy these criteria:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Multiple references in articles and textbooks;&lt;/li&gt;&#xA;&lt;li&gt;Inclusion in well-known network analysis software packages;&lt;/li&gt;&#xA;&lt;li&gt;Sufficient time of existence;&lt;/li&gt;&#xA;&lt;li&gt;Usage in a number of courses on graph analysis.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Concerning my field of interest, I also need labeled classes for vertices and/or precomputed (or predefined) &quot;authority scores&quot; (i.e. centrality estimates). After asking this question I continued searching, and here are some suitable examples:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://networkdata.ics.uci.edu/data.php?id=105&quot;&gt;Zachary's Karate Club&lt;/a&gt;: introduced in 1977, cited more than 1.5k times (according to Google Scholar), vertexes have attribute Faction (which can be used for clustering).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.orgnet.com/Erdos.html&quot;&gt;Erdos Collaboration Network&lt;/a&gt;: unfortunately, I haven't find this network in form of data-file, but it's rather famous, and if someone will enrich network with  mathematicians' specialisations data, it also could be used for testing clustering algorithms.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a political campaign where dozens of volunteers will be conducting door-knocking promotions over the next few weeks. Given a list with names, addresses and long/lat coordinates, what algorithms can be used to create an optimized walk list.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm building a recommender system and using SVD as one of the preprocessing techniques.&#xA;However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I take the SVD (A = USV^T), is there a standard way to normalize the matrix 'A' between 0 and 1? Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I want all of my similarity measurements to give results between 0 and 1 and my normalized euclidean distance in particular fails if the input matrix does not have values between 0 and 1.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;datasciencetoolKIT.org -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;datasciencetoolBOX.org -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and &lt;a href=&quot;http://en.wikipedia.org/wiki/Multilayer_perceptron&quot; rel=&quot;noreferrer&quot;&gt;Multilayer perceptron&lt;/a&gt; (MLP) that I put together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of &lt;code&gt;lambda * current_weight&lt;/code&gt; is added as well before the combined delta is subtracted at the end of the batch, where &lt;code&gt;lambda&lt;/code&gt; is the regularisation parameter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. &lt;em&gt;Does a smaller regularisation term per item work just as well?&lt;/em&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance &lt;code&gt;lambda * current_weight / N&lt;/code&gt; where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I understand Hadoop MapReduce and its features but I am confused about R MapReduce.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One difference I have read is that R utilizes maximum RAM. So do perform parallel processing integrated R with Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;My doubt is:&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;R can do all stats, math and data science related stuff, but why R MapReduce?&lt;/li&gt;&#xA;&lt;li&gt;Is there any new task I can achieve by using R MapReduce instead of Hadoop MapReduce? If yes, please specify.&lt;/li&gt;&#xA;&lt;li&gt;We can achieve the task by using R with Hadoop (directly) but what is the importance of MapReduce in R and how it is different from normal MapReduce?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[A B B C A]  &#xA;[A A B A A]  &#xA;[A B B C C]  &#xA;[A A A A A]  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There would be two separate clusters for A, two separate clusters for C, and one cluster for B.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output I'm looking for would ideally assign a unique ID to each cluster, something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[1 2 2 3 4]  &#xA;[1 1 2 4 4]  &#xA;[1 2 2 5 5]  &#xA;[1 1 1 1 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I'm convinced it must be out there.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering if there's a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that's being classified?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, I'm using precision/recall as a measure of performance, so might have something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                    Precision Recall&#xA;Person classifier   65%       40%&#xA;Company classifier  98%       90%&#xA;Cheese classifier   10%       50%&#xA;Egg classifier      100%      100%&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the dataset I'm running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say we had hundreds of such classifiers, I guess I'm looking for a good way to answer questions like &quot;Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they're underperforming?&quot;. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How can I choose an algorithm for the overall classifier?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am new to machine learning. I have a task at hand of predicting click probability given user information like city, state, OS version, OS family, device, browser family, browser version, etc. I have been advised to try logit since logit seems to be what MS and Google are using. I have some questions regarding logistic regression:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Click and non click is a very very unbalanced class and the simple GLM predictions do not look good. How can I make the data work better with the GLM?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. How can I deal with this distribution of categorical variables?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the variables that we get is device ID. This is a very unique feature that can be translated to a user's identity. How can I make use of it in logit, or should it be used in a completely different model based on user identity?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently working on a project that would benefit from personalized predictions.  Given an input document, a set of output documents, and a history of user behavior, I'd like to predict which of the output documents are clicked.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, I'm wondering what the typical approach to this kind of personalization problem is.  Are models trained per user, or does a single global model take in summary statistics of past user behavior to help inform that decision?  Per user models won't be accurate until the user has been active for a while, while most global models have to take in a fixed length feature vector (meaning we more or less have to compress a stream of past events into a smaller number of summary statistics).  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently searching for labeled datasets to use to train a model to extract named entities from informal text (think something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I'm looking for out of domain data that's a bit more &quot;informal&quot; than the news articles and journal entries that many of today's state of the art named entity recognition systems are trained on.  Any recommendations?  So far I've only been able to locate 50k tokens from twitter published here: &lt;a href=&quot;https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt&quot;&gt;https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;For a recommendation system I'm using cosine similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general &quot;average&quot; category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for example if these were my observations (rows are observations, cols are features):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[0, 0, 0, 1, 1, 1, 0, 1, 0],&#xA; [1, 0, 1, 0, 0, 0, 1, 0, 0],&#xA; [1, 1, 1, 1, 0, 1, 0, 1, 1],&#xA; [0, 0, 1, 0, 0, 1, 0, 1, 0]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosine similarity. Though, at the moment, I'm having trouble reasoning my way through angles between lines in high dimensional space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to build a nonparametric density function for a fairly large dataset that can be evaluated efficently, and can be updated efficiently when new points are added. There will only ever be a maximum of 4 independent variables, but we can start off with 2. Lets use a gaussian kernel. Let the result be a probability density function, i.e. its volume will be 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In each evaluation, we can omit all points for which the evaluation point is outside a certain ellipsoid corresponding to the minimum gaussian value we care about. We can change this threshold for accuracy or performance, and the maximum number of points inside the threshold will depend on the chosen covariance matrix of the kernel. Then, we can evaluate the distribution approximately using the subset of points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use a fixed kernel, then we can use the eigenvalues and eigenvectors we get from the covariance matrix to transform each point so that the threshold ellipsoid is a fixed circle. We can then shove all the transformed points into a spatial index, and efficiently find all points within the required radius of the evaluation point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, we would the kernel to be variable for two reasons. (1) to fit the data better, and (2) because adding or modifying points would require the fixed kernel to be updated, which would mean that the entire data set would need to be reindexed. With a variable kernel, we could make new/updated points only affect the closest points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, is there a spatial index that can efficiently find ellipses surrounding a given point from a set of around 10 million ellipses of different shapes and sizes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More generally though, does my approach look sound? I am open to answers like &quot;give up and precalculate a grid of results&quot;. Answers much appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to find which classification methods, that do not use a training phase, are available. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The scenario is gene expression based classification, in which you have a matrix of gene expression of m genes (features) and n samples (observations).&#xA;A signature for each class is also provided (that is a list of the features to consider to define to which class belongs a sample).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An application (non-training) is the &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015543&quot; rel=&quot;nofollow&quot;&gt;Nearest Template Prediction&lt;/a&gt; method. In this case it is computed the cosine distance between each sample and each signature (on the common set of features). Then each sample is assigned to the nearest class (the sample-class comparison resulting in a smaller distance). No already classified samples are needed in this case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A different application (training) is the &lt;a href=&quot;http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot; rel=&quot;nofollow&quot;&gt;kNN&lt;/a&gt; method, in which we have a set of already labeled samples. Then, each new sample is labeled depending on how are labeled the k nearest samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other non-training methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I would like to use ANN to automate trading currencies, preferably USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some papers and done some experiments but without much luck. I would like to get advice from EXPERTS to make this work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is what I did so far:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.&lt;/li&gt;&#xA;&lt;li&gt;Extracted all ticks for the time frame 12PM to 14PM for all days.&lt;/li&gt;&#xA;&lt;li&gt;From this data, created a data set where each entry consists of n bid values in sequence. &lt;/li&gt;&#xA;&lt;li&gt;Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.&lt;/li&gt;&#xA;&lt;li&gt;The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 output neuron. Input layer had linear TF, hidden had log TF and output had linear TF.&lt;/li&gt;&#xA;&lt;li&gt;Trained the network with back propagation with n-125 first and then 10.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For both n, the MSE did not drop below 0.5 and stayed at that value during full training. Assuming that this could be due to the time series being totally random, I used the R package to find partial autocorrelation on the data set (pacf). This gave non zero values for 2 and 3 lags only. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 1: What does this mean exactly? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I used hurst exponent to evaluate the randomness. In R, hurst(values) showed values above 0.9.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 2: It is supposed to be nearly random. Should it have a value closer to 0.5?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the (n-1)th bid value. It looks like ANN just takes the last bid as the next bid! I tried different network structures (all multilayer perceptions), different training parameters, etc, but results are same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 3: How can I improve the accuracy? Are there any other training methods than backpropagation?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Suppose I want to use CART as classification tree (I want a categorical response). I have the training set, and I split it using observation labels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, to build the decision tree (classification tree) how are selected the features to decide which label apply to testing observations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Supposing we are working on gene expression matrix, in which each element is a real number, is that done using features that are more distant between classes?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I would like to use non-atomic data, as a feature for a prediction. &#xA;Suppose I have a Table with these features:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;- Column 1: Categorical - House&#xA;- Column 2: Numerical - 23.22&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xA;- Column 5: A List [ 122, Boolean ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to predict/classify, for instance, Column 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am making something to automatically respond to questions, any type of question, like &quot;Where was Foo Born?&quot; ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I first make a query to a search engine, then I get some text data as a result, then I do all the parsing stuff (tagging, stemming, parsing, splitting ... )&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first approach was to make a table, each row with a line of text and a lot of features, like &quot;First Word&quot;, &quot;Tag of First Word&quot;, &quot;Chunks&quot;, etc...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But with this approach I am missing the relationships between the sentences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know if there is an algorithm that looks inside the tree structures (or vectors) and makes the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I m new to RHadoop and also to RMR...&#xA;I had an requirement to write a Mapreduce Job in R Mapreduce. I have Tried writing but While executing this it gives an Error.&#xA;Tring to read the file from hdfs&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Error:&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  : &#xA;   hadoop streaming failed with error code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Code :&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Sys.setenv(HADOOP_HOME=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop&quot;)&#xA;Sys.setenv(HADOOP_CMD=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/bin/hadoop&quot;)&#xA;&#xA;Sys.setenv(HADOOP_STREAMING=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar&quot;)&#xA;library(rmr2)&#xA;library(rhdfs)&#xA;hdfs.init()&#xA;day_file = hdfs.file(&quot;/hdfs/bikes_LR/day.csv&quot;,&quot;r&quot;)&#xA;day_read = hdfs.read(day_file)&#xA;c = rawToChar(day_read)&#xA;&#xA;XtX =&#xA;  values(from.dfs(&#xA;    mapreduce(&#xA;      input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xA;      map=&#xA;        function(.,Xi){&#xA;         yi =c[Xi[,1],]&#xA;         Xi = Xi[,-1]&#xA;         keyval(1,list(t(Xi)%*%Xi))&#xA;       },&#xA;  reduce = function(k,v )&#xA;  {&#xA;    vals =as.numeric(v)&#xA;    keyval(k,sum(vals))&#xA;  } ,&#xA;  combine = TRUE)))[[1]]&#xA;&#xA;XtY =&#xA; values(from.dfs(&#xA;    mapreduce(&#xA;     input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xA;     map=&#xA;       function(.,Xi){&#xA;         yi =c[Xi[,1],]&#xA;         Xi = Xi[,-1]&#xA;        keyval(1,list(t(Xi)%*%yi))&#xA;       },&#xA;     reduce = TRUE ,&#xA;     combine = TRUE)))[[1]]&#xA;solve(XtX,XtY)&#xA;&#xA;&#xA;&#xA;Input:&#xA;------------&#xA;&#xA;instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt&#xA;1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,985&#xA;2,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,801&#xA;3,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,1349&#xA;4,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,1562&#xA;5,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,1600&#xA;6,2011-01-06,1,0,1,0,4,1,1,0.204348,0.233209,0.518261,0.0895652,88,1518,1606&#xA;7,2011-01-07,1,0,1,0,5,1,2,0.196522,0.208839,0.498696,0.168726,148,1362,1510&#xA;8,2011-01-08,1,0,1,0,6,0,2,0.165,0.162254,0.535833,0.266804,68,891,959&#xA;9,2011-01-09,1,0,1,0,0,0,1,0.138333,0.116175,0.434167,0.36195,54,768,822&#xA;10,2011-01-10,1,0,1,0,1,1,1,0.150833,0.150888,0.482917,0.223267,41,1280,1321&#xA;&#xA;&#xA;&#xA; Please Suggest me any mistakes.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I download and installed CDH 5 package succesfully on a single linux node in pseudo-distributed Mode on my CentOS 6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Starting Hadoop and verifying it is Working Properly as in &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I succesfully finished the following steps&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 1: Format the NameNode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 2: Start HDFS&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 3: Create the /tmp Directory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 4: Create the MapReduce system directories:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 5: Verify the HDFS File Structure&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 6: Start MapReduce&lt;/p&gt;&#xA;&#xA;&lt;p&gt;while following command in step 7 I get the following error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 7: Create User Directories&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;mkdir: '/user/hadoopuser': No such file or directory&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(where hadoopuser is my linux login username)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to success the step 7:?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please provide the sloution to procced the remaining installation.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Note that I am doing everything in R. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem goes as follow: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My original idea:  &lt;em&gt;make this a supervised learning problem&lt;/em&gt;. &#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those &lt;em&gt;unlabelled data&lt;/em&gt;, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&#xA;Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this approach wrong ? Please correct me if you think my approach is wrong. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 3: The tricky part is: how to &lt;strong&gt;identify and extract the keywords&lt;/strong&gt; ? Using the &lt;code&gt;tm&lt;/code&gt; package in R ? what algorithm is the &lt;code&gt;tm&lt;/code&gt;   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas would be great.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;R base function &lt;code&gt;glm()&lt;/code&gt; uses Fishers Scoring for MLE, while the &lt;code&gt;glmnet&lt;/code&gt; appears to use the coordinate descent method to solve the same equation. Coordinate descent is more time-efficient than Fisher Scoring, as Fisher Scoring calculates the second order derivative matrix, in addition to some other matrix operations. which makes expensive to perform, while coordinate descent can do the same task in O(np) time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why would R base function use Fisher Scoring? Does this method have an advantage over other optimization methods? How does coordinate descent and Fisher Scoring compare? I am relatively new to do this field so any help or resource will be helpful.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I was building a model that predicts user churn for a website, where I have data on all users, both past and present.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can build a model that only uses those users that have left, but then I'm leaving 2/3 of the total user population unused.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good way to incorporate data from these users into a model from a conceptual standpoint?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a &lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_linear_regression&quot; rel=&quot;nofollow noreferrer&quot;&gt;Simple Linear Regression&lt;/a&gt; algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm new to data science, would appreciate any pointers to move further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit: nfmcclure's suggestions applied&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Before applying the suggestions&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZsyyQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the snapshot of what I've got after splitting the dataset where the reset occurs, and the slope of two set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/OEQCw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;finding the mean of the two slopes and drawing the line from the mean.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/i2qv5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this OK?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to use the sklearn_pandas module to extend the work I do in pandas and dip a toe into machine learning but I'm struggling with an error I don't really understand how to fix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was working through the following dataset on &lt;a href=&quot;https://www.kaggle.com/c/data-science-london-scikit-learn/data&quot; rel=&quot;nofollow&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's essentially an unheadered table (1000 rows, 40 features) with floating point values.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pdfrom sklearn import neighbors&#xA;from sklearn_pandas import DataFrameMapper, cross_val_score&#xA;path_train =&quot;../kaggle/scikitlearn/train.csv&quot;&#xA;path_labels =&quot;../kaggle/scikitlearn/trainLabels.csv&quot;&#xA;path_test = &quot;../kaggle/scikitlearn/test.csv&quot;&#xA;&#xA;train = pd.read_csv(path_train, header=None)&#xA;labels = pd.read_csv(path_labels, header=None)&#xA;test = pd.read_csv(path_test, header=None)&#xA;mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])&#xA;mapper_train&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DataFrameMapper(features=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',&#xA;       n_neighbors=3, p=2, weights='uniform'))])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So far so good. But then I try the fit&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mapper_train.fit_transform(train, labels)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;TypeError                                 Traceback (most recent call last)&#xA;&amp;lt;ipython-input-6-e3897d6db1b5&amp;gt; in &amp;lt;module&amp;gt;()&#xA;----&amp;gt; 1 mapper_train.fit_transform(train, labels)&#xA;&#xA;//anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)&#xA;    409         else:&#xA;    410             # fit method of arity 2 (supervised transformation)&#xA;--&amp;gt; 411             return self.fit(X, y, **fit_params).transform(X)&#xA;    412 &#xA;    413 &#xA;&#xA;//anaconda/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)&#xA;    116         for columns, transformer in self.features:&#xA;    117             if transformer is not None:&#xA;--&amp;gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xA;    119         return self&#xA;    120 &#xA;&#xA;TypeError: fit() takes exactly 3 arguments (2 given)`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong? While the data in this case is all the same, I'm planning to work up a workflow for mixtures categorical, nominal and floating point features and sklearn_pandas seemed to be a logical fit.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;When I say &quot;document&quot;, I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I made a similar question asking about distance between &quot;documents&quot; (Wikipedia articles, news stories, etc.).  I made this a separate question because search queries are considerably smaller than documents and are considerably noisier.  I hence don't know (and doubt) if the same distance metrics would be used here.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either vanilla lexical distance metrics or state-of-the-art semantic distance metrics are preferred, with stronger preference for the latter.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have 2 datasets, one with positive instances of what I would like to detect, and one with unlabeled instances. What methods can I use ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example, suppose we want to understand detect spam email based on a few structured email characteristics. We have one dataset of 10000 spam emails, and one dataset of 100000 emails for which we don't know whether they are spam or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we tackle this problem (without labeling manually any of the unlabeled data) ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can we do if we have additional information about the proportion of spam in the unlabeled data (i.e. what if we estimate that between 20-40% of the 100000 unlabeled emails are spam) ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am very new to machine learning and in my first project have stumbled across a lot of issues which I really want to get through.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using logistic regression with R's &lt;code&gt;glmnet&lt;/code&gt; package and alpha = 0 for ridge regression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn't much of a difference (0.61).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dependent variable/output is probability of click, based on if there is a click or not in historical data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version and OS family.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of these, for prediction I'm using state, device, user age, user gender, IP carrier, browser version, browser family, OS version and OS family; I am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. I am not using city because they are too many or mobile manufacturer because they are too few.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is that okay or should I be using the rejected variables?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To start, I create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After training the model, I save the coefficients and intercept. These are used for new incoming requests using the formula for logistic regression:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/AZBRq.png&quot; alt=&quot;1 / (1+e^-1*sum(a+k(ith)*x(ith)))&quot;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;a&lt;/code&gt; is intercept, &lt;code&gt;k&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th coefficient and &lt;code&gt;x&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th variable value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is my approach correct so far?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple GLM in R (that is where there is no regularized regression, right?) gave me 0.56 AUC. With regularization I get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn't happen is almost always greater than the max probability that a click happened.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So basically what should I do?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read how stochastic gradient descent is an effective technique in logit so how do I implement stochastic gradient descent in R? If it's not straightforward, is there a way to implement this system in Python? Is SGD implemented after generating a regularized logistic regression model or is it a different process altogether?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also there is an algorithm called follow the regularized leader (FTRL) that is used in click-through rate prediction. Is there a sample code and use of FTRL that I could go through?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using Neural Networks to solve different Machine learning problems. I'm using Python and &lt;a href=&quot;http://pybrain.org/&quot; rel=&quot;noreferrer&quot;&gt;pybrain&lt;/a&gt; but this library is almost discontinued. Are there other good alternatives in Python?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to run some analysis with some big datasets (eg 400k rows vs. 400 columns) with R (e.g. using neural networks and recommendation systems).&#xA;But, it's taking too long to process the data (with huge matrices, e.g. 400k rows vs. 400k columns).&#xA;What are some free/cheap ways to improve R performance?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm accepting packages or web services suggestions (other options are welcome).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a set of datapoints from the unit interval (i.e. 1-dimensional dataset with numerical values). I receive some additional datapoints online, and moreover the value of some datapoints might change dynamically. I'm looking for an ideal clustering algorithm which can handle these issues efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm&quot;&gt;sequential k-means clustering&lt;/a&gt; copes with the addition of new instances, and I suppose with minor modification it can work with dynamic instance values (i.e. first taking the modified instance from the respective cluster, then updating the mean of the cluster and finally giving the modified instance as an input to the algorithm just as the addition of an unseen instance).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern with using the k-means algorithm is the requirement of supplying the number of clusters as an input. I know that they beat other clustering algorithms (GAs, MSTs, Hierarchical Methods etc.) in time&amp;amp;space complexity. Honestly I'm not sure, but maybe I can get away with using one of the aforementioned algorithms. Even that my datasets are relatively large, the existence of a single dimension makes me wonder.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More specifically a typical test case of mine would contain about 10K-200K 1-dimensional datapoints. I would like to complete the clustering preferably under a second. The dynamic changes in the value points are assumed to be smooth, i.e. relatively small. Thus being able to use existing solutions (i.e. being able to continue clustering on the existing one when a value is changed or new one is added) is highly preferred. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So all in all:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Can you think of an algorithm which will provide a sweet spot between computational efficiency and the accuracy of clusters wrt. the problem defined above?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Are there some nice heuristics for the k-means algorithm to automatically compute the value of K beforehand?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have generated a dataset of pairwise distances as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id_1 id_2 dist_12&#xA;id_2 id_3 dist_23&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven't been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In my university, we have an HPC computing cluster. I use the cluster to train classifiers and so on. So, usually, to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash script that contains (among others) a command like &lt;code&gt;qsub script.py&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository, so I get the same python script there. Then I write that Bash script or edit it, so I can run the bash script.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you see this is really frustrating since, for every little update for the python script, I need to do many steps to have it executed at the computing cluster. Of course the task gets even more complicated when I have to put the data on the server and use the datasets' path on the server.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Extractive Multi-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Extractive Single-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Generative Single-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Generative Multi-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;This question is in response to a comment I saw on another question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And Wikipedia has this: &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine#Issues&quot;&gt;http://en.wikipedia.org/wiki/Support_vector_machine#Issues&lt;/a&gt; . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have installed cloudera CDH5 Quick start VM on VM player. When I login through HUE in the first page I am the following error&lt;/p&gt;&#xA;&#xA;&lt;p&gt;“Potential misconfiguration detected. Fix and restart Hue.”&lt;img src=&quot;https://i.stack.imgur.com/vnq5P.png&quot; alt=&quot;Potential misconfiguration detected. Fix and restart Hue&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to solve this issue.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I know that there is no a clear answer for this question, but let's suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The &quot;best&quot; way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like correlation measures (&lt;a href=&quot;http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf&quot;&gt;http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf&lt;/a&gt;) etc?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using an experimental design to test the robustness of different classification methods, and now I'm searching for the correct definition of such design.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. Then, I run each classification method on every subset. Finally, I estimate the accuracy of each method as how many classifications on subsets are in agreement with the classification on the full dataset. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Classification-full     1    2    3    2    1    1    2&#xA;&#xA;Classification-subset1  1    2         2    3    1   &#xA;Classification-subset2       2    3         1    1    2&#xA;...&#xA;&#xA;Accuracy                1    1    1    1  0.5    1    1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a correct name to this methodology? I thought it can fall under &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_(statistics)&quot; rel=&quot;nofollow&quot;&gt;bootstrapping&lt;/a&gt; but I'm not sure about this.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to understand a neuroscience article:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Friston, Karl J., et al. &quot;Action and behavior: a free-energy formulation.&quot; &lt;em&gt;Biological cybernetics&lt;/em&gt; 102.3 (2010): 227-260. (&lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs00422-010-0364-z&quot; rel=&quot;nofollow noreferrer&quot;&gt;DOI 10.1007/s00422-010-0364-z&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Energy minus entropy&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Divergence plus surprise&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Complexity minus accuracy&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Mathematically, these correspond to:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/0eQgg.png&quot; alt=&quot;\\\\begin {aligned} F &amp;amp;= - \\\\langle \\\\ln p(\\\\tilde s, \\\\Psi \\\\vert m) \\\\rangle _q + \\\\langle \\\\ln q(\\\\Psi \\\\vert \\\\mu) \\\\rangle _q \\\\ &amp;amp;= D(q(\\\\Psi \\\\vert \\\\mu) \\\\parallel p(\\\\Psi \\\\vert \\\\tilde s, m)) - \\\\ln p(\\\\tilde s \\\\vert m) &amp;amp; \\\\qquad \\\\qquad &amp;amp; (5) \\\\ &amp;amp;= D(q(\\\\Psi \\\\vert \\\\mu) \\\\parallel p(\\\\Psi \\\\vert m)) - \\\\langle \\\\ln p(\\\\tilde s \\\\vert \\\\Psi, m) \\\\rangle _q \\\\end{aligned}&quot;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The things I am struggling with at this point are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the meaning of the || in the 2nd and 3rd versions of the equations;&lt;/li&gt;&#xA;&lt;li&gt;and the negative logs.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;As far as I know the development of algorithms to solve the Frequent Pattern Mining (FPM) problem, the road of improvements have some main checkpoints. Firstly, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Apriori_algorithm&quot;&gt;Apriori&lt;/a&gt; algorithm was proposed in 1993, by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=170072&quot;&gt;Agrawal et al.&lt;/a&gt;, along with the formalization of the problem. The algorithm was able to &lt;em&gt;strip-off&lt;/em&gt; some sets from the &lt;code&gt;2^n - 1&lt;/code&gt; sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Later, on year 1997, &lt;a href=&quot;http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html&quot;&gt;Zaki et al.&lt;/a&gt; proposed the algorithm &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm&quot;&gt;Eclat&lt;/a&gt;, which &lt;em&gt;inserted&lt;/em&gt; the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In 2000, &lt;a href=&quot;http://dl.acm.org/citation.cfm?doid=335191.335372&quot;&gt;Han et al.&lt;/a&gt; proposed an algorithm named &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm&quot;&gt;FPGrowth&lt;/a&gt;, along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to &lt;em&gt;strip-off&lt;/em&gt; non-frequent itemsets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strike&gt;As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered &quot;state-of-the-art&quot;? What is the &lt;em&gt;intuition&lt;/em&gt;/&lt;em&gt;main-contribution&lt;/em&gt; of such algorithms?&lt;/strike&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the FPGrowth algorithm still considered &quot;state of the art&quot; in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;When I started with artificial neural networks (NN) I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even beat my score on random forest!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For implementing NN I use Theano Stacked Auto Encoder with &lt;a href=&quot;https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py&quot;&gt;the code from tutorial&lt;/a&gt; that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at &lt;a href=&quot;http://deeplearning.net/tutorial/deeplearning.pdf&quot;&gt;tutorial&lt;/a&gt;, chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100-&gt;100-&gt;100, 60-&gt;60-&gt;60, 60-&gt;30-&gt;15, etc.), different learning and pre-train rates, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can it be that my deep NN with pre-training behaves so badly? What should I try? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a dataset which contains ~100,000 samples of 50 classes. I have been using SVM with an RBF kernel to train and predict new data. The problem though is the dataset is skewed towards different classes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, Class 1 - 30 (~3% each), Class 31 - 45 (~0.6% each), Class 46 - 50 (~0.2% each)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that the model tends to very rarely predict the classes which occur less frequent in the training set, even though the test set has the same class distribution as the training set. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am aware that there are technique such as 'undersampling' where the majority class is scaled down to the minor class. However, is this applicable here where there are so many different classes? Are there other methods to help handle this case?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to use ARMA/ARIMA with the &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/tsa.html#descriptive-statistics-and-tests&quot; rel=&quot;nofollow noreferrer&quot;&gt;statsmodel Python package&lt;/a&gt;, in order to predict the gas consumption. I tried with &lt;a href=&quot;https://github.com/denadai2/Gas-consumption-outliers/blob/master/exportWeb.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;a dataset&lt;/a&gt; of this format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ZUvBlUP.png&quot; alt=&quot;with this format&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using only the gas column.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pandas.tseries.offsets import *&#xA;&#xA;arma_mod20 = sm.tsa.ARMA(januaryFeb[['gas [m3]']], (5,3)).fit()&#xA;predict_sunspots = arma_mod20.predict('2012-01-13', '2012-01-14', dynamic=True)&#xA;ax = januaryFeb.ix['2012-01-13 00:00:00':'2012-01-15 22:00:00']['gas [m3]'].plot(figsize=(12,8))&#xA;ax = predict_sunspots.plot(ax=ax, style='r--', label='Dynamic Prediction');&#xA;ax.legend();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/oCPonu7.png&quot; alt=&quot;result&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is the prediction so bad?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I asked a data science question regarding how to decide on the best variation of a split test on the Statistics section of StackExchange. I hope I will have better luck here. The question is basically, &quot;Why is mean revenue per user the best metric to make your decision on in a split test?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original question is here: &lt;a href=&quot;https://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean&quot;&gt;https://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since it was not well received/understood I simplified the problem to a discrete set of purchases and phrased it as a classical probability problem. That question is here: &lt;a href=&quot;https://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn&quot;&gt;https://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The mean may be the best metric for such a decision but I am not convinced. We often have a lot of prior information so a Bayesian method would likely improve our estimates. I realize that this is a difficult question but Data Scientists are doing such split tests everyday. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am learning about matrix factorization for recommender systems and I am seeing the term &lt;code&gt;latent features&lt;/code&gt; occurring too frequently but I am unable to understand what it means. I know what a feature is but I don't understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UCI letter recognition:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 5.3% &lt;/li&gt;&#xA;&lt;li&gt;Boost -  13% &lt;/li&gt;&#xA;&lt;li&gt;MLP -    7.9% &lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   6.5%  &lt;/li&gt;&#xA;&lt;li&gt;Bayes -  11.5%  &lt;/li&gt;&#xA;&lt;li&gt;SVM -    3.3%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Parameters used:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,&#xA;min_sample_count=1&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Boost -  boost_type=REAL, weak_count=200, weight_trim_rate=0.95,&#xA;max_depth=7&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;MLP -    method=BACKPROP, param=0.001, max_iter=300 (default values - too&#xA;slow to experiment) &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;kNN(k=3) -   k=3&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Bayes -  none&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;SVM -    RBF kernel, C=10, gamma=0.01&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Digits:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 5.1%&lt;/li&gt;&#xA;&lt;li&gt;Boost -  23.4%&lt;/li&gt;&#xA;&lt;li&gt;MLP -    4.3%&lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   7.3%&lt;/li&gt;&#xA;&lt;li&gt;Bayes -  17.7%&lt;/li&gt;&#xA;&lt;li&gt;SVM -    4.2%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;MNIST:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 1.4%&lt;/li&gt;&#xA;&lt;li&gt;Boost -  out of memory&lt;/li&gt;&#xA;&lt;li&gt;MLP -    1.0%&lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   1.2%&lt;/li&gt;&#xA;&lt;li&gt;Bayes -  34.33%&lt;/li&gt;&#xA;&lt;li&gt;SVM -    0.6%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a data science project using Python.&#xA;The project has several stages.&#xA;Each stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.&#xA;I store the code in git, so that part is covered.&#xA;I would like to hear about:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tools for data version control.&lt;/li&gt;&#xA;&lt;li&gt;Tools enabling to reproduce stages and experiments.&lt;/li&gt;&#xA;&lt;li&gt;Protocol and suggested directory structure for such a project.&lt;/li&gt;&#xA;&lt;li&gt;Automated build/run tools.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am given a time series data vector (ordered by months and years),which contains only &lt;code&gt;0&lt;/code&gt;s and &lt;code&gt;1&lt;/code&gt;s. &lt;code&gt;1&lt;/code&gt; s represent a person changes his job at a particular a month. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would love to gather more information on how to model something like this. Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What is the right approach and clustering algorithm for geolocation clustering?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using the following code to cluster geolocation coordinates:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;from scipy.cluster.vq import kmeans2, whiten&#xA;&#xA;coordinates= np.array([&#xA;           [lat, long],&#xA;           [lat, long],&#xA;            ...&#xA;           [lat, long]&#xA;           ])&#xA;x, y = kmeans2(whiten(coordinates), 3, iter = 20)  &#xA;plt.scatter(coordinates[:,0], coordinates[:,1], c=y);&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it right to use K-means for geolocation clustering, as it uses Euclidean distance, and not &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot; rel=&quot;noreferrer&quot;&gt;Haversine formula&lt;/a&gt; as a distance function?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;t-SNE, as in [1], works by progressively reducing the Kullback-Leibler (KL) divergence, until a certain condition is met.&#xA;The creators of t-SNE suggests to use KL divergence as a performance criterion for the visualizations:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence [2]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I tried two implementations of t-SNE:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;python&lt;/strong&gt;: sklearn.manifold.TSNE().&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;: tsne, from library(tsne).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Both these implementations, when verbosity is set, print the error (Kullback-Leibler divergence) for each iteration. However, they don't allow the user to get this information, which looks a bit strange to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;from sklearn.manifold import TSNE&#xA;X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])&#xA;model = TSNE(n_components=2, verbose=2, n_iter=200)&#xA;t = model.fit_transform(X)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;produces:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[t-SNE] Computing pairwise distances...&#xA;[t-SNE] Computed conditional probabilities for sample 4 / 4&#xA;[t-SNE] Mean sigma: 1125899906842624.000000&#xA;[t-SNE] Iteration 10: error = 6.7213750, gradient norm = 0.0012028&#xA;[t-SNE] Iteration 20: error = 6.7192064, gradient norm = 0.0012062&#xA;[t-SNE] Iteration 30: error = 6.7178683, gradient norm = 0.0012114&#xA;...&#xA;[t-SNE] Error after 200 iterations: 0.270186&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, as far as I understand, &lt;strong&gt;0.270186&lt;/strong&gt; should be the KL divergence. However i cannot get this information, neither from &lt;strong&gt;model&lt;/strong&gt; nor from &lt;strong&gt;t&lt;/strong&gt; (which is a simple numpy.ndarray).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To solve this problem I could: i) Calculate KL divergence by my self, ii) Do something nasty in python for capturing and parsing TSNE() function's output [3]. However: i) would be quite stupid to re-calculate KL divergence, when TSNE() has already computed it, ii) would be a bit unusual in terms of code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any other suggestion? Is there a standard way to get this information using this library?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I mentioned I tried &lt;em&gt;R&lt;/em&gt;'s tsne library, but I'd prefer the answers to focus on the &lt;em&gt;python&lt;/em&gt; sklearn implementation.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;References&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] &lt;a href=&quot;http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[2] &lt;a href=&quot;http://homepage.tudelft.nl/19j49/t-SNE.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://homepage.tudelft.nl/19j49/t-SNE.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[3] &lt;a href=&quot;https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call&quot;&gt;https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;New to the Data Science forum, and first poster here!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may be kind of a specific question (hopefully not too much so), but one I'd imagine others might be interested in.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for a way to basically query GitHub with something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Give me a collection of all of the public repositories that have more than 10 stars, at&#xA;least two forks, and more than three committers.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result could take any viable form: a JSON data dump, a URL to the web page, etc. It more than likely will consist of information from 10,000 repos or something large.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this sort of thing possible using the API or some other pre-built way, or am I going to have to build out my own custom solution where I try to scrape every page? If so, how feasible is this and how might I approach it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I read in this post &lt;a href=&quot;https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data&quot;&gt;Is the R language suitable for Big Data&lt;/a&gt; that big data constitutes &lt;code&gt;5TB&lt;/code&gt;, and while it does a good job of providing information about the feasibility of working with this type of data in &lt;code&gt;R&lt;/code&gt; it provides very little information about &lt;code&gt;Python&lt;/code&gt;. I was wondering if &lt;code&gt;Python&lt;/code&gt; can work with this much data as well. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I'm almost sure this pattern was created by a disk testing program, but I'd like to reverse-engineer it anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already know that the pattern is aligned, with a periodicity of 256 characters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel's color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a snapshot of the pattern (you can see more than one), seen through &lt;code&gt;xxd&lt;/code&gt; (32x16):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/zOFSK.gif&quot; alt=&quot;Pattern to analyze&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either way, I am trying to find a way of visualizing this information. This probably isn't hard for anyone into signal analysis, but I can't seem to find a way using open-source software.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to avoid Matlab or Mathematica and I'd prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Update, 2014-07-25: given Emre's answer below, this is what the pattern looks like, given the first 30MB of the pattern, aligned at 512 instead of 256 (this alignment looks better):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4tDIA.png&quot; alt=&quot;Graphical pattern&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any further ideas are welcome!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a timeseries with hourly gas consumption. I want to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model&quot; rel=&quot;nofollow noreferrer&quot;&gt;ARMA&lt;/a&gt;/&lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot; rel=&quot;nofollow noreferrer&quot;&gt;ARIMA&lt;/a&gt; to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with &lt;a href=&quot;https://www.otexts.org/fpp/6/5&quot; rel=&quot;nofollow noreferrer&quot;&gt;Seasonal and Trend decomposition using Loess&lt;/a&gt; (STL)?)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/hYyH8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to find stock data to practice with, is there a good resource for this? I found this: &lt;a href=&quot;ftp://emi.nasdaq.com/ITCH/&quot;&gt;ftp://emi.nasdaq.com/ITCH/&lt;/a&gt; but it only has the current year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already have a way of parsing the protocol, but would like to have some more data to compare with. It doesn't have to be in the same format, as long as it has price, trades, and date statistics.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I cleared up some &quot;shared library&quot; problems by adding this to yarn-env.sh and hadoop-env.sh:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;export HADOOP_HOME=&quot;/usr/local/hadoop&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I also added this to hadoop-env.sh:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;based on the advice of this post at horton works &lt;a href=&quot;http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/&quot; rel=&quot;nofollow&quot;&gt;http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;starting yarn daemons&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;starting resourcemanager, &#xA;  logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;localhost: starting nodemanager, &#xA;  logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The only problem is, JPS says that the Resource Manager isn't running. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's going on here?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;senior, primary, lead, head, vp, director, stuff, principal, chief, &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;or the words &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;analyst, expert, modeler, researcher, scientist, developer, engineer, architect.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How can I get all such possible words with their distance ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How can &lt;a href=&quot;http://en.wikipedia.org/wiki/NoSQL&quot;&gt;NoSQL&lt;/a&gt; databases like &lt;a href=&quot;http://en.wikipedia.org/wiki/MongoDB&quot;&gt;MongoDB&lt;/a&gt; be used for data analysis? What are the features in them that can make data analysis faster and powerful?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Basically, both are software systems that are based on data and algorithms.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I've found that some claim its advantage is mostly marginal in that it is best for catching 'lucky' models with high standard accuracy measurements and low AUC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So should I avoid relying on AUC for validating models or would a combination be best? Thanks for all your help.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;i want to become a &lt;strong&gt;data scientist&lt;/strong&gt;. I studied applied &lt;strong&gt;statistics&lt;/strong&gt; (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in &lt;strong&gt;Computer Science&lt;/strong&gt; focus in Intelligent Systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my study plan:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Machine learning&lt;/li&gt;&#xA;&lt;li&gt;Advanced machine learning&lt;/li&gt;&#xA;&lt;li&gt;Data mining&lt;/li&gt;&#xA;&lt;li&gt;Fuzzy logic&lt;/li&gt;&#xA;&lt;li&gt;Recommendation Systems&lt;/li&gt;&#xA;&lt;li&gt;Distributed Data Systems&lt;/li&gt;&#xA;&lt;li&gt;Cloud Computing&lt;/li&gt;&#xA;&lt;li&gt;Knowledge discovery&lt;/li&gt;&#xA;&lt;li&gt;Business Intelligence&lt;/li&gt;&#xA;&lt;li&gt;Information retrieval&lt;/li&gt;&#xA;&lt;li&gt;Text mining&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for the answers.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should choose a similar data setup for training my models as well. But I came across a research paper or two (in my area of work) which have used a &quot;class balancing&quot; data approach to training the models, implying an equal number of instances of benign and malicious traffic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, if I am building machine learning models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone shed more light on the &lt;em&gt;pros&lt;/em&gt; and &lt;em&gt;cons&lt;/em&gt; of both the choices and how to decide which one to go choose?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking about writing a program to generate a svg. Any other suggestions?  &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm currently facing a project that I could solve with a relational database in a relatively painful way. Having heard so much about NOSQL, I'm wondering if there is not a more appropriate way of tackling it:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose we are tracking a group of animals in a forest (n ~ 500) and would like to keep a record of a set of observations (this is a fictional scenario).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We would like to store the following information in a database:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a unique identifier for each animal&lt;/li&gt;&#xA;&lt;li&gt;a description of the animal with structured fields: Species, Genus, Family, ...&lt;/li&gt;&#xA;&lt;li&gt;a free text field with additional information&lt;/li&gt;&#xA;&lt;li&gt;each time-point at which it was detected close to a reference point&lt;/li&gt;&#xA;&lt;li&gt;a picture of the animal&lt;/li&gt;&#xA;&lt;li&gt;an indication whether two given animals are siblings&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;And:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;there might be additional features appearing later as more data comes in&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;We would like to be able to execute the following types of queries:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;return all the animals spotted between in a given time interval&lt;/li&gt;&#xA;&lt;li&gt;return all the animals of a given Species or Family&lt;/li&gt;&#xA;&lt;li&gt;perform a text search on the free text field&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which particular database system would you recommend ? Is there any tutorial / examples that I could use as a starting point ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Apologies if this is very broad question, what I would like to know is how effective is A/B testing (or other methods) of effectively measuring the effects of a design decision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance we can analyse user interactions or click results, purchase/ browse decisions and then modify/tailor the results presented to the user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We could then test the effectiveness of this design change by subjecting 10% of users to the alternative model randomly but then how objective is this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we avoid influencing the user by the model change, for instance we could decided that search queries for 'David Beckham' are probably about football so search results become biased towards this but we could equally say that his lifestyle is just as relevant but this never makes it into the top 10 results that are returned.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious how this is dealt with and how to measure this effectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts are that you could be in danger of pushing a model that you think is correct and the user obliges and this becomes a self-fulfilling prophecy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've read an article on this: &lt;a href=&quot;http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/&quot;&gt;http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/&lt;/a&gt; and also the book: &lt;a href=&quot;http://shop.oreilly.com/product/0636920028529.do&quot;&gt;http://shop.oreilly.com/product/0636920028529.do&lt;/a&gt; which discussed this so it piqued my interest.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;There was a recent furore with &lt;a href=&quot;http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840&quot;&gt;facebook experimenting on their users to see if they could alter user's emotions&lt;/a&gt; and now &lt;a href=&quot;http://www.bbc.co.uk/news/technology-28542642&quot;&gt;okcupid&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whilst I am not a professional data scientist I read about &lt;a href=&quot;http://columbiadatascience.com/2013/11/25/data-science-ethics/&quot;&gt;data science ethics&lt;/a&gt; from &lt;a href=&quot;http://shop.oreilly.com/product/0636920028529.do&quot;&gt;Cathy O'Neill's book 'Doing Data Science'&lt;/a&gt; and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science &lt;em&gt;accidentally&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The article discusses a code of practice and mentions the &lt;a href=&quot;http://www.datascienceassn.org/code-of-conduct.html&quot;&gt;Data Science Association's Code of conduct&lt;/a&gt;, is this something that is in use? Rule 7 is of particular interest (quoted from their website):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;(a) A person who consults with a data scientist about the possibility&#xA;  of forming a client-data scientist relationship with respect to a&#xA;  matter is a prospective client.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(b) Even when no client-data scientist relationship ensues, a data&#xA;  scientist who has learned information from a prospective client shall&#xA;  not use or reveal that information.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(c) A data scientist subject to paragraph (b) shall not provide&#xA;  professional data science services for a client with interests&#xA;  materially adverse to those of a prospective client in the same or a&#xA;  substantially related industry if the data scientist received&#xA;  information from the prospective client that could be significantly&#xA;  harmful to that person in the matter&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a &lt;a href=&quot;http://www.cnet.com/news/senator-asks-ftc-to-investigate-facebooks-mood-study/&quot;&gt;senator&lt;/a&gt; to react to such things before something happens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Most vehicle license/number plate extractors I've found involve reading a plate from an image (OCR) but I'm interested in something that could tag instances of license plates in a body of text. Are there any such annotators out there?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;While running the below pig script I am getting error in line4: &#xA;If it is GROUP then I am getting error.&#xA;If I change from 'GROUP' TO 'group' in line4, then the script is running.&#xA;What is the difference between group and GROUP.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDSGROUPED = GROUP WORDS BY ZIPS;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DUMP WORDSSORT;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Have you heard of the &quot;Data Science Association&quot;? &#xA;&lt;br&gt;URL: &lt;a href=&quot;http://www.datascienceassn.org/&quot; rel=&quot;nofollow&quot;&gt;http://www.datascienceassn.org/&lt;/a&gt;&#xA;&lt;br&gt;Do you expect it to become a professional body like the Actuaries Institute?&#xA;&lt;br&gt;If yes, then why?&#xA;&lt;br&gt;If no, then why not and do you see anyone else becoming the professional body?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, is this question &quot;on-topic&quot; ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I'm currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, how should I adjust the data so that this is able to be plotted?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The times for an action can be between 2 seconds and a minute. And, at any given time I would estimate there could be about 100 actions taking place. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm open to any advice on how to think about this issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to build a recommendation engine using collaborative filtering. I have the usual [user, movie, rating] information. I would like to incorporate an additional feature like 'language' or 'duration of movie'. I am not sure what techniques I could use for such a problem. Please suggest references or packages in python/R. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;While running the below pig script I am getting error in line4: If it is &lt;code&gt;GROUP&lt;/code&gt; then I am getting error. If I change from &lt;code&gt;GROUP&lt;/code&gt; TO &lt;code&gt;group&lt;/code&gt; in line4, then the script is running. What is the difference between group and GROUP.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&#xA;&#xA;WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&#xA;&#xA;WORDSGROUPED = GROUP WORDS BY ZIPS;&#xA;&#xA;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&#xA;&#xA;WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&#xA;&#xA;DUMP WORDSSORT;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The calculation completes in IPython but inspection shows the results all come back as NaN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest the best approach to tackle the problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Here's my code thus far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;path = &quot;../../reuters.db&quot;&#xA;%pylab inline&#xA;import pandas as pd&#xA;import numpy as np&#xA;import pandas.io.sql as psql&#xA;import sqlite3 as lite&#xA;con = lite.connect(path)&#xA;with con:&#xA;    sql = &quot;SELECT * FROM Frequency&quot;&#xA;    df = psql.frame_query(sql, con)&#xA;    print df.shape&#xA;df = df.rename(columns={&quot;term&quot;:&quot;term_id&quot;, &quot;count&quot;:&quot;count_id&quot;})&#xA;pivoted = df.pivot('docid', 'term_id', 'count_id')&#xA;pivoted.to_sparse()&#xA;similarity_matrix = pivoted.dot(pivoted.T)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A mix of Data Science and Machine learning would be great.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;so I'm using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PySpark worker failed with exception:&#xA;Traceback (most recent call last):&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py&quot;, line 77, in main&#xA;    serializer.dump_stream(func(split_index, iterator), outfile)&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 191, in dump_stream&#xA;    self.serializer.dump_stream(self._batched(iterator), stream)&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 123, in dump_stream&#xA;    for obj in iterator:&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 180, in _batched&#xA;    for item in iterator:&#xA;TypeError: __init__() takes exactly 3 arguments (2 given)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the code for serializers is available &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and my code is &lt;a href=&quot;https://github.com/seashark97/Scalable-Sentiment-Analysis/blob/master/spark_test.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;h2&gt;Image Similarity based on Color Palette Distribution&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I am trying to compute similarity between two images based on their color palette distribution, let's say I have two sets of key value pairs as follows,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Img1: &lt;code&gt;{'Brown': 14, 'White': 13, 'Black': 40, 'Gray': 31}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Img2: &lt;code&gt;{'Pink': 82, 'Brown': 8, 'White': 7}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where the numbers denote the % of that color present in the image. What would be the best way to compute similarity on a scale of 0-100 between the two images?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to make the same NER for different language. The problem here is that I can't use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to use methods for unsupervised feature learning with CRF algorithm? Did anyone try this and got any good result? Is there any article or tutorial about this matter?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I still don't completely understand this way of feature creation so I don't want to spend to much time for something that won't work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the different classes of data science problems that can be solved using mapreduce programming model?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I need to build parse tree for some source code (on Python or any program language that describe by CFG).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have source code on some programming language and BNF this language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody give some advice how can I build parse tree in this case?&#xA;Preferably, with tools for Python.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goals are to:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;&lt;/li&gt;&#xA;&lt;li&gt;Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;&lt;/li&gt;&#xA;&lt;li&gt;Do the same as 1 and 2, but with procedures and/or diagnoses.&lt;/li&gt;&#xA;&lt;li&gt;Preferably, the results would be interpretable by a doctor&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I have looked at things like the &lt;a href=&quot;https://www.heritagehealthprize.com/c/hhp/details/milestone-winners&quot; rel=&quot;nofollow noreferrer&quot;&gt;Heritage Health Prize Milestone papers&lt;/a&gt; and have learned a lot from them, but they are focused on predicting hospitalizations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT #2 to add plaintext table:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;CKD is the target condition, &quot;chronic kidney disease&quot;, &quot;.any&quot; denotes that they have acquired that condition at any time, &quot;.isbefore.ckd&quot; means they had that condition before their first diagnosis of CKD.  The other abbreviations correspond with other conditions identified by ICD-9CM code groupings.  This grouping occurs in SQL during the import process. Each variable, with the exception of patient_age, is binary.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Does anyone know what (from your experience) is the best open source natural language generators (NLG) out there? What are the relative merits of each?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;So, I'm just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I've seen how an artificial neural network can parse image data and categorize the images (&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html&quot;&gt;demo with convnetjs&lt;/a&gt;), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm trying to wrap my head around if this is possible to be done with string inputs? The use-case I've got is a &quot;recommendation engine&quot; for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine &quot;downsampling&quot; the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I'd need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I've seen don't allow adding new neurons after the system has been trained?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what's a better tool for pattern-matching in string data)?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am attempting to use the tm package to convert a vector of text strings to a corpus element.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code looks something like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Corpus(d1$Yes)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where d1$Yes is a factor with 124 levels, each containing a text string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, d1$Yes[246] = &quot;So we can get the boat out!&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm receiving the following error:&#xA;&quot;Error: inherits(x, &quot;Source&quot;) is not TRUE&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure how to remedy this.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Well this looks like the most suited place for this question.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every website collect data of the user, some just for usability and personalization, but the majority like social networks track every move on the web, some free apps on your phone scan text messages, call history and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All this data siphoning is just for selling your profile for advertisers?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am working on a data set that has multiple traffic speed measurements per day. My data is from the city of chicago, and it is taken every minute for about six months. I wanted to consolidate this data into days only, so this is what I did:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;traffic &amp;lt;- read.csv(&quot;path.csv&quot;,header=TRUE)&#xA;traffic2 &amp;lt;- aggregate(SPEED~DATE, data=traffic, FUN=MEAN)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;this was perfect because it took all of my data and averaged it by date. For example, my original data looked something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DATE        SPEED  &#xA;12/31/2012   22&#xA;12/31/2012   25&#xA;12/31/2012   23&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the final looked like this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DATE        SPEED &#xA;10/1/2012    22&#xA;10/2/2012    23&#xA;10/3/2012    22&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The only problem, is my data is supposed to start at 9/1/2012. I plotted this data, and it turns out the data goes from 10/1/2012-12/31/2012 and then 9/1/2012-9/30/2012.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What in the world is going on here?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am currently on a project that will build a model (train and test) on Client-side Web data, but evaluate this model on Sever-side Web data.  Unfortunately building the model on Server-side data is not an option, nor is it an option to evaluate this model on Client-side data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This model will be based on metrics collected on specific visitors.  This is a real time system that will be calculating a likelihood based on metrics collected while visitors browse the website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for approaches to ensure the highest possible accuracy on the model evaluation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have the following ideas,&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Clean the Server-side data by removing webpages that are never seen Client-side.&lt;/li&gt;&#xA;&lt;li&gt;Collect additional data Server-side data to make the Server-side data more closely resemble Client-side data.&lt;/li&gt;&#xA;&lt;li&gt;Collect data on the Client and send this data to the Server.  This is possible and may be the best solution, but is currently undesirable. &lt;/li&gt;&#xA;&lt;li&gt;Build one or more models that estimate Client-side Visitor metrics from Server-side Visitor metrics and use these estimates in the Likelihood model.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any other thoughts on evaluating over one Population while training (and testing) on another Population?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am building a regression model and I need to calculate the below to check for correlations&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Correlation between 2 Multi level categorical variables&lt;/li&gt;&#xA;&lt;li&gt;Correlation between a Multi level categorical variable and&#xA;continuous variable &lt;/li&gt;&#xA;&lt;li&gt;VIF(variance inflation factor) for a Multi&#xA;level categorical variables&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please answer the below questions&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Which correlation coefficient works best for the above cases ? &lt;/li&gt;&#xA;&lt;li&gt;VIF calculation only works for continuous data so what is the&#xA;alternative? &lt;/li&gt;&#xA;&lt;li&gt;What are the assumptions I need to check before I use the correlation coefficient you suggest? &lt;/li&gt;&#xA;&lt;li&gt;How to implement them in SAS &amp;amp; R?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I assume that each person on Facebook is represented as a node (of a Graph) in Facebook, and relationship/friendship between each person(node) is represented as an edge between the involved nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that there are millions of people on Facebook, how is the Graph stored?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am trying to implement the Brown Clustering Algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Paper details: &quot;Class-Based n-gram Models of Natural Language&quot; by Brown et al&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm is supposed to in &lt;code&gt;O(|V|k^2)&lt;/code&gt; where &lt;code&gt;|V|&lt;/code&gt; is the size of the vocabulary and k is the number of clusters. I am unable to implement it this efficiently. In fact, the best I can manage is &lt;code&gt;O(|V|k^3)&lt;/code&gt; which is too slow. My current implementation for the main part of the algorithm is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for w = number of clusters + 1 to |V|&#xA;{&#xA;   word = next most frequent word in the corpus&#xA;&#xA;   assign word to a new cluster &#xA;&#xA;   initialize MaxQuality to 0&#xA;&#xA;   initialize ArgMax vector to (0,0)&#xA;&#xA;   for i = 0 to number of clusters - 1 &#xA;   {&#xA;      for j = i to number of clusters&#xA;      {&#xA;         Quality = Mutual Information if we merge cluster i and cluster j&#xA;&#xA;         if Quality &amp;gt; MaxQuality&#xA;         {&#xA;            MaxQuality = Quality &#xA;&#xA;            ArgMax = (i,j) &#xA;         }&#xA;      }&#xA;   }&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I compute quality as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. Before entering the second loop compute the pre-merge quality i.e. quality before doing any merges.&#xA;2. Every time a cluster-pair merge step is considered:&#xA;    i. assign quality := pre-merge quality&#xA;   ii. quality = quality - any terms in the mutual information equation that contain cluster i or cluster j (pre-merge)&#xA;  iii. quality = quality + any terms in the mutual information equation that contain (cluster i U cluster j)  (post-merge)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my implementation, the first loop has approx |V| iterations, the second and third loop approx k iterations each. To compute quality at each step requires approx a further k iterations. In total it runs in &lt;code&gt;(|V|k^3)&lt;/code&gt; time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you get it to run in &lt;code&gt;(|V|k^2)&lt;/code&gt;?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:  In addition to generalizing about a population, given a sample, I'm interested in generalizing about an individual's overall search behavior, given a time slice.  Theory and paper references are a plus!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution. &#xA;In fact the distribution resembles more closely the Landau Distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But my question is: &#xA;&lt;strong&gt;At what point can one say that the t-test is not a good method of significance testing?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don't require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: &lt;img src=&quot;https://i.stack.imgur.com/Edh2e.png&quot; alt=&quot;sample dashboard&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but applied to web analytics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions on what packages to use creating these dashboard reports? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first intuition is to use R markdown and knitr, but perhaps you've found a better solution. I can't seem to find rich examples of dashboards generated from R. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   y    x1  x2 x3 x4 x5 x6 ... x40&#xA;87169   14  0  1  0  0  2  ... 0 &#xA;46449   0   0  4  0  1  4  ... 12&#xA;846449  0   0  0  0  0  3  ... 0&#xA;....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am currently using a Genetic Algorithm to solve this and the results are coming out &#xA;with roughly a factor of two difference between observed and expected. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Data set looks like:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;25000 observations&lt;/li&gt;&#xA;&lt;li&gt;up to 15 predictors of different types: numeric, multi-class categorical, binary&lt;/li&gt;&#xA;&lt;li&gt;target variable is binary&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which cross validation method is typical for this type of problems?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default I'm using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;There is a text summarization project called SUMMARIST. Apparently it is able to perform abstractive text summarization. I want to give it a try but unfortunately the demo links on the website do not work. Does anybody have any information regarding this? How can I test this tool?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.isi.edu/natural-language/projects/SUMMARIST.html&quot; rel=&quot;nofollow&quot;&gt;http://www.isi.edu/natural-language/projects/SUMMARIST.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regards,&#xA;PasMod&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am fitting a model in R.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;use &lt;code&gt;createFolds&lt;/code&gt; method to create several &lt;code&gt;k&lt;/code&gt; folds from the data set&lt;/li&gt;&#xA;&lt;li&gt;loop through the folds, repeating the following on each iteration:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;train&lt;/code&gt; the model on k-1 folds&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;predict&lt;/code&gt; the outcomes for the i-th fold&lt;/li&gt;&#xA;&lt;li&gt;calculate prediction accuracy&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;average the accuracy&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Does R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have  set of documents and I want classify them to true and false &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is I have to take the whole words in the documents then I classify them depend on the similarity words in these documents or I can take only some words that I interested in then I compare it with the documents. Which one is more efficient in classify document and can work with SVM.       &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm working on the dataset with lots of NA values with sklearn and pandas.DataFrame. I implemented different imputation strategies for different columns of the dataFrame based column names. For example NAs predictor 'var1' I impute with 0's and for 'var2' with mean.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try to cross validate my model using train_test_split it returns me a nparray which does not have column names. How can I impute missing values in this nparray?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. I do not impute missing values in the original data set before splitting on purpose so I keep test and validation sets separately.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have train and test sets of chronological data consisting of 305000 instances and 70000,appropriately. There are 15 features in each instance and only 2 possible class values ( NEW,OLD). The problem is that there are only 725 OLD instances in the train set and 95 in the test. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only algorithm which succeeds for me to handle imbalance is NaiveBayes in Weka (0.02 precision for OLD class), others (trees) classify each instance as NEW.&#xA;What is the best approach to handle the imbalance and the appropriate algorithm in such a case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am attempting to compile code using Knitr in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code below is returning the following error, and causes errors in the rest of the document.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;miss&amp;lt;-sample$sensor_glucose[!is.na(sample$sensor_glucose)]&#xA;# Error: &quot;## Warning: is.na() applied to non-(list or vector) of type 'NULL'&quot;&#xA;&#xA;str(miss)&#xA;# int [1:103] 213 113 46 268 186 196 187 153 43 175 ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Does anyone know how to remedy this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I was wondering does scikit-learn have forward selection/stepwise regression algorithm?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;We are storing the information about our users showing interest in our items. Based on this information, we would like to create a simple recommendation engine that will take the items I1, I2, I3 etc of the current user, search for all other users that had shown interest in those items, and then output the items I4, I5, I6 etc of the other users, sorted by their decreasing popularity. So, basically, the standard &quot;other buyer were also interested in...&quot; functionality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm asking myself what kind of a database is suitable for a realtime recommendation engine like this. My current idea is to build a trie of item IDs, then sort the item IDs of the current user (as the order of items is irrelevant) and to go down the trie; the children of the last trie node will build the needed output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that we have 2 million items so that according to our estimation the trie will have at least 1E12 nodes, so that we probably need a distributed sharded database to store it. Before we reinvent the wheel, are there any ready-to-use databases or generally, non-cloud solutions for recommendation engines out there?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;We have a classification algorithm to categorize Java exceptions in Production.&#xA;This algorithm is based on hierarchical human defined rules so when a bunch of text forming an exception comes up, it determines what kind of exception is (development, availability, configuration, etc.) and the responsible component (the most inner component responsible of the exception). In Java an exception can have several causing exceptions, and the whole must be analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, given the following example exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;com.myapp.CustomException: Error printing ...&#xA;... (stack)&#xA;Caused by: com.foo.webservice.RemoteException: Unable to communicate ...&#xA;... (stack)&#xA;Caused by: com.acme.PrintException: PrintServer002: Timeout ....&#xA;... (stack)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;First of all, our algorithm splits the whole stack in three isolated exceptions. Afterwards it starts analyzing these exceptions starting from the most inner one. In this case, it determines that this exception (the second caused by) is of type &lt;code&gt;Availability&lt;/code&gt; and that the responsible component is a &quot;print server&quot;. This is because there is a rule that matches containing the word &lt;code&gt;Timeout&lt;/code&gt; associated to the &lt;code&gt;Availability&lt;/code&gt; type. There is also a rule that matches &lt;code&gt;com.acme.PrintException&lt;/code&gt; and determines that the responsible component is a print server. As all the information needed is determined using only the most inner exception, the upper exceptions are ignored, but this is not always the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see this kind of approximation is very complex (and chaotic) as a human have to create new rules as new exceptions appear. Besides, the new rules have to be compatible with the current ones because a new rule for classifying a new exception must not change the classification of any of the already classified exceptions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are thinking about using Machine Learning to automate this process. Obviously, I am not asking for a solution here as I know the complexity but I'd really appreciate some advice to achieve our goal.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Cross posting this from Cross Validated:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've seen this question asked before, but I have yet to come across a definitive source answering the specific questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What's the most appropriate statistical test to apply to a small A/B test?&lt;/li&gt;&#xA;&lt;li&gt;What's the R code and interpretation to analyze a small A/B test?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm running a small test to figure out which ads perform better. I have the following results:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 1:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,26&#xA;row-3,767 7&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 2:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,16&#xA;row-3,767 13&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 3:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,2&#xA;row-3,767 7&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it's safe to say these numbers are small and likely to be not normally distributed. Also, it's click data so there's a binary outcome of clicked or not and the trials are independent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Appropriate test&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In analyzing each position for significance, I think comparison with a binomial or Poisson distribution makes the most sense. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;http://www.openintro.org/stat/textbook.php?stat_book=os&quot; rel=&quot;nofollow&quot;&gt;the OpenIntro Stats&lt;/a&gt; (and other sources) book, a variable follows a Poisson distribution &quot;... if the event being considered is rare, the population is large, and the events occur independently of each other.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same source classifies a binomial variable approximately the same way adding that the probability of success is the same and the number of trials is fixed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I appreciate this is not an either/or decision and analysis can be done using both distributions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given A/B (split) testing is a science that has been practiced for several years, I imagine that there is a canonical test. However, looking around the internet, I mostly come across analysis that uses the standard normal distribution. That just seems wrong :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a canonical test to use for A/B tests with small #'s of clicks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Interpretation and R code&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've used the following R code to test significance for each position:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Position 1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;binom.test(7, 767, p=(26/753))&#xA;&#xA;Exact binomial test&#xA;&#xA;data:  7 and 767&#xA;number of successes = 7, number of trials = 767, p-value = 1.077e-05&#xA;alternative hypothesis: true probability of success is not equal to 0.03452855&#xA;95 percent confidence interval:&#xA; 0.003676962 0.018713125&#xA;sample estimates:&#xA;probability of success &#xA;           0.009126467&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I interpret this result to mean: The probability of success in the test group is indeed different than the control group with a 95% confidence interval that the success probability is between .368% and 1.87%&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ppois(((26-1)/753), lambda=(7/767), lower.tail = F)&#xA;[1] 0.009084947&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I interpret this result to mean: Given a Poisson distribution with a click rate of 7 per 767 trials, there is a 0.9% chance of having a click rate of 26 or more per 753 trials in the same distribution. Contextualized in the ad example,&#xA;there is a .1% chance that the control ad actually performs the same as the test ad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the above interpretation correct? Does the test and interpretation change with the different positions (i.e. are the results of the Poisson test more appropriate for Position 3 given the small numbers)?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the tools I've found are (in no particular order):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scalding&lt;/li&gt;&#xA;&lt;li&gt;Breeze&lt;/li&gt;&#xA;&lt;li&gt;Spark&lt;/li&gt;&#xA;&lt;li&gt;Saddle&lt;/li&gt;&#xA;&lt;li&gt;H2O&lt;/li&gt;&#xA;&lt;li&gt;Spire&lt;/li&gt;&#xA;&lt;li&gt;Mahout&lt;/li&gt;&#xA;&lt;li&gt;Hadoop&lt;/li&gt;&#xA;&lt;li&gt;MongoDB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If I need to be more specific to make the question answerable: I'm not particularly interested in clusters and Big Data at this moment, but I'm interested in sizable data (up to 100 GB) for information integration and predictive analytics.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am facing this bizarre issue while using &lt;code&gt;Apache Pig&lt;/code&gt; &lt;strong&gt;rank&lt;/strong&gt; utility. I am executing the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;email_id_ranked = rank email_id;&#xA;store email_id_ranked into '/tmp/';&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, basically I am trying to get the following result&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1,email1&#xA;2,email2&#xA;3,email3&#xA;... &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Issue is sometime pig dumps the above result but sometimes it dumps only the emails without the rank. Also when I dump the data on screen using &lt;code&gt;dump&lt;/code&gt; function pig returns both the columns. I don't know where the issue is. Kindly advice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if you need any more information. Thanks in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Pig version: Apache Pig version 0.11.0-cdh4.6.0&lt;/strong&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I need to do coreference resolution for German texts and I plan to use OpenNLP to perform this task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know OpenNLP coreference resolution does not support the German language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which components/data do I need to adapt the code such that it is possible to perform coreference resolution for German texts?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;does anybody know a libarary for performing coreference resolution on German texts?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know OpenNLP and Standord NLP are not able to perform coreference resolution for German Texts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only tool that I know is &lt;a href=&quot;http://www.cl.uzh.ch/static/news.php?om=view&amp;amp;nid=163&quot; rel=&quot;nofollow&quot;&gt;CorZu&lt;/a&gt; which is a python library.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I know that ARIMA can't detect multiple seasonality, but it is possible to &lt;a href=&quot;http://robjhyndman.com/hyndsight/dailydata/&quot; rel=&quot;nofollow&quot;&gt;use fourier functions to add a second seasonality&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to forecast gas consumption composed by a daily, weekly (week days-weekend), yearly seasonality. Does it make sense to apply three times the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html&quot; rel=&quot;nofollow&quot;&gt;STL decomposition by LOESS&lt;/a&gt;? The reason is that I applied the fourier method and I have bad results but I don't know if it is only because I applied it wrong.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm interested in the theoretical explanation, but here you find also the code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ARIMA + 2 STL:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;b &amp;lt;- ts(drop(coredata(dat.ts)), deltat=1/12/30/24, start=1)&#xA;fit &amp;lt;- stl(b, s.window=&quot;periodic&quot;)&#xA;b &amp;lt;- seasadj(fit)&#xA;dat.ts &amp;lt;- xts(b, index(dat.ts))&#xA;&#xA;# The weekdays are extracted&#xA;dat.weekdays &amp;lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xA;dat.weekdaysTS &amp;lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xA;fit &amp;lt;- stl(dat.weekdaysTS, s.window=&quot;periodic&quot;)&#xA;dat.weekdaysTS &amp;lt;- seasadj(fit)&#xA;&#xA;arima &amp;lt;- Arima(dat.weekdaysTS, order=c(3,0,5))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With fourier:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dat.weekdays &amp;lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xA;dat.weekdaysTS &amp;lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xA;z &amp;lt;- fourier(ts(dat.weekdaysTS, frequency=365.25), K=5)&#xA;arima &amp;lt;- Arima(dat.weekdaysTS, order=c(3,0,5),xreg=z)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lBxL5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are 4 different weight matrices that I got after training a &lt;strong&gt;restricted Boltzman machine (RBM)&lt;/strong&gt; with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though &lt;em&gt;none&lt;/em&gt; of weights are &lt;em&gt;exactly&lt;/em&gt; the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: what is the &lt;strong&gt;most likely reason&lt;/strong&gt; for weights to get &lt;strong&gt;very similar values&lt;/strong&gt;? Do they all just get to some local minimum? Or is it a sign of overfitting? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I currently use a kind of Gaussian-Bernoulli RBM, code may be found &lt;a href=&quot;https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPD.&lt;/strong&gt; My dataset is based on &lt;a href=&quot;http://www.pitt.edu/~emotion/ck-spread.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;CK+&lt;/a&gt;, which contains &gt; 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/FQMSp.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;img src=&quot;https://i.stack.imgur.com/Hjcaw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When training RBM, I take only non-zero pixels, so outer black region is ignored. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I know the difference between clustering and classification in machine learning, but I don't understand the difference between text classification and topic modeling for documents. Can I use topic modeling over documents to identify a topic? Can I use classification methods to classify the text inside these documents?  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;For experimenting we'd like to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Emoji&quot;&gt;Emoji&lt;/a&gt; embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Note that SentiWordNet does allow for &lt;em&gt;ambiguous&lt;/em&gt; meanings, too. Consider e.g. &lt;a href=&quot;http://sentiwordnet.isti.cnr.it/search.php?q=funny&quot;&gt;funny&lt;/a&gt;, which is not just positive: &quot;this tastes funny&quot; is probably not positive... same will hold for &lt;code&gt;;-)&lt;/code&gt; for example. But I don't think this is harder for Emoji than it is for regular words...)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for a product that allows us to take in a collection of datastreams, and then after some event, will find any data that changes or correlates with that event. (For example, having a headache, and identifying that I drank too much beer last night and didn't drink enough water)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am using the &lt;code&gt;stepAIC&lt;/code&gt; function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;          Df     Sum of Sq    RSS       AIC&#xA;&amp;lt;none&amp;gt;                        350.71   -5406.0&#xA;- aaa      1     0.283        350.99   -5405.9&#xA;- bbb      1     0.339        351.05   -5405.4&#xA;- ccc      1     0.982        351.69   -5400.5&#xA;- ddd      1     0.989        351.70   -5400.5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Question Are the values listed under &lt;code&gt;Df&lt;/code&gt;, &lt;code&gt;Sum of Sq&lt;/code&gt;, &lt;code&gt;RSS&lt;/code&gt;, and &lt;code&gt;AIC&lt;/code&gt; the values for a model where only one variable would be considered as the independent variable (i.e. y ~&#xA;aaa, y ~ bbb, etc.)? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;While finding frequent subgraphs in single large graph, subgraph isomorphism (test) is not considered because its not anti-monotone. How and why subgraph isomorphism is not anti-monotone ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I like to find the weight vector for input-space features in a &lt;a href=&quot;http://en.wikipedia.org/wiki/Structured_support_vector_machine&quot; rel=&quot;nofollow&quot;&gt;structured SVM&lt;/a&gt;. The idea is to identify the most important set of input-space features (based on the magnitude of their corresponding weights). I know that in a binary SVM the weight vector can be written as a &lt;a href=&quot;http://pyml.sourceforge.net/doc/howto.pdf&quot; rel=&quot;nofollow&quot;&gt;linear combination of examples&lt;/a&gt;, and the magnitude of those weights represents how much they were effective for the prediction problem at hand. But how do you compute the same for an SSVM?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I'm exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I'm considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)&lt;/li&gt;&#xA;&lt;li&gt;Using locality-sensitive hashing (LSH)&lt;/li&gt;&#xA;&lt;li&gt;Reducing first the dimensionality of the problem with a PCA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'd appreciate any thoughts or advices about possible other ways in which I could tackle this problem.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Let me show you an example of a hypothetical online clustering application:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZVOQN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/sQj4h.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This finally leads to my questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Does this &quot;problem&quot; have a name that it can be referred to?&lt;/li&gt;&#xA;&lt;li&gt;Are there &quot;standard&quot; solutions to this and ...&lt;/li&gt;&#xA;&lt;li&gt;... is there maybe even an R package for that?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Reasonable Inheritance of Cluster Identities in Repetitive Clustering&lt;/a&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying now the simplest features with just list of words and distance between sentences defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/dHB9X.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(A and B are corresponding sets of words in sentence A and B)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make sense at all? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to apply &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mean-Shift&lt;/a&gt; algorithm from scikit library to this distance, as it does not require number of clusters in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have no knowledge about the climate or soil. And I just want to find out more about these kind of dataset. I heard that Climate Corporation asked its candidates to perform statistical analysis on various climate dataset. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why I am asking this question. Please do not get me wrong. I am not trying to get the dataset to prepare myself for an interview, as I know they give out different dataset to people from different background. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that Climate Corporation only hires PHD, which I am not. I only want to play around with their dataset such that I can learn and implement &lt;strong&gt;time series analysis&lt;/strong&gt;. That's it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, if anyone does not mind sharing their dataset. Please post the link them below. Thank you very much. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a question about classifying documents using supervised learning and unsupervised learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: - I have a bunch of documents talking about football.&lt;br&gt;&#xA;As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.&lt;br&gt;&#xA;My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to create more accurate categorizations because if I want to test a new document I want know if this document can be related to these categorizations or not.  &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren't garbage. Scaling the features didn't help as well. My current thinking is that these kind of features don't mix well with the (sparse) bag of words features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn't seem to be the issue.&#xA;I have also tried changing the 'C' parameter value from 1 to 1e3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am facing similar issues with all svm implementations using scikit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I not waiting enough for it to complete ?&#xA;How much time should this execution take ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my experience it shouldn't require over a few minutes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my system configuration:&#xA;Ubuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've been analyzing a data set of ~400k records and 9 variables The dependent variable is binary. I've fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this so? I'm guessing that it's because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Where can I find free spatio-temporal dataset for download so that I can play with it in R ? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Caveat: I am a complete beginner when it comes to machine learning, but eager to learn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a large dataset and I'm trying to find pattern in it. There may / may not be correlation across the data, either with known variables, or variables that are contained in the data but which I haven't yet realised are actually variables / relevant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm guessing this would be a familiar problem in the world of data analysis, so I have a few questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The 'silver bullet' would be to throw this all this data into a stats / data analysis program and for it to crunch the data looking for known / unknown patterns trying to find relations. Is SPSS suitable, or are there other applications which may be better suited.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Should I learn a language like R, and figure out how to manually process the data. Wouldn't this comprimise finding relations as I would have to manually specify what and how to analyse the data?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How would a professional data miner approach this problem and what steps would s/he take?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I recently read &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf&quot; rel=&quot;nofollow&quot;&gt;Similarity Measures for Short Segments of Text&lt;/a&gt; (Metzler et al.).  It describes basic methods for measuring query similarity, and in the paper, the data consists of queries and their top results. Results are lists of page urls, page titles, and short page snippets.  In the paper, the authors collect 200 results per query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.  There's a substantial difference between 10 and 200.  Hence, how much data is commonly used in practice to measure query similarity (e.g., how many results per query)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;References are a plus!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to scrape some data from a website. &#xA;I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I've just started reading about AB testing, as it pertains to optimizing website design.  I find it interesting that most of the methods assume that changes to the layout and appearance are independent of each other.  I understand that the most common method of optimization is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multi-armed_bandit&quot; rel=&quot;nofollow&quot;&gt;'multi-armed bandit'&lt;/a&gt; procedure.  While I grasp the concept of it, it seems to ignore the fact that changes (changes to the website in this case) are not independent to each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if company is testing the placement and color of the logo on the website, they find the optimal color first then the optimal placement.  Not that I'm some expert on human psychology, but shouldn't these be related? Can the multi-armed bandit method be efficiently used in this case or more complicated cases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first instinct is to say no.  On that note, why haven't people used heuristic algorithms to optimize over complicated AB testing sample spaces?  For an example, I thought someone might have used a genetic algorithm to optimize a website layout, but I can find no examples of something like this out there. This leads me to believe that I'm missing something important in my understanding of AB testing as it applies to website optimization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why isn't heuristic optimization used on more complicated websites?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have installed &lt;a href=&quot;https://github.com/Factual/drake&quot; rel=&quot;nofollow&quot;&gt;Drake&lt;/a&gt; on Windows 7 64-bit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using JDK 1.7.0_51.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried both using the pre-compiled jar file and&#xA;compiling from the Clojure source using &lt;a href=&quot;https://github.com/technomancy/leiningen&quot; rel=&quot;nofollow&quot;&gt;leiningen&lt;/a&gt;.&#xA;The resulting Drake version is 0.1.6, the current development version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When running Drake, I get the current version number.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next, I tried to go through &lt;a href=&quot;https://github.com/Factual/drake/wiki/Tutorial&quot; rel=&quot;nofollow&quot;&gt;the tutorial&lt;/a&gt;. The command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java -jar drake.jar  -w .\\\\workflow.d&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;results in the following Exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.Exception: no input data found in locations: D:\\\\tools\\\\drake\\\\in.c&#xA;sv&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Even though the file exists and has text inside it. &#xA;The same scenario works in a similar installation on Ubuntu 12.04.&#xA;Am I doing something wrong, or is this a Windows-specific bug?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm studying reinforcement learning in order to implement a kind of time series pattern analyzer such as market.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most examples I have seen are based on the maze environment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another question is about buy-sell modeling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume that the agent randomly buy at time $t$ and sell at time $t + \\\\alpha$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's simple to calculate reward.&#xA;The problem is how can I model $Q$ matrix and how can I model signals between buy and sell actions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you share some source code or guidance for similar situation?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am hoping to model the characteristics of the users of a specific page on Facebook, which has roughly 2 million likes. I have been looking at the Facebook SDK/API, but I can't really see if what I would like to do is possible. It seems that the users share quite different amounts of data so I probably discard a lot of users and only use the ones with a quite open public profile. I would like to have the following data:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) See the individuals that have 'liked' the page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) See the list of friends for each person that have 'liked' the page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) See gender for each person (optional)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) See other pages that each person has liked (optional)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone tell me if it is possible to get this data? As mentioned earlier it is okay if I discard data for users that don't like to share this data.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. They can be considered to be noisy variations of a canonical string. I am looking for an algorithm or a library that will convert each list into this canonical string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is one such list.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Star Wars: Episode IV A New Hope | StarWars.com&lt;/li&gt;&#xA;&lt;li&gt;Star Wars Episode IV - A New Hope (1977)&lt;/li&gt;&#xA;&lt;li&gt;Star Wars: Episode IV - A New Hope - Rotten Tomatoes&lt;/li&gt;&#xA;&lt;li&gt;Watch Star Wars: Episode IV - A New Hope Online Free&lt;/li&gt;&#xA;&lt;li&gt;Star Wars (1977) - Greatest Films&lt;/li&gt;&#xA;&lt;li&gt;[REC] 4 poster promises death by outboard motor - SciFiNow&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For this list, any string matching the regular expression &lt;code&gt;^Star Wars:? Episode IV (- )?A New Hope$&lt;/code&gt; would be acceptable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked at Andrew Ng's course on Machine Learning on Coursera, but I was not able to find a similar problem.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've fit a GLM (Poisson) to a data set where one of the variables is categorical for the year a customer bought a product from my company, ranging from 1999 to 2012. There's a linear trend of the coefficients for the values of the variable as the year of sale increases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any problem with trying to improve predictions for 2013 and maybe 2014 by extrapolating to get the coefficients for those years?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have been developing a chess program which makes use of alpha-beta pruning algorithm and an evaluation function that evaluates positions using the following features namely material, kingsafety, mobility, pawn-structure and trapped pieces etc..... My evaluation function is derived from the &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$f(p) = w_1 \\\\cdot \\\\text{material} + w_2 \\\\cdot \\\\text{kingsafety} + w_3 \\\\cdot \\\\text{mobility} + w_4 \\\\cdot \\\\text{pawn-structure} + w_5 \\\\cdot \\\\text{trapped pieces}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $w$ is the weight assigned to each feature. At this point i want to tune the weights of my evaluation function using temporal difference, where the agent plays against itself and in the process gather training data from its environment (which is a form of reinforcement learning). I have read some books and articles in order to have an insight on how to implement this in Java but they seem to be theoretical rather than practical. I need a detailed explanation and pseudo codes on how to automatically tune the weights of my evaluation function based on previous games.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Which one will be the dominating programming language for next 5 years for analytics , machine learning . R verses python verses SAS. Advantage and disadvantage.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am exploring different types of parse tree structures. The two widely known parse tree structures are &#xA;a) Constituency based parse tree and &#xA;b) Dependency based parse tree structures. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am exploring how to model a data set using normal distributions with both mean and variance defined as linear functions of independent variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Something like N ~ (f(x), g(x)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generate a random sample like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def draw(x):&#xA;    return norm(5 * x + 2, 3 *x + 4).rvs(1)[0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I want to retrieve 5, 2 and 4 as the parameters for my distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generate my sample:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;smp = np.zeros((100,2))&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for i in range(0, len(smp)):&#xA;    smp[i][0] = i&#xA;    smp[i][1] = draw(i)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The likelihood function is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def lh(p):&#xA;    p_loc_b0 = p[0]&#xA;    p_loc_b1 = p[1]&#xA;    p_scl_b0 = p[2]&#xA;    p_scl_b1 = p[3]&#xA;&#xA;    l = 1&#xA;    for i in range(0, len(smp)):&#xA;        x = smp[i][0]&#xA;        y = smp[i][1]&#xA;        l = l * norm(p_loc_b0 + p_loc_b1 * x, p_scl_b0 + p_scl_b1 * x).pdf(y)&#xA;&#xA;    return -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the parameters for the linear functions used in the model are given in the p 4-variable vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using scipy.optimize, I can solve for the MLE parameters using an extremely low xtol, and already giving the solution as the starting point:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fmin(lh, x0=[2,5,3,4], xtol=1e-35)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which does not work to well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Warning: Maximum number of function evaluations has been exceeded.&#xA;array([ 3.27491346,  4.69237042,  5.70317719,  3.30395462])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Raising the xtol to higher values does no good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So i try using a starting solution far from the real solution:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; fmin(lh, x0=[1,1,1,1], xtol=1e-8)&#xA;Optimization terminated successfully.&#xA;         Current function value: -0.000000&#xA;         Iterations: 24&#xA;         Function evaluations: 143&#xA;array([ 1.,  1.,  1.,  1.])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which makes me think:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PDF are largely clustered around the mean, and have very low gradients only a few standard deviations away from the mean, which must be not too good for numerical methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how does one go about doing these kind of numerical estimation in functions where gradient is very near to zero away from the solution?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have created external table in Hive in the hdfs path 'hdfs://localhost.localdomain:8020/user/hive/training'. If I apply describe command I can find the table path as shown below. But when I browse through the namenode web page, the table name does not showing up in the path.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hive&amp;gt; describe extended testtable4;&#xA;OK&#xA;firstname   string  &#xA;lastname    string  &#xA;address string  &#xA;city    string  &#xA;state   string  &#xA;country string  &#xA;&#xA;    ***Detailed Table Information  Table(tableName:testtable4, dbName:default, owner:cloudera, createTime:1408765301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:firstname, type:string, comment:null), FieldSchema(name:lastname, type:string, comment:null), FieldSchema(name:address, type:string, comment:null), FieldSchema(name:city, type:string, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], location:hdfs://localhost.localdomain:8020/user/hive/training, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,, line.delim=    &#xA;    }), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1408765301}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)       &#xA;    Time taken: 0.7 seconds***&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am seeking a basic list of key data analysis methods used for studying social media platforms online. Are there such key methods, or does this process generally vary according to topic? And is there a standard order in which these methods are applied?(The particular context I'm interested in is how the news is impacting on social media)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;As what I described in the title, we are especially interested in those for dealing with big data----ts efficiency and stability, and used in industry not in experiment or university. Thanks!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Have anyone used Shark as repository from resulting datasets from Apache Spark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm starting some tests with Spark and read about this database tecnology. Have anyone been using it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;strong&gt;General description of the problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a graph where some vertices are labeled with a type with 3 or 4 possible values. For the other vertices, the type is unknown.&#xA;My goal is to use the graph to predict the type for vertices that are unlabeled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible framework&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suspect this fits into the general framework of label propagation problems, based on my reading of the literature (e.g., see &lt;a href=&quot;http://lvk.cs.msu.su/~bruzz/articles/classification/zhu02learning.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; and &lt;a href=&quot;http://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/slides/pdf/Frequent_Subgraph_Mining.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another method that is mentioned often is &lt;code&gt;Frequent Subgraph Mining&lt;/code&gt;, which includes algorithms like &lt;code&gt;SUBDUE&lt;/code&gt;,&lt;code&gt;SLEUTH&lt;/code&gt;, and &lt;code&gt;gSpan&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Found in R&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only label propagation implementation I managed to find in &lt;code&gt;R&lt;/code&gt; is &lt;code&gt;label.propagation.community()&lt;/code&gt; from the &lt;code&gt;igraph&lt;/code&gt; library. &#xA;However, as the name suggests, it is mostly used to find communities, not for classifying unlabeled vertices.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There also seems to be several references to a &lt;code&gt;subgraphMining&lt;/code&gt; library (here for example), but it looks like it is missing from CRAN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know of a library or framework for the task described?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I would like to &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/base/html/summary.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;summarize&lt;/a&gt; (as in R) the contents of a CSV (possibly after &lt;a href=&quot;http://www.endmemo.com/program/R/readcsv.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;loading&lt;/a&gt; it, or storing it somewhere, that's not a problem). The summary should contain the quartiles, mean, median, min and max of the data in a CSV file for each numeric (integer or real numbers) dimension. The standard deviation would be cool as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also like to generate some plots to visualize the data, for example 3 plots for the 3 pairs of variables that are more correlated (&lt;a href=&quot;http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient&quot; rel=&quot;nofollow noreferrer&quot;&gt;correlation coefficient&lt;/a&gt;) and 3 plots for the 3 pairs of variables that are least correlated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;R requires only a few lines to implement this. Are there any libraries (or tools) that would allow a similarly simple (and efficient if possible) implementation in Java or Scala?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PD: This is a specific use case for a &lt;a href=&quot;https://datascience.stackexchange.com/questions/948/any-clear-winner-for-data-science-in-scala&quot;&gt;previous (too broad) question&lt;/a&gt;.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was wondering if there is any research or study made to calculate the volume of space is used by all scientific articles. It could be in pdf, txt, compressed, or any other format. Is there even a way to measure it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can some one point me towards realizing this study?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regards and thanks.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a question regarding the use of neural network. I am currently working with R (&lt;a href=&quot;http://cran.r-project.org/web/packages/neuralnet/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;neuralnet package&lt;/a&gt;) and I am facing the following issue.&#xA;My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?&#xA;Maybe something is wrong in my analysis&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I use the daily log return &#xA;&lt;img src=&quot;https://i.stack.imgur.com/A3r16.gif&quot; alt=&quot;r(t) = ln(s(t)/s(t-1))&quot;&gt;&lt;/li&gt;&#xA;&lt;li&gt;I normalise my data with the sigmoid function (sigma and mu computed on my whole set)&lt;/li&gt;&#xA;&lt;li&gt;I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for the best solution to manage and host datasets for journalistic pursuits. I am assessing &lt;a href=&quot;https://www.documentcloud.org&quot; rel=&quot;nofollow&quot;&gt;https://www.documentcloud.org&lt;/a&gt; and &lt;a href=&quot;http://datahub.io/&quot; rel=&quot;nofollow&quot;&gt;http://datahub.io/&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain the differences between them, or recommend a superior solution?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to evaluate and compare several different machine learning models built with different parameters (i.e. downsampling, outlier removal) and different classifiers (i.e. Bayes Net, SVM, Decision Tree).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am performing a type of cross validation where I randomly select 67% of the data for use in the training set and 33% of the data for use in the testing set. I perform this for several iterations, say, 20.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, from each iteration I am able to generate a confusion matrix and compute a kappa. My question is, what are some ways to aggregate these across the iterations? I am also interested in aggregating accuracy and expected accuracy, among other things.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the kappa, accuracy, and expected accuracy, I have just been taking the average up to this point. One of the problems is that when I recompute kappa with the aggregated average and expected average, it is not the same with the aggregated kappa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the confusion matrix, I have been first normalizing the confusion matrix from each iteration and then averaging them, in an attempt to avoid an issue of confusion matrices with different numbers of total cases (which is possible with my cross validation scheme).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I recompute the kappa from this aggregated confusion matrix, it is also different from the previous two.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which one is most correct? Is there another way of computing an average kappa that is more correct?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks, and if more concrete examples are needed in order to illustrate my question please let me know.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am interested in knowing the differences in &lt;strong&gt;functionality&lt;/strong&gt; between SAP HANA and Exasol. Since this is a bit of an open ended question let me be clear. I am not interested in people debating which is &quot;better&quot; or faster. I am only interested in what each was designed to do so please keep your opinions out of it. I suspect it is a bit like comparing HANA to Oracle Exalytics where there is some overlap but the functionality goals are different. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;How is the concept of data different for different disciplines? Obviously, for physicists and sociologists, &quot;data&quot; is something different.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking for packages (either in python, R, or a standalone package) to perform online learning to predict stock data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have found and read about Vowpal Wabbit (&lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit/wiki&quot;&gt;https://github.com/JohnLangford/vowpal_wabbit/wiki&lt;/a&gt;),&#xA;which seems to be quite promising but I am wondering if there are any other packages out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;In SVMs the polynomial kernel is defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(scale * crossprod(x, y) + offset)^degree&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do the scale and offset parameters affect the model and what range should they be in? (intuitively please)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are the scale and offset for numeric stability only (that's what it looks like to me), or do they influence prediction accuracy as well?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can good values for scale and offset be calculated/estimated when the data is known or is a grid search required? The caret package always sets the offset to 1, but it does a grid search for scale. (Why) is an offset of 1 a good value?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS.: Wikipedia didn't really help my understanding:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For degree-d polynomials, the polynomial kernel is defined as&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/3stCR.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;where x and y are vectors in the input space, i.e. vectors of features&#xA;  computed from training or test samples, &lt;img src=&quot;https://i.stack.imgur.com/34dwi.png&quot; alt=&quot;&quot;&gt; &lt;strong&gt;is a constant trading&lt;/strong&gt;&#xA;  &lt;strong&gt;off the influence of higher-order versus lower-order terms&lt;/strong&gt; in the&#xA;  polynomial. When &lt;img src=&quot;https://i.stack.imgur.com/79cDG.png&quot; alt=&quot;&quot;&gt;, the kernel is called homogeneous.(&lt;strong&gt;A further&lt;/strong&gt;&#xA;  &lt;strong&gt;generalized polykernel divides &lt;img src=&quot;https://i.stack.imgur.com/Ps0qt.png&quot; alt=&quot;&quot;&gt; by a user-specified scalar&lt;/strong&gt;&#xA;  &lt;strong&gt;parameter &lt;img src=&quot;https://i.stack.imgur.com/sjfS1.png&quot; alt=&quot;&quot;&gt;.&lt;/strong&gt;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Neither did ?polydot's explanation in R's help system:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt;: The scaling parameter of the polynomial and tangent kernel is a&#xA;  convenient way of normalizing patterns (&amp;lt;-!?) without the need to modify the&#xA;  data itself&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;strong&gt;offset&lt;/strong&gt;: The offset used in a polynomial or hyperbolic tangent kernel (&amp;lt;- lol thanks)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Background:&#xA;I run a product that compares sets of data (data matching and data reconciliation).&#xA;To get the result we need to compare each row in a data set with every N rows on the opposing data set&#xA;Now however we get sets of up to 300 000 rows of data in each set to compare and are getting 90 Billion computations to handle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is this:&#xA;Even though we dont have the data volumes to use Hadoop, we have the computational need for something distributed. Is Hadoop a good choice for us?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm going to classify unstructured text documents, namely web sites of unknown structure. The number of classes to which I am classifying is limited (at this point, I believe there is no more than three). Does anyone have a suggested for how I might get started?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the &quot;bag of words&quot; approach feasible here? Later, I could add another classification stage based on document structure (perhaps decision trees).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am somewhat familiar with Mahout and Hadoop, so I prefer Java-based solutions. If needed, I can switch to Scala and/or Spark engine (the ML library).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm searching for data sets for evaluating text retrieval quality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TF-IDF is a popular similarity measure, but is it the best choice? And which &lt;em&gt;variant&lt;/em&gt; is the best choice? &lt;a href=&quot;https://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html&quot; rel=&quot;nofollow&quot;&gt;Lucenes Scoring&lt;/a&gt; for example uses IDF^2, and IDF defined as 1+log(numdocs/(docFreq+1)). TF in lucene is defined as sqrt(frequency)...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many more variants exist, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Okapi_BM25&quot; rel=&quot;nofollow&quot;&gt;Okapi BM25&lt;/a&gt;, which is used by the &lt;a href=&quot;http://xapian.org/docs/bm25.html&quot; rel=&quot;nofollow&quot;&gt;Xapian search engine&lt;/a&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to study the different variants, and I'm looking for &lt;strong&gt;evaluation data sets&lt;/strong&gt;. Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was curious about the ANOVA RBF kernel provided by kernlab package available in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tested it with a numeric dataSet of 34 input variables and one output variable. For each variable I have 700 different values. Comparing with other kernels, I got very bad results with this kernel.&#xA;For example using the simple RBF kernel I could predict with 0,88 R2 however with the anova RBF I could only get 0,33 R2.&#xA;I thought that ANOVA RBF would be a very good kernel. Any thoughts? Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;set.seed(100) #use the same seed to train different models&#xA;svrFitanovaacv &amp;lt;- train(R ~ .,&#xA;                       data = trainSet,&#xA;                       method = SVManova,&#xA;                       preProc = c(&quot;center&quot;, &quot;scale&quot;),&#xA;                       trControl = ctrl, tuneLength = 10) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = &quot;ROC&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;define custom model in caret package:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(caret)&#xA;#RBF ANOVA KERNEL&#xA;SVManova &amp;lt;- list(type = &quot;Regression&quot;, library = &quot;kernlab&quot;, loop = NULL)&#xA;prmanova &amp;lt;- data.frame(parameter = c(&quot;C&quot;, &quot;sigma&quot;, &quot;degree&quot;, &quot;epsilon&quot;),&#xA;                     class = rep(&quot;numeric&quot;, 4),&#xA;                     label = c(&quot;Cost&quot;, &quot;Sigma&quot;, &quot;Degree&quot;, &quot;Epsilon&quot;))&#xA;SVManova$parameters &amp;lt;- prmanova&#xA;svmGridanova &amp;lt;- function(x, y, len = NULL) {&#xA;library(kernlab)&#xA;sigmas &amp;lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)&#xA;expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,&#xA;            C = 2^(-40:len), degree = 1:2) # len = tuneLength in train&#xA;}&#xA;SVManova$grid &amp;lt;- svmGridanova&#xA;svmFitanova &amp;lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {&#xA;  ksvm(x = as.matrix(x), y = y,&#xA;       kernel = &quot;anovadot&quot;,&#xA;       kpar = list(sigma = param$sigma, degree = param$degree),&#xA;       C = param$C, epsilon = param$epsilon,&#xA;       prob.model = classProbs,&#xA;       ...) #default type = &quot;eps-svr&quot;&#xA;}&#xA;SVManova$fit &amp;lt;- svmFitanova&#xA;svmPredanova &amp;lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xA;  predict(modelFit, newdata)&#xA;SVManova$predict &amp;lt;- svmPredanova&#xA;svmProb &amp;lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xA;  predict(modelFit, newdata, type=&quot;probabilities&quot;)&#xA;SVManova$prob &amp;lt;- svmProb&#xA;svmSortanova &amp;lt;- function(x) x[order(x$C), ]&#xA;SVManova$sort &amp;lt;- svmSortanova&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;load data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataA2&amp;lt;-read.csv(&quot;C:/results/A2.txt&quot;,header = TRUE, &#xA;                             blank.lines.skip = TRUE,sep = &quot;,&quot;)&#xA;set.seed(1)&#xA;inTrainSet &amp;lt;- createDataPartition(dataA2$R, p = 0.75, list = FALSE) #[[1]]&#xA;trainSet &amp;lt;- dataA2[inTrainSet,]&#xA;testSet &amp;lt;- dataA2[-inTrainSet,]&#xA;#-----------------------------------------------------------------------------&#xA;#K-folds resampling method for fitting svr&#xA;ctrl &amp;lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10,&#xA;                     allowParallel = TRUE) #10 separate 10-fold cross-validations&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;link to data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;wuala.com/jpcgandre/Documents/Data%20SVR/?key=BOD9NTINzRHG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;We have ~500 biomedical documents each of some 1-2 MB. We want to use a non query-based method to rank the documents in order of their unique content score. I'm calling it &quot;unique content&quot; because our researchers want to know from which document to start reading. All the documents are of the same topic, in the biomedical world we know that there is always a lot of content overlap. So all we want to do is to arrange the documents in the order of their unique content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most Information Retrieval literature suggest query-based ranking which does not fit our need.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;It may be unlikely that anyone knows this but I have a specific question about Freebase.  Here is the Freebase page from the &lt;a href=&quot;http://www.freebase.com/m/014_d3&quot; rel=&quot;nofollow&quot;&gt;Ford Taurus automotive model&lt;/a&gt; .  It has a property called &quot;Related Models&quot;.  Does anyone know how this list of related models was compiled.  What is the similarity measure that they use?  I don't think it is only about other wikipedia pages that link to or from this page.  Alternatively, it may be that this is user generated.  Does anyone know for sure?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What is the best technology to be used to create my custom bag of words with N-grams to apply to. I want to know a functionality that can be achieved over GUI. I cannot use spot fire as it is not available in the organization. Though i can get SAP Hana or R-hadoop. But R-hadoop is bit challenging, any suggessions.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's &lt;a href=&quot;http://scikit-learn.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;scikit-learn&lt;/a&gt; for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in &lt;a href=&quot;https://stackoverflow.com/q/20106940/1435804&quot;&gt;this&lt;/a&gt; Stackoverflow post.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my machine learning task, I create a set of predictors.&#xA;Predictors come in &quot;bundles&quot; - multi-dimensional measurements (3 or 4 - dimensional in my case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The hole &quot;bundle&quot; makes sense only if it has been measured, and taken all together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is, different 'bundles' of predictors can be measured only for small part of the sample, and those parts don't necessary intersect for different 'bundles'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As parts are small, imputing leads to considerable decrease in accuracy(catastrophical to be more accurate)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible solutions&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could create dummy variables that would mark whether the measurement has taken place for each variable. The problem is, when random forests draws random variables, it does so individually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So there are two basic ways to solve this problem:&#xA;1) Combine each &quot;bundle&quot; into one predictor. That is possible, but it seems information will be lost. &#xA;2) Make random forest draw variables not individually, but by obligatory &quot;bundles&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Problem for random forest&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As random forest draws variables randomly, it takes features that are useless (or much less useful) without other from their &quot;bundle&quot;. I have a feeling that leads to a loss of accuracy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example I have variables &lt;code&gt;a&lt;/code&gt;,&lt;code&gt;a_measure&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;,&lt;code&gt;b_measure&lt;/code&gt;.&#xA;The problem is, variables &lt;code&gt;a_measure&lt;/code&gt; make sense only if variable &lt;code&gt;a&lt;/code&gt; is present, same for &lt;code&gt;b&lt;/code&gt;. So I either have to combine &lt;code&gt;a&lt;/code&gt;and &lt;code&gt;a_measure&lt;/code&gt; into one variable, or make random forest draw both, in case at least one of them is drawn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the best practice solutions for problems when different sets of predictors are measured for small parts of overall population, and these sets of predictors come in obligatory &quot;bundles&quot;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;The problem refers to decision trees building. According to Wikipedia '&lt;a href=&quot;http://en.wikipedia.org/wiki/Gini_coefficient&quot;&gt;Gini coefficient&lt;/a&gt;' should not be confused with '&lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity&quot;&gt;Gini impurity&lt;/a&gt;'. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) 'Gini impurity' - it is a standard decision-tree splitting metric (see in the link above);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) 'Gini coefficient' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking at pybrain for taking server monitor alarms and determining the root cause of a problem. I'm happy with training it using supervised learning and curating the training data sets. The data is structured something like this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;A&lt;/strong&gt; #1&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type 1&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;A&lt;/strong&gt; #2&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type 1&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;B&lt;/strong&gt; #1&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type &lt;strong&gt;99&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So there are &lt;em&gt;n&lt;/em&gt; servers, with &lt;em&gt;x&lt;/em&gt; alarms that can be &lt;code&gt;UP&lt;/code&gt; or &lt;code&gt;DOWN&lt;/code&gt;. Both &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; are variable. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If Server A1 has &lt;em&gt;alarm 1 &amp;amp; 2&lt;/em&gt; as &lt;code&gt;DOWN&lt;/code&gt;, then we can say that &lt;em&gt;service a&lt;/em&gt; is down on that server and is the cause of the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If &lt;em&gt;alarm 1&lt;/em&gt; is down on all servers, then we can say that &lt;em&gt;service a&lt;/em&gt; is the cause.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There can potentially be multiple options for the cause, so straight classification doesn't seem appropriate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also like to tie later sources of data to the net. Such as just scripts that ping some external service.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the appropriate alarms may not be triggered at once, due to serial service checks, so it can start with one server down and then another server down 5 minutes later.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to do some basic stuff at first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pybrain.tools.shortcuts import buildNetwork&#xA;from pybrain.datasets import SupervisedDataSet&#xA;from pybrain.supervised.trainers import BackpropTrainer&#xA;&#xA;&#xA;INPUTS = 2&#xA;OUTPUTS = 1&#xA;&#xA;# Build network&#xA;&#xA;# 2 inputs, 3 hidden, 1 output neurons&#xA;net = buildNetwork(INPUTS, 3, OUTPUTS)&#xA;&#xA;&#xA;# Build dataset&#xA;&#xA;# Dataset with 2 inputs and 1 output&#xA;ds = SupervisedDataSet(INPUTS, OUTPUTS)&#xA;&#xA;&#xA;# Add one sample, iterable of inputs and iterable of outputs&#xA;ds.addSample((0, 0), (0,))&#xA;&#xA;&#xA;&#xA;# Train the network with the dataset&#xA;trainer = BackpropTrainer(net, ds)&#xA;&#xA;# Train 1000 epochs&#xA;for x in xrange(10):&#xA;    trainer.train()&#xA;&#xA;# Train infinite epochs until the error rate is low&#xA;trainer.trainUntilConvergence()&#xA;&#xA;&#xA;# Run an input over the network&#xA;result = net.activate([2, 1])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I[m having a hard time mapping variable numbers of alarms to static numbers of inputs. For example, if we add an alarm to a server, or add a server, the whole net needs to be rebuilt. If that is something that needs to be done, I can do it, but want to know if there's a better way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another option I'm trying to think of, is have a different net for each type of server, but I don't see how I can draw an environment-wide conclusion, since it will just make evaluations on a single host, instead of all hosts at once. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which type of algorithm should I use and how do I map the dataset to draw environment-wide conclusions as a whole with variable inputs?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&quot;Oversaw car manufacturing&quot; gets tagged as NNP-NN-NN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Rather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what's the best way to accomplish accuracy improvement?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Are there better models out there for POS/NER that can be incorporated into CoreNLP?&lt;/li&gt;&#xA;&lt;li&gt;Should I switch to other NLP tools?&lt;/li&gt;&#xA;&lt;li&gt;Or create training models with exception rules?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have been tasked with creating a pipeline chart with the live data and the budgeted numbers.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know what probability of each phase of reaching the next.  The problem is I have no Idea what to do about the pipeline budgeting with regards to time.  For instance what period of time should I have closed sales in the chart.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have honestly been working on trying to figure it out.  Each successive revision gets me farther from the answer.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am currently trying to implement logistic regression with iteratively reweightes LS, according to &quot;Pattern Recognition and Machine Learning&quot; by C. Bishop. In a first approach I tried to implement it in C#, where I used Gauss' algorithm to solve eq. 4.99. For a single feature it gave very promising (nearly exact) results, but whenever I tried to run it with more than one feature my system matrix became singular, and the weights did not converge. I first thought that it was my implementation, but when I implemented it in SciLab the results sustained. The SciLab (more concise due to matrix operators) code I used is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;phi = [1; 0; 1; 1];&#xA;t = [1; 0; 0; 0];&#xA;w= [1];&#xA;&#xA;w' * phi(1,:)'&#xA;&#xA;for in=1:100&#xA;    y = [];&#xA;    R = zeros(size(phi,1));&#xA;    R_inv = zeros(size(phi,1));&#xA;&#xA;    for i=1:size(phi,1)&#xA;        y(i) = 1/(1+ exp(-(w' * phi(i,:)')));&#xA;        R(i,i) = y(i)*(1 - y(i));&#xA;        R_inv(i,i) = 1/R(i,i);&#xA;    end&#xA;&#xA;    z = phi * w - R_inv*(y - t)&#xA;    w = inv(phi'*R*phi)*phi'*R*z&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the values for phi (input/features) and t (output/classes), it yields a weight of  -0.6931472, which is pretty much 1/3, which seems fine to me, for there is 1/3 probability of beeing assigned to class 1, if feature 1 is present (please forgive me, if my terms do not comply with ML-language completely, for I am an software developer). If I now added an intercept feature, which would accord to&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;phi = [1, 1; 1, 0; 1, 1; 1, 1];&#xA;w = [1; 1];&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;my R-matrix becomes singular and the last weights value is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;w  =&#xA;  - 5.8151677  &#xA;  1.290D+30  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which - to my reading - would mean, that the probability of belonging to class 1 would be close to 1 if feature 1 is present about 3% for the rest. There has got to be any error I made, but I do not get which one. For both implementations yield the same results I suspect that there is some point I've been missing or gotten wrong, but I do not understand which one.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am having an HTML string and want to find out if a word I supply is relevant in that string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Relevancy could be measured based on frequency in the text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example to illustrate my problem:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;this is an awesome bike store&#xA;bikes can be purchased online.&#xA;the bikes we own rock.&#xA;check out our bike store now&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to test a few other words:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bike repairs&#xA;dog poo&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;bike repairs&lt;/code&gt; should be marked as relevant whereas &lt;code&gt;dog poo&lt;/code&gt; should not be marked as relevant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How could this be done?&lt;/li&gt;&#xA;&lt;li&gt;How to I filter out ambiguous words like &lt;code&gt;in&lt;/code&gt; or &lt;code&gt;or&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks for your ideas!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess it's something Google does to figure out what keywords are relevant to a website. I am basically trying to reproduce their on-page rankings.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some good answers about sub-sampling and weighted random forest are given here: &lt;a href=&quot;https://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase&quot;&gt;What are the implications for training a Tree Ensemble with highly biased datasets?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which classification methods besides RF can handle the problem in the best way?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;As mentioned &lt;a href=&quot;https://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets&quot;&gt;before&lt;/a&gt;, I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.&#xA;I have trained a Generalized Boosted Regression model using &lt;code&gt;gbm()&lt;/code&gt; from the &lt;code&gt;gbm&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; and get the following output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xA;  1                  50       0.906     0.523  0.00978      0.0512  &#xA;  1                  100      0.91      0.561  0.0108       0.0517  &#xA;  1                  150      0.91      0.572  0.0104       0.0492  &#xA;  2                  50       0.908     0.569  0.0106       0.0484  &#xA;  2                  100      0.91      0.582  0.00965      0.0443  &#xA;  2                  150      0.91      0.584  0.00976      0.0437  &#xA;  3                  50       0.909     0.578  0.00996      0.0469  &#xA;  3                  100      0.91      0.583  0.00975      0.0447  &#xA;  3                  150      0.911     0.586  0.00962      0.0443  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That's clear.&#xA;And what is not transparent: how Kappa is calculated.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? &lt;/li&gt;&#xA;&lt;li&gt;What do &lt;code&gt;Accuracy SD&lt;/code&gt; and &lt;code&gt;Kappa SD&lt;/code&gt; mean?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to cluster a set of long-tailed / pareto-like data into several bins (actually the bin number is not determined yet). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which algorithm or model would anyone recommend?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a general methodological question. I have two columns of data, with one a column a numeric variable for age and another column a short character variable for text responses to a question. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to group the age variable (that is, create cut points for the age variable), based on the text responses. I'm unfamiliar with any general approaches for doing this sort of analysis. What general approaches would you recommend? Ideally I'd like to categorize the age variable based on linguistic similarity of the text responses.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/RFM_(customer_value)&quot; rel=&quot;nofollow&quot;&gt;RFM&lt;/a&gt; - is a ranking model when all customers are ranked according to their purchasing &lt;strong&gt;F&lt;/strong&gt; requency, &lt;strong&gt;R&lt;/strong&gt; recency and &lt;strong&gt;M&lt;/strong&gt; monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;predicting which customer will most likely spend more&lt;/li&gt;&#xA;&lt;li&gt;who is going to upgrade/renew subscribtion/refund etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update2&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I understand, this is simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.&lt;/li&gt;&#xA;&lt;li&gt;Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Suppose I am interested in classifying a set of instances composed by different content types, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a piece of &lt;strong&gt;text&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;an &lt;strong&gt;image&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;as &lt;code&gt;relevant&lt;/code&gt; or &lt;code&gt;non-relevant&lt;/code&gt; for a specific class &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my classification process I perform the following steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Given a sample, I subdivide it in text and image&lt;/li&gt;&#xA;&lt;li&gt;A first SVM binary classifier (&lt;code&gt;SVM-text&lt;/code&gt;), trained only on text, classifies the text as &lt;code&gt;relevant&lt;/code&gt;/&lt;code&gt;non-relevant&lt;/code&gt; for the class &lt;code&gt;C&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;A second SVM binary classifier (&lt;code&gt;SVM-image&lt;/code&gt;), trained only on images, classifies the image as &lt;code&gt;relevant&lt;/code&gt;/&lt;code&gt;non-relevant&lt;/code&gt; for the class &lt;code&gt;C&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Both &lt;code&gt;SVM-text&lt;/code&gt; and &lt;code&gt;SVM-image&lt;/code&gt; produce an estimate of the probability of the analyzed content (text or image) of being relevant for the class &lt;code&gt;C&lt;/code&gt;. Given this, I am able to state whether the text is relevant for &lt;code&gt;C&lt;/code&gt; and the image is relevant for &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, these estimates are valid for segments of the original sample (either the text or the image), while it is not clear how to obtain a general opinion on the whole original sample (text+image). How can I combine conveniently the opinions of the two classifiers, so as to obtain a classification for the whole original sample?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am working on a text classification problem using Random Forest as classifiers, and a bag-of-words approach. &#xA;I am using the basic implementation of Random Forests (the one present in scikit), that creates a binary condition on a single variable at each split. Given this, is there a difference between using simple tf (term frequency) features. where each word has an associated weight that represents the number of occurrences in the document, or tf-idf (term frequency * inverse document frequency), where the term frequency is also multiplied by a value that represents the ratio between the total number of documents and the number of documents containing the word)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my opinion, there should not be any difference between these two approaches, because the only difference is a scaling factor on each feature, but since the split is done at the level of single features this should not make a difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I right in my reasoning? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am interested in the field of &lt;strong&gt;named entity disambiguation&lt;/strong&gt; and want to learn more about it. I have heard that there are contests organised by various associations on these kind of research topics. These contests are very helpful as they give a practical experience in these fields. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found one such contest organised by Microsoft research &lt;a href=&quot;http://web-ngram.research.microsoft.com/erd2014/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; though the dates have already passed. Can anyone point me to any other such contests ? Also, is there a site which catalogues these contests so that one can just go there and know about all upcoming contests ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.  &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit... (Attempt to better explain what I meant by an algebraic system...)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about &quot;abstract algebras&quot; as discussed in Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Algebra#Abstract_algebra&quot;&gt;http://en.wikipedia.org/wiki/Algebra#Abstract_algebra&lt;/a&gt;&#xA;&lt;a href=&quot;http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts&quot;&gt;http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Boolean Algebras are (very simple) algebras that range over Boolean values.  A simple example of such an algebra would consist the values True and False and the operations AND, OR and NOT. One might argue this algebra is 'complete' as, from these two constants (free-variables) and three basic operations, arbitrary boolean functions can be constructed/described.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested to discover algebras where the values are (time-domain) time-series.  I'd like it to be possible to construct &quot;arbitrary&quot; functions, that map time-series to time-series, from a few operations which, individually, map time-series to time-series.  I am open to liberal interpretations of &quot;arbitrary&quot;. I would be especially interested in examples of these algebras where the operations consist 'higher-order functions' - where such operations have been developed for a specific domain.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a research scholar in data mining. I'm interested in C# implementation of K-Means clustering algorithm for mixed numeric and categorical data.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a rather large commute every day - it ranges between about an hour and about an hour and half of driving.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been tracking my driving times, and want to continue to do so. I am capturing the date, my time of departure, my time of arrival, the route I took (there are two or three possible ones), weather conditions (wet/dry and clear/hazy/foggy), and whether I stopped (and if so, for what reason - fuel/toilet break/food break, and for how long) for every journey to and from work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to create a system to analyse this data and suggest an optimal departure time (for the next journey) based on day of the week, weather conditions, and whether i need to stop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anecdotally, I can see that Tuesday mornings are worse than other mornings, the earlier I leave the more likely I am to take a toilet break or a food break, and obviously that the journey takes longer on rainy or foggy days than on clear and dry days - but I would like the system to empirically tell me that!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume this is a machine-learning and statistical analysis problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have absolutely no knowledge of machine-learning, or statistical methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What statistical methods should I use to do this kind of analysis to the point where the data will lead to suggestions like &quot;tomorrow is Tuesday and it is going to rain, so you must leave home between 7.50 and 8.00, and take route XYZ, to get the optimal driving time. Oh and chances are you will need a toilet break - and I have factored that in&quot;? (assume that I manually enter tomorrow’s weather forecast - I’ll look into integrating with a weather service later)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this is life-hacking for me, trying to optimise the hell out of a tedious process, and it is very personal - specific to me and my habits, specific to this route, and specific to the morning/evening commute times. Google Maps with Traffic, TomTom with IQ, and Waze do very well in the more open-ended situations of ad-hoc driving-time prediction. Even Apple is happy to tell me on my iPhone notification screen how long it will take me to get home if I leave right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note, it appears to me that traffic is not a consideration - that is to say, I do not think I need to know the actual traffic conditions - traffic is a function of day of the week and weather. For example, there are more people on the roads on Monday and Tuesday mornings, and people drive more slowly, and more people are in cars (opting to drive instead of cycle or take public transport) when it rains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To what extent can I let the data do all the talking? I have a somewhat ambiguous hidden agenda which may not be apparent from the data;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I should be at work at 9.30 (i.,e. 9.15 +/- 15 minutes) every day, but the occasional 10am arrival is OK&lt;/li&gt;&#xA;&lt;li&gt;I want to leave home as late as possible, and yet arrive at work as early as possible&lt;/li&gt;&#xA;&lt;li&gt;I want to leave work as early as possible, and yet have done at least 8 hours’ work&lt;/li&gt;&#xA;&lt;li&gt;it is OK for me to, say, leave half an hour early on one day but stay late on another to compensate&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I think I can come up with a procedural formula that can encompass all of these rules, but my gut feeling is that statistical analysis can make it a lot smarter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apart from the methods of analysis, the technology stack is not an issue. Java is my language of choice - I am quite familiar with programming in it, and in creating web applications.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming that it is possible, are there Java libraries that can provide the requisite methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What limitations are there? I want to keep capturing more and more data every day, making the data set bigger, hopefully, making the prediction more accurate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What other ways are there to do it? Can I push this data into, say, Wolfram Programming Cloud, or maybe something Google provides to get the desired results?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have time series data from mobile sensors for different motions such as walking, pushups, dumbellifts, rowing and so on. All these motions have different length of time series. For classifying them using &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_time_warping&quot;&gt;Dynamic Time Warping (DTW)&lt;/a&gt;, how do I choose an appropriate window size that will give good results?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking to choose my career in the area of decision science or predictive modeling and I am aware that this is kind of opinion based but I would like to have some suggestion from experts that I can use it to build my career in correct path. What are the tools should I know like R, SAS or any other. What are the thinks I should know to work in a data science or machine learning or predictive modeling. For me I am having problem in identifying steps that I should follow. Please suggest me some steps to follow.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Recently I was introduced to the field of Data Science (its been 6 months approx), and Ii started the journey with Machine Learning Course by Andrew Ng and post that started working on the Data Science Specialization by JHU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On practical application front, I have been working on building a predictive model that would predict attrition. So far I have used glm, bayesglm, rf in an effort to learn and apply these methods, but I find a lot of gap in my understanding of these algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My basic dilemma is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Whether I should focus more on learning the intricacies of a few algorithms or should I use the approach of knowing a lot of them as and when and as much as required?&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please guide me in the right direction, maybe by suggesting books or articles or anything that you think would help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would be grateful if you would reply with an idea of guiding someone who has just started his career in the field of Data Science, and wants to be a person who solves practical issues for the business world.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would read (as many as possible) resources (books,articles) suggested in this post and would provide a personal feed back on the pros and cons of the same so as to make this a helpful post for people who come across a similar question in future,and i think it would be great if people suggesting these books can do the same.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;One of the discussed nice aspects of the procedure that Vowpal Wabbit uses for updates to sgd &#xA;&lt;a href=&quot;http://lowrank.net/nikos/pubs/liw.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt; is so-called weight invariance, described in the linked as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Among these updates we mainly focus on a novel&#xA;set of updates that satisfies an additional invariance&#xA;property: for all importance weights of h, the update&#xA;is equivalent to two updates with importance weight&#xA;h/2. We call these updates importance invariant.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this mean and why is it useful?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am working on a text classification problem on tweets. At the moment I was only considering the content of the tweets as a source of information, and I was using a simple bag of words approach using term frequencies as features, using Random Forests (this is something I cannot change). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now my idea is to try to incorporate information present in the URLs used in tweets. Now, not all the tweets have URLs, and if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLs. For this reason, I suppose that having a single set of features containing both the tweet term frequencies and the URL term frequencies could be bad. Besides I'll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any suggestions regarding this issue? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm a Java developer and I want to pursue career in Data Science and machine learning. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please advise me where and how to begin? What subjects and mathematical/statistical skills are required and so on?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a problem and I'm having trouble representing it - first I thought I should use graph theory (nodes and edges) and now I'm not sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data is some tanks names and it's volumes, those tanks are connected by pipelines which I have the names and length.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;------(pipeline 1)------.-----(pipeline 2)------.----(pipeline 3)---&#xA;  |                     |    |                           |         |&#xA;[R tank 1]        [S tank 1] [S tank 2]            (pipeline 4) [S tank 3]&#xA;                                                         |&#xA;                                                     [S tank 4]&#xA;&#xA;R tank is sink (receiver) and S tank is source (sender)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Problem is the pipe names change doesn't occur where there is a tank - they change name because historical reasons, size or connections...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if I want to graphically show that S tank 2 is connected to pipeline 2 at point X and pipeline 2 connects to pipeline and the content goes to R tank 1, how should I do this? (I think the point X may not be relevant but if I had some way to get the distance travelled would be great).  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I couldn't quite think of how best to title this, so recommendations are welcome. Same goes for the tags (I don't have the reputation to use the tags that I thought were appropriate). The question is this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Suppose you have N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -&gt; y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD then PCA to reduce the data dimension for efficiency. But MATLAB and Octave cannot load such a large dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tools I can use to do SVD with such a large amount of data? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm going to start a Computer Science phd this year and for that I need a research topic. I am interested in Predictive Analytics in the context of Big Data. I am interested by the area of Education (MOOCs, Online courses...). In that field, what are the unexplored areas that can help me choose a strong topic? Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know if there is such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenization, POS tagging, parsing, training, testing .... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I am new to NLP and I need a tool which let me experiment, get familiar and progress&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Are there any general open-source programs or libraries (e.g., a Python library) for analyzing user search behavior?  By &quot;search behavior&quot;, I mean a user's interaction with a search engine, such as querying, clicking relevant results, and spending time on those results.  I'd like something with the following properties - it doesn't have to be all of them, but the more the merrier:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Models individual user behavior (aggregate and time-based)&lt;/li&gt;&#xA;&lt;li&gt;Models group user behavior&lt;/li&gt;&#xA;&lt;li&gt;Simulates individual user behavior, given a model&lt;/li&gt;&#xA;&lt;li&gt;Is easily extensible (to accept data input formats, user models, document models, etc., that end-users define)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Links are a plus!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Not sure if this is Math, Stats or Data Science, but I figured I would post it here to get the site used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a programmer, when you have a system/component implemented, you might want to allow some performance monitoring. For example to query how often a function call was used, how long it took and so on. So typically you care about count, means/percentile, max/min and similiar statistics. This could be measurements since startup, but also a rolling average or window.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if there is a good data structure which can be updated efficiently concurrently which can be used as the source for most of those queries. For example having a ringbuffer of rollup-metrics (count, sum, min, max) over increasing periods of time and a background aggregate process triggered regularly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The focus here (for me) is on in-memory data structures with limited memory consumption. (For other things I would use a RRD type of library).&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have post already the question few months ago about my project that I'm starting to work on. This post can be see here: &#xA;&lt;a href=&quot;https://datascience.stackexchange.com/questions/211/human-activity-recognition-using-smartphone-data-set-problem&quot;&gt;Human activity recognition using smartphone data set problem&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I know this is based around multivariate time series analysis and tasks are to classify and cluster the data. I have gathered some materials (e-books, tutorials etc.) on this but still can't see a more detailed picture of how even I should start. Here's the tutorial that looks like it might be helpful but the thing is my data looks differently and I'm not really sure if this can be applied to my work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So basically, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How I can start on some very basic analysis? How to read data so it any meaning for me.&#xA;Any tips and advises will be much appreciated!&#xA;Note: I'm just the beginner in data science.  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to implement item based collaborative filtering. Do any distance calculations allow for weighting of certain ranges of values within each vector? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I would like to be able to say values 10..22 within each vector are more significant than values within the range 0..10. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been experimenting with Pearson, Tanimoto and Euclidean algorithms, but they all seem to assume equal weighting for each value within the vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I approaching this problem in the right way, and if not, how do others deal with this problem? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want the text-based semantic clustering EMD do.&#xA;Is there a better way of using LDA to detect topics in text, there are so provide better results?&#xA;I'm going to do my EMD on discovery topics.&#xA;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm interested in discovering some kind of dis-associations between the periods of a time series based on its data, e.g., find some (unknown number of) periods where the data is not similar with the data from another period.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also I would like to compare the same data but over 2 years (something like DTW?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I get my data Excel as a two-column list:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1=date (one per each day of the year), c2=Data To Analyze&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, what algorithms could I use and in what software?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update/Later edit:&lt;/strong&gt;&#xA;I'm looking for dates as cut-off points from which the DataToAnalyze could be part of another cluster of consecutive dates. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2014-1-1 --&amp;gt; 2014-3-10&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;are part of &lt;em&gt;Cluster_1&lt;/em&gt; based on DataToAnalyze. And:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2014-3-11 --&amp;gt; 2014-5-2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;are part of &lt;em&gt;Cluster_2&lt;/em&gt; based on DataToAnalyze, and so on. So, clusters of consecutive dates should be automatically determined based on some algorithms, which is what I'm looking for. Which ones (or which software) would be applicable to this problem?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to implement GD for standard task of NN training :) The best papers for practioneer I've founded so far are:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) &quot;Efficient BackProp&quot; by Yann LeCun et al.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) &quot;Stochastic Gradient Descent Tricks&quot; by Leon Bottou&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there some other must read papers on this topic?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am planning to use scikit linear support vector machine (SVM) classifier for text classification on a corpus consisting of 1 million labeled documents. What I am planning to do is, when a user enters some keyword, the classifier will first classify it in a category, and then a subsequent information retrieval query will happen in within the documents of that category catagory. I have a few questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How do I confirm that classification will not take much time? I don't want users to have to spend time waiting for a classification to finish in order to get better results.&lt;/li&gt;&#xA;&lt;li&gt;Is using Python's scikit library for websites/web applications suitable for this?&lt;/li&gt;&#xA;&lt;li&gt;Does anyone know how amazon or flipkart perform classification on user queries, or do they use a completely different logic?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a client that is managing several campaigns. However I'm not clear what percentage should be applied to each channel that bring traffic to my website, when assessing their participation in the objectives. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For those interested, here I leave the link to my profile on linkedin. &lt;a href=&quot;https://www.linkedin.com/pub/mario-mu%C3%B1oz-ahumada/52/287/28b&quot; rel=&quot;nofollow&quot;&gt;Specialist online markegin in Bogotá&lt;/a&gt;.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Below is the dataset where the response variable is play with two labels (yes, and no), &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;No. outlook temperature humidity    windy   play&#xA;1   sunny       hot     high        FALSE   no&#xA;2   sunny       hot     high        TRUE    no&#xA;3   overcast    hot     high        FALSE   yes&#xA;4   rainy       mild    high        FALSE   yes&#xA;5   rainy       cool    normal      FALSE   yes&#xA;6   rainy       cool    normal      TRUE    no&#xA;7   overcast    cool    normal      TRUE    yes&#xA;8   sunny       mild    high        FALSE   no&#xA;9   sunny       cool    normal      FALSE   yes&#xA;10  rainy       mild    normal      FALSE   yes&#xA;11  sunny       mild    normal      TRUE    yes&#xA;12  overcast    mild    high        TRUE    yes&#xA;13  overcast    hot     normal      FALSE   yes&#xA;14  rainy       mild    high        TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here are the decisions with their respective classifications: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1: (outlook,overcast) -&amp;gt; (play,yes) &#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]&#xA;&#xA;2: (humidity,normal), (windy,FALSE) -&amp;gt; (play,yes)&#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]&#xA;&#xA;3: (outlook,sunny), (humidity,high) -&amp;gt; (play,no) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]&#xA;&#xA;4: (outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 4]&#xA;&#xA;5: (outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 11]&#xA;&#xA;6: (outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Statement of problem: An ambulance is at the hospital dropping off a patient. The goal of the paramedic is to get released from the hospital as soon as possible. I am curious, what are the factors in how long an ambulance off loads a patient at the hospital? Can I predict how long an offload will take given certain variables. And how confident can I be in this model? The Dependent Variable is HospitalTime, it is a ratio type of data and is measured in seconds. The Independent Variables are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hospital, a nominal type of data recoded into integers, 1 would stand&#xA;for Lee Memorial.&lt;/li&gt;&#xA;&lt;li&gt;Ambulance, a nominal type of data recoded into integers, 9 would&#xA;stand for ambulance #9&lt;/li&gt;&#xA;&lt;li&gt;PatientPriority is an ordinal type of data recoded into integers. A 1&#xA;is a high priority, 2 is a medium priority and 3 is low acuity.&lt;/li&gt;&#xA;&lt;li&gt;MonthOfCall is an interval type of data recoded into integers. A 6&#xA;would be June and 12 is December. A 12 (December) is not twice as&#xA;much as a 6 (June) in this case.&lt;/li&gt;&#xA;&lt;li&gt;HourOfCall is an interval type of data recoded into integers. Once&#xA;again, an offload happening at 10:00 pm is not more than something&#xA;happening at 10:00 am.&lt;/li&gt;&#xA;&lt;li&gt;Officer1 and Officer2 are nominal data and are integers representing&#xA;an EMT and a paramedic.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My question is this: Given this type of data and my goal to predict the off loading time at the hospital, what kind of regression model should I look into?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked at my statistics books from university days and they are all using ratio data. My data is mixed with nominal, ordinal, interval and ratio.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have as much data as you could ask for. I have at least 100,000 observations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please push me in the right direction? What kind of model should I use with this type of data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Shown below are observations to give you a tiny peek at my data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;IncidentID,HospitalTime,Hospital,Ambulance,PatientPriority,MonthOfCall,HourOfCall,Officer1,Officer2&#xA;757620,1849,7,11,2,10,10,234,771,chr(10) 802611,2625,7,11,3,1,18,234,777,chr(10) &#xA;765597,1149,7,12,3,11,2,234,777,chr(10) 770926,1785,7,12,3,11,15,234,777,chr(10) &#xA;771689,3557,7,12,2,11,14,234,777,chr(10) 822758,1073,7,20,3,3,13,777,307,chr(10) &#xA;767249,2570,7,22,2,11,11,560,778,chr(10) 767326,1998,7,22,1,11,18,560,777,chr(10) &#xA;785903,1660,7,22,3,12,12,234,777,chr(10) 787644,2852,7,22,3,12,17,234,777,chr(10) &#xA;760294,1327,7,23,2,10,14,498,735,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) &#xA;994677,3653,7,32,2,2,15,181,159,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) &#xA;788471,2053,5,9,2,1,3,498,777,chr(10) 759983,1342,5,11,2,10,8,474,777,chr(10)&#xA;791243,1635,5,11,2,1,18,234,777,chr(10) 800796,1381,5,11,3,1,11,234,777,chr(10)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;P.S. This question is cross-posted in Stack-Overflow under the same title and author.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;The dataset that I am experimenting with is in the form of a table with columns userid and itemid. If there is a row for a given user and a given item, that means the user accessed the item (like in an online store). I am trying to cluster similar items based on this data. If a pair of items is accessed together often, then the items are similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because this is a case of a high dimensionality (# of users and items will be in 10,000's) I think I am justified in trying to use SVD as a pre-clustering step and then do some classical clustering. When I tried doing this I got poor clustering results when compared with simple hierarchical clustering. Items that weren't very similar were being bucketed together in one dimension, while there were available dimensions that weren't used. The results weren't completely random, but they were definitely worse than the output from the hierarchical clustering. I attempted the SVD step with Mahaut and Octave and the results were similar. For the hierarchical clustering I used the Jaccard measure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this point I am starting to doubt the notion of SVD as a way to reduce dimensionality. Do you think that SVD cannot be used effectively in this case (and why?) or do you think that I made some mistake along the way?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have several datasets with thousands of variables. This different datasets have different variables for the same thing. Is there a way to automatically/semi-automatically check compatible variables and make them consistent?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there is such thing, that would save me months of tedious work. The data is stored in SPSS format.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm coding a program that tests several classifiers over a database weather.arff, I found rules below, I want classify test objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand how the classification, it is described:&#xA;&quot;In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects. &quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to classify test objects?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;No. outlook temperature humidity    windy   play&#xA;1   sunny       hot     high        FALSE   no&#xA;2   sunny       hot     high        TRUE    no&#xA;3   overcast    hot     high        FALSE   yes&#xA;4   rainy       mild    high        FALSE   yes&#xA;5   rainy       cool    normal      FALSE   yes&#xA;6   rainy       cool    normal      TRUE    no&#xA;7   overcast    cool    normal      TRUE    yes&#xA;8   sunny       mild    high        FALSE   no&#xA;9   sunny       cool    normal      FALSE   yes&#xA;10  rainy       mild    normal      FALSE   yes&#xA;11  sunny       mild    normal      TRUE    yes&#xA;12  overcast    mild    high        TRUE    yes&#xA;13  overcast    hot     normal      FALSE   yes&#xA;14  rainy       mild    high        TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Rule found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1: (outlook,overcast) -&amp;gt; (play,yes) &#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]&#xA;&#xA;2: (humidity,normal), (windy,FALSE) -&amp;gt; (play,yes)&#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]&#xA;&#xA;3: (outlook,sunny), (humidity,high) -&amp;gt; (play,no) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]&#xA;&#xA;4: (outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 4]&#xA;&#xA;5: (outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 11]&#xA;&#xA;6: (outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Dung&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Many discussions of missing data in supervised (and unsupervised) learning deal with various methods of imputation, like mean values or EM. But in some cases the data will be missing as a necessary consequence of the data generation process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, let's say I'm trying to predict students' grades, and one of the inputs I want to analyze is the average grades of the student's siblings. If a particular student is an only child, then that value will be missing, not because we failed to collect the data, but because logically there is no data to collect. This is distinct from cases where the student has siblings, but we can't find their grades. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other examples abound: say we're in college admissions and we want to include students' AP exam results, but not all students took AP exams. Or we're looking at social network data, but not all subjects have facebook and/or twitter accounts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These data are missing, but they're certainly not missing at random. And many algorithms, such as all supervised learning packages in scikit-learn, simply demand that there be no missing values at all in the data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How have people dealt with this in the past, and what off-the-shelf solutions are there? For instance, I believe the gradient boosting algorithm in R uses trees with three possible branches: left, right, and missing. Any other alternatives out there?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've made a Naive Bayes classifier that uses the bag-of-words technique to classify spam posts on a message board. It works, but I think I could get much better results if my models considered the word orderings and phrases. (ex: 'girls' and 'live' may not trigger a high spam score, even though 'live girls' is most likely junk). How can I build a model that takes word ordering into account?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've considered storing n-grams (check-out-these, out-these-live, these-live-girls), but this seems to radically increase the size of the dictionary I keep score in and causes inconsistency as phrases with very similar wording but different order will slip through.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not tied to Bayesian classification, but I'd like something that someone without a strong background in statistics could grok and implement. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I would like to do some data mining and NLP experiments to do some research&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have decided to use NLTK or related tools and software&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which environment or operating system do you suggest for my purpose? I mean doing research on NLP&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Windows or Linux? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to Linux&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is your experience and your preferred OS?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As NLTK is in Python I thought Python is a good language for my purpose, do you suggest Python too?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Next week I'm going to begin prototyping a recommendation engine for work. I've implemented/completed the Netflix Challenge in Java before (for college) but have no real idea what to use for a production/enterprise level recommendation engine. Taking into consideration everything from a standalone programming language to things like Apache Mahout and Neo4j, does anyone have any advice on how to proceed?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Okay, here is the background:&#xA;I am doing text mining, and my basic flow is like this:&#xA;extract feature (n-gram), reduce feature count, score (tf-idf) and classify. for my own sake i am doing comparison between SVM and neural network classifiers. here is the weird part (or am i wrong and this is reasonable?), if i use 2gram the classifiers' result (accuracy/precision) is different and the SVM is the better one; but when i use 3-gram the results are exactly the same. what causes this? is there any explanation? is it the case of very separable classes?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for a supervised learning algorithm that can take 2d data for input and output. As an example of something similar to my data, consider a black image with some sparse white dots. Blur that image using a full range of grayscale. Then create a machine that can take the blurred image as input and produce the original sharp image as output. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could make some sample 1D data by taking a region/radius around the original sharp point, but I don't know the exact radius. It would be significant data duplication and a lot of guessing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any good algorithm suggestions for this problem? Thanks for your time.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a hands on researcher and I like testing out viable solutions, so I tend to run a lot of experiments. For example, if I am calculating a similarity score between documents, I might want to try out many measures. In fact, for each measure I might need to make several runs to test the effect of some parameters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I've been tracking the runs inputs and their results by writing out the results into  files with as much info about the inputs. The problem is that retrieving a specific result becomes a challenge sometimes, even if I try to add the input info to th filename. I tried using a spreadsheet with links to results but this isn't making a huge difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tools/process do you use for the book keeping of your experiments?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a 35 year old IT professional who is purely technical. I am good at programming, learning new technologies, understanding them and implementing. I did not like mathematics at school, so I didn't score well in mathematics. I am very much interested in pursuing a career in Big Data analytics. I am more interested in Analytics rather than Big Data technologies (Hadoop etc.), though I do not dislike it.  However, when I look around in the internet, I see that, people who are good in analytics (Data Scientists) are mainly Mathematics graduates  who have done their PHds and sound like intelligent creatures, who are far far ahead of  me. I get scared sometimes to think whether my decision is correct, because learning advance statistics on your own is very tough and requires a of hard work and time investment. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know whether my decision is correct, or should I leave this piece of work to only intellectuals who have spend their life in studying in prestigious colleges and earned their degrees and PHDs.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm curious if anyone else has run into this. I have a data set with about 350k samples, each with 4k sparse features. The sparse fill rate is about 0.5%. The data is stored in a &lt;code&gt;scipy.sparse.csr.csr_matrix&lt;/code&gt; object, with &lt;code&gt;dtype='numpy.float64'&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using this as an input to sklearn's Logistic Regression classifier. The &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt; indicates that sparse CSR matrices are acceptable inputs to this classifier. However, when I train the classifier, I get extremely bad memory performance; the memory usage of my process explodes from ~150 MB to fill all the available memory and then everything grinds to a halt as memory swapping to disk takes over.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know why this classifier might expand the sparse matrix to a dense matrix? I'm using the default parameters for the classifier at the moment, within an updated anacoda distribution. Thanks!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scipy.__version__ = '0.14.0'&#xA;sklearn.__version__ = '0.15.2'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have product purchase count data which looks likes this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;user item1 item2&#xA;   a     2     4&#xA;   b     1     3&#xA;   c     5     6&#xA;   ...   ...   ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These data are imported into &lt;code&gt;python&lt;/code&gt; using &lt;code&gt;numpy.genfromtxt&lt;/code&gt;. Now I want to process it to get the correlation between &lt;code&gt;item1&lt;/code&gt; purchase amount and &lt;code&gt;item2&lt;/code&gt; purchase amount -- basically for each value &lt;code&gt;x&lt;/code&gt; of &lt;code&gt;item1&lt;/code&gt; I want to find all the users who bought &lt;code&gt;item1&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt; quantity then average the &lt;code&gt;item2&lt;/code&gt; over the same users. What is the best way to do this? I can do this by using &lt;code&gt;for&lt;/code&gt; loops but I thought there might be something more efficient than that. Thanks!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to figure out a good (and fast) solution to the following problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two models I'm working with, let's call them players and teams. A player can be on multiple teams and a team can have multiple players). I'm working on creating a UI element on a form that allows a user to select multiple teams (checkboxes). As the user is selecting (or deselecting) teams, I'd like to display the teams grouped by the players.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for examples:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;If the selected teams have no players that intersect, each team would have its own section. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If the user selects two teams and they have the same players, there would be one section containing the names of the two teams and all the players.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If TEAM_A has players [1, 2, 4, 5] and TEAM_B has players [1, 3, 5, 6]. There would be the following sections: SECTION_X = [TEAM_A, TEAM_B, 1, 5], SECTION_Y = [TEAM_A, 2, 3], SECTION _Z = [TEAM_B, 3, 5]&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I hope that's clear. Essentially, I want to find the teams that players have in common and group by that. I was thinking maybe there is a way to do this by navigating a bipartite graph? Not exactly sure how though and I might be overthinking it. I was hoping to do this by creating some type of data structure on the server and using it on the client. I would love to hear your suggestions and I appreciate any help you can give!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Lets say I have a database of users who rate different products on a scale of 1-5. Our recommendation engine recommends products to users based on the preferences of other users who are highly similar. My first approach to finding similar users was to use Cosine Similarity, and just treat user ratings as vector components. The main problem with this approach is that it just measures vector angles and doesn't take rating scale or magnitude into consideration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My question is this:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there any drawbacks to&lt;/strong&gt; &lt;strong&gt;just using the percentage difference between the vector components of two vectors as a measure of similarity&lt;/strong&gt;? What disadvantages, if any, would I encounter if I used that method, instead of Cosine Similarity or Euclidean Distance?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For Example, why not just do this:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n = 5 stars&#xA;a = (1,4,4)&#xA;b = (2,3,4)&#xA;&#xA;similarity(a,b) = 1 - ( (|1-2|/5) + (|4-3|/5) + (|4-4|/5) ) / 3 = .86667&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Instead of Cosine Similarity :&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a = (1,4,4)&#xA;b = (2,3,4)&#xA;&#xA;CosSimilarity(a,b) = &#xA;(1*2)+(4*3)+(4*4) / sqrt( (1^2)+(4^2)+(4^2) ) * sqrt( (2^2)+(3^2)+(4^2) ) = .9697&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;how to get the Polysemes of a word in wordnet or any other api. I am looking for any api. with java any idea is appreciated?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I know there is the normal &lt;em&gt;subtract the mean and divide by the standard deviation&lt;/em&gt; for standardizing your data, but I'm interested to know if there are more appropriate methods for this kind of discrete data. Consider the following case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 5 items that have been ranked by customers. First 2 items were ranked on a 1-10 scale. Others are 1-100 and 1-5. To transform everything to a 1 to 10 scale, is there another method better suited for this case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the data has a central tendency, then the standard would work fine, but what about when you have more of a halo effect, or some more exponential distribution?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I do movement building work for Effective Altruism (&lt;a href=&quot;http://en.m.wikipedia.org/wiki/Effective_altruism&quot; rel=&quot;nofollow&quot;&gt;http://en.m.wikipedia.org/wiki/Effective_altruism&lt;/a&gt;), and would like to level up our growth strategy. It occurred to me that a social network visualization tool which allowed us to strategically find and recruit new influencers/donors would be mega useful. I'd love to find something (preferably free), similar to InMaps, which would allow us to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Combine all of our social media connections into a single map&lt;/li&gt;&#xA;&lt;li&gt;Easily see who the superconnectors are&lt;/li&gt;&#xA;&lt;li&gt;Weight each person by their degree of social influence (perhaps some function of things like Klout score * amount of social media connections * number of Google mentions, etc) &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Does such a thing exist? If not, is anyone interested in pro bono work for an amazing cause? =)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I am a data science noob, so preferably the solution would be one with a nice GUI and minimal involvement of R or Python.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Suppose, for example, that the first search result on a page of Google search results is swapped with the second result. How much would this change the click-through probabilities of the two results? How much would its click-through probability drop if the fifth search result was swapped with the sixth? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can we say something, with some level of assurance, about how expected click-through probabilities change if we do these types of pairwise swaps within pages of search results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What we seek is a measure of the contribution to click-through rates made specifically by position bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Likely, how position ranking would affect the sales in Amazon or other online shopping website? If we cast the sales into two parts, the product quality and its ranking effect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;sales = alpha*quality + beta*position + epsilon&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we quantify the beta?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;When ML algorithms, e.g. Vowpal Wabbit or some of the factorization machines winning click through rate competitions (&lt;a href=&quot;https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution/55862#post55862&quot;&gt;Kaggle&lt;/a&gt;), mention that features are 'hashed', what does that actually mean for the model? Lets say there is a variable that represents the ID of an internet add, which takes on values such as '236BG231'. Then I understand that this feature is hashed to a random integer. But, my question is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is the integer now used in the model, as an integer (numeric) OR&lt;/li&gt;&#xA;&lt;li&gt;is the hashed value actually still treated like a categorical variable and one-hot-encoded? Thus the hashing trick is just to save space somehow with large data?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;let's assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or &quot;epoch&quot;, I use each training sample exactly once after randomly reordering the whole training set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody know any vectorized implementation of stochastic gradient descent? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: I've been asked why would I like to use online gradient descent if the size of my dataset is fixed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From [1], one can see that online gradient descent converges slower than batch gradient descent to the minimum of the empirical cost. However, it converges faster to the minimum of the expected cost, which measures generalization performance. I'd like to test the impact of these theoretical results in my particular problem, by means of cross validation. Without a vectorized implementation, my online gradient descent code is much slower than the batch gradient descent one. That remarkably increases the time it takes to the cross validation process to be completed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: I include here the pseudocode of my on-line gradient descent implementation, as requested by ffriend. I am solving a regression problem.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Method: on-line gradient descent (regression)&#xA;Input: X (nxp matrix; each line contains a training sample, represented as a length-p vector), Y (length-n vector; output of the training samples)&#xA;Output: A (length-p+1 vector of coefficients)&#xA;&#xA;Initialize coefficients (assign value 0 to all coefficients)&#xA;Calculate outputs F&#xA;prev_error = inf&#xA;error = sum((F-Y)^2)/n&#xA;it = 0&#xA;while abs(error - prev_error)&amp;gt;ERROR_THRESHOLD and it&amp;lt;=MAX_ITERATIONS:&#xA;    Randomly shuffle training samples&#xA;    for each training sample i:&#xA;        Compute error for training sample i&#xA;        Update coefficients based on the error above&#xA;    prev_error = error&#xA;    Calculate outputs F&#xA;    error = sum((F-Y)^2)/n&#xA;    it = it + 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;[1] &quot;Large Scale Online Learning&quot;, L. Bottou, Y. Le Cunn, NIPS 2003.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I hope you can help me, as I have some questions on this topic. I'm new in the field of deep learning, and while I did some tutorials, I can't relate or distinguish concepts from one another.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I need some help with a single layered perceptron with multiple classes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need to do is classify a dataset with three different classes, by now I just learnt how to do it with two classes, so I have no really a good clue how to do it with three.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The dataset have three different classes: Iris-setosa, Iris-versicolor and Iris-versicolor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The url with the dataset and the information is in : &lt;a href=&quot;http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data&quot; rel=&quot;nofollow&quot;&gt;http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I really appreciate any help anyone can give to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have some very complicated data about some movie sales online, first for each data entry, I have a key which is a combination of five keys, which are territory, day, etc, and then, for each key I have the sales for a period of time, and other information, like the movie's box office and genre. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each day, there is a delay for the data loading to the database, around ten hours, I try to fill the gap, do some data extrapolations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each movie we sell, there is some decay of selling since the new release of the movie, i.e. usually for each movie, it follows some sales decay pattern.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a recent day, I pulled some data, and I found that some decay pattern:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/H0lSJ.png&quot; alt=&quot;decay curve 1&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/L5DAf.png&quot; alt=&quot;decay curve 2&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/RJPjK.png&quot; alt=&quot;decay curve 3&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And for that day, the sales for each key can range from around $150000 to $0. The pic is as follow:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/o7tsq.png&quot; alt=&quot;one day sales curve&quot;&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the picture, the 15000 means there are around 15000 keys for each day. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;found this article,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&amp;amp;Pyo.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&amp;amp;Pyo.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to predict for each key, the sales amount, like for a movie, territory, day etc combination, the sales amount, how much dollars, means for that movie, that territory, that day, how much money we get from selling online. I tried ARIMA time series model, but there is some concerns for that model, seen from the pics, there is some seasonal thing, and decay thing for the movie, so the sales prediction can not be always flat, there may be a pump after a going down, it may happens on a weekend, since there is seasonal thing, and the decay trend, etc, how to capture these things. Thank you for your reply!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not sure whether can be applied, and how to be applied here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot in advance. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;First of all I know the question may be not suitable for the website but I'd really appreciate it if you just gave me some pointers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm a 16 years old programmer, I've had experience with many different programming languages, a while ago I started a course at Coursera, titled introduction to machine learning and since that moment i got very motivated to learn about AI, I started reading about neural networks and I made a working perceptron using Java and it was really fun but when i started to do something a little more challenging (building a digit recognition software), I found out that I have to learn a lot of math, I love math but the schools here don't teach us much, now I happen to know someone who is a math teacher do you think learning math (specifically calculus) is necessary for me to learn AI or should I wait until I learn those stuff at school?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also what other things would be helpful in the path of me learning AI and machine learning? do other techniques (like SVM) also require strong math?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry if my question is long, I'd really appreciate if you could share with me any experience you have had with learning AI.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I was wondering if someone could point me to suitable database formats for building up a user database:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;basically I am collecting logs of impressions data, and I want to compile a user database &lt;/p&gt;&#xA;&#xA;&lt;p&gt;which sites user visits, country/gender/..? and other categorisations with the aim of &#xA;a) doing searches: give me all users visiting games sites from france...&#xA;b) machine learning: eg clustering users by the sites they visit&lt;/p&gt;&#xA;&#xA;&lt;p&gt;so I am interested in storing info about 100's of millions of users&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with indexes? on user, sites, geo-location&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what are suitable database systems. Can someone suggest suitable reading material? &#xA;I was imagining Hbase might be suitable...&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am new to Natural Language Processing, I think NLP is a challenging field, the syntax and semantic ambiguities could cause a lot of problems. For example I think for these problems machine translation is a hard task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore there are probably many approaches and methods that have been applied to this field. But what are the latest and most promising approaches and methods in the field of NLP?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are these techniques highly dependent on the target language?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking for an online console for the language R. Like I write the code and the server should execute and provide me with the output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similar to the website Datacamp.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a variable whose value I would like to predict, and I would like to use only one variable as predictor. For instance, predict traffic density based on weather.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, I thought about using &lt;a href=&quot;http://en.wikipedia.org/wiki/Self-organizing_map&quot;&gt;Self-Organizing Maps&lt;/a&gt; (SOM), which performs unsupervised clustering + regression. However, since it has an important component of dimensionality reduction, I see it as more appropriated for a large number of variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this &lt;em&gt;simple&lt;/em&gt; case: I used &quot;Data Mining&quot; instead of &quot;machine learning&quot; in the title of my question, because I think maybe a linear regression could do the job...&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&quot; rel=&quot;nofollow noreferrer&quot;&gt;Automated Time Series Forecasting for Biosurveillance&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;in the above paper, page 4, two models, non-adaptive regression model, adaptive regression model, the non-adaptive regression model's parameter estimation method is &quot;least squares&quot;, what is the parameter estimation for the adaptive regression model? is there any package in R to do parameter estimation for this kind of adaptive regression model? If I add more predictors in the adaptive regression model, can R still solve it? and how?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How would I do parameter estimation and prediction for the adaptive regression model using R, as in the 4th page of the paper linked below?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&quot; rel=&quot;nofollow&quot;&gt;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone clarify this for me?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you know adaptive regression models very well, share some useful link, or describe the model/parameter estimation/prediction, that would be very helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you so much!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I want to identifies different queries in sentences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like - &lt;code&gt;Who is Bill Gates and where he was born?&lt;/code&gt; or &lt;code&gt;Who is Bill Gates, where he was born?&lt;/code&gt; contains two queries &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Who is Bill Gates?&lt;/li&gt;&#xA;&lt;li&gt;Where Bill Gates was born&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I worked on Coreference resolution, so I can identify that &lt;code&gt;he&lt;/code&gt; points to &lt;code&gt;Bill Gates&lt;/code&gt; so resolved sentence is &quot;Who is Bill Gates, where Bill Gates was born&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like wise&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MGandhi is good guys, Where he was born?&#xA;single query&#xA;who is MGandhi and where was he born?&#xA;2 queries&#xA;who is MGandhi, where he was born and died?&#xA;3 quries&#xA;India won world cup against Australia, when?&#xA;1 query (when India won WC against Auz)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can perform Coreference resolution (Identifying and converting &lt;code&gt;he&lt;/code&gt; to &lt;code&gt;Gandhi&lt;/code&gt;) but not getting how can I distinguish queries in it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to do this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to find &quot;Sentence disambiguation&quot; like &quot;word sense disambiguation&quot;, but nothing exist like that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help or suggestion would be much appreciable. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Recently in a data analytic job interview for an e-commerce site, they asked me, do i have some knowledge of buyer classification problem. Unfortunately i heard this term for the first time.&lt;br&gt;&#xA;After interview i tried to search a lot about it over google but didn't find something meaningful. Please any one let me know if you have heard this term before and paste some links explaining this concept. Thanks   &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am kind of a newbie on machine learning and I would like to ask some questions based on a problem I have .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say I have x y z as variable and I have values of these variables as time progresses like :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;t0  = x0 y0 z0  &lt;br&gt;&#xA;t1  = x1 y1 z1  &lt;br&gt;&#xA;tn  = xn yn zn  &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want a model that when it's given 3 values of x , y , z I want a prediction of them like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input : x_test y_test z_test &#xA;Output : x_prediction y_prediction z_prediction&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These values are float numbers. What is the best model for this kind of problem? &#xA;Thanks in advance for all the answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More details:&#xA;Ok so let me give some more details about the problems so as to be more specific.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have run certain benchmarks and taken values of performance counters from the cores of a system per interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The performance counters are the x , y , z in the above example.They are dependent to each other.Simple example is x = IPC , y  =  Cache misses , z  = Energy at Core.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I got this dataset of all these performance counters per interval .What I want to do is create a model that after learning from the training dataset , it will be given a certain state of the core ( the performance counters) and predict the performance counters that the core will have in the next interval.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to setup a big data infrastructure using Hadoop, Hive, Elastic Search (amongst others), and I would like to run some algorithms over certain datasets. I would like the algorithms themselves to be scalable, so this excludes using tools such as Weka, R, or even RHadoop. The &lt;a href=&quot;https://mahout.apache.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;Apache Mahout Library&lt;/a&gt; seems to be a good option, and it features &lt;a href=&quot;https://mahout.apache.org/users/basics/algorithms.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;algorithms for regression and clustering tasks&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I am struggling to find is a solution for anomaly or outlier detection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since Mahout features Hidden Markov Models and a variety of clustering techniques (including K-Means) I was wondering if it would be possible to build a model to detect outliers in time-series, using any of this. I would be grateful if somebody experienced on this could advice me&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;if it is possible, and in case it is&lt;/li&gt;&#xA;&lt;li&gt;how-to do it, plus&lt;/li&gt;&#xA;&lt;li&gt;an estimation of the effort involved and&lt;/li&gt;&#xA;&lt;li&gt;accuracy/problems of this approach.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for a topic for my masters thesis. Machine learning is my primary domain and I want to work on probabilistic models and applied probability in Machine Learning. Please suggest some exciting new topics that would make for a good masters thesis subject.&#xA;Anything related to Markov chains Monte Carlo, Bayesian methods, Probabilistic graphical models, Markov models and so on in context of machine learning would be great!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've built a toy Random Forest model in &lt;code&gt;R&lt;/code&gt; (using the &lt;code&gt;German Credit&lt;/code&gt; dataset from the &lt;code&gt;caret&lt;/code&gt; package), exported it in &lt;code&gt;PMML 4.0&lt;/code&gt; and deployed onto Hadoop, using the &lt;code&gt;Cascading Pattern&lt;/code&gt; library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've run into an issue where &lt;code&gt;Cascading Pattern&lt;/code&gt; scores the same data differently (in a binary classification problem) than the same model in &lt;code&gt;R&lt;/code&gt;. Out of 200 observations, 2 are scored differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this? Could it be due to a difference in the implementation of Random Forests? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to analyze the effectiveness and efficiency of kernel methods for which I would require 3 different data-set in 2 dimensional space for each of the following cases:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;BAD_kmeans: The data set for which the kmeans clustering algorithm&#xA;will not perform well.&lt;/li&gt;&#xA;&lt;li&gt;BAD_pca: The data set for which the Principal Component Analysis&#xA;(PCA) dimension reduction method upon projection of the original&#xA;points into 1-dimensional space (i.e., the first eigenvector) will&#xA;not perform well.&lt;/li&gt;&#xA;&lt;li&gt;BAD_svm: The data set for which the linear Support Vector Machine&#xA;(SVM) supervised classification method using two classes of points&#xA;(positive and negative)  will not perform well.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Which packages can I use in R to generate the random 2d data-set for each of the above cases ? A sample script in R would help in understanding&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I would like to learn both Python and R for usage in data science projects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently unemployed, fresh out of university, scouting around for jobs and thought it would be good if I get some Kaggle projects under my profile.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have very little knowledge in either language. Have used Matlab and C/C++ in the past. But I haven't produced production quality code or developed an application or software in either language. It has been dirty coding for academic usage all along.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have used a little bit of Python, in a university project, but I dont know the fundamentals like what is a package , etc etc. ie havent read the intricacies of the language using a standard Python Textbook etc..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have done some amount of coding in C/C++ way back (3-4 years back then switched over to Matlab/Octave).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to get started in Python Numpy Scipy scikit-learn and pandas etc. but just reading up Wikipedia articles or Python textbooks is going to be infeasible for me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And same goes with R, except that I have zero knowledge of R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any suggestions?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is there a good java library for doing time series energy consumption forecasting based on weather data and other variables?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Most Treebank conversion which I found in the web are from constituency treebank to dependency treebank, I wonder why there is little jobs in the opposite direction?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am looking for a thesis to complete my master M2, I will work on a topic in the big data's field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some topics or project that would make for a good masters thesis subject.&lt;br&gt;&#xA;I add that I have bases in data warehouses, databases, data mining, good skills in programming, system administration and cryptography ... &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Where is the difference between one-class, binary-class and multinominal-class classification?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Couldn't I just use all the methods that I mentioned above to reach my goal?&#xA;e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C1 -&gt; 1 or 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C2 -&gt; 1 or 0&#xA;... and so on ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But where is the difference between this two methods? (except of that I have to use different algorithms)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;A Random Forest (RF) is created by an ensemble of Decision Trees's (DT). By using bagging, each DT is trained in a different data subset. Hence, &lt;strong&gt;is there any way of implementing an on-line random forest by adding more decision tress on new data?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, we have 10K samples and train 10 DT's. Then we get 1K samples, and instead of training again the full RF, we add a new DT. The prediction is done now by the Bayesian average of 10+1 DT's.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition, if we keep all the previous data, the new DT's can be trained mainly in the new data, where the probability of picking a sample is weighted depending how many times have been already picked.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Where is the difference between one-class, binary-class and multinominal-class classification?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Couldn't I just use all the methods that I mentioned above to reach my goal?&#xA;e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C1 -&gt; 1 or 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C2 -&gt; 1 or 0&#xA;... and so on ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But where is the difference between this two methods? (except of that I have to use different algorithms)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am drawing samples from two classes in the two-dimensional Cartesian space, each of which has the same covariance matrix $[2, 0; 0, 2]$.  One class has a mean of $[1.5, 1]$ and the other has a mean of $[1, 1.5]$.  If the priors are $4/7$ for the former and $3/7$ for the later, how would I derive the equation for the ideal decision boundary?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it turns out that misclassifying the second class is twice as expensive as the first class, and the objective is to minimize the expected cost, what equation would I use for the best decision boundary?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Are there any good sources that explain how decision trees can be implemented in a scalable way on a distributed computing system.  Where in a given source is this explained?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Can anyone explain how field-aware factorization machines (FFM) compare to standard Factorization Machines (FM)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Standard:&#xA;&lt;a href=&quot;http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&quot;&gt;http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Field Aware&quot;:&#xA;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf&quot;&gt;http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf&lt;/a&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;There're many data points, each of which is associated with two coordinates and a numeral value, or three coordinates. And I wish it is coloured.&#xA;I checked packages &quot;scatterplot3d&quot; and &quot;plot3D&quot; but I couldn't find one like the example I give. It is like it has a fitting surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data is basically like the following. In this way I think this kind of plot is gonna be perfectly suitble for this data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ki,kt,Top10AverageF1Score&#xA;    360,41,0.09371256716549396&#xA;    324,41,0.09539634212851525&#xA;    360,123,0.09473510831594467&#xA;    36,164,0.09773486852645874&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I also may have one more additional variable, which makes it like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    NeighborhoodSize,ki,kt,Top10AverageF1Score&#xA;    10,360,41,0.09371256716549396&#xA;    15,324,41,0.09539634212851525&#xA;    15,360,123,0.09473510831594467&#xA;    20,36,164,0.09773486852645874&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Do you also have any good idea for visualizing the second case? What kind of plot and which packages and functions, etc.&lt;/strong&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/5whM7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want to analyze &lt;a href=&quot;http://grouplens.org/datasets/movielens/&quot; rel=&quot;nofollow&quot;&gt;MovieLens data set&lt;/a&gt; and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according &lt;code&gt;'userId'&lt;/code&gt; and &lt;code&gt;'Time'&lt;/code&gt; columns. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [36]: df.head(10)&#xA;Out[36]: &#xA;        userId  movieId  Rating       Time                         movieName  \\\\&#xA;40034        1      150       5  978301777                  Apollo 13 (1995)   &#xA;77615        1     1028       5  978301777               Mary Poppins (1964)   &#xA;550485       1     2018       4  978301777                      Bambi (1942)   &#xA;400889       1     1962       4  978301753         Driving Miss Daisy (1989)   &#xA;787274       1     1035       5  978301753        Sound of Music, The (1965)   &#xA;128308       1      938       4  978301752                       Gigi (1958)   &#xA;497972       1     3105       5  978301713                 Awakenings (1990)   &#xA;28417        1     2028       5  978301619        Saving Private Ryan (1998)   &#xA;6551         1     1961       5  978301590                   Rain Man (1988)   &#xA;35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)   &#xA;&#xA;                            genre  &#xA;40034                       Drama  &#xA;77615   Children's|Comedy|Musical  &#xA;550485       Animation|Children's  &#xA;400889                      Drama  &#xA;787274                    Musical  &#xA;128308                    Musical  &#xA;497972                      Drama  &#xA;28417            Action|Drama|War  &#xA;6551                        Drama  &#xA;35492        Action|Crime|Romance  &#xA;&#xA;[10 rows x 6 columns]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can not understand that the same user with user Id 1 see or rated the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id:2018) exactly at the same time (up to the milleseconds). If somebody works already with this data set, please, clear this situation.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am not sure whether I formulated the question correctly. Basically, what I want to do is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's suppose I have a list of 1000 strings which look like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;cvzxcvzx&lt;strong&gt;string&lt;/strong&gt;cvzcxvz&lt;/p&gt;&#xA;&#xA;&lt;p&gt;otortorotr&lt;strong&gt;string&lt;/strong&gt;grptprt&lt;/p&gt;&#xA;&#xA;&lt;p&gt;vmvmvmeop&lt;strong&gt;string2&lt;/strong&gt;vmrprp&lt;/p&gt;&#xA;&#xA;&lt;p&gt;vccermpqp&lt;strong&gt;string2&lt;/strong&gt;rowerm&lt;/p&gt;&#xA;&#xA;&lt;p&gt;proororor&lt;strong&gt;string3&lt;/strong&gt;potrprt&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mprto2435&lt;strong&gt;string3&lt;/strong&gt;famerpaer&lt;/p&gt;&#xA;&#xA;&lt;p&gt;etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to extract these reoccuring strings that occur on the list. What solution should I use? Does anyone know about algorithm that could do this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I need some serious help. I am supposed to implement a project (Non-Existing as of now) for my Machine Learning course. I have no basics in AI or Data mining or Machine learning. I have been searching for a while and unable to find something that i can finish implementing in 3-4 weeks time. It carries a huge chunk of my final marks and no matter how much i try i am unable to understand how it works!&#xA;Can the machine learning masters please help me out with this. I need a project suggestion to start with. And i want to know how to proceed after gathering the data set. I am totally blank and running out of time for my graduation :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Appreciate your suggestions! Thanks in advance.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a question about memory usage. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to do 4 things:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1) make a dataframe from one of several columns from a datasource, say a json string&#xA;2) make the third column of the original dataset the index to the dataframe&#xA;3) change the name of another column&#xA;4) change the series i've created to a dataframe&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question is about memory efficiency. It seems that for step 1), I am first loading a whole dataframe, then run a concat command to concatenate the columns I want. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For step 2, I again need to resave the new dataframe as another object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For step 3, it seems to stick so nothing there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please advise on a more efficient way to go about this, if that exists.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   df = pd.DataFrame(jsonobject)&#xA;   df = df.set_index(&quot;columnC&quot;)&#xA;   df.index.names= [&quot;foo&quot;]&#xA;   df1 = df[&quot;foo&quot;].map(lambda x:x[&quot;id&quot;])&#xA;   df2 = pd.DataFrame(df1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to change this to a model of daily exception reporting and weekly trend reporting. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Features might include: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;User report consolidation (so there's only one daily email)&lt;/li&gt;&#xA;&lt;li&gt;report ordering based upon level of variance from past performance (see the most important stuff first)&lt;/li&gt;&#xA;&lt;li&gt;HTML email support (with my audience, pretty counts)&lt;/li&gt;&#xA;&lt;li&gt;Web interface to allow preference changes, including LDAP support (make administration easier)&lt;/li&gt;&#xA;&lt;li&gt;Unsubscribe feature at the report level&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here's what I'd like to know:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What are the practical problems I might run into?&lt;/li&gt;&#xA;&lt;li&gt;What is the best way to display the new reports?&lt;/li&gt;&#xA;&lt;li&gt;How should  I define an &quot;exception&quot;? How can I know if my definition is a good one?&lt;/li&gt;&#xA;&lt;li&gt;I assume I'd be using a mix of Python, SQL, and powershell. Anything else I should consider, e.g. R? What are some good resources?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have Train and Test data, how to calculate classification accuracy with confusion matrix ? Thanks&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@attribute outlook {sunny, overcast, rainy}&#xA;@attribute temperature {hot, mild, cool}&#xA;@attribute humidity {high, normal}&#xA;@attribute windy {TRUE, FALSE}&#xA;@attribute play {yes, no}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Train:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1   sunny       hot     high    FALSE   no&#xA;2   sunny       hot     high    TRUE    no&#xA;3   overcast    hot     high    FALSE   yes&#xA;4   rainy       mild    high    FALSE   yes&#xA;5   rainy       cool    normal  FALSE   yes&#xA;6   rainy       cool    normal  TRUE    no&#xA;7   sunny       cool    normal  FALSE   yes&#xA;8   rainy       mild    normal  FALSE   yes&#xA;9   sunny       mild    normal  TRUE    yes&#xA;10  overcast    mild    high    TRUE    yes&#xA;11  overcast    hot     normal  FALSE   yes&#xA;12  rainy       mild    high    TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Test:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;overcast    cool    normal  TRUE    yes&#xA;sunny       mild    high    FALSE   no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Rules found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(humidity,normal), (windy,FALSE) -&amp;gt; (play,yes) [Support=0.33 , Confidence=1.00 , Correctly Classify= 4, 8, 9, 12]&#xA;(outlook,overcast) -&amp;gt; (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 2, 11]&#xA;(outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 3]&#xA;(outlook,sunny), (temperature,hot) -&amp;gt; (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 0, 1]&#xA;(outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) [Support=0.17 , Confidence=1.00 , Correctly Classify= 10]&#xA;(outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 5, 13]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;For my Computational Intelligence class, I'm working on classifying short text. One of the papers that I've found makes a lot of use of &lt;em&gt;granular computing&lt;/em&gt;, but I'm struggling to find a decent explanation of what exactly it is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I can gather from the paper, it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I'm asking about rough sets as well, because I'm curious about them and how they relate to fuzzy sets. If at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: &lt;a href=&quot;http://ijcai.org/papers11/Papers/IJCAI11-298.pdf&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the paper I'm referencing.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have data collected from a computer simulation of football games which seem to have recurring patterns of the following form.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;if madrid plays arsernal and the match ends under 3 goal, then on their next match against each others, madrid will win. if madrid happens to loose and then plays against chelsea next, they will win 90% of the time.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;how do I find such inferences from simulation generated data like this. There are other forms of hidden patterns that I believe exists in the dataset.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I cant seem to figure out why I have a high percentage error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to get a perceptron between X1 and X2 which are Gaussian distributed data sets with distinct means and identical co-variances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;N=200;&#xA;C= [2 1; 1 2]; %Covariance&#xA;m1=[0 2];&#xA;m2=[1.5 0];%mean&#xA;X1 = mvnrnd(m1, C, N/2);&#xA;X2 = mvnrnd(m2, C, N/2);&#xA;&#xA;X = [X1; X2];&#xA;X = [X ones(N,1)]; %bias&#xA;y = [-1*ones(N/2,1); ones(N/2,1)]; %classification&#xA;&#xA;%Split data into training and test &#xA;ii = randperm(N);&#xA;Xtr = X(ii(1:N/2),:);&#xA;ytr = X(ii(1:N/2),:);&#xA;Xts = X(ii(N/2+1:N),:);&#xA;yts = y(ii(N/2+1:N),:);&#xA;Nts = N/2;&#xA;&#xA;w = randn(3,1);&#xA;eta = 0.001;&#xA;%learn from training set&#xA;for iter=1:500 &#xA;j = ceil(rand*N/2);&#xA;if( ytr(j)*Xtr(j,:)*w &amp;lt; 0)&#xA;w = w + eta*Xtr(j,:)'; &#xA;end&#xA;end&#xA;&#xA;%apply what you have learnt to test set&#xA;yhts = Xts * w;&#xA;disp([yts yhts])&#xA;PercentageError = 100*sum(yts .*yhts &amp;lt; 0)/Nts;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong and how can I address this challenge?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Sorry, if this topic is not connected directly to Data Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to understand how the &lt;a href=&quot;http://graphlab.com/learn/gallery/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Graphlab tool&lt;/a&gt; works. Firstly I want to execute the toy examples from the Gallery site. When I try to execute the example code, everything is OK except one command: I can not see the graphlab plot after &lt;code&gt;show()&lt;/code&gt;. The command &lt;code&gt;show()&lt;/code&gt; returns to me some kind of object in IPython and nothing in the IPython Notebook.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the example code has the plot, which depends directly on the matplotlib module, I can produce the real plots and save it on my machine. Consequently, I suppose the main error depends on the graphlab (or object from its class). &#xA;If somebody already used this tool and rendered the plot, can he/she tell me, how I can execute the plots command?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [8]: import graphlab&#xA;&#xA;In [9]: from IPython.display import display&#xA;&#xA;        from IPython.display import Image&#xA;&#xA;        graphlab.canvas.set_target('ipynb')&#xA;&#xA;In [10]:import urllib&#xA;&#xA;        url = 'https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv'&#xA;&#xA;        urllib.urlretrieve(url, filename='freebase_performances.csv')  # downloads an 8MB file to the working directory&#xA;&#xA;Out[10]: ('freebase_performances.csv', &amp;lt;httplib.HTTPMessage instance at 0x7f44e153cf38&amp;gt;)&#xA;&#xA;In [11]: data = graphlab.SFrame.read_csv('remote://freebase_performances.csv', column_type_hints={'year': int})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;...&#xA;...&#xA;...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [15]:data.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;No plot after this line&#xA;...&#xA;...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [19]:print data.show()&#xA;&#xA;&amp;lt;IPython.core.display.Javascript object at 0x7f44e14c0850&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The object of graphlab (?) after print command&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Our system allows an admin to manage a database of university courses. These courses have multiple fields, like the department, a title, and a description.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am adding the ability to add learning objectives to a course. To simplify the problem, let's say that learning objectives are just tags. Courses can have more than one learning objective associated with them. So a course like CHEM 101 might have &quot;chemistry&quot;, &quot;technology&quot;, &quot;science&quot;, and several others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming I can reduce a course to a set of features, (using keywords/stemming/nlp, I suppose?), what kind of problem is this and what algorithm would you suggest? It seems very similar to a classification problem, but I want to provide a sorted list of suggestions with the most relevant at the top.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm training a NN with 8 features and 8000 training examples with a single output (0, 1) using the scipy.optimise CG algorithm and the results are somewhat inconsistent. The goal is to get the NN to be as 'precise' as possible (recall doesn't really matter too much) so I've set the threshold for y value quite high (0.75). Most of the time it gets a precision of around 80%, however sometimes it fails (using exactly the same parameters, lambda etc..) to generate any outputs which are above the 0.75 threshold, meaning the precision equals 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've successfully trained NNs before without these unusual results (albeit the goal was a somewhat more conventional multi-class classifier with many more features).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering if the training NNs with fewer features increases the chances of it getting stuck at a local optima; or getting stuck at local optima has a more significant impact on NNs with fewer features?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any thoughts on what's going on!?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the common/best practices to handle time data for machine learning application?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if in data set there is a column with timestamp of event, such as &quot;2014-05-05&quot;, how you can extract useful features from this column if any?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;The most online tutorials like to use a simple example to introduce to machine learning by classify unknown text in spam or not spam. They say that this is a binary-class problem. But why is this a binary-class problem? I think it is a one-class problem! I do only need positive samples of my inbox to learn what is not spam. If I do take a bunch of not spam textes as positiv samples and a bunch of spam-mails as negativ samples, then of course it's possible to train a binary-classifier and make predictions from unlabeled data, but where is the difference to the onc-class-approach? There I would just define a training-set of all non spam examples and train some one-class classifier. What do you think?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm doing some data analysis in a Statistical Pattern Recognition course using PRML. We analyzed a lot of matrix properties, like eigenvalues, column independence, positive semi-definite matrix, etc. When we are doing, for example, linear regression, we need to calculate some of those properties, and fit them into the equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is, my question is about the intuition behind these matrix properties, and their implications in the ML/DM literature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone could answer, can you teach me what is the importance of eigenvalue, positive semi-definite matrix, and column independence for ML/DM. And possibly, other important matrix properties you think important in study the dataset, and why.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be really appreciated if someone can answer this question.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm examining the activity of customers over the years which have about one event per year. This results is many short time-series for which I found the distributions (hit/miss over 4 years sorted by probability in the data):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0000 : 0.31834&#xA;0001 : 0.17582&#xA;0010 : 0.13605&#xA;0100 : 0.13554&#xA;1000 : 0.12886&#xA;0011 : 0.01717&#xA;1100 : 0.01650&#xA;0110 : 0.01578&#xA;0101 : 0.01220&#xA;1010 : 0.01117&#xA;1001 : 0.00883&#xA;0111 : 0.00571&#xA;1110 : 0.00565&#xA;1111 : 0.00496&#xA;1101 : 0.00384&#xA;1011 : 0.00351&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Apparently a purely uncorrelated binomial model wouldn't do, but one can observe that if both, the number of 1's and 11's coincide, then the probabilities are approximately equal (apart from a small recency effect of 0001).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you see a way to approach such data to deduce a probabilistic model? Basically where I have only a few probability parameters which roughly explain this distribution?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm planing to write a classification program that is able to classify unknown text in around 10 different categories and if none of them fits it would be nice to know that. It is also possible that more then one category is right.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My predefined categories are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1 = &quot;politics&quot;&#xA;c2 = &quot;biology&quot;&#xA;c3 = &quot;food&quot;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about the right approach in how to represent my training-data or what kind of classification is the right one. The first challenge is about finding the right features. If I only have text (250 words each) what method would you recommend to find the right features? My first approach is to remove all stop-words and use the POS-Tagger (&lt;a href=&quot;http://nlp.stanford.edu/software/tagger.shtml&quot; rel=&quot;nofollow&quot;&gt;Stanford NLP POS-Tagger&lt;/a&gt;) to find nouns, adjective etc. I count them an use all frequently appeared words as features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;e.g. politics, I've around 2.000 text-entities. With the mentioned POS-Tagger I found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;law:           841&#xA;capitalism:    412&#xA;president:     397&#xA;democracy:     1007&#xA;executive:     112&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Would it be right to use only that as features? The trainings-set would then look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Training set for politics:&#xA;feature law         numeric&#xA;feature capitalism  numeric&#xA;feature president   numeric&#xA;feature democracy   numeric&#xA;feature executive   numeric&#xA;class politics,all_others&#xA;&#xA;sample data:&#xA;politics,5,7,1,9,3&#xA;politics,14,4,6,7,9&#xA;politics,9,9,9,4,2,1&#xA;politics,5,8,0,7,6&#xA;...&#xA;all_others,0,2,4,1,0&#xA;all_others,0,0,1,1,1&#xA;all_others,7,4,0,0,0&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Would this be a right approach for binary-classification? Or how would I define my sets? Or is multi-class classification the right approach? Then it would look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Training set for politics:&#xA;feature law         numeric&#xA;feature capitalism  numeric&#xA;feature president   numeric&#xA;feature democracy   numeric&#xA;feature executive   numeric&#xA;feature genetics    numeric&#xA;feature muscle      numeric&#xA;feature blood       numeric&#xA;feature burger      numeric&#xA;feature salad       numeric&#xA;feature cooking     numeric &#xA;class politics,biology,food&#xA;&#xA;sample data:&#xA;politics,5,7,1,9,3,0,0,2,1,0,1&#xA;politics,14,4,6,7,9,0,0,0,0,0,1&#xA;politics,9,9,9,4,2,1,1,1,1,0,3&#xA;politics,5,8,0,7,6,2,2,0,1,0,1&#xA;...&#xA;biology,0,2,4,1,0,4,19,5,0,2,2&#xA;biology,0,0,1,1,1,12,9,9,2,1,1&#xA;biology,7,4,0,0,0,10,10,3,0,0,7&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What would you say?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a newbie to data science with a typical problem. I have a data set with metric1, metric2 and metric3. All these metrics are interdependent on each other. I want to detect anomalies in metric3. Currently, I am using Nupic from numenta.org for my analysis and it doesn't seem to be effective. Is there any ML library which can detect anomalies in multiple parameters?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I tried to use OMP algorithm available in scikit-learn. My net datasize which includes both target signal and dictionary ~ 1G. However when I ran the code, it exited with mem-error.&#xA;The machine has 16G RAM, so I don't think this should have happened. I tried with some logging where the error came and  found that the data got loaded completely into numpy arrays. And it was the algorithm itself that caused the error. Can someone help me with this&#xA;or sugggest more memory efficient algorithm for feature selection, or is subsampling the &#xA;data my only option. Are there some deterministic good subsampling techniques.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;Relevant code piece:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n=8;&#xA;y=mydata[:,0];&#xA;X=mydata[:,[1,2,3,4,5,6,7,8]];&#xA;#print y;&#xA;#print X;&#xA;print &quot;here&quot;;&#xA;omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5,copy_X = False, normalize=True);&#xA;omp.fit(X,y);&#xA;coef = omp.coef_;&#xA;print omp.coef_;&#xA;idx_r, = coef.nonzero();&#xA;for id in idx_r:&#xA;        print coef[id], vars[id],&quot;\\\\n&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;/usr/local/lib/python2.7/dist-packages/sklearn/base.py&quot;, line 324, in score&#xA;return r2_score(y, self.predict(X), sample_weight=sample_weight)&#xA;File &quot;/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.py&quot;, line 2332, in r2_score&#xA;numerator = (weight * (y_true - y_pred) ** 2).sum(dtype=np.float64)&#xA;MemoryError&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;In most data acquisition settings it is useful to tag your data with time and location. If I write the data to csv file, what are the best formats that I can use for this two variables if I want to create a heatmap on Google Maps? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a continuous variable, sampled over a period of a year at irregular intervals. Some days have more than one observation per hour, while other periods have nothing for days. This makes it particularly difficult to detect patterns in the time series, because some months (for instance October) are highly sampled, while others are not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/7MEXt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is what would be the best approach to model this time series?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I believe most time series analysis techniques (like ARMA) need a fixed frequency. I could aggregate the data, in order to have a constant sample or choose a sub-set of the data that is very detailed. With both options I would be missing some information from the original dataset, that could unveil distinct patterns.&lt;/li&gt;&#xA;&lt;li&gt;Instead of decomposing the series in cycles, I could feed the model&#xA;with the entire dataset and expect it to pick up the patterns. For&#xA;instance, I transformed the hour, weekday and month in categorical&#xA;variables and tried a multiple regression with good results (R2=0.71)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I have the idea that machine learning techniques such as ANN can also pick these patterns from uneven time series, but I was wondering if anybody has tried that, and could provide me some advice about the best way of representing time patterns in a Neural network.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'd like to apply some of the more complex supervised machine learning techniques in python - deep learning, generalized addative models, proper implementation of regularization, other cool stuff I dont even know about, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any recommendations how I could find expert ML folks that would like to collaborate on projects?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I had a conversation with someone recently and mentioned my interest in data analysis and who I intended to learn the necessary skills and tools. They suggested to me that while it is great to learn the tools and build the skills there is little point in doing so unless i have specialized  knowledge in a specific field. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;They basically summed it to that I'd just be like a builder with a pile of tools who could build a few wooden boxes and may be build better things (cabins, cupboards etc), but without knowledge in a specific field I'd never be a builder people would come to for a specific product.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone found this or have any input on what to make of this ? It would seem if it was true one would have to learn the data science aspects of things and then learn  a new field just to  become specialized.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;h1&gt;My Background&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I am a graduate student in Civil Engineering. For the analyses of road traffic data (vehicle trajectories as time series) I work with big data sets mostly about a million data points or more.&lt;br&gt;&#xA;I started using R language when MS Excel could not open the big data files. Using basic statistics knowledge and R code I developed few algorithms to identify certain patterns in the data which worked for many applications. But I still lack serious programming skills in R.&lt;br&gt;&#xA;Now, I am familiar with basic inferential statistics and R packages (plyr, dplyr, ggplot2, etc). Recently I came to know that Machine Learning algorithms also help in defining patterns in the data through supervised/ unsupervised learning and their application might improve the accuracy of prediction of certain 'behaviors' of drivers using the traffic data.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Question&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Having the basic knowledge of Statistics and R, I want to learn about the data science/ machine learning as a beginner. I know that some concepts in Stats. and ML overlap and that might bridge the gap in my learning of ML. Keeping my background in mind, what resources (books/ online courses) would you recommend me to start learning data science and apply it in my field?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I would like to ask your opinion on how to choose a similarity measure. I have a set of vectors of length N, each element of which can contain either 0 or 1. The vectors are actually ordered sequences, so the position of each element is important. Suppose I have three vectors of length 10, x_1 x2, x3: x1 has three 1 at positions 6,7,8 (indexes start from 1. Both x2 and x3 have an additional 1, but x2 has it in position 9 while x3 has it in position 1. I am looking for a metric according to which x1 is more similar to x2 than to x3, in that the additional 1 is closer to the &quot;bulk&quot; of ones. I guess this is a relatively common problem, but I am confused on the best way to approach it.&#xA;Many thanks in advance!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm just starting to work on a relatively large dataset after ML course in Coursera.&#xA;Trying to work on &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&quot; rel=&quot;nofollow&quot;&gt;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&lt;/a&gt;.&#xA;Got an accuracy of 5.2 in training and test set with linear regression using gradient descent in octave.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried adding all possible quadratic features (515345 instances and 4275 features), but the code just won't stop executing in my HP Pavilion g6 2320tx, with 4GB RAM in Ubuntu 14.04.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this beyond the data size capacity of Octave ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a data set that is pivoted in to the following format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[key] [id] [0] [1] [5] [10] [15] [60] [120] [180],.. [365]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So key could be&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Products] [1000] [15,000] [4000]... etc&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where products is the category of item being reviewed and key is the identifier for the product; the only fields (0, 1,... 180,.. [365]) are individual daily samples identify how many of &quot;x&quot; product were logged as either sold, in-stock etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need to do is perform some kind of analysis on an entire slew of products and their inventory levels. i.e. each import of data I need to make sure the incoming data is accurate or predictably accurate and that some human did not typo a stock level. The problem is, using a simple average or rolling average can introduce significant variance and smoothing out the average renders my analysis less reliable. Ideally this analysis would trigger an alarm that someone would have to investigate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better and more accurate way of performing this analysis?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;The meaning of multi-class classification rules&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Example: I have two classification rules (Refund is a predictor and Cheat is a binary response):&#xA;(Refund, No) → (Cheat, No) Support = 0.4, Confidence = 0.57&#xA;(Refund, No) → (Cheat, Yes) Support = 0.3, Confidence = 0.43&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;=&gt; multi-class classification rules:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(Refund, No) → (Cheat, No) v (Cheat, Yes)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;By &quot;large&quot;, I mean in the range of 100m to 10b rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently using both Hadoop MapReduce and Amazon RedShift. MapReduce has been a little disappointing here. Redshift works very well if the data is distributed well for the given query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there other technologies that I should be looking at here? If so, what are the trade offs?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was going through an IEEE Research paper which has used Fuzzy ARTMAP for predicting the price of electricity given some highly correlated data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per my basic understanding about Fuzzy ARTMAP it is a classification algorithm, so how will it be able to predict continuous data?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The text from research paper is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In the architecture of the FA network, the preprocessing stages take&#xA;  the input vector and contribute to produce complement coding, which&#xA;  avoids category proliferation, i.e., the creation of a relatively&#xA;  large number of categories to represent the training data. A sequence&#xA;  of input vectors (price and demand) and their respective target&#xA;  vectors are introduced to the FA network in order to classify the&#xA;  input pattern correctly. The classiﬁed input patterns are then grouped&#xA;  into labels using membership functions.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I was using MATLAB to implement the same, so is there a library in MATLAB to approach towards the solution. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I need to collect several large datasets (thousands of samples, dozens of features) for regression with only categorical inputs. I already look for such datasets in the UCI repository, but I did not find any suitable one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody know of any such dataset, or of any additional dataset repository on the Internet?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;n this paper (&lt;a href=&quot;http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;)  they suppose a &quot;unified architecture for NLP&quot; with deep neural networks with multitask learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is to understand the layer architecture in figure 1, see below:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/RoU0P.png&quot; alt=&quot;unified deep learning architecture for NLP&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is someone able to give me a concrete, reproducible example how this architecture processing 3 sentences through their layers?&lt;/li&gt;&#xA;&lt;li&gt;What are the outputs after each layer? &lt;/li&gt;&#xA;&lt;li&gt;Why they choose which layer?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Thans in advance!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29&quot; rel=&quot;nofollow&quot;&gt;database&lt;/a&gt; of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to formulate a supervised classifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My present approach is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[date] [CDR/Position] [#calls] [#sms] [#internetConnections]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My purpose is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I'd like to find the reason they are similar/dissimilar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I apply?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a biosemi bdf of EEG data which contains 32 channel. &#xA;I've opened it using biosig, everything works great, a first list is channel and inside each list there are eeg data.&#xA;But if I open it using MNE it the first list is eeg data, and the second list (inside the list of eeg data) are two list of eeg data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;this is how I open the data using MNE&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;raw_file=read_raw_edf(&quot;E:\\\\eegDATA\\\\\\\\256\\\\s02_reduced.bdf&quot;,preload=True,verbose=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Am I missing something here?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to create a logistic regression model in jpmml, then write the PMML to a file. The problem I'm having, is that I can't find any way to create a custom tag, such as &quot;shortForm&quot; and &quot;longForm&quot; in the following example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;MapValues outputColumn=&quot;longForm&quot;&amp;gt;&#xA;  &amp;lt;FieldColumnPair field=&quot;gender&quot; column=&quot;shortForm&quot;/&amp;gt;&#xA;  &amp;lt;InlineTable&amp;gt;&#xA;    &amp;lt;row&amp;gt;&amp;lt;shortForm&amp;gt;m&amp;lt;/shortForm&amp;gt;&amp;lt;longForm&amp;gt;male&amp;lt;/longForm&amp;gt;&#xA;    &amp;lt;/row&amp;gt;&#xA;    &amp;lt;row&amp;gt;&amp;lt;shortForm&amp;gt;f&amp;lt;/shortForm&amp;gt;&amp;lt;longForm&amp;gt;female&amp;lt;/longForm&amp;gt;&#xA;    &amp;lt;/row&amp;gt;&#xA;  &amp;lt;/InlineTable&amp;gt;&#xA;&amp;lt;/MapValues&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's what I have so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MapValues mv = new MapValues(&quot;output&quot;)&#xA;  .withFieldColumnPairs(&#xA;        new FieldColumnPair( new FieldName(&quot;gender&quot;), &quot;shortForm&quot; )&#xA;  ).withInlineTable(&#xA;        new InlineTable().withRows(&#xA;                new Row().with???( new ??? )&#xA;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In short, I am asking for an API call I can use to instantiate the &quot;shortForm&quot; element in the example, and attach it to the &quot;row&quot; object. I've been all through the API, examples, and Google/SO, and can't find a thing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;We have a ruby-on-rails platform (w/ postgreSQL db) for people to upload various products to trade. Of course, many of these products listed are the same, while they are described differently by the consumer (either through spelling, case etc.) &quot;lots of duplicates&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the purposes of analytics and a better UX, we're aiming to create an evolving &quot;master product list&quot;, or &quot;whitelist&quot;, if you will, that will have users select from an existing list of products they are uploading, OR request to add a new one. We also plan to enrich each product entry with additional information from the web, that would be tied to the &quot;master product&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some methods we're proposing to solve this problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A) Take all the &quot;items&quot; listed in the website (~90,000), de-dupe as much as possible by running select &quot;distinct&quot; queries (while maintaining a key-map back to original data by generating an array of item keys from each distinct listing in a group-by.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;THEN&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A1) Running this data through mechanical turk, and asking each turk user to list data in a uniform format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OR&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A2) Running each product entry through the Amazon products API and asking the user to identify a match.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A3) A better method?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two questions regarding such a dataset: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What algorithms can we use to ensure that the model not only predicts the fraudulent transactions accurately, but also predicts the value of fraud associated with these.  &lt;/li&gt;&#xA;&lt;li&gt;Assuming that we can quantify the cost involved in each false positive (tagging a non-fraudulent transaction as fraudulent) and cost incurred due to a false negative (tagging a fraudulent transaction as non-fraudulent), how can we optimize the model to maximize savings (or minimize losses)?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;We're currently using Redshift as our data warehouse, which we're very happy with. However, we now have a requirement to do machine learning against the data in our warehouse. Given the volume of data involved, ideally I'd want to run the computation in the same location as the data rather than shifting the data around, but this doesn't seem possible with Redshift. I've looked at MADlib, but this is not an option as Redshift does not support UDFs (which MADlib requires). I'm currently looking at shifting the data over to EMR and processing it with the Apache Spark machine learning library (or maybe H20, or Mahout, or whatever). So my questions are: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;is there a better way?&lt;/li&gt;&#xA;&lt;li&gt;if not, how should I make the data accessible to Spark? The options I've identified so far include: use Sqoop to load it into HDFS, use DBInputFormat, do a Redshift export to S3 and have Spark grab it from there. What are the pros/cons for these different approaches (and any others) when using Spark?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that this is off-line batch learning, but we'd like to be able to do this as quickly as possible so that we can iterate experiments quickly.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a data set that keeps track of who referred someone to a program, and includes the geo coordinates of both parties for each record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What will be the best way to visualize this kind of data set? This visualization should also be able to use the geo coordinates to place this entities in the map to form clusters, or to superimpose them on a real map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested in an algorithm and/or a library that will help me do this. Library should be preferably written in Java, Python, Scala, or NodeJS. The record count can be as big as a thousand or hundreds of thousands.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to use Latent Dirichlet Allocation for a project and I am using Python with the gensim library. After finding the topics I would like to cluster the documents using an algorithm such as k-means(Ideally I would like to use a good one for overlapping clusters so any recommendation is welcomed). I managed to get the topics but they are in the form of:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;0.041*Minister + 0.041*Key + 0.041*moments + 0.041*controversial + 0.041*Prime&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to apply a clustering algorithm, and correct me if I'm wrong, I believe I should find a way to represent each word as a number using either tfidf or word2vec.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any ideas of how I could &quot;strip&quot; the textual information from e.g. a list, in order to do so and then place them back in order to make the appropriate multiplication?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance the way I see it if the word Minister has a tfidf weight of 0.042 and so on for any other word within the same topic I should be to compute something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;0.041*0.42 + ... + 0.041*tfidf(Prime) and get a result that will be later on used in order to cluster the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your time.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have measurements of 4 devices at two different points of time. A measurement basically consists of an array of ones and zeros corresponding to a bit value at the corresponding location:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;whos measurement1_dev1_time1&#xA;&#xA;Name                         Size               Bytes  Class      Attributes&#xA;&#xA;measurement1_dev1_time1      4096x8             32768  logical&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I assume that for a specific device the changes between time 1 and 2 of the measurements are unique. However, since I am dealing with 32768 bits at different locations, it is quite hard to visualize if there is some kind of dependency. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As every bit at location &lt;code&gt;x&lt;/code&gt;can be regarded as one dimension of an observation I thought to use PCA to reduce the number of dimensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, for every of the 5 devices:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I randomly sample &lt;code&gt;n&lt;/code&gt; measurements at point &lt;code&gt;t1&lt;/code&gt;and &lt;code&gt;t2&lt;/code&gt; seperatly&lt;/li&gt;&#xA;&lt;li&gt;I prepare an array as input for &lt;code&gt;pca()&lt;/code&gt; with &lt;code&gt;m&lt;/code&gt;*n columns (&lt;code&gt;m&lt;/code&gt;&amp;lt; 32768; its a subset of all the observed bits, as the original data might be too big for pca) and 4 rows (one row for each device).&lt;/li&gt;&#xA;&lt;li&gt;On this array &lt;code&gt;A&lt;/code&gt; I calculate the pca: ``[coeff score latent] = pca(zscore(A))```&lt;/li&gt;&#xA;&lt;li&gt;Then I try to visualize it using &lt;code&gt;biplot&lt;/code&gt;: &lt;code&gt;biplot(coeff(:,1:2), 'score', score(:,1:2))&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;However, this gives me really strange results. Maybe PCA is not the right approach for this problem? I also modified the input data to do the PCA not on the logical bit array itself. Instead, I created a vector, which holds the indices where there is a '1' in the original measurement array. Also this produces strange results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I am completely new to PCA I want to ask you if you either see a flaw in the process or if PCA is just not the right approach for my goal and I better look for other dimension reduction approaches or clustering algorithms.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I did small survey and get such data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-------------| Yes | No | Dont_Know |  &#xA;|-------------|     |    |           |  &#xA;| Employee    | 60  | 5  | 5         |  &#xA;| Workers     | 17  | 0  | 1         |  &#xA;| Businessmen | 71  | 5  | 10        |  &#xA;| Jobless     | 4   | 30 | 0         |  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;R code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dt &amp;lt;- data.frame(workers = c(&quot;Employee&quot;,&#xA;                             &quot;Workers&quot;, &#xA;                             &quot;Businessmen&quot;, &#xA;                             &quot;Jobless&quot;), &#xA;                 yes = c(60,17,71,4), &#xA;                 no = c(5,0,5,30), &#xA;                 dont_know = c(5,1,10,0)&#xA;                )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What kind of test I must do, if I want to show, that the Jobless people are often choosing &lt;strong&gt;No&lt;/strong&gt; answer?   &lt;/li&gt;&#xA;&lt;li&gt;Is the difference between Jobless and Businessmen answers significant? &lt;/li&gt;&#xA;&lt;li&gt;And what is about other groups?  &lt;/li&gt;&#xA;&lt;li&gt;What another information I can get from such data or what kind questions I can ask from such data?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have two classes (A,B) that I want to classify using a SVM. Say that I have a class C and a function f. Can I do this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;A' =  f(A,C) = |A-C|&#xA;B' =  f(B,C) = |B-C|&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and then perform the classification on A' and B' instead? In the context of my problem A and B are classes where elements are vectors. The f function measures the Mahalanobis distance of each vector with respect to the distribution imposed by C.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am dealing with a lot of categorical data right now and I would like to use an appropriate data mining method in any tool [preferably R] to find the effect of each parameter [categorical parameters] over my target variable. To give a brief notion about the data that am dealing with, my target variable denotes the product type [say, disposables and non-disposables] and I have parameters like root cause,symptom,customer name, product name etc. As my target can be considered as a binary value, I tried to find the combination of values leading to the desired categories using Apriori but, I have more than 2 categories in that attribute and I want to use all of them and find the effect of the mentioned parameters over each category. I really wanted to try SVM and use hyperplanes to separate the content and get n-dimensional view. But, I do not have enough knowledge to validate the technique, functions am using to do the analysis. Currently I have like 9000 records and each of them represents a complaint from the user. There are lot of columns available in the dataset which is what I am trying to use to determine the target variable [ myForumla &amp;lt;- Target~. ] I tried with just 4 categorical columns too. Not getting a proper result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can just the categorical variables be used to develop a SVM model and get visualization with n hyper planes? Is there any appropriate data mining technique available for dealing with just the categorical data?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I think this is something that experienced programmers do all the time. But, given my limited programming experience, please bear with me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an excel file which has particular cell entries that read &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;[[{&quot;from&quot;: &quot;4&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;8&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;9&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;3&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}], [{&quot;from&quot;: &quot;14&quot;, &quot;response&quot;: false, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;15&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;17&quot;, &quot;response&quot;: false, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;13&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}]]&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, for each such entry I want to take the information in each of the curly brackets and make a row of data out of it. Each such row would have 3 columns. For example, the row formed from the first entry within curly brackets should have the entries &quot;4&quot; &quot;true&quot; and &quot;20&quot; respectively. The part I posted should give me 6 such rows, and for n such repetitions I should end up with a matrix of 6n rows, and 4 columns ( an identifier, plus the 3 columns mentioned).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be most efficient way to do this? By &quot;doing this&quot; I mean learning the trick, and then implementing it. I have access to quite a few software packages(Excel, Stata, Matlab, R) in my laboratory, so that should not be an issue.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am new to D3 programming (any programming, for that matter). I have protein-protein interaction data in JSON format and csv format. I would like to use that data for network visualization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data attributes: Protein Name, Protein Group, Protein type, Protein Source Node, Protein Target Node&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest good network visualizations for such data. How does it work with hive plots?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I was wondering if anyone was aware of any methods for visualizing an SVM model where there are more than three continuous explanatory variables. In my particular situation, my response variable is binomial, with 6 continuous explanatory variables (predictors), one categorical explanatory variable (predictor). I have already reduced the number of predictors and I am primarily using R for my analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I am unaware if such a task is possible/ worth pursuing.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your time.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;this is my first ever stack exchange question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to build a tool right now and one of the features of the tool is the ability to break down a product or service into it's associated attributes/properties/classes/keywords/entities. (Choose which word best suits, as I have no idea).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example if we had a Camera as the product. I would like to be able to generate a breakdown of everything that is associated to a camera. Such as;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Digital, Film, Optical, LCD, Glass, CCD, CMOS, RGB, Lens, Shutter, Negative, Polaroid, Darkroom, Flash, Resolution, Stabilisation, Batteries, Zoom, Angle, Telephoto, Macro, Filters, Memory, CF, SD&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The list could go on for quite some time, those were jsut a few off the top of my head.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How on earth could I go about retrieving such attributes automatically? Is there a database out there that has such info? Are there any special tricks anyone has up their sleeve to be able to accumulate datasets such as the example above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Very interested in your answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks :)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The questionnaire for the data is &lt;a href=&quot;http://www.cc.gatech.edu/gvu/user_surveys/survey-1997-10/questions/general.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The clean data is available &lt;a href=&quot;http://wikisend.com/download/586046/DataRaw.arff&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&#xA;NB: The Column CompuPlat has missing values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;part of dataset&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;CMFam CMHobb  CMNone  CMOther CMPol   CMProf  CMRel&#xA;0   0   1   0   0   0   0&#xA;0   0   0   0   0   0   0&#xA;1   1   0   0   0   1   0&#xA;0   0   0   1   0   0   0&#xA;0   0   0   0   1   1   0&#xA;1   0   0   0   0   1   1&#xA;Community Membership_Family&#xA;Community Membership_Hobbies&#xA;Community Membership_None&#xA;Community Membership_Other&#xA;Community Membership_Political&#xA;Community Membership_Professional&#xA;Community Membership_Religious&#xA;Community Membership_Support&#xA;&lt;/code&gt;&#xA;I want to club all of them in a variable CM&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Data sample contains a single feature: random integer number from 1 to 4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possble to change &lt;code&gt;1,2,3,4&lt;/code&gt; representation on the filter card to some custom names, say: &lt;code&gt;Type1,Type2,Type3,Type4&lt;/code&gt;? (not changing data set)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4ZPYM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or starting point on how to do this using Neuroph or maybe in other framework/approaches?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit for more info as suggested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have very little experience on neural networks though I have some introduction with it in college. However I am very much familar with java&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data is around 3 megabytes only and consists of rules and relationships for a single domain. This means that the data is complex but relatively limited though still free-form English language.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have 1-4 gram text data from wikipedia for 14 categories, which I am using for NE classification.&#xA;I feed named entity from sentence to lucene indexer which searches named entity from these 14 categories. &#xA;Issue I am facing is, for single entity I get multiple classes as a result with same score.&#xA;like while search &lt;code&gt;titanic&lt;/code&gt;, indexer gives this result&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Book&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Movie&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Product&lt;/p&gt;&#xA;&#xA;&lt;p&gt;now problem is which class to be considered?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already tried with classifiers (NB,ME in nltk,scikit learn), but as it consider each entity from dataset as feature, it works as indexer only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why lucene?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Tz8Uy.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am running SVM algorithm in R.It is taking long time to run the algorithm.I have system with 32GB RAM.How can I use that whole RAM memory to speed my process.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for an (ideally free) API that would have time series avg/median housing prices by zip code or city/state. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quandl almost fits the bill, but it returns inconsistent results across different zip codes and the data is not as up to date as I'd like (it's mid November, and the last month is August).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also looked at Zillow, but storing their data is against TOS, and at 1,000 calls daily--it would take forever to pull in the necessary data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions (even if they aren't free) would be much appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a list user data: user name, age, sex, address, location etc., and &lt;/p&gt;&#xA;&#xA;&lt;p&gt;a set of product data: Product name, Cost, description etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I would like to build a recommendation engine that will be able to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1 Figure out similar products&lt;/p&gt;&#xA;&#xA;&lt;p&gt;eg :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;name  :   category   :  cost    :   ingredients&lt;/p&gt;&#xA;&#xA;&lt;p&gt;x     :     x1   :        15  :       xx1, xx2, xx3&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y     :    y1   :        14   :     yy1, yy2, yy3&lt;/p&gt;&#xA;&#xA;&lt;p&gt;z     :    x1  :          12   :     xx1, xy1 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;here x and z are similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2 Recommend relevant products from the product list to a user&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I implement this using mahout?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I do at the moment some data experiments with the &lt;a href=&quot;http://graphlab.com/products/create/docs/&quot; rel=&quot;nofollow&quot;&gt;Graphlab toolkit&lt;/a&gt;. I have at the first next SFrame, with the three columns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Users Items Rating&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The pair in the same row from every &lt;code&gt;Users&lt;/code&gt; and &lt;code&gt;Items&lt;/code&gt; values build the unique key and the &lt;code&gt;Rating&lt;/code&gt; is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)&lt;/li&gt;&#xA;&lt;li&gt;Take the logarithm by every rating value&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Afterward I create a recommender model and evaluate the basic metrics for it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dataset is comming from some music site, the users rated some tracks. I have approximately 100 000 users and 300 000 tracks. Total number of ratings is over 3 millions (actually the matrix is sparse). This is the most simple data set, which I analyze now. In the future I can (and will) use some additional information about the users and tracks (f.e. duration, year, genre, band etc). At the moment I just interest to collect some methods for rating normalisation without to use additional information (users &amp;amp; items features). My problem is, the data set doesn't have any &lt;code&gt;Rating&lt;/code&gt; at the first. I create someself the column &lt;code&gt;Rating&lt;/code&gt;, based on the number of events for unique &lt;code&gt;User-Item&lt;/code&gt; pair (I have this information). You can of course understand that some users can hear some tracks many times, and another users only one time. Consequently the dispersion is very high and I want to reduce it (normalise the ratings value).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working through the &lt;a href=&quot;https://www.coursera.org/course/nlp&quot; rel=&quot;nofollow&quot;&gt;Coursera NLP course by Jurafsky &amp;amp; Manning&lt;/a&gt;, and the &lt;a href=&quot;https://class.coursera.org/nlp/lecture/32&quot; rel=&quot;nofollow&quot;&gt;lecture on Good-Turing smoothing&lt;/a&gt; struck me odd.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example given was:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;You are fishing (a scenario from Josh Goodman), and caught:&lt;br&gt;&#xA;  10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish&lt;br&gt;&#xA;  ...&lt;br&gt;&#xA;  How likely is it that the next species is new (i.e. catfish or bass)&lt;br&gt;&#xA;  Let's use our estimate of things-we-saw-once to estimate the new things.&lt;br&gt;&#xA;  3/18 (because N_1=3)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I get the intuition of using the count of uniquely seen items to estimate the number of unseen item types (N = 3), but the next steps seem counterintuitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is the denominator left unchanged instead of incremented by the estimate of unseen item types? I.e., I would expect the probabilities to become:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Carp : 10 / 21&lt;br&gt;&#xA;  Perch : 3 / 21&lt;br&gt;&#xA;  Whitefish : 2 / 21&lt;br&gt;&#xA;  Trout : 1 / 21&lt;br&gt;&#xA;  Salmon : 1 / 21&lt;br&gt;&#xA;  Eel : 1 / 21&lt;br&gt;&#xA;  Something new : 3 / 21&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It seems like the Good-Turing count penalizes seen items too much (trout, salmon, &amp;amp; eel are each taken down to 1/27); coupled with the need to adjust the formula for gaps in the counts (e.g., Perch &amp;amp; Carp would be zeroed out otherwise), it just feels like a bad hack.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to build an item-item similarity matching recommendation engine with mahout. The data set is as in the following format ( attributes are in text not in numerals format )&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name : category : cost : ingredients&#xA;&#xA;x : xx1 : 15 : xxx1, xxx2, xxx3&#xA;&#xA;y : yy1 : 14 : yyy1, yyy2, yyy3&#xA;&#xA;z : xx1 : 12 : xxx1, xxy1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So in-order to use this data set for mahout to train, what is the right way to convert this in to numeric (as CSV Boolean data set) format accepted by mahout.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to build a data set on several log files of one of our products.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The different log files have their own layout and own content; I successfully grouped them together, only one step remaining...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Indeed, the log &quot;messages&quot; are the best information. I don't have the comprehensive list of all those messages, and it's a bad idea to hard code based on those because that list can change every day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I would like to do is to separate the indentification text from the value text (for example: &quot;Loaded file XXX&quot; becomes (identification: &quot;Loaded file&quot;, value: &quot;XXX&quot;)). Unfortunately, this example is simple, and in real world there are different layouts and sometimes multiple values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about using string kernels, but it is intended for clustering ... and cluseting is not applicable here (I don't know the number of different types of messages and eventhough, it would be too much).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any idea?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S: For those who programs, this can be easier to understand. Let's say that the code contains as logs printf(&quot;blabla %s&quot;, &quot;xxx&quot;) -&gt; I would like to have &quot;blabla&quot; and &quot;xxx&quot; seperatated&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I would like to run an R script using a single command (e.g. bat file or shortcut).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This R script asks the user to choose a file and then plots information about that file. All is done via dialog boxes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't want the user to go inside R - because they don't know it at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I was using r cmd and other similar stuffs, but as soon as the plots are displayed, R exits and closes the plots.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I do?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently finishing up a B.S. in mathematics and would like to attend graduate school (a master's degree for starters, with the possibility of a subsequent Ph.D.) with an eye toward entering the field of data science. I'm also particularly interested in machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the graduate degree choices that would get me to where I want to go?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a consensus as to whether a graduate degree in applied mathematics, statistics, or computer science would put me in a better position to enter the field of data science?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you all for the help, this is a big choice for me and any input is very much appreciated. Typically I ask my questions on Mathematics Stack Exchange, but I thought asking here would give me a broader and better rounded perspective.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;From &lt;a href=&quot;http://nshorter.com/ResearchPapers/MachineLearning/A_Roadmap_to_SVM_SMO.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;A_Roadmap_to_SVM_SMO.pdf&lt;/a&gt;, pg 12.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://s13.postimg.org/9dx9t4w47/whatwhat.png&quot; alt=&quot;a busy cat&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assume I am using linear kernel, how will I be able to get both the first and second inner product?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My guess, inner product of datapoint with datapoint j labelled class A for the first inner product of the equation and inner product of datapoint j with datapoints labelled class B for second inner product?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;To all:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been wracking my brain at this for a while and thought maybe someone here would know of a package or algorithm to handle the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have nominal multivariant timeseries that look like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;          Time Var1 Var2 Var3 Var4 Var5 ... VarN&#xA;             0     A     A   B    C    A   ... H&#xA;             1     A     A   B    D    D   ... H&#xA;             2     B     A   C    D    D   ... H&#xA;             ..&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And so on from times 0 to 1,000,000. What I would like to do is search the time series for rules of the type:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given Var3 is in state B in the previous step and Var5 is in state D in the previous step, than Var1 will be in state B. What I want to do is have the rules that include the time interval explicitly. A simpler case of interest would simply be to reduce the time series to &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;               Time    Var1 Var2 Var3 Var4 Var5 ... VarN&#xA;                0        0    0    0     0   0   ... 0&#xA;                1        0    0    0     1   1   ... 0&#xA;                2        1    0    1     0   0   ... 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where the the variable is 1 if its state is different from the previous step and zero otherwise. Then I just want to have rules that say something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If Var4 and Var5 changed in the previous step than Var1 will change in the current step. Which would be easy for a lag of one, as I could just make the data into something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   Var1 Var2 Var3 Var4 Var5 ... VarN Var1_t-1 Var2_t-1 Var3_t-1 ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then do sequence mining, but if I want to have rules that aren't just a single lag but could be lags from 1 to 500 than my data set begins to be a little difficult to work with. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be greatly appreciated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit to respond to comment:&#xA;Each column could be in one of 7 different states. As far as a target, it is non-specific, any rules between the columns would be of interest. However, predicting columns 30-40 and 62-75 would be particularly interesting.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I browsed a sample for available data at &lt;a href=&quot;http://dbpedia.org/page/Sachin_Tendulkar&quot; rel=&quot;nofollow&quot;&gt;http://dbpedia.org/page/Sachin_Tendulkar&lt;/a&gt;. I wanted these properties as columns, so I downloaded the CSV files from &lt;a href=&quot;http://wiki.dbpedia.org/DBpediaAsTables&quot; rel=&quot;nofollow&quot;&gt;http://wiki.dbpedia.org/DBpediaAsTables&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when I browse the data for the same entity &quot;Sachin_Tendulkar&quot;, I find that many of the properties are not available. e.g. the property &quot;dbpprop:bestBowling&quot; is not present.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I get all the properties that I can browse through the direct resource page.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Common model validation statistics like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov–Smirnov test&lt;/a&gt; (KS), &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;AUROC&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Gini_coefficient&quot; rel=&quot;nofollow&quot;&gt;Gini coefficient&lt;/a&gt; are all functionally related. However, my question has to do with proving how these are all related. I am curious if anyone can help me prove these relationships. I haven't been able to find anything online, but I am just genuinely interested how the proofs work. For example, I know Gini=2AUROC-1, but my best proof involves pointing at a graph. I am interested in formal proofs. Any help would be greatly appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am having some difficulty in seeing connection between PCA on second order moment matrix in estimating parameters of Gaussian Mixture Models. Can anyone connect the above??&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Hi this is my first question in the Data Science stack. I want to create an algorithm for text classification. Suppose i have a large set of text and articles. Lets say around 5000 plain texts. I first use a simple function to determine the frequency of all the four and above character words. I then use this as the feature of each training sample. Now i want my algorithm to be able to cluster the training sets to according to their features, which here is the frequency of each word in the article. (Note that in this example, each article would have its own unique feature since each article has a different feature, for example an article has 10 &quot;water and 23 &quot;pure&quot; and another has 8 &quot;politics&quot; and 14 &quot;leverage&quot;). Can you suggest the best possible clustering algorithm for this example?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;&lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&quot; rel=&quot;nofollow&quot;&gt;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the description given in the above link, &#xA;the Attribute information specifies &quot;average and covariance over all 'segments', each segment being described by a 12-dimensional timbre vector&quot;. So the covariance matrix should have 12*12 = 144 elements. But why is the number of timbre covariance features only 78 ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;GBMs, like random forests, build each tree on a different sample of the dataset and hence, going by the spirit of ensemble models, produce higher accuracies. However, I have not seen GBM being used with dimension sampling at every split of the tree like is common practice with random forests. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there some tests that show that dimensional sampling with GBM would decrease its accuracy because of which this is avoided, either in literature form or in practical experience? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have historic error of time series.  I want to analyze error series to improve forecast series. Are there any methods to do this?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;What is the standard way for evaluating and comparing different algorithms while developing recommendation system? Whether we need to have a predetermined annotated ranked dataset and then compare with precision/recall/F measure of  different algorithms ? Is this the best way for evaluation ? Or is there any other way to compare results of various recommendation algorithms ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm from programming background. I'm now learning Analytics. I'm learning concepts from basic statistics to model building like linear regression, logistic regression, time-series analysis, etc.,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As my previous experience is completely on programming, I would like to do some analysis on the data which programmer has.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say, Lets have the details below(I'm using SVN repository)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;personname, code check-in date, file checked-in, number of times checkedin, branch, check-in date and time, build version, Number of defects, defect date, file that has defect, build version, defect fix date, defect fix hours, (please feel free to add/remove how many ever variables needed)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I Just need a trigger/ starting point on what can be done with these data. can I bring any insights with this data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;or can you provide any links that has information about similar type of work done.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to determine whether or not we are 90% confident that the mean of a &lt;em&gt;proposed&lt;/em&gt; &lt;em&gt;population&lt;/em&gt; is at least 2 times that of the mean of the &lt;em&gt;incumbant&lt;/em&gt; &lt;em&gt;population&lt;/em&gt; based on samples from each population which is all the data I have right now. Here are the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;incumbantvalues = (7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;proposedvalues =  (17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have no idea if either population is or will be normal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The ratio of the &lt;em&gt;sample&lt;/em&gt; means does exceed 2.0 but how does that translate to confidence that the proposed population mean will be at least twice that of the mean of the incumbant population with 90% confidence ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can re-sampling (bootstrapping with replacement) help answer this question ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am doing a text classification task(5000 essays evenly distributed by 10 labels). I explored &lt;code&gt;LinearSVC&lt;/code&gt; and got an accuracy of 80%. Now I guess whether accuracy could be raised by using &lt;code&gt;ensemble&lt;/code&gt; classifier with &lt;code&gt;SVM&lt;/code&gt; as base estimator?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I do not know how to employ an &lt;code&gt;ensemble&lt;/code&gt; classifier incorporating all the features? Please note that I do not want to combine the different features directly in a single vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, &lt;strong&gt;My first question:&lt;/strong&gt; in order to improve the current accuracy, is it possible to use &lt;code&gt;ensemble&lt;/code&gt; classifier with &lt;code&gt;svm&lt;/code&gt; as base estimator?&#xA;&lt;strong&gt;My second question&lt;/strong&gt; How to employ an &lt;code&gt;ensemble&lt;/code&gt; classifier incorporating all features?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What are some possible techniques for smoothing proportions across very large categories, in order to take into account the sample size? The application of interest here is to use the proportions as input into a predictive model, but I am wary of using the raw proportions in cases where there is little evidence and I don't want to overfit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example, where the ID denotes a customer and impressions and clicks are the number of ads shown and clicks the customer has made, respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/3oHzQ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a visualization problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Creating a comparison report of PR event efficiency. Say, show or exhibition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are two dimensions of comparison:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;compare vs the same event performance in the past years &lt;/li&gt;&#xA;&lt;li&gt;compare vs another type of analogical/competitive events &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;There is also a number of comparison aspects:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Audience&lt;/li&gt;&#xA;&lt;li&gt;Media Coverage&lt;/li&gt;&#xA;&lt;li&gt;Social Buzz&lt;/li&gt;&#xA;&lt;li&gt;ROI&lt;/li&gt;&#xA;&lt;li&gt;.... etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Each aspect is a set of some final KPI-s (just numbers, which can be compared vs another &quot;dimensions&quot;), plus maybe some descriptive text and pictures (which couldn't be a metric but should be attached to the report).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So finaly it looks like a three-dimensional coube:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Years&lt;/li&gt;&#xA;&lt;li&gt;Another Events&lt;/li&gt;&#xA;&lt;li&gt;Aspects&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;If I put it in plain Word or PPT it will look like a document with dozen of slides/papers and linear structure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas how to compile an elegant user-friendly report?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;A short while ago, I came across this ML framework that has implemented several different algorithms ready for use. The site also provides a handy API that you can access with an API key.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have need of the framework to solve a website classification problem where I basically need to categorize several thousand websites based on their HTML content. As I don't want to be bound to their existing API, I wanted to use the framework to implement my own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, besides some introductory-level data mining courses and associated reading, I know very little as to what exactly I would need to use. Specifically, I'm at a loss as to what exactly I need to do to train the classifier and then model the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The framework already includes some classification algorithms like NaiveBayes, which I know is well suited to the task of text classification, but I'm not exactly sure how to apply it to the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me a rough guidelines as to what exactly I would need to do to accomplish this task?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to make a prediction for the result of the parliamentary elections. My output will be the % each party receives. There is more than 2 parties so logistic regression is not a viable option. I could make a separate regression for each party but in that case the results would be in some manner independent from each other. It would not ensure that the sum of the results would be 100%.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What regression (or other method) should I use? Is it possible to use this method in R or Python via a specific library?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have some text files containing moview reviews I need to find out whether the review is good or bad. I tried the following code but its not working:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import nltk&#xA;with open(&quot;c:/users/user/desktop/datascience/moviesr/movies-1-32.txt&quot;, 'r') as m11:&#xA;    mov_rev = m11.read()&#xA;mov_review1=nltk.word_tokenize(mov_rev)&#xA;bon=&quot;crap aweful horrible terrible bad bland trite sucks unpleasant boring dull moronic dreadful disgusting distasteful flawed ordinary slow senseless unoriginal weak wacky uninteresting unpretentious &quot;&#xA;bag_of_negative_words=nltk.word_tokenize(bon)&#xA;bop=&quot;Absorbing Big-Budget Brilliant Brutal Charismatic Charming Clever Comical Dazzling Dramatic Enjoyable Entertaining Excellent Exciting  Expensive Fascinating Fast-Moving First-Rate Funny Highly-Charged Hilarious Imaginative Insightful Inspirational Intriguing Juvenile Lasting Legendary Pleasant Powerful Ripping Riveting Romantic Sad  Satirical Sensitive  Sentimental Surprising Suspenseful Tender Thought Provoking Tragic Uplifting Uproarious&quot;&#xA;bop.lower()&#xA;bag_of_positive_words=nltk.word_tokenize(bop)&#xA;vec=[]&#xA;for i in bag_of_negative_words:&#xA;    if i in mov_review1:&#xA;        vec.append(1)&#xA;    else:&#xA;        for w in bag_of_positive_words:&#xA;            if w in moview_review1:&#xA;                vec.append(5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;so i am trying to check whether the review contains a positive word or a negative word. If it contains negative word then a value 1 will be assigned to the vector vec else a value of 5 will be assigned.&#xA;but the output i am getting is an empty vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;please help..&#xA;Also please suggest others way of solving this problem.&#xA;thanks in advance&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;In this &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF&quot; rel=&quot;nofollow&quot;&gt;wiki page&lt;/a&gt; there is a function &lt;code&gt;corr()&lt;/code&gt; that calculates the Pearson coefficient of correlation, but my question is that: is there any function in Hive that enables to calculate the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kendall%27s_W&quot; rel=&quot;nofollow&quot;&gt;Kendall coefficient&lt;/a&gt; of correlation of a pair of a numeric columns in the group?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for API suggestions for enriching data on companies. Currently I use the Crunchbase API to look up a company's name or domain and I am trying to gather the domain/name (if I don't already have both), contact email (this one is a long shot), and the location of their headquarters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works incredibly well if Crunchbase has the company in their API, but I'd say this only happens about 25% of the time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd love to get some suggestions on some free APIs that I could use along with Crunchbase. I'd also love to see if anyone has had positive or negative experiences with paid APIs! &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have some question regarding to the choice of the better implementation. I would know the differences and advantages of &lt;a href=&quot;https://mahout.apache.org/&quot; rel=&quot;nofollow&quot;&gt;Mahout Apache&lt;/a&gt; (Java implementation) versus &lt;a href=&quot;http://graphlab.com/index.html&quot; rel=&quot;nofollow&quot;&gt;Graphlab&lt;/a&gt; (Python implementation) in the area of the data sciences. Specially in the area of recommenders and classifiers. Can anybody here get some (qualified) feedback about both possibilities?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Which of the following is best (or widely used) for calculating item-item similarity measure in mahout and why ?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Pearson Correlation&#xA;Spearman Correlation&#xA;Euclidean Distance&#xA;Tanimoto Coefficient&#xA;LogLikelihood Similarity&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any thumb-rule to chose from these set of algorithm also how to differentiate each of them ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to match new product description with the existing ones. Product description looks like this: ￼Panasonic DMC-FX07EB digital camera silver. These are steps to be performed:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tokenize description and recognize attributes: Panasonic =&gt; Brand, DMC-FX07EB =&gt; Model, etc.&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;Get few candidates with similar features&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;Get the best candidate.&lt;br/&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am having problem with the first step (1). In order to get 'Panasonic =&gt; Brand', DMC-FX07EB =&gt; Model, silver =&gt; color, I need to have index where each token of the product description correspond to certain attribute name (Brand, model, color, etc.) in the existing database. The problem is that in my database product descriptions are presented as one atomic attribute e.g. 'description' (no separated product attributes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically I don't have training data, so I am trying to build index of all product attributes so I can build training data. So far I have attributes from bestbuy.com and semantics3.com APIs, but both sources lack most of attributes or contain irrelevant ones. Any suggestions for better APIs to get product attributes? Better approach to do this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. For every product there is a matched product description in the Database, which is as well in a form of one atomic attribute. I have checked this &lt;a href=&quot;https://stackoverflow.com/questions/18496925/how-to-parse-product-titles-unstructured-into-structured-data&quot;&gt;question on SO&lt;/a&gt;, it helped me and it seems we have same approach but I am still trying to get training data. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am currently using SVM and scaling my training features to the range of [0,1].&#xA;I first fit/transform my training set and then apply the &lt;strong&gt;same&lt;/strong&gt; transformation to my testing set. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ### Configure transformation and apply to training set&#xA;    min_max_scaler = MinMaxScaler(feature_range=(0, 1))&#xA;    X_train = min_max_scaler.fit_transform(X_train)&#xA;&#xA;    ### Perform transformation on testing set&#xA;    X_test = min_max_scaler.transform(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Let assume that a given feature in the training set has a range of [0,100], and that same feature in the testing set has a range of [-10,120]. In the training set that feature will be scaled appropriately to [0,1], while in the testing set that feature will be scaled to a range outside of that first specified, something like [-0.1,1.2].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering what the consequences of the testing set features being out of range of those being used to train the model? Is this a problem?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm very passionate about how computers can be made able to think intelligently and independently (in our favour, of course!). I'm currently studying Bachelors science of information technology at UTS (University of Technology:Sydney). I have two months before I start my second year, and have not yet been able to decide on which major should I select that can lead myself towards dedicated study of Artificial Intelligence (which I love with my life).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the following majors available:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Internetworking and Applications &lt;/li&gt;&#xA;&lt;li&gt;Data Analytics&lt;/li&gt;&#xA;&lt;li&gt;(there are other two as well, but business oriented).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://uts.edu.au&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the link to my subjects. I believe that being able to play with data is a sign of intelligence (I may be wrong too!). Will one of these subjects form me a good foundation for my further study in A.I.? Or should I jump into Engineering? Or Pure Science?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is there a method/class available in Apache Mahout to perform n-fold cross validation?&#xA;If yes how it can be done?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I would like to pose a question about how to treat additional holders in the propensity-to-buy models of banking products.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Up to now I was only taking into considerations the clients as first holders. &#xA;For example, if a client ‘1’ appears as the first holder of a saving account ‘A’ with a balance at the end of the month of 100€ and as an additional holder of a saving account ‘B’ with a balance at the end of the month of 50€, the saving balance at the end of the month for the client is considered to be just 100€.&#xA;Moreover, if a client only appears as an additional holder (and he/she is not a first account holder of ANY product), he/she is dismissed by the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I have been told to include additional holders in the models (additional holders have the same rights of the first holders).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possibility is to recalculate all the variables summing up the position as first and additional holder (in the previous example, the balance at the end of the month of client ‘1’ would be 150€). Together with this, I would create some variable that represents the maximum degree of intervention of the client in the account (ex. 'first holder', 'second holder').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another possibility would be to “double” all the variables, considering the client as first and additional holder (in the example, we would create two variables:  the balance at the end of the month as FH =100€, :  the balance at the end of the month as AH =50€).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Did any of you encounter a similar problem?It would be very helpful to understand how you solved it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want to create a model to predict the propensity to buy a certain product. As my proportion of 1's is very low, I decided to apply oversampling (to get a 10% of 1's and a 90% of 0's). Now, I want to discretize some of the variables. To do so I run a tree for each variable against the target. My question is...shall I define the prior probabilities when I do this (run the trees), or it doesn't matter and I can use the over-sampled dataset just like that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to write a data-mining service in &lt;a href=&quot;http://golang.org&quot;&gt;Google Go&lt;/a&gt; which collects data through scraping and APIs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However as Go lacks good ML support I would like to do the ML stuff in Python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having a web background I would connect both services with something like RPC but as I believe that this is a common problem in data science I think that there is some better solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example most (web) protocols lack at:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;buffering between processes&lt;/li&gt;&#xA;&lt;li&gt;clustering over multiple instances&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So what (type of libraries) do data scientists use to connect different languages/processes?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bodo&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I've been toying with this idea for a while. I think there is probably some method in the text mining literature, but I haven't come across anything just right...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is/are some methods for tackling a problem where the number of variables it its self a variable.  This is not a missing data problem, but one where the nature of the problem fundamentally changes. Consider the following example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I want to predict who will win a race, a simple multinomial classification problem. I have lots of past data on races, plenty to train on.  Lets further suppose I have observed each contestant run multiple races. The problem however is that the number or racers is variable. Sometimes there are only 2 racers, sometimes there are as many as 100 racers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One solution might be to train a separate model for each number or racers, resulting in 99 models in this case, using any method I choose.  E.g. I could have 100 random forests. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another solution might be to include an additional variable called 'number_of_contestants' and have input field for 100 racers and simply leave them blank when no racer is present.  Intuitively, it seems that this method would have difficulties predicting the outcome of a 100 contestant race if the number of racers follows a Poisson distribution (which I didn't originally specify in the problem, but I am saying it here).    &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thoughts?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I would like to know how exactly mahout user based and item based recommendation differ from each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It defines that&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://mahout.apache.org/users/recommender/userbased-5-minutes.html&quot;&gt;User-based&lt;/a&gt;: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html&quot;&gt;Item-based&lt;/a&gt;: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed off line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But though there are two kind of recommendation available, what I understand is that both these will take some data model ( say 1,2 or 1,2,.5 as item1,item2,value or user1,user2,value where value is not mandatory) and will perform all calculation as the similarity measure and recommender build-in function we chose and we can run both user/item based recommendation on the same data ( is this a correct assumption ?? ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I would like to know how exactly and in which all aspects these two type of algorithm differ. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am interested in graph problems like 2-color, max-clique, stable sets, etc but the documentation for scipy.optimize.anneal seems to be for ordinary functions. How would one apply this library towards graph formulations?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I came across a package in R which has a function called &lt;code&gt;sann&lt;/code&gt; for simulated annealing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;sann&lt;/code&gt; uses parameters &lt;code&gt;fn&lt;/code&gt; and &lt;code&gt;gr&lt;/code&gt; to optimize and to select new points, respectively. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For something like the max-clique or max-stable set problems, &lt;code&gt;fn&lt;/code&gt; would be a summing function, but it's less clear how one would formulate &lt;code&gt;gr&lt;/code&gt; to fix these graph computations. In these cases, how would &lt;code&gt;gr&lt;/code&gt; &quot;select&quot;?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;My data looks like this: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/9LgwU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this error showing up?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Can anyone suggest any good books to learn hadoop and map reduce basics?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also something for Spark, and Spark Streaming?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Although I have seen a few good questions asked about data anonymization, I was wondering if there were answers to this more specific variant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am seeking a tool (or to design one) that will anonymize human names from a specific country: particularly first names in unstructured text. Many of the tools that I have seen have considered the wider dimensions of data anonymization; with an equal focus on dates of birth, addresses, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An imperative aspect is that it needs to have near absolute recall. The major pitfalls, as far as I can see, are diminutive variants (&quot;Tommy&quot; instead of &quot;Thomas&quot;, &quot;Ben&quot; instead of &quot;Benjamin&quot;, etc.) and typos. These two factors prevent a simple regex based on a database of names (based on censuses, etc.)&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Few things in life give me pleasure like scraping structured and unstructured data from the Internet and making use of it in my models. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, the Data Science Toolkit (or &lt;code&gt;RDSTK&lt;/code&gt; for R programmers) allows me to pull lots of good location-based data using IP's or addresses and the &lt;code&gt;tm.webmining.plugin&lt;/code&gt; for R's &lt;code&gt;tm&lt;/code&gt; package makes scraping financial and news data straightfoward. When going beyond such (semi-) structured data I tend to use &lt;code&gt;XPath&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm constantly getting throttled by limits on the number of queries you're allowed to make. I think Google limits me to about 50,000 requests per 24 hours, which is a problem for Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From a &lt;em&gt;technical&lt;/em&gt; perspective getting around these limits is easy -- just switch IP addresses and purge other identifiers from your environment. However, this presents both ethical and financial concerns (I think?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a solution that I'm overlooking?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;The &lt;code&gt;mnlogit&lt;/code&gt; package in R allows for the fast estimation of multinomial logit models.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The specification of forumlas is a bit different from most other regression models/packages in R, however. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the &lt;code&gt;Fish&lt;/code&gt; dataset as a reproducible example, &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; require(mnlogit)&#xA;Loading required package: mnlogit&#xA;&#xA;Package: mnlogit&#xA;Version: 1.1.1&#xA;Multinomial Logit Choice Models.&#xA;Scientific Computing Group, Sentrana Inc, 2013.&#xA;&#xA;&amp;gt; data(Fish, package ='mnlogit')&#xA;&amp;gt; head(Fish)&#xA;           mode   income     alt   price  catch chid&#xA;1.beach   FALSE 7083.332   beach 157.930 0.0678    1&#xA;1.boat    FALSE 7083.332    boat 157.930 0.2601    1&#xA;1.charter  TRUE 7083.332 charter 182.930 0.5391    1&#xA;1.pier    FALSE 7083.332    pier 157.930 0.0503    1&#xA;2.beach   FALSE 1250.000   beach  15.114 0.1049    2&#xA;2.boat    FALSE 1250.000    boat  10.534 0.1574    2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm trying to understand the difference between the model specification of &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fm &amp;lt;- formula(mode ~ 0 + price | income | catch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fm &amp;lt;- formula(mode ~ 0 + price | income + catch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;while the documentation covers the detail of such changes  in the general coeffcient area of the forumla (i.e. where &lt;code&gt;price&lt;/code&gt; is), I don't see an explanation of how operators like &lt;code&gt;+&lt;/code&gt; affect the alternative-specific area of the formula/code, relative to &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to create 3D bars on this map. Can anyone please advise if this is possible, and how?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://leafletjs.com/examples/choropleth.html&quot; rel=&quot;nofollow&quot;&gt;http://leafletjs.com/examples/choropleth.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data: UFO sightings in the USA (location wise).&#xA;Count of these sightings per location will be the height of the 3D bar.&#xA;Base map is a choropleth with US population density values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't mind integrating Javascript or d3.js into the code to create the 3D bars.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Using SAS Studio (online, student version)...&#xA;Need to do a &quot;nested likelihood ratio test&quot; for a logistic regression. &#xA;Entirety of instructions are: &quot;Perform a nested likelihood ratio test comparing your full model (all predictors included)to a reduced model of interest.&quot;&#xA;The two models I have are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Proc Logistic Data=Project_C;&#xA;Model Dem (event='1') = VEP TIF Income NonCit Unemployed Swing;&#xA;Run;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Proc Logistic Data=Project_C;&#xA;Model Dem (Event='1') = VEP TIF Income / clodds=Wald clparm=Wald expb rsquare;&#xA;Run;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I honestly have no idea where to even start. &#xA;Any suggestions would be appreciated. &#xA;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In recent years, the term &quot;data&quot; seems to have become a term widely used without specific definition. Everyone seems to use the phrase. Even people as technology-impaired as my grandparents use the term and seem to understand words like &quot;data breach.&quot; But I don't understand what makes &quot;data science&quot; a new discipline. Data has been the foundation of science for centuries. Without data, there would be no Mendel, no Schrödinger, etc. You can't have science without interpreting  and analyzing data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But clearly it means something. Everyone is talking about it.  So what exactly do people mean by data when they use terms like &quot;big data&quot; and why has this become a discipline in itself? Also, if it is an emerging discipline, where can I find more serious/in-depth information so I can better educate myself? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;For simplicity let's assume the feature space is the XY plane.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm running a test on MapReduce algorithm in different environments, like Hadoop and MongoDB, and using different types of data. What are the different methods or techniques to find out the execution time of a query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I'm inserting a huge amount of data, consider it to be 2-3GB, what are the methods to find out the time for the process to be completed.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to set up a cluster (1 namenode, 1 datanode) on AWS.&#xA;I'm using free one year trial period of AWS, but the challenge is, instance is created with 1GB of RAM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I'm a student, I cannot afford much. Can anyone please suggest me some solution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, it would be great if you could provide any links for setting up multi cluster hadoop with spark on AWS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I cannot try in GCE as my trial period is exhausted. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Having:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a set of soft fuzzy classifiers (classification onto overlapping sets) $C_i(x) \\\\to [0,1]$;&lt;/li&gt;&#xA;&lt;li&gt;a corresponding set of weak estimators $R_i(z)$ of the form $R_i(z) = \\\\mathit{EX}(y\\\\mid z)$.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The estimators $R_i$ are just some kind of regression, Kalman or particle filters. The classifiers $C_i$ are fixed and static. How to make a strong estimator out of a weighted combination of the form:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$L(x, z) = \\\\sum_{i}C_i(x)R_i(z)Q_i$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words how to choose the weights $Q_i$? Is there some kind of online approach to this problem? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is brief description of a practical application. When an event $E$ is registered, multiple measurements are made. Based on these measurements, the classifiers $C_i$ make a soft assignment of the event to multiple overlapping categories. What we get is fit ratios for the soft clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now there is some chance that event $E$ may trigger a subsequent event $D$, depending on another variable $z$ -- independent from the event $E$. We know that all the soft cluster &quot;memberships&quot; may influence the probability of event $D$ being triggered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We want to estimate the probability that $E$ triggers $D$, given the $C_i$ fitness ratios and value of $z$.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Are there any algorithms which were developed using partial differential equations for tackling some of the machine learning problems? Most works I see online are in the field of computer vision and a few bizarre ones in topic modelling. But just curious if someone has used or seen it being used for some decision making process or classification problems?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm new to apache spark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to configure multi cluster spark without hadoop?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, can you please provide the steps. &#xA;I would like to create clusters on Google Compute Engine  (1-master, 1-worker)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Say I used spectral clustering to cluster a data-set $D$ of points $X_0 - X_n$ into a number $C$ of clusters. How can I efficiently assign a new single point $X_{n+1}$ to his convenient cluster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set $X_0 - X_{n+1}$), or is there an optimized way to extend to the point $X_{n+1}$?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Are there any articles or discussions about extracting part of text that holds the most of information about current document.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I have a large corpus of documents from the same domain. There are parts of text that hold the key information what single document talks about. I want to extract some of those parts and use them as kind of a summary of the text. Is there any useful documentation about how to achieve something like this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be really helpful if someone could point me into the right direction what I should search for or read to get some insight in work that might have already been done in this field of Natural language processing.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;&lt;br&gt;&#xA;Im traying to integrate Hadoop and R, I was install the pachages rJava and Rhipe in R, I do this steps to start Hadoop and R:&lt;br&gt;&#xA;-starting Hadoop services.,&lt;br&gt;&#xA;-loading rJava and Rhipe packages by library function.&lt;br&gt;&#xA;-Calling rhinit() to initialize Rhipe.&lt;br&gt;&#xA;the problem here is when I call rhinit() funtion, it show this error:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;blockquote&gt;&#xA;    &lt;p&gt;Initializing Rhipe v0.73&lt;br&gt;&#xA;    Error in .jnew(&quot;org/godhuli/rhipe/PersonalServer&quot;) : &#xA;     java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream'&lt;/p&gt;&#xA;  &lt;/blockquote&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;please some helps to fixe this problem.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering if there is a web framework well suited for placing recommendations on content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In most cases, a data scientist goes through after the fact and builds (or uses) a completely different tool to create recommendations. This involves analyzing traffic logs, a history of shopping cart data, ratings, and so forth. It usually comes from multiples sources (the web server, the application's database, Google Analytics, etc) and then has to be cleaned up and processed, THEN delivered back to the application in way it understands.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a web framework on the market which handles collecting this data up front, as to minimize the retrospective data wrangling?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a huge file of customer complaints about the products my company owns and I would like to do a data analysis on those descriptions and tag a category to each of them. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: I need to figure out the number of complaints on &lt;strong&gt;&lt;em&gt;Software&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Hardware&lt;/em&gt;&lt;/strong&gt; side of my product from the customer complaints. Currently, I am using excel to do the data analysis which do seek a significant amount of manual work to get a tag name to the complaints.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I'm still new to this topic and confuse the terms quite often. So here is the changed question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine that we pick kNN for some dataset. I want to remain as general as possible, thus $k$ will not be specified here. Further we select, at some point, an observation where the number of neighbors that fulfill the requirement to be in the neighbourhood are actually more than the specified $k$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighborhood (number of neighbors). Which observations will be left out and why? Also is this a problem that occurs often, or is it something of an anomaly?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How can I connect to Titan database from Python ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I understand is that Titan (Graph database) provides an interface (Blueprint) to Cassandra (Column Store) and &#xA;bulb is a python interface to graph DB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how can I start programming in python to connect with titan DB?&#xA;Is there any good documentation/tutorial available ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I know that Spark is fully integrated with Scala. It's use case is specifically for large data sets. Which other tools have good Scala support? Is Scala best suited for larger data sets? Or is it also suited for smaller data sets? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to find an equivalent of Hinton Diagrams for multilayer networks to plot the weights during training. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The trained network is somewhat similar to a Deep SRN, i.e. it has a high number of multiple weight matrices which would make the simultaneous plot of several Hinton Diagrams visually confusing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of a good way to visualize the weight update process for recurrent networks with multiple layers? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't found much papers on the topic. I was thinking to display time-related information on the weights per layer instead if I can't come up with something. E.g. the weight-delta over time for each layer (omitting the use of every single connection). PCA is another possibility, though I'd like to not produce much additional computations, since the visualization is done online during training.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently going into the world of machine learning and Neural Networks, thanks to &lt;a href=&quot;https://github.com/cazala/synaptic&quot; rel=&quot;nofollow&quot;&gt;synaptic (js)&lt;/a&gt; that interests me a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I read a lot, wikipedia links and &lt;a href=&quot;https://github.com/cazala/synaptic/wiki/Neural-Networks-101&quot; rel=&quot;nofollow&quot;&gt;synaptic's NN 101&lt;/a&gt;, but there's a lot of basics questions that I don't understand (but I'd like to) in the use of machine learning (NN) and the point of these technologies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say, I wan't my network to (kind of) learn (something like) gravity, so to train it I set in input 10 objects with a mass, and a position x, y (and z) and I set output the new x, y (and z) of each objects.&#xA;I guess I should give it several configurations and everything but &lt;strong&gt;here is the question; can it, then, be able to compute the interactions between 10000, 100000 objects?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this stage in my learning, what I don't clearly get is what is the point of teaching/training neurons to compute XOR like it's shown in &lt;a href=&quot;https://github.com/cazala/synaptic#documentation&quot; rel=&quot;nofollow&quot;&gt;synaptic's documentation&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var trainingSet = [&#xA;    {&#xA;        input: [0,0],&#xA;        output: [0]&#xA;    },&#xA;    {&#xA;        input: [0,1],&#xA;        output: [1]&#xA;    },&#xA;    {&#xA;        input: [1,0],&#xA;        output: [1]&#xA;    },&#xA;    {&#xA;        input: [1,1],&#xA;        output: [0]&#xA;    },&#xA;];&#xA;&#xA;var trainer = new Trainer(myNetwork);&#xA;trainer.train(trainingSet);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Were we just give it all the possible inputs and outputs to a XOR.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Well, as I'm all new to the technologies I think my questions are full of non-sense and everything but thanks for reading and help you might bring :)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found out that &lt;a href=&quot;http://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot; rel=&quot;noreferrer&quot;&gt;tf-idf&lt;/a&gt; could be very useful for this. However, I was unsure as to &lt;em&gt;when&lt;/em&gt; exactly to use it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Retrieve site content, parse for plain text &lt;/li&gt;&#xA;&lt;li&gt;Normalize and stem content &lt;/li&gt;&#xA;&lt;li&gt;Tokenize into unigrams (maybe bigrams too)&lt;/li&gt;&#xA;&lt;li&gt;Retrieve a count of each unigram for the given document, filtering low length and low occurrence words&lt;/li&gt;&#xA;&lt;li&gt;Train a classifier such as NaiveBayes on the resulting set&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My question is the following: &lt;strong&gt;Where would tf-idf fit in here&lt;/strong&gt;? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any insight would be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Upon closer inspection, I think I may have run into a misunderstanding at to how TF-IDF operates. At the above step 4 that I describe, would I have to feed the &lt;em&gt;entirety&lt;/em&gt; of my data into TF-IDF at once? If, for example, my data is as follows: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[({tokenized_content_site1}, category_string_site1), &#xA; ({tokenized_content_site2}, category_string_site2), &#xA;...&#xA; ({tokenized_content_siten}, category_string_siten)}]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here, the outermost structure is a list, containing tuples, containing a dictionary (or hashmap) and a string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would I have to feed the &lt;em&gt;entirety&lt;/em&gt; of that data into the TF-IDF calculator at once to achieve the desired effect? Specifically, I have been looking at the &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html&quot; rel=&quot;noreferrer&quot;&gt;scikit-learn&lt;/a&gt; TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have used Neo4J to implement a content recommendation engine. I like Cypher, and find graph databases to be intuitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at scaling to a larger data set, I am not confident No4J + Cypher will be performant. Spark has the GraphX project, which I have not used in the past.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anybody switched from Neo4J to Spark GraphX? Do the use cases overlap, aside from scalability? Or, does GraphX address a completely different problem set than Neo4J?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to connect with cassandra in R using &lt;a href=&quot;http://cran.r-project.org/web/packages/RJDBC/index.html&quot; rel=&quot;nofollow&quot;&gt;RJDBC&lt;/a&gt;. When I execute &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;casscon &amp;lt;- dbConnect(cassdrv, &quot;jdbc:cassandra://ipaddrs:9160/demodb&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in .jcall(drv@jdrv, &quot;Ljava/sql/Connection;&quot;, &quot;connect&quot;, as.character(url)[1],&#xA;: java.lang.StringIndexOutOfBoundsException: String index out of range: -1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can't figure out the problem. I need a solution for this.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a corpus of text with a corresponding topics. For example &lt;code&gt;&quot;A rapper Tupac was shot in LA&quot;&lt;/code&gt; and it was labelled as &lt;code&gt;[&quot;celebrity&quot;, &quot;murder&quot;]&lt;/code&gt;. So basically each vector of features can have many labels (not the same amount. The first feature vector can have 3 labels, second 1, third 5).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I would have just one label corresponded to each text, I would try a &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes classifier&lt;/a&gt;, but I do not really know how should I proceed if I can have many labels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to transform Naive Bayes into multi label classification problem (if there is a better approach - please let me know)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; few things about the data I have.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;approximately 10.000 elements in the dataset&lt;/li&gt;&#xA;&lt;li&gt;text is approximately 2-3 sentences&lt;/li&gt;&#xA;&lt;li&gt;maximum 7 labels per text&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm wonder if it's possible to export a model trained in R, to OpenCV's Machine Learning (ML) library format? The latter appears to save/read models in &lt;a href=&quot;http://docs.opencv.org/modules/ml/doc/statistical_models.html#cvstatmodel-load&quot; rel=&quot;nofollow&quot;&gt;XML/YAML&lt;/a&gt;, whereas the former might be exportable via &lt;a href=&quot;http://cran.r-project.org/web/packages/pmml/index.html&quot; rel=&quot;nofollow&quot;&gt;PMML&lt;/a&gt;. Specifically, I'm working with Random Forests, which are classifiers available both in R and OpenCV's ML library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any advice on how I can get the two to share models would be greatly appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm working on a project which asks fellow students to share their original text data for further analysis using data mining techniques, and, I think it would be appropriate to anonymize student names with their submissions.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Setting aside the better solutions of a url where students submit their work and a backend script inserts the anonymized ID, &lt;strong&gt;What sort of solutions could I direct students to implement on their own to anonymized their own names?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm still a noob in this area. I don't know what are the norms. I was thinking the solution could be a hashing algorithm. That sounds like a better solution than making up a fake name as two people could pick the same fake name.possible people could pick the same fake name. &lt;strong&gt;What are some of the concerns I should be aware of?&lt;/strong&gt; &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&#xA;For an imbalanced set of data is it better to choose an L1 or L2 regularization?&lt;br&gt;&#xA;Is there a cost function more suitable to imbalanced datasets to improve the model score (log_loss in particular)?&lt;br&gt;&lt;br&gt;  &#xA;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;In &lt;a href=&quot;http://web.cse.ohio-state.edu/~mbelkin/papers/PLM_UCTHESIS_03.pdf&quot; rel=&quot;nofollow&quot; title=&quot;Belkin&amp;#39;s thesis about Laplacian Eigenmaps&quot;&gt;his thesis&lt;/a&gt; (section 2.3.3) Belkin uses the heat equation to derive an approximation for $\\\\mathcal{L}f$:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\\\\mathcal{L}f(x_i)\\\\approx \\\\frac{1}{t}\\\\Big(f(x_i)-\\\\alpha \\\\sum_{x_j, ||x_i-x_j||&amp;lt;\\\\epsilon}e^{-\\\\frac{||x_i-x_j||^2}{4t}}f(x_j)\\\\Big)$$&#xA;where $$\\\\alpha=\\\\Big(\\\\sum_{x_j, ||x_i-x_j||&amp;lt;\\\\epsilon}e^{-\\\\frac{||x_i-x_j||^2}{4t}}\\\\Big)^{-1}$$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm not sure how these considerations lead to this choice of weights for the weight matrix (which will be used to construct the Laplacian):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$W_{ij} =&#xA;\\\\begin{cases} &#xA;e^{-\\\\frac{||x_i-x_j||^2}{4t}} &amp;amp; if\\\\ ||x_i-x_j||&amp;lt;\\\\epsilon \\\\\\\\ &#xA;0 &amp;amp; otherwise &#xA;\\\\end{cases}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A very vague idea of mine was that the factors $\\\\alpha$ and $\\\\frac{1}{t}$ don't change for a given $x_i$ so if one choses the weights like above the resulting discrete Laplacian would (let aside those two constants) converge to the continuous version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or tips what I'd have to read up to in order to get a better understanding?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Is the k-nearest neighbour algorithm a discriminative or a generative classifier? My first thought on this was that it was generative, because it actually used Bayes's theorem to compute the posterior. Searching further for this, it seems like it is a discriminative model, but I couldn't find the explanation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is KNN discriminative first of all? And if it is, is that because it doesn't model the the priors or the likelihood?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to get to grips with sci-kit learn for some simple machine learning projects but I'm coming unstuck with Pipelines and wonder what I've done wrong...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to work through a &lt;a href=&quot;http://www.kaggle.com/c/data-science-london-scikit-learn/data&quot; rel=&quot;nofollow&quot;&gt;tutorial on Kaggle&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;&#xA;train = pd.read_csv(local path to training data) train_labels =&#xA;pd.read_csv(local path to labels)&#xA;&#xA;&#xA;from sklearn.decomposition import PCA&#xA;from sklearn.svm import LinearSVC&#xA;from sklearn.grid_search import GridSearchCV&#xA;&#xA;pca = PCA()&#xA;clf = LinearSVC()&#xA;&#xA;n_components = arange(1, 39)&#xA;loss = ['l1','l2']&#xA;penalty = ['l1','l2']&#xA;C = arange(0, 1, .1)&#xA;whiten = [True, False]&#xA;&#xA;from sklearn.pipeline import Pipeline&#xA;&#xA;#set up pipeline&#xA;pipe = Pipeline(steps=[('pca', pca), ('clf', clf)])&#xA;&#xA;#set up GridsearchCV&#xA;estimator = GridSearchCV(pipe, dict(pca__n_components = n_components, pca__whiten = whiten, clf__loss = loss, clf__penalty = penalty, clf__C = C)) &#xA;&#xA;&amp;gt; estimator&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Returns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=None,&#xA;       estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,&#xA;     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',&#xA;     random_state=None, tol=0.0001, verbose=0))]),&#xA;       fit_params={}, iid=True, loss_func=None, n_jobs=1,&#xA;       param_grid={'clf__penalty': ['l1', 'l2'], 'clf__loss': ['l1', 'l2'], 'clf__C': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9]), 'pca__n_components': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,&#xA;       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,&#xA;       35, 36, 37, 38]), 'pca__whiten': [True, False]},&#xA;       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,&#xA;       verbose=0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But when I try to train data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;estimator.fit(train, train_labels)&#xA;The error is:&#xA;&#xA;    428         for test_fold_idx, per_label_splits in enumerate(zip(*per_label_cvs)):&#xA;    429             for label, (_, test_split) in zip(unique_labels, per_label_splits):&#xA;--&amp;gt; 430                 label_test_folds = test_folds[y == label]&#xA;    431                 # the test split can be too big because we used&#xA;    432                 # KFold(max(c, self.n_folds), self.n_folds) instead of&#xA;&#xA;IndexError: too many indices for array&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can anyone point me in the right direction?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When classifying instances, the attribute with the missing value is simply not included in the probability calculation (&lt;a href=&quot;http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In training, &quot;the instance [with the missing data] is not included in frequency count for attribute value-class combination.&quot; (&lt;a href=&quot;http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html&quot; rel=&quot;nofollow&quot;&gt;http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does that mean that particular training record simply isn't included in the training phase? Or does it mean something else?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking to graph and interactively explore live/continuously measured data. There are quite a few options out there, with plot.ly being the most user-friendly. Plot.ly has a fantastic and easy to use UI (easily scalable, pannable, easily zoomable/fit to screen), but cannot handle the large sets of data I'm collecting. Does anyone know of any alternatives?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have MATLAB, but don't have enough licenses to simultaneously run this and do development at the same time. I know that LabVIEW would be a great option, but it is currently cost-prohibitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database. But it seams Neo4j doesn't scale well. The alternative I found out are Titan  and oriebtDB. I have gone through &lt;a href=&quot;http://db-engines.com/en/system/Neo4j%3BOrientDB%3BTitan&quot; rel=&quot;noreferrer&quot;&gt;this&lt;/a&gt; comparison on these three Databases, But I would like to get more details on  these databases. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support of these databases. Also is there any other good database options ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;With increasingly sophisticated methods that work on large scale datasets, financial applications are obvious. I am aware of machine learning being employed on financial services to detect fraud and flag fraudulent activities but I have a lesser understanding of how it helps to predict the price of the stock the next day and how many stocks of a particular company to buy. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do the hedge funds still employ portfolio optimization techniques that are right out of the mathematical finance literature or have they started to use machine learning to hedge their bets? More importantly, what are the features that are used by these hedge funds and what is a representative problem set up? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I was wondering if anyone knew which piece of software is being used in this video? It is an image recognition system that makes the training process very simple.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098&quot; rel=&quot;nofollow&quot;&gt;http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example is with car images, though the video should start at the right spot.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm training random forest models in R using randomForest() with 1000 trees and data.frames with about 20 predictors and 600K rows. On my laptop everything works fine but when I move to amazon ec2 to run the same thing i get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error: cannot allocate vector of size 5.4 Gb&#xA;Execution halted&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm using the c3.4xlarge instance type so it's pretty beefy... does anyone know a workaround for this to get it to run on this instance? I would love to know the memory nuances that causes this problem only on the ec2 instance and not on my laptop (OS X 10.9.5 Processor  2.7 GHz Intel Core i7; Memory  16 GB 1600 MHz DDR3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a non-function (not in closed form) that takes in a few parameters (about 20) and returns a real value. A few of these parameters are discrete while others are continuous. Some of these parameters can only be chosen from a finite space of values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I don't have the function in closed form, I cannot use any gradient based methods. However, the discrete nature and the boxed constraints on a few of those parameters restrict even the number of derivative free optimization techniques at my disposal. I am wondering what are the options in terms of optimization methods that I can use. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;My apologies in advance as I am new to this. I have searched the internet and tried various processes and nothing seems to work or address this situation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset of 30,000 transactions and 500,000 items. Average item size for a transaction is 50. The dataset is sparse, so the support number must be set quite low. Furthermore, the rules become more valuable the larger the number of items in the rule. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried running this in arules and the tests fail after exceeding 64 gb of RAM (the limit of the machine). I have tried reducing items and transactions to smaller subsets, but still hit this memory limit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately, I am looking for ways to cluster large groups of similar accounts by selection of items and generate confidence and lift of various next items selected from those clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question: are there alternative, more efficient ways to do this, or other approaches to consider?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;h3&gt;Background&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I'm working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the things I've been working on is clustering of these time series. My work is academic for the moment, and while I'm doing other analysis of the data as well, I have a specific goal to carry out some clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I've found several papers related to this.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Question&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case the above is unclear, consider these examples:&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example 1&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example 2&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Possible Directions&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I've wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I'd need to be able to detect such changes. Also, I just don't know if this is a suitable way or working with the data.&lt;/li&gt;&#xA;&lt;li&gt;I've also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I'd then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I &lt;em&gt;think&lt;/em&gt; this would work, but it's potentially quite onerous and I'd hate to go down this path if there's a better way that I'm missing.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Further Notes&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;These are things that have come up in comments, or things I've thought of due to comments, which might be relevant. I'm putting them here so people don't have to read through everything to get relevant information.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I'm working in Python, but have rpy for those places where R is more suitable. I'm not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I'm happy to figure out implementation details myself.&lt;/li&gt;&#xA;&lt;li&gt;I have a lot of working &quot;rough draft&quot; code - I've done some DTW runs, I've done a couple of different types of clustering, etc. I think I largely understand the direction I'm taking, and what I'm really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).&lt;/li&gt;&#xA;&lt;li&gt;I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: &lt;a href=&quot;http://www.cs.ucr.edu/~eamonn/selected_publications.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.cs.ucr.edu/~eamonn/selected_publications.htm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is that I have an over-represented category which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one category, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My first question:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My second question:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as &quot;Entertainment&quot;, it might receive &quot;News&quot; now). Is this indicative of an error in my implementation, or expected?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to figure out a strange phenomenon, when I use matrix factorization (the Netflix Prize solution) for a rating matrix: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$R = P^T * Q + B_u + B_i$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with ratings ranging from 1 to 10.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I evaluate the model by each label's absolute mean average error in test set, the first column is origin_score, the second(we don't transform the data, then train and its  prediction error), the third(we transform the data all by dividing 2, train, and when I use this model to make prediction, firstly reconstruct the matrix and then just multiply 2 and make it back to the same scale)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you see, in grade 3-4 (most samples are label from 3-4), it's more precise while in high score range(like 9 and 10, just 2% of the whole traiing set), it's worse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;+----------------------+--------------------+--------------------+&lt;br&gt;&#xA;| rounded_origin_score | abs_mean_avg_error | abs_mean_avg_error  | &#xA;+----------------------+--------------------+---------------------+&lt;br&gt;&#xA;| 1.0                  | 2.185225396100167  |  2.559125413626183  |&#xA;| 2.0                  | 1.4072212825108161 |  1.5290497332538155 |&#xA;| 3.0                  | 0.7606073396581479 |  0.6285151230269825 |&#xA;| 4.0                  | 0.7823491986435621 |  0.6419077576969795 |&#xA;| 5.0                  | 1.2734369551159568 |  1.256590210555053  |&#xA;| 6.0                  | 1.9546560495715863 |  2.0461809588933835 |&#xA;| 7.0                  | 2.707229888048017  |  2.8866856489147494 |&#xA;| 8.0                  | 3.5084244741417137 |  3.7212155956153796 |&#xA;| 9.0                  | 4.357185793060213  |  4.590550124054919  |&#xA;| 10.0                 | 5.180752400467891  |  5.468600926567884  |&#xA;+----------------------+--------------------+---------------------+&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've re-train the model several times, and got same result, so I think it's not effect by randomness.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    date        timeframe  fruit_amt   veg_amt &amp;lt;br/&amp;gt; &#xA;4352  05/23/2013  before     0.25        0.75 &amp;lt;br/&amp;gt; &#xA;5002  05/24/2014  after      0.06        0.25 &amp;lt;br/&amp;gt; &#xA;4352  04/16/2014  after      0           0 &amp;lt;br/&amp;gt; &#xA;4352  05/23/2013  after      0.06        0.25 &amp;lt;br/&amp;gt; &#xA;5002  05/24/2014  before     0.75        0.25 &amp;lt;br/&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Desired:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    B_fr05/23/2013   B_veg05/23/2013  A_fr05/23/2013  A_veg05/23/2013   B_fr05/24/2014   B_veg05/24/2014   (etc)  &amp;lt;br/&amp;gt;&#xA;4352  0.25             0.75             0.06            0.25              .                .  &amp;lt;br/&amp;gt;&#xA;5002  .                .                .               .                 0.75             0.25 &amp;lt;br/&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I recently read a lot about the &lt;strong&gt;n-armed bandit problem&lt;/strong&gt; and its solution with various algorithms, for example for webscale content optimization. Some discussions were referring to '&lt;strong&gt;contextual bandits&lt;/strong&gt;', I couldn't find a clear definition what the word 'contextual' should mean here. Does anyone know what is meant by that, in contrast to 'usual' bandits?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I was looking to learn about Bayesian theory in decision tree and how it avoids overfitting but couldn't find any tutorials for someone just starting. Do you know any resources to learn about it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I would like to pick up on the topic of deep learning. Should I begin from the topic of AI before working my way into Deep learning?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am working on a project with two data sets. A time vs. speed data set (let's call it traffic), and a time vs. weather data set (called weather). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking to find a correlation between these two sets using Pig. However the traffic data set has the time field, D/M/Y hr:min:sec, and the weather data set has the time field, D/M/Y. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to this I would like to average the speed per day and put it into a single D/M/Y value inside the traffic file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then plan to use:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;data = JOIN speed BY day, JOIN weather BY day with 'merge'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I will then find the correlation using: (I am borrowing this code from elsewhere)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;set = LOAD 'data.txt' AS (speed:double, weather:double)&#xA;rel = GROUP set ALL&#xA;cor = FOREACH rel GENERATE COR(set.speed, set.weather)&#xA;dump cor;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my first experience with Pig (I've never even used SQL), so I would like to know a few things:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. How can I merge the rows of my traffic file (ie. average D/M/Y hr:min:sec into D/M/Y)?&#xA;2. Is there a better way to find a correlation between the fields of different datasets?&#xA;3. Are the JOIN BY and the COR() functions used appropriately in my above code?  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am new to data science/ machine learning world. I know that in Statistics we assume that a certain event/ process has some particular distribution and the samples of that random process are part of some sampling distribution. The findings from the data could then be generalized by using confidence intervals and significance levels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we generalize our findings once we &quot;learn&quot; the patterns in the data set? What is the alternative to confidence levels here?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to determine what is the best number of hidden neurons for my MATLAB neural network. I was thinking to adopt the following strategy:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Loop for some values of hidden neurons, e.g. 1 to 40;&lt;/li&gt;&#xA;&lt;li&gt;For each NN with a fixed number of hidden neurons, perform a certain number of   training (e.g. 40, limiting the number of epoch for time reasons: I was thinking to doing this because the network seems to be hard to train, the MSE after some epochs is very high)&lt;/li&gt;&#xA;&lt;li&gt;Store the MSE obtained with all the nets with different number of hidden neurons&lt;/li&gt;&#xA;&lt;li&gt;Perform the previous procedure more than 1 time, e.g. 4, to take into account the initial random weight, and take the average of the MSEs&lt;/li&gt;&#xA;&lt;li&gt;Select and perform the &quot;real&quot; training on a NN with a number of hidden neurons such that the MSE previously calculated is minimized&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The MSE that I'm referring is the validation MSE: my samples splitting in trainining, testing and validation to avoid overfitting is 70%, 15% and 15% respectively)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other informations related to my problem are:&lt;br&gt;&#xA;fitting problem&lt;br&gt;&#xA;9 input neurons&lt;br&gt;&#xA;2 output neurons&lt;br&gt;&#xA;1630 samples  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This strategy could be work? Is there any better criterion to adopt? Thank you&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: Test done, so the result suggest me to adopt 12 neurons? (low validation MSE and  number of neurons lower than 2*numberOfInputNeurons? but also 18 could be good...&#xA;&lt;img src=&quot;https://i.stack.imgur.com/qxOSV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In &lt;a href=&quot;http://www.wired.com/2014/01/how-to-hack-okcupid/all/&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; article, Chris McKinlay says he used AdaBoost to choose the proper &quot;importances&quot; of questions he answered on okcupid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you haven't read and don't want to read the article, or are unfamiliar with okcupid and the question system, here's the data and problem he had:&#xA;The goal is to &quot;match&quot; as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions. These questions may have between 2 and 4 answers each, and for the sake of simplicity, let's pretend that the formula for a match% $\\\\ M $ between you and another user is given by&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\ M = Q_a/Q_c $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\\\\ Q_c $ is the number of questions you and the other user have in common, and&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\ Q_a $ is the number of questions you both answered with the same value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The real formula is slightly more complex, but the approach would be the same regarding &quot;picking&quot; a correct answer (he actually used boosting to find the ideal &quot;importance&quot; to place on a given question, rather than the right answer).&#xA;In any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of $\\\\ M $ over all users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I've watched the MIT course on AI up to and including the lecture on boosting, but I don't understand how you would apply it to a problem like this. Honestly I don't even know where to begin with choosing rules for the weak learners. I don't have any &quot;rules&quot; about what values to choose for each question (if the user is under 5'5, choose A, etc) - I'm just trying to fit the data I have.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this not the way boosting is supposed to be used? Is there likely some other optimization left out of how he figured this out?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;If I execute the following code I have no problem:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require(foreign)&#xA;require(nnet)&#xA;require(ggplot2)&#xA;require(reshape2)&#xA;&#xA;ml &amp;lt;- read.dta(&quot;http://www.ats.ucla.edu/stat/data/hsbdemo.dta&quot;)&#xA;ml$prog2 &amp;lt;- relevel(ml$prog, ref = &quot;academic&quot;)&#xA;test &amp;lt;- multinom(prog2 ~ ses + write, data = ml)&#xA;predict(test, newdata = dses, &quot;probs&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but if I try:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require(caret)&#xA;ml &amp;lt;- read.dta(&quot;http://www.ats.ucla.edu/stat/data/hsbdemo.dta&quot;)&#xA;ml$prog2 &amp;lt;- relevel(ml$prog, ref = &quot;academic&quot;)&#xA;test &amp;lt;- train(prog2 ~ ses + write,method=&quot;multinom&quot; ,data = ml)&#xA;predict(test$finalModel, newdata = dses, &quot;probs&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it returns &lt;code&gt;Error in eval(expr, envir, enclos) : object 'sesmiddle' not found&lt;/code&gt;, why?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I have done so far is to break each serie into daily data, for exemple: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;from:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2013-03-03 - 2013-03-09 37&lt;/p&gt;&#xA;&#xA;&lt;p&gt;to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2013-03-03 37 &#xA;2013-03-04 37&#xA;2013-03-05 37&#xA;2013-03-06 37&#xA;2013-03-07 37&#xA;2013-03-08 37&#xA;2013-03-09 37&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other approach would be to &quot;merge&quot; daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don't know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So three questions: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel like breaking weekly/monthly data into daily data like i've done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last but not least: &lt;strong&gt;when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?&lt;/strong&gt; I think this is a compromise between the number of data and the complexity of the model but I can't see any strong argument to choose between those options. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to predict clients comportement from market rates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The value of the products depends on the actual rate but this is not enough. The comportement of the client also depends on their awareness wich depends on the evolution of rates. I've added this in model using past 6 month rates as features in polynomial regression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact media coverage of rate mostly depends on rate variations and I wanted to add that in my model.  The idea would be to add a derivative/variation of rate as a feature. But I anticipated something wrong, example with only two month , my variation will be of the form $x_n - x_{n-1}$ that is a simple linear combination of actual and past rates. So for a 1d polynomial regression i will have:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ x_{n+1} = a * x_{n} +  b * x_{n-1} + c * (x_{n} - x_{n-1})$$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;instead of: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ x_{n+1} = a_0 * x_{n} +  b_0 * x_{n-1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;wich is strictly equivalent with $ a + c = a_0 $ and $b-c= b_0$. Higher polynomial degree results in a more or less equivalent result. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking about a way to include derivative information but it seems not possible. So I am wondering if all the information is included in my curve. Is this a general idea ? all information is somewhat directly contained in data and modifications of features will result in higher order objective function ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to predict a time serie from another one. My approach is based on a moving windows. I predict the output value of the serie from the following features: the previous value and the 6 past values of the source serie. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it usefull to add the previous value of the time serie ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel like I don't use all the information contained in the curve to predict futures values. But I don't see how it would be possible to use all previous data to predict a value (first, the number of features would be growing trough time...).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the caveats of a 6 month time-window approach ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any paper about differents method of feature selection for time-series ? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Let's assume I'm building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it's a history of which pages each user has viewed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I'd like to explore alternatives to see which method gives the most relevant recommendations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition to my simple traversal, I'm curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I've looked at &lt;a href=&quot;http://snap.stanford.edu/&quot; rel=&quot;nofollow&quot;&gt;SNAP&lt;/a&gt;, it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Recently, I studied a paper called &quot;What Does Your Chair Know About Your Stress Level?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be download at the link below.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;at page 210, Fig.5 (picture 5) mentioned that&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Spectra of the norm of the CoP vector during the experiment (stages 3–7) for the same subject used in Fig. 4.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How did they do to transform Fig.4 to Fig.5? And what do x-axis and y-axis mean in Fig.5?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fig4&#xA;&lt;img src=&quot;https://i.stack.imgur.com/Iq7qc.png&quot; alt=&quot;Fig4&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fig5&#xA;&lt;img src=&quot;https://i.stack.imgur.com/7HPxu.png&quot; alt=&quot;Fig5&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a &lt;strong&gt;dataset of xyz coordinates with a date component&lt;/strong&gt; in a pandas dataframe&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ex:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;date1: $[x_1,y_1,z_1]$,&lt;/li&gt;&#xA;&lt;li&gt;date2: $[x_2,y_2,z_2]$,&lt;/li&gt;&#xA;&lt;li&gt;date3: $[x_3,y_3,z_3]$,&#xA;..&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I would like to &lt;strong&gt;classify a sample of object positions over the period of a week&lt;/strong&gt;&#xA;(using indexes to re-map the classification label back to the date), like this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Week 1: $[x_1,y_1,z_1], [x_2, y_2, z_2], [x_3,y_3,z_3], [x_4,y_4,z_4], [x_5,y_5,z_5], [x_6,y_6,z_6], [x_7,y_7,z_7]$,&lt;/li&gt;&#xA;&lt;li&gt;Week 2: $[x_8,y_8,z_8],[x_9,y_9,z_9],[x_{10},y_{10},z_{10}],[x_{11},y_{11},z_{11}],[x_{12},y_{12},z_{12}],[x_{13},y_{13},z_{13}],[x_{14},y_{14},z_{14}]$,&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;When I try to run KMeans it returns&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;k_means = KMeans(n_clusters=cclasses)&#xA;k_means.fit(process_set.hpc)&#xA;date_classes = k_means.labels_&#xA;&#xA;ValueError: Found array with dim 3. Expected &amp;lt;= 2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?&lt;/li&gt;&#xA;&lt;li&gt;Are there any other methods I could use?&lt;/li&gt;&#xA;&lt;li&gt;Am I doing everything completely backwards and should consider a different approach, any thoughts?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Imagine modeling the &quot;&lt;em&gt;input(plaintext) - output(ciphertext)&quot;&lt;/em&gt; pairs of an encryption algorithm as a data science problem. Very informally, the strength of an encryption scheme is measured by the randomness (unpredictability, or entropy) of the output. This is counter-intuitive to classic regression problems, which are frequently used for prediction. Say, informally, the strength of an encryption scheme is determined by the number of such input-output pairs needed beyond which it becomes predictable. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we model this as data science problem? Given all pairs of two different encryption schemes, can we determine which is stronger just by using the input-output pairs of both the schemes? Is there any other way apart from regression to solve this problem?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;while comparing two different algorithms to feature selection I stumbled upon the follwing question: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a given dataset with a discrete class variable we want to train a naive bayes classifier. We decide to conduct feature selection during preprocessing using naive bayes in a wrapper approach. &#xA;Does this method of feature selection consider the size of the used feature subsets? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/2Vh2Q.png&quot; alt=&quot;Naive Bayes Classification&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When considering how NB classifies a given instance, the size of the feature subset being used for classification only influences the number of parts that the product of the conditional dependencies has but that does not make a difference, or does it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It'd be great if someone could offer a solid explanation since for me it's more of a gut feeling at the moment.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the &lt;strong&gt;captions in the properties&lt;/strong&gt; section.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Create statement:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LOAD CSV WITH HEADERS FROM &quot;File Location&quot; AS row CREATE (:NodeName {DATE_OCCURED: row.Date});&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Query to visualize the relationship between nodes:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MATCH (a)-[:`REL NAME`]-&amp;gt;(b) RETURN a,b LIMIT 25;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This gives me the values the node &quot;a&quot; and random numbers for all the node &quot;b&quot;. I don't know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don't get the actual values of the related nodes.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/xxKVi.png&quot; alt=&quot;Sample output for the relationship query&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;What's the best / easiest way to create a graph from address data?  For example, if I have 100 houses from all across a city is there any easy way to determine the shortest distance between two houses and all that good stuff?  Would this require changing the data into coordinates and using GIS software or can I get away with using Python or R?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering if e-commerce companies where products are offered by users, such as EBay, are using Object Recognition to ensure that an uploaded image corresponds to an specific type of object (clothing, shoes, glasses, etc) either to classify automatically or more importantly to filter undesired images (such as non related or even illegal types).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, which algorithms and/or open platforms could be use for doing so? From what I've looked it seems that HOG+Exemplar SVM might be one of the most accurate methods developed so far (&lt;a href=&quot;http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf&lt;/a&gt;), even having couple of public repo's with Matlab implementations (&lt;a href=&quot;https://github.com/quantombone/exemplarsvm&quot; rel=&quot;nofollow&quot;&gt;https://github.com/quantombone/exemplarsvm&lt;/a&gt;), but I'm still wondering if this is being used in industry.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;This refers to a system described in a book by Nick Lawrence titled &quot;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0975276107&quot; rel=&quot;nofollow&quot;&gt;Correlithm Object Technology&lt;/a&gt;&quot;. The author coined the term &quot;correlithm&quot; as a combination of &quot;correlation&quot; and &quot;algorithm&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Correlithm Objects (&quot;COs&quot;) described in the book are, in the author's view, &quot;primary data tokens&quot; in biological neural systems and are central to &quot;all high-level data representation, storage, and manipulation&quot;. In addition to this the author describes the COs as &quot;important mathematical objects of the statistics of bounded, high-dimensional spaces&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Considering these descriptions it seems like these COs could be useful for AI. What I was curious about is whether they are actually used anywhere in the industry, and if so, in what kind of situations?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am reading up about lambda architecture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It makes sense. we have queue based data ingestion. we have an in-memory store for data which is very new and we have HDFS for old data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we have our entire data set. in our system. very good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but the architecture diagram shows that the merge layer is able to query both the batch layer and the speed layer in one shot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your batch layer is probably a map reduce job or a HIVE query. The speed layer query is probably a scala program which is execution on the spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how will you merge these?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any guidance.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;For an upcoming project, I'm mining textual posts from an online forum, using Scrapy. What is the best way to store this text data? I'm thinking of simply exporting it into a JSON file, but is there a better format? Or does it not matter?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Im doing my academic project. im having the base paper for reference  the paper is IEEE paper &quot;effective and efficient clustering methods for correlated probabilistic graph&quot;. i wish to do this in R tool. in this paper two algorithm are implemented. i like to implement the peedr algorithm in the paper. how can i give the input for that algorithm.? suggest the packgages in R tool&#xA; the paper can be found here&lt;br&gt;&#xA;&lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474&quot; rel=&quot;nofollow&quot;&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474&lt;/a&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to develop my neural network with both early stopping and bayesian regularization (matlab implementation, lm algorithm is used for both).&#xA;Since in bayesian regularization I have not the validation set, how can I compare the generalization capability of the networks obtained with the two methodologies? &#xA;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm building a neural network to analyze a business' sales. I'm normalizing all input values to the range &lt;code&gt;{0,1}&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm struggling with the day of the week column. Business days are identified by a number ranging &lt;code&gt;{1-5}&lt;/code&gt; (1=Monday). Normalizing these values to the range &lt;code&gt;{0,1}&lt;/code&gt; is straightforward, but results in a major bias in the final output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason is that the full range of normalized values for the business day column is explored with every week worth of data, whereas other price-related column explore their full range of normalized values infrequently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The business day column ends up being the largest contributor to the final output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I normalize it to make its contribution more in tune with the rest of the inputs?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;It is often pointed out that &lt;em&gt;sample&lt;/em&gt; is an overloaded term in statistics and the sciences being supported by statistics.  In my field (geological sciences) as in most other sciences, the process of collecting meaningful data is critical and discussions about the traps and pitfalls in that process talk about &lt;em&gt;sampling&lt;/em&gt;.  Not far down the road from that, particularly when lab results are back, conversations involving statisticians, data scientists, geomathematicians, GIS analysts and even &lt;em&gt;normal&lt;/em&gt; geologists are likely to attempt to include multiple meanings of &lt;em&gt;sample&lt;/em&gt; in the same sentence!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Q: Have any data scientists (or statisticians) found practical ways to communicate these different meanings?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way is to always add soil, rock, statistical and so on before &lt;em&gt;sample&lt;/em&gt;.  But I was curious if there are any other approaches to effective communication that are in use.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am interested in finding a statistic that tracks the unpredictability of a time series. For simplicity sake, assume that each value in the time series is either 1 or 0. So for example, the following two time series are entirely predictable&#xA;TS1: 1 1 1 1 1 1 1 1&#xA;TS2: 0 1 0 1 0 1 0 1 0 1 0 1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the following time series is not that predictable:&#xA;TS3: 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for a statistic that given a time series, would return a number between 0 and 1 with 0 indicating that the series is completely predictable and 1 indicating the series in completely unpredictable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked at some entropy measures like Kolmogorov Complexity and Shannon entropy, but neither seem to fit my requirement. In Kolmogorov complexity, the statistic value changes depending on the length of the time series (as in &quot;1 0 1 0 1&quot; and &quot;1 0 1 0&quot; have different complexities, so its not possible to compare predictability of two time series with differing number of observations). In Shannon entropy, the order of observations didn't seem to matter. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers on what would be a good statistic for my requirement?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a data set, in excel format, with &lt;strong&gt;account names, reported symptoms, a determined root cause and a date in month year format&lt;/strong&gt; for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Account : &lt;em&gt;X&lt;/em&gt; Symptoms : &lt;em&gt;AB, AD, AB, AB&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Account : &lt;em&gt;Y&lt;/em&gt;  Symptoms : &lt;em&gt;AE, AE, AB, AB, EA&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the sake of this example, let's assume that all the dates are this month. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;O/P: Account : X Symptom: AE&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here both of them have reported &lt;em&gt;AB&lt;/em&gt; 2 or more times. I could fix such number as a threshold to look for probable symptoms. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to do a correlation analysis between inputs and outputs inspecting the data in order to understand which input variables to include. What could be a threshold in the correlation value to consider a variable eligible to be an input for my Neural Network?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is there any way to use package &quot;dplyr&quot; on RStudio having R base 3.0.2 ?  I am not interested  in &quot;plyr&quot; package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&#xA;Navin&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Could anyone recommend a good similarity measure for objects which have multiple classes, where each class is part of a hierarchy?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, let's say the classes look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 Produce&#xA;  1.1 Eggs&#xA;    1.1.1 Duck eggs&#xA;    1.1.2 Chicken eggs&#xA;  1.2 Milk&#xA;    1.2.1 Cow milk&#xA;    1.2.2 Goat milk&#xA;2 Baked goods&#xA;  2.1 Cakes&#xA;    2.1.1 Cheesecake&#xA;    2.1.2 Chocolate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;An object might be tagged with items from the above at any level, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Omelette: eggs, milk (1.1, 1.2)&#xA;Duck egg omelette: duck eggs, milk (1.1.1, 1.2)&#xA;Goat milk chocolate cheesecake: goat milk, cheesecake, chocolate (1.2.2, 2.1.1, 2.1.2)&#xA;Beef: produce (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the classes weren't part of a hierarchy, I'd probably I'd look at cosine similarity (or equivalent) between classes assigned to an object, but I'd like to use the fact that different classes with the same parents also have some similarity value (e.g. in the example above, beef has some small similarity to omelette, since they both have items from the class '1 produce').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it helps, the hierarchy has ~200k classes, with a maximum depth of 5.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a project where we would like to take the ratio of two measurements A/B and subject these ratios to a ranking algorithm. The ratio is normalized prior to ranking (though the ranking/normalization are not that import to my question).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In most cases measurement A (the starting measurement) is a count with values greater than 1000.  We expect an increase for measurement B for positive effects and a decrease in measurement B for negative effects.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the issue, some of our starting counts are nearly zero which we believe is an artifact of experimental preparation.  This of course leads to some really high ratios/scaling issues for these data points.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best way to adjust these values in order to better understand the real role in our experiment?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One suggestion we received was to add 1000 to all counts (from measurement A and B) to scale the values and remove the bias of such a low starting count,  is this a viable option?  Thank you in advance for your assistance, let me know if I am not being clear enough.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I would like to know if you people have some good tutorials (fast and straightforward) about topic models and LDA, teaching intuitively how to set some parameters, what they mean and if possible, with some real examples.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;(Me: Never learned calculus or advanced math and I started Stanford openclasses for machine learning. I know basic matrix calculations.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One chapter of my course is about cost function. I have been trying to find any example calculation of it with numbers. Googling only finds the same formula everytime, and also on Octave. But I want to do the same thing first with pen+paper and without it, I cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I require a cost function calculation example for following sample dataset:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#Rooms = Rent&#xA;1 = 4000&#xA;2 = 10000&#xA;3 = 22000&#xA;4 = 30000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to build a home server/workstation to run my R projects. Based on what I have gathered, it should probably be Linux based. I want to buy the hardware now, but I am confused with the many available options for processors/ram/motherboards. I want to be able to use parallel processing, at least 64GB? of memory and enough storage space (~10TB?). Software wise, Ubuntu?, R, RStudio, PostgreSQL, some NOSQL database, probably Hadoop. I do a lot of text/geospatial/network analytics that are resource intensive. Budget ~$3000US.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My Questions:&lt;/strong&gt;&lt;BR&gt;&#xA;What could an ideal configuration look like? (Hardware + Software)&lt;br&gt;&#xA;What type of processor?&lt;BR&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;BR&gt;&#xA;No, I don't want to use a cloud solution.&lt;BR&gt;&#xA;I know it is a vague question, but any thoughts will help, please?&lt;BR&gt;&#xA;If it is off-topic or too vague, I will gladly delete.&lt;BR&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cheers B&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it's hard to get a bird's eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All these vector space models tend to use cosine-similarity as the similarity function, however I have seen some literature discussing similarity functions which are more exotic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this basically where we currently are in attacking this particular type of problem? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a mysql database with the following format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id      string&#xA;1        foo1...&#xA;2        foo2...&#xA;..       ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are &gt;100k entries in this db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to do is for each string, compare it to each other string and store some metric of the comparison.  Doing this will essentially yield a 2D matrix of size &lt;code&gt;NxN&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is the number of row in the db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My initial thought was creating another db where each index corresponds to the string of the index in the first db and each column is the value from comparing the two strings. For example, id 1 column 2 in the second db would be the value outputted from comparing id1 and id2 in the first db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The format of the second db:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id    col1    col2    col3    ....&#xA;1       1      0.4     0.5    .....&#xA;...    ...     ...      ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This way of creating the second db would result in 100k rows x 100k columns, which is the issue at hand.  What is the best way to handle large data sets like this?  Is storing the data in a text file more efficient (say each text file corresponds to one row in the second db.)&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am currently collecting second by second data regarding buyer vs seller initiated trades for different financial instruments (securities mostly). If there are more buyer initiated trades in a given second, then that second's data point would contain a positive value in the pertinent feature. If there are more seller initiated trades, then there would be a negative value. And if either there is an equal amount of buy vs seller initiated trades OR if there are simply not any trades in a given second, there will be a 0 for the feature in that data point. Along with this feature, there are several other features that are based on what occurred in the preceding seconds (eg if the value discussed above was 12 for the data point immediately preceding the current point, then the second feature for the current data point would be 12 - please let me know if this is not clear) After much troubleshooting, I have concluded that if there are too many data points with too many 0's for features, the classifier simply wont work. When I print out the probabilities of evaluation data points falling into different classes, I simply get&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0:NaN,1:NaN&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;for all model evaluation points I try to classify. (I am using logistic regression from apache-mahout. In total have 183 features, but over 40million data points. There are three categories to which the data point can be classified)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have found that if I set the default value to 1, then I no longer encounter this error, e.g. if there are no trades, the value will be a 1, if there is one seller initiated trade, the value will be 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So with all this in mind, I have two related questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Has anyone else encountered this issue? e.g. if you have a vector with x features, and for a majority of the data points, a majority of the features contain 0's, is this know to give issues?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is shifting all values up by a constant (such as 1) a valid fix to this issue? I assume that if this constant is applied to all values, then it shouldn't skew the data, but I figure it won't hurt to check with the experts.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Also, I'm new to this, so if you believe that my question could use more info please let me know, and if you could give me ideas of what information to include, it would be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;thanks in advance&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am very new in machine learning. I have annotated data with category, aspect, opinion word and sentiment. for example, for the bellow text&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;The apple was really tasty&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have category-&gt;food, aspect-&gt; apple, opinion word -&gt;tasty and sentiment-&gt;positive. I have training data like this format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I train a SVM classifier using this format of training set?&#xA;How to extract features like n-gram, POS and sentiment word to train the classifier?&#xA;Could you please suggest any beginning step for this aspect based sentiment analysis using machine learning algorithms?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have 11 lottery tickets (used) and I have discovered that in each ticket, the 3rd digit's value is +1 of the value of the 6th digit. I have 11 tickets, each ticket is composed of 16 digits. Would someone (anyone) help me find the logic used here, in creating the other digits and their interrelation? I am a complete noob in data analysis, and any help would be greatly appreciated. For those who would like to know, I incidentally discovered the certain pattern, and made me realize these numbers are not totally random and if I could find the underlying pattern I'd be able to predict. The excel file is here &lt;a href=&quot;http://s000.tinyupload.com/index.php?file_id=39010250377074241779&quot; rel=&quot;nofollow&quot;&gt;Excel file&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How can I get information about an entity from DBpedia using Python?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eg: I need to get all DBpedia information about USA  (&lt;a href=&quot;http://dbpedia.org/page/United_States&quot; rel=&quot;nofollow&quot;&gt;http://dbpedia.org/page/United_States&lt;/a&gt;) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PREFIX db: &amp;lt;http://dbpedia.org/resource/&amp;gt;&#xA;SELECT ?p ?o&#xA;WHERE { db:United_States ?p ?o }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But here all DBpedia information is not displaying.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I do this and which all are the possible plugins/api available for python to connect with DBpedia ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also what will be the SPARQL query for generating the above problem result?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Given a dataset that has a binary (0/1) dependent variable and a large collection of continuous and categorical independent variables, is there a process and ideally a R package that can find combinations/subsets/segments of the IVs that are highly correlated with the DV?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple example: &#xA;DV: college education (0/1), and IVs: age (20 to 120), income (0 to 1 million), race (white, black, hispanic etc), gender (0/1), state, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then finding correlations combining IVs and subsets of IVs (e.g. women between 30 and 50, with incomes over 100k are highly positively correlated with the DV), and then being able to compare the combinations (e.g. to find out women between 30 and 40, with incomes over 100k have a higher correlation than women between 40 and 50, with incomes over 100k)&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have data, which looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqH9I.jpg&quot; alt=&quot;Data table&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These data are only for one subject. I will have a lot more.&lt;br&gt;&#xA;These data will be analyzed in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I'm storing them like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;subject &amp;lt;- rep(1, times = 24)&#xA;measurement &amp;lt;- factor(x = rep(x = 1:3, each = 8), &#xA;                      labels = c(&quot;Distance&quot;, &quot;Frequency&quot;, &quot;Energy&quot;))&#xA;speed &amp;lt;- factor(x = rep(x = 1:2, each = 4, times = 3), &#xA;                labels = c(&quot;speed1&quot;, &quot;speed2&quot;))&#xA;condition &amp;lt;- factor(x = rep(x = 1:2, each = 2, times = 6), &#xA;                    labels = c(&quot;Control&quot;, &quot;Experm&quot;))&#xA;Try &amp;lt;- factor(x = rep(x = 1:2, times = 12), &#xA;              labels = c(&quot;Try1&quot;, &quot;Try2&quot;))&#xA;result &amp;lt;- c(1:8, &#xA;            11:18, &#xA;            21:28)&#xA;&#xA;dt &amp;lt;- data.frame(subject, measurement, speed, condition, Try, result)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the appropriate way to store these data in R (in a data frame)? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the proper name for that? Which are examples of algorithms for the job?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&#xA;More specifically, I want to train audio source separation. The input is a mixed sound (spectrogram) and the output is the sound with some energy removed in certain frequencies. The function needs to recognize some pattern in the input and decide what to remove.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering if there's a way to automatically generate a list of tags for live chat transcripts without domain knowledge.  I've tried applying NLP chunking to the chat transcripts and keep only the noun phrases as tag candidates.  However, this approach would generate too many useless noun phrases.  I could use some rules to prune out some of them, but it would be hard to generalize the rules.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;This question might sound silly. But I have been wondering why do we assume that there is&#xA;a hidden probability distribution between input-output pairs in machine learning setup ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if we want to learn a function $f: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{Y}$, we generally tend to assume a probability distribution $\\\\rho(x,y)$ on $Z=\\\\mathcal{X} \\\\times \\\\mathcal{Y} $ and try to minimize the error &#xA;$$&#xA;\\\\mathcal{E}(f) = \\\\int (f(x)-y)^2 \\\\ d\\\\rho(x,y)&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the probability distribtution $\\\\rho$ inherent to the very nature of $Z$ or depends on $f$ ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone please provide a good intuitive explanation for this ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am currently working on a recommendation system for daily news. At first, I evaluated all the recommender algorithms and their corresponding settings (e.g., similarities, factorizers, ...etc) implemented in Mahout. Since we want to recommend daily news for users, we use the reading behavior of each user collected two days ago as training set, data of the next day as the testing set. The evaluated RMSE is good, the best recommender is SVD+SGD, so we implemented the recommender on our system for several days of trial run.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the result, the actually recommended news, seems to be not very attractive for real users (&quot;not attractive&quot; here means, the users feel like &quot;why you recommend this to me?&quot;). So we decided another approach: use the tags and categories and their relationship to do the main job of recommendation, the result from CF is for just supporting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This makes me wonder if CF if not appropriate for some kind of content. Because I also worked on movie and music recommendation, CF is a good tool. But for news, it seems not the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain why this happening, and also give some guideline about how to choose appropriate recommendation methods? Thanks:)&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I was wondering whether we could list machine learning winning methods to apply in many fields of interest: NLP, image, vision, medical, deep package inspection, etc. I mean, if someone will get started a new ML project, what are the ML methods that cannot be forgotten?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a data set of video watching records in a 3G network. In this data set, 2 different kind of features are included: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;user-side information, e.g., age, gender, data plan and etc; &lt;/li&gt;&#xA;&lt;li&gt;Video watching records of these users, each of  which associated with a download ratio and some detailed network condition metrics, say, download speed, RTT, and something similar.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, Given this data set, I want to predict the download ratio of each video.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since it is a regression problem, so I use &lt;em&gt;gradient boosting regression tree&lt;/em&gt; as model and run 10-fold cross validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have tried different model parameter configurations and even different models (linear regression, decision regress tree), the best root-mean-square error I can get is 0.3790, which is quite high, because if I don't use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890. There is not obvious difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For this problem, I have some questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Does this high error rate imply that the label in data set is unpredictable? &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Can anybody tell me what is the purpose of feature generation? and why feature space enrichment is needed before classifying an image? Is it a necessary step?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any method to enrich feature space?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;is there any comprehensive framework matching tasks with approaches or methods related to different types of machine learning related problems?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn a simple Gaussian?&lt;/strong&gt; &#xA;Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn a mixture of Gaussians (MoG)?&lt;/strong&gt; Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn any density?&lt;/strong&gt; Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l  ́ 2 error; Kernel density estimation (KDE), optimal kernel, KDE theory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I predict a continuous variable (regression)?&lt;/strong&gt; Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I predict a discrete variable (classification)?&lt;/strong&gt; Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which loss function should I use?&lt;/strong&gt; Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which model should I use?&lt;/strong&gt; AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (combined) models?&lt;/strong&gt; Ensemble learning theory; boosting; bagging; stacking&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (nonlinear) models?&lt;/strong&gt; Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (compositional) models?&lt;/strong&gt; Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I reduce or relate features?&lt;/strong&gt; Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I create new features?&lt;/strong&gt; principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I reduce or relate the data?&lt;/strong&gt; Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat time series?&lt;/strong&gt; ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat non-ideal data?&lt;/strong&gt; covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize the parameters?&lt;/strong&gt; Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize linear functions?&lt;/strong&gt; computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize with constraints?&lt;/strong&gt; Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I evaluate deeply-nested sums?&lt;/strong&gt; Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I evaluate large sums and searches?&lt;/strong&gt; Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat even larger problems?&lt;/strong&gt; Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I apply all this in the real world?&lt;/strong&gt; Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I want to visualize goal achievment progress.&#xA;This is my first idea:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;use area chart to show progress in current metric&lt;/li&gt;&#xA;&lt;li&gt;use horizontal band to show the goal value&lt;/li&gt;&#xA;&lt;li&gt;colorize areas under/above the band into &quot;positive&quot; and &quot;negative&quot; colors&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/h7fUa.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/EIhmz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is this approach informative enough? Are there better choises?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additional info:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;charts made in Tableau&lt;/li&gt;&#xA;&lt;li&gt;two data sources: metric progress &amp;amp; goals&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am COMPLETELY new to the field of Data Science, mainly because every employer I have worked for, simply COULDN'T sell any customers anything that would use techniques learned in this field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of particular interest to me is machine learning/Predictive Analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have attempted many &quot;test projects&quot; myself, but I seem to NEED some sort of outside &quot;catalyst&quot; to tell me a specific goal, and a specific set of guidelines, when I am trying to learn something.&#xA;Otherwise, I tend to lose focus, and jump from one interesting topic to the next, without ever gaining any experience.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;In SIFT feature extraction how the key points will be generated and how the features will be stored in the database. In image will the bag of visual words be images or text words?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;As there are numerous tools available for data science tasks, and it's cumbersome to install everything and build up a perfect system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a Linux/Mac OS image with Python, R and other open-source data science tools installed and available for people to use right away? An Ubuntu or a light weight OS with latest version of Python, R (including IDEs), and other open source data visualization tools installed will be ideal. I haven't come across one in my quick search on Google.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if there are any or if someone of you have created one for yourself? I assume some universities might have their own VM images. Please share such links.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In the big data world there is lot of talk about implementing an &quot;active archive&quot;. I see cloudera talk about it a lot. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The value prop is that you move the low value (and less used) data from EDW to Hadoop and then save on expensive EDW storage by using Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in my company say we keep 10 years of data on EDW and say we don't use anything below 2 years very actively. So I move 8 years of data from EDW to Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can setup an Impala (or equivalent) product to query the 8 years of data as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the problem is how do you order, sort a query which requires some data form EDW and some data from Impala?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;because this kind of grouping, sorting ordering etc will have to be done in memory and the apps which queried the EDW were not written for operations and don't have the capacity as well to sort, group and process so much of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how are people implementing the Active Archive?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What happens when we train a basic support vector machine (linear kernel and no soft-margin) on non-linearly separable data? The optimisation problem is not feasible, so what does the minimisation algorithm return?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Due to various &lt;a href=&quot;http://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curses of dimensionality&lt;/a&gt;, the accuracy and speed of many of the common predictive techniques degrade on high dimensional data. What are some of the most useful techniques/tricks/heuristics that help deal with high-dimensional data effectively? For example,&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do certain statistical/modeling methods perform well on high-dimensional datasets?&lt;/li&gt;&#xA;&lt;li&gt;Can we improve the performance of our predictive models on high-dimensional data by using certain (that define alternative notions of distance) or &lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernels&lt;/a&gt; (that define alternative notions of dot product)?&lt;/li&gt;&#xA;&lt;li&gt;What are the most useful techniques of dimensionality reduction for high-dimensional data?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;How does varying the regularization parameter in an SVM change the decision boundary for a non-separable dataset? A visual answer and/or some commentary on the limiting behaviors (for large and small regularization) would be very helpful.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;What are the most useful techniques for learning a binary classifier from a dataset with a high degree of imbalance (i.e., a dataset with the &quot;target&quot; class being much rarer than the &quot;background&quot; class)? For example,&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Should one first down-sample the majority/background class to reduce its frequency and then readjust the probabilities reported by the learning algorithm? How should one do the readjustment?&lt;/li&gt;&#xA;&lt;li&gt;Should one use different approaches for different learning algorithms, i.e., are there different techniques for dealing with imbalance in SVM, random forests, logistic regression, etc.?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I hope this is a question appropriate for SO.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The article in question: &lt;a href=&quot;http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I can tell, the only publicly available data from Google Search is through their Trends API.  The help page states that&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don't represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However in the article, the author reports (absolute) &quot;average monthly searches&quot;.  The source is stated as:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;All monthly search numbers are approximate and derived from anonymous and aggregate web activity.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Source: analysis of Google data by (author)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, how did he get this &quot;anonymous and aggregate web activity&quot;?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am looking for a method to parse semi-structured textual data, i.e. data poorly formatted but usually having a visual structure of a matrix which may vary a lot in content and number of items in it, which may have headers or not, which may be interpreted sometimes column-wise or row-wise, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read about the WHISK information extraction paper : &lt;a href=&quot;https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdf&quot;&gt;https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but unfortunately, it is not very detailed and I have not been able to find a real-system implementing it, or even snippets of code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anybody have an idea where I can find such help? Or suggest an alternative approach which may be suited to my problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance for your reply!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The author built a model for each gesture of each player to guess which gesture would be played next. Calculating over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable for cross-fold validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that legitimate? That seems like cheating. I would think you would have to exclude the target variables from your test set when calculating features in order to not &quot;cheat&quot;.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;My company provides managed services to a lot of its clients. Our customers typically uses following monitoring tools to monitor their servers/webapps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;OpsView&lt;/li&gt;&#xA;&lt;li&gt;Nagios&lt;/li&gt;&#xA;&lt;li&gt;Pingdom&lt;/li&gt;&#xA;&lt;li&gt;Custom shell scripts&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Whenever any issue is found, an alert mail comes to our Ops team so that they act upon rectifying the issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As we manage thousands of servers, our Ops teams' inbox is flooded with email alerts all the time. Even a single issue which has a cascading effect, can trigger 20-30 emails.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, what I want to do is to implement a system which will be able to extract important features out of an alert email - like server IP address, type of problem, severity of problem etc. and also classify the emails into proper category, like &lt;code&gt;CPU-Load-Customer1-Server2, MySQL-Replication-Customer2-DBServer3&lt;/code&gt; etc. We will then have a pre-defined set of debugging steps for each category, in order to help the Ops team to rectify the problem faster. Also, the feature extractor will provide input data to the team for a problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have been able to train &lt;em&gt;NaiveBayesClassifier&lt;/em&gt; with supervised learning techniques i.e. labeled training data(cluster data), and able to classify new unseen emails into its proper cluster/category. As the emails are based on certain templates, the accuracy of the classifier is very high. But we also get alert emails from custom scripts, which may not follow the templates. So, instead of doing supervised learning, I want to try out unsupervised learning for the same. I am looking into &lt;em&gt;KMeans clustering&lt;/em&gt;. But again the problem is, we won't know the number of clusters beforehand. &lt;strong&gt;So, which algorithm will be best for this use case?&lt;/strong&gt; Right now I am using Python's TextBlob library for classification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, for feature extraction out of an alert email, I am looking into NLTK (&lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nltk.org/book/ch07.html&lt;/a&gt;) library. I tried it out, but it seems to work on proper English paragraphs/texts well, however, for alert emails, it extracted a lot of unnecessary features. &lt;strong&gt;Is there already any existing solution for the same? If not, what will be the best way to implement the same? Which library, which algorithm?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;PS: I am not a Data Scientist.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Sample emails:&lt;/strong&gt;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.10.0.100  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: CRITICAL  Date &amp;amp; Time: Sat Oct 4 07:02:06 UTC 2014    Additional Information:     CRITICAL - load average: 41.46, 40.69, 37.91&#xA;RECOVERY: OK - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.1.1.100  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: OK  Date &amp;amp; Time: Sat Oct 4 07:52:05 UTC 2014    Additional Information:     OK - load average: 0.36, 0.23, 4.83&#xA;PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.100.10.10  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: CRITICAL  Date &amp;amp; Time: Sat Oct 4 09:29:05 UTC 2014    Additional Information:     CRITICAL - load average: 29.59, 26.50, 18.49&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Classifier code:(format of csv - email, &amp;lt;disk/cpu/memory/mysql&amp;gt;)&lt;/strong&gt;    &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from textblob import TextBlob&#xA;from textblob.classifiers import NaiveBayesClassifier&#xA;import csv&#xA;train = []&#xA;with open('cpu.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;// this can be done in a loop, but for the time being let it be&#xA;with open('memory.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;with open('disk.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;with open('mysql.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;cl = NaiveBayesClassifier(train)&#xA;cl.classify(email)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Feature extractor code taken from:&lt;/strong&gt; &lt;a href=&quot;https://gist.github.com/shlomibabluki/5539628&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/shlomibabluki/5539628&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if any more information is required here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Do you know of any machine learning add-ins that I could use within Excel? For example I would like to be able to select a range of data and use that for training purposes and then use another sheet for getting the results of different learning algorithms.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I found that Apache-Spark very powerful in Big-Data processing. but I want to know about Dryad (Microsoft) benefits. Is there any advantage for this framework than Spark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why we must use Dryad instead of Spark?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Let's say I'm trying to predict a person's electricity consumption, using the time of day as a predictor (hours 00-23), and further assume I have a hefty but finite amount of historical measurements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I'm trying to set up a linear model akin to &lt;/p&gt;&#xA;&#xA;&lt;p&gt;power_used = hr_of_day * alpha_hr + temperature * beta&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problem: using the hr_of_day as a numerical value is a very bad idea for many reasons, the fact that 23 and 0 are actually quite close values is one problem that can be solved with a simple transformation [1]. The fact that electrical consumption is often bi-modal is another problem which isn't solved by a simple transformation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A possible solution that works rather well is to treat the time of day as a categorical variable. That does the trick, but it suffers from a significant drawback in that there's no information sharing between neighbouring hours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what I'm asking is this: does anyone know of a &quot;soft&quot; version of categorical values? I'm suggesting something quite loosely defined: Ideally I would have some parameter alpha that reduces the regression to numerical regression where alpha = 1 and reduces to categorical regression where alpha = 0, and behaves &quot;in between&quot; if it's some other number. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now the only answer I can think of is to alter the weights in the regression in such a way that they tend towards zero the further away the quasi-categorical value is from the desired value. Surely there are other approaches?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] &#xA;introduce the hour variable as two new variables: Cos(time_of_day/24) and Sin(time_of_day/24)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Both Apache-Spark and Apache-Flink projects claim pretty much similar capabilities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what is the difference between these projects. Is there any advantage in either Spark or Flink?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Twitter is a popular source of data for many applications, especially involving sentiment analysis and the like.  I have some things I'm interested in doing with Twitter data, but here's the issue:  To get all Tweets, you have to get special permission from Twitter (which, as I understand it, is never granted) or pay big bucks to Gnip or the like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OTOH, &lt;a href=&quot;https://dev.twitter.com/streaming/firehose&quot; rel=&quot;nofollow&quot;&gt;Twitter's API documentation&lt;/a&gt; says:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Few applications require this level of access. Creative use of a combination of other resources and various access levels can satisfy nearly every application use case.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the filter api with keyword tracking seems like something that would be a big part of this, but you obviously can't enumerate every keyword.  Using a User stream on many User accounts that follow a lot of people might be an option as well, and I'm not sure if it makes sense to think about using the search API in addition.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here's the question &quot;What combination of other resources and access levels is the best way to get the maximum amount of data from Twitter&quot;?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an array of edges and weights:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[['a', 'b', 4],&#xA; ['a', 'c', 3],&#xA; ['c', 'a', 2], &#xA; ...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have about 100,000 edges and weights are between 1 and 700, most around 100.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking of using Markov Cluster Algorithm however wanted to reach out to see if this is the best to use. What about Affinity Propagation? In either case, what is the workflow? Do you typically have a way to measure how well clustered the results. Is there an equivalent to a silhouette score? Is there a way to visualize the clusters?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I found that Apache-Storm, Apache-Spark, Apache-Flink and TIBCO StreamBase are some powerful frameworks for stream processing. but I don't know which one of them has the best performance and capabilities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know Apache-Spark and Apache-Flink are two general purpose frameworks that support stream processing. but Apache-Storm and TIBCO StreamBase are built for stream processing specially. Is there any considerable advantage between these frameworks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;When considering Support Vector Machine, in an take in multiple inputs. Can each of these inputs be a vector??&#xA;What i am trying to say is, can the input be a 2 dimensional vector??&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I need to simulate for an academical project how the traffic fluxes (input/output with respect to a monitored area, measured in number of cars) of a city area evolves in correspondence of an event (i.e. the opening of a restricted traffic area to decongest the traffic).&#xA;I have some simulated sensors that provide the data: I was thinking to use a combination of a fuzzy system (to assign a membership function to each type of data, e.g. PM10 value and CO2 value) and a markov process: I would need to modify the probability to decrement the number of car in the monitored area (simulating that a car is going out the congested area, towards the new opened area) basing on decisions made by means of a fuzzy system.&#xA;So my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It is a good way to interpret the problem or there are better ideas that I have not taken into account yet?&lt;/li&gt;&#xA;&lt;li&gt;How to implement such a combination of markov chain and fuzzy systems in matlab?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have been working in NLTK for a while using Python. The problem I am facing is that their is no help available on training NER in NLTK with my custom data. They have used MaxEnt and trained it on ACE corpus. I have searched on the web a lot but I could not find any way that can be used to train NLTK's NER. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone can provide me with any link/article/blog etc which can direct me to Training Datasets Format used in training NLTK's NER so I can prepare my Datasets on that particular format. And if I am directed to any link/article/blog etc which can help me TRAIN NLTK's NER for my own data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a question widely searched and least answered. Might be helpful for someone in the future whose working with NER.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am currently working on a multi-class classification problem with a large training set. However, it has some specific characteristics, which induced me to experiment with it, resulting in few versions of the training set (as a result of re-sampling, removing observations, etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to perform pre-processing of the data, that is to scale, center and impute (not much imputation though) values. This is the point where I've started to get confused.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been taught that you should always pre-process the test set in the same way you've pre-processed the training set, that is (for scaling and centering) to measure the mean and standard deviation on the training set and apply those values to the test set. This seems reasonably to me. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what to do in case when you have shrinked/resampled training set? Should one focus on characteristics of the data that is actually feeding the model (that is what would 'train' function in R's caret package suggest, as you can put the pre-processing object in there directly) and apply these to the test set, or maybe one should capture the real characteristics of the data (from the whole untouched training set) and apply these? If the second option is better, maybe it would be worth it to capture the characteristics of the data by merging the training and test data together just for pre-processing step to get as accurate estimates as possible (I've actually never heard of anyone doing that though)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know I can simply test some of the approaches specified here, and I surely will, but are there any suggestions based on theory or your intuition/experience on how to tackle this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also have one additional and optional question. Does it make sense to center but NOT scale the data (or the other way around) in any case? Can anyone present any example where that approach would be reasonable?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you very much in advance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a weekly dataset and I have to normalize this data. &#xA;Data is something like this : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. week   50&#xA;2. week   51&#xA;3. week   50&#xA;4. week   54&#xA;5. week   150&#xA;6. week   155&#xA;7. week   ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The important thing is, the difference between week 3 and week 4 (50-54) is not same with week 5 and week 6. And also there is a huge different between week 4 and week 5. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is how can i handle all of this things ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the standard normalization functions(for example scikit normalization) can do it for me and should I normalize this data 0-1 or -1 to 1 ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html&quot; rel=&quot;nofollow&quot;&gt;Sklearn normalization page&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; I am working with python and generally scikit-learn library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I'm trying to dice it quite finely and pick things with cosine similarity 0.1 and above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I'm struggling to see how I implement this practically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I have so far is as follows&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user's score)&lt;/li&gt;&#xA;&lt;li&gt;I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0&lt;/li&gt;&#xA;&lt;li&gt;I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits.&lt;/li&gt;&#xA;&lt;li&gt;Next I look for similar sketches by doing the following&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I split the sketch vector into b bands of r bits&lt;/li&gt;&#xA;&lt;li&gt;Each band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket.&lt;/li&gt;&#xA;&lt;li&gt;I then look in each bucket. Any movies that are in the same bucket are candidate pairs.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The book suggests I can &quot;amplify&quot; my results by adding more AND and OR steps, but I'm at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me understand how to do this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;We can access HDFS file system and YARN scheduler In the Apache-Hadoop. But Spark has a higher level of coding. Is it possible to access HDFS and YARN in Apache-Spark too?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I found that Apache-Spark has pretty much simple interface and easy to use. But I want to know about other interfaces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me a ranking of Big-Data frameworks in base of simplicity of their interfaces. also this is useful to express most simple and complex interfaces in base of your experiences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. Detailed comparison is so lengthy and this is not my purpose. Just a selection or ranking on your opinions is sufficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Maybe it is a bit general question. I am trying to solve various regression tasks and I try various algorithms for them. For example, multivariate linear regression or an SVR. I know that the output can't be negative and I never have negative output values in my training set, though I could have 0's in it (for example, I predict 'amount of cars on the road' - it can't be negative but can be 0). Rather often I face a problem that I am able to train relatively good algorithm (maybe fit a good regression line to my data) and I have relatively small average squared error on training set. But when I try to run my regression algorithm against new data I sometimes get a negative output. Obviously, I can't accept negative output since it is not a valid value. The question is - what is the proper way of working with such output? Should I think of negative output as a 0 output? Is there any general advice for such cases?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm going to start my degree thesis and I want to do a fault detector system using machine learning techniques. I need datasets for my thesis but I don't know where I can get that data. I'm looking for historical operation/maintenance/fault datasets of any kind of machine in the oil &amp;amp; gas industry (drills, steam injectors etc) or electrical companies (transformators, generators etc).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am wondering if there is a way to proceed 2 exectuions in 1 step in hive.&#xA;For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT * FROM TABLE1&#xA;SELECT * FROM TABLE2&#xA;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do this in one window, and do not have to open 2 hive windows to execute each line separetly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can it be done on HUE?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Our main use case is object detection in 3d lidar point clouds i.e. data is not in RGB-D format. We are planning to use CNN for this purpose using theano. Hardware limitations are CPU: 32 GB RAM Intel 47XX 4th Gen core i7 and GPU: Nvidia quadro k1100M 2GB. Kindly help me with recommendation for architecture. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking in the lines of 27000 input neurons on basis of 30x30x30 voxel grid but can't tell in advance if this is a good option.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additional Note: Dataset has 4500 points on average per view per point cloud&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Recently I read about path ranking algorithm in a paper (source: &lt;a href=&quot;https://www.cs.cmu.edu/~nlao/publication/2014.kdd.pdf&quot; rel=&quot;nofollow&quot;&gt;Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this paper was a table (Table 3) with facts and I tried to understand how they were calculated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;F1 (harmonic mean of precision and recall) = 0.04&lt;br/&gt;&#xA;P (precision) = 0.03&lt;br/&gt;&#xA;R (recall) = 0.33&lt;br/&gt;&#xA;W (weight given to this feature by logistic regression)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found a formula for F1 via Google which is&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$F1 = 2 * \\\\frac{precision * recall}{precision + recall}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I get the result of 0.055 with this formula, but not the expected result of 0.04.&#xA;Can someone help me to get this part?&#xA;Also, does someone know how 'W' can be calculated?&#xA;Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've recently become interested in possibly of developing some sort of method for ranking athletes of sports such as American football and determining which players are better than others in terms of specific statistics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts are that there are two ways to go about doing this. The first would be some sort of mathematical formula which would take in the statistics of a given player and provide some sort of standardized score which could be compared with other players to determine which is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My other idea would be to have some machine learning algorithm go through historical data and determine the patterns which indicate how well a certain combination of statistics would perform in the following week of play by using the patterns it recognizes over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure which approach would be more effective and so I'm hoping that someone has an idea or any advice as to which would be best to look into. Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;With respect to &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;ROC&lt;/a&gt; can anyone please tell me what the phrase &quot;discrimination threshold of binary classifier system&quot; means? I know what a binary classifier is.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am learning about Data Science and I love the Healthcare part. That's why I have started a blog and my third entry is about using Genetic Algorithm for solving NP-Problems. &#xA;This post is &lt;a href=&quot;https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/&quot; rel=&quot;noreferrer&quot;&gt;https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some expertise with GA package solving problems like the TSP, but do you know any most powerful R package?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks so much!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'am trying t create a regression based prediction (like booking website): predict the number of clicks for each hotel.&#xA;I have to generate a .csv file containing two columns: hotel_id, predicted_number_of_clicks for all hotel_ids.&#xA;My first question question is : should I put the Id_hotel as feature in the predictive model. I think that I must to drop it no?&#xA;Second question is : how can write in the csv file only this 2 columns &quot;hotel_id&quot;, &quot;predicted_number_of_clicks&quot; if I drop hotel_id from the model feature?&#xA;Thank you very much for your reply in advance&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to use a particular cost function (based on doubling rate of wealth) for a classification problem, and the solution works well in MATLAB. See &lt;a href=&quot;https://github.com/acmyers/compareCostFXs&quot; rel=&quot;nofollow&quot;&gt;https://github.com/acmyers/compareCostFXs&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try to do this in Python 2.7.6 I don't get any errors, but it only returns zeros for the theta values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the cost function and optimization method I've used in Python:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def costFunctionDRW(theta, X, y):&#xA;&#xA;    # Initialize useful values&#xA;    m = len(y)&#xA;    # Marginal probability of acceptance&#xA;    marg_pA = sum(y)/m&#xA;    # Marginal probability of rejection&#xA;    marg_pR = 1 - marg_pA&#xA;&#xA;    # =============================================================&#xA;    pred = sigmoid(np.dot(X,theta))&#xA;    final_wealth_individual = (pred/marg_pA)*y + ((1-pred)/marg_pR)*(1-y)&#xA;    final_wealth = np.prod(final_wealth_individual)&#xA;    final_wealth = -final_wealth&#xA;&#xA;    return final_wealth&#xA;&#xA;result = scipy.optimize.fmin(costFunctionDRW, x0=initial_theta, \\\\&#xA;                   args=(X_array, y_array), maxiter=1000, disp=False, full_output=True )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any advice would be much appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y$, which is a continuous value from zero to one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, I have tried different models, &lt;em&gt;e.g.,&lt;/em&gt; decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and &lt;em&gt;etc.&lt;/em&gt; However, all of these models also do not perform well. (RMSE $\\\\approx $0.35, there is not significant difference with linear regression)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: &lt;strong&gt;how can I examine whether it is caused by bad data quality?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, for the size of data set, there are more than 10K data points, each of which associated with 105 features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also tried to investigate importance of each feature by using decision-tree-based regression, it turns out that, only one feature (which should not be the most outstanding feature in my knowledge to this problem) have an importance of 0.2, while the rest of them only have an importance less than 0.1.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a research project that deals with American military casualties during WWII. Specifically, I am attempting to construct a count of casualties for each service at the county level. There are two sources of data here, each presenting their own challenges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1. Army and Air Force data.&lt;/strong&gt; The National Archives hosts lists of Army and Air Force servicemen killed in action by state and county. There are .gif images of the report available online. &lt;a href=&quot;http://media.nara.gov/media/images/29/19/29-1891a.gif&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a sample for several counties in Texas. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I DO NOT need to recover the names or any other information. I simply need to count the number of names (each on its own line, and listed in groups of five) under each County. There are hundreds of these images (50 states - 30-100 for each state). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been unable to find an OCR program that can tackle this problem adequately. How would you suggest I approach this challenge? (I have some programming expertise in Python and Java, but would prefer to use any off-the-shelf solutions that may exist).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2. Navy and Marine Core data.&lt;/strong&gt; This data is organized differently. Each state has long lists of casualties with the address of their next of kin. &lt;a href=&quot;http://media.nara.gov/media/images/27/31/27-3023a.gif&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a sample for Texas again. For these images, I need to BOTH count the number of dead and recover their hometown, which is typically the last word in each entry. I can then match these hometowns to counties and merge with database 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, the usual OCR programs have proved inadequate. Any help on this (admittedly more difficult) problem would be very much appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance experts!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm doing some work trying to extract commonly occurring words from a set of human classified documents and had a couple questions for anyone who might know something about NLP or statistical analysis of text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have a set of a bunch of documents, and users have classified them as either good or bad. What I'd like to do is figure out what words are common to the good documents, but not necessarily the other ones.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could, for example, use the (frequency within good documents / total frequency) which would essentially normalize the effect of a word being generally common. This, unfortunately, gives very high precedence to words that occur in only a few good documents &amp;amp; not at all in the other documents. I could add some kind of minimum threshold for # of occurrences in good docs before evaluating the total frequency, but it seems kind of hacky.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know what the best practice equation or model to use in this case is? I've done a lot of searching and found a lot of references to TF-IDF but that seems more applicable for assessing the value of a term on a single document against the whole set of docs. Here I'm dealing with a set of docs that is a subset of the larger collection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, I'd like to identify which words are uniquely or more important to the class of good documents.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;strong&gt;Example Data&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset (in &lt;code&gt;R&lt;/code&gt; as a &lt;code&gt;data frame&lt;/code&gt;) of race results for athletes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;athlete racedistance time    location tracktype       date    coach&#xA;A          100       10.0       UK     typeA       2014-01-01  carlos&#xA;A          200       20.0       US     typeB       2014-02-01  carla&#xA;A          100        9.5      AUS     typeC       2014-03-01  chris&#xA;B          100       11.0       UK     typeA       2014-01-01  carla&#xA;B          200       21.0       US     typeB       2014-02-01  carlos&#xA;B          400       61.0      AUS     typeC       2014-03-01  carla&#xA;B          100       10.5      GER     typeA       2014-04-01  clive&#xA;C          100        9.5       UK     typeA       2014-01-01  clive&#xA;C          200       21.5       US     typeB       2014-02-01  chris&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an appropriate machine learning algorithm or method that can use the previous results of each athlete as a feature, when trying to predict the &lt;code&gt;time&lt;/code&gt; for an athlete in a future race? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, &lt;code&gt;athlete A&lt;/code&gt; has three races, with one month rest between them. In the third race he performs slightly better than the first race over the same distance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can an algorithm learn that the second race had an effect on the athlete, which meant he performed better in the third race?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I've read on the subject and the training examples I've completed it would appear that each 'row' of data should be independent, is this the case for all ML algorithms? Is there another prediction technique I should be considering to solve this type of problem?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I once knew some Java, but that was close to 10 years ago.  Assuming I can learn a language to get into data analytics.... what language do you recommend?  &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am not certain, if this is the right place to ask the following question. I am looking for some practical scenarios in social networks where the following information propagation model can arise:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/mERQh.jpg&quot; alt=&quot;A toy example of the desired information propagation model&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I want to build a chatbot that serves as a first line customer support on a  retail website. I have a large log of chat sessions between customers and support professionals that I can use. I am wondering what is the best way to go about building this chatbot. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some ideas I have looked into;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The first thing I did was to breakdown the chat logs into Q&amp;amp;A pairs. Treat each question as a document, compute a term-document matrix and use it to retrieve the most similar question when a customer asks sometihng. The idea was to then simply pick the answer given to the question by the human (with some modifications). However, this gives really abysmal results and does not match to previous questions very well. Even if I get this approach to work well, we would be limited to the questions that already been asked. &lt;/li&gt;&#xA;&lt;li&gt;Another thing I thought about was to modify something like ALICE and write some AIML for customer support related QA. However, that would take a lot of very precise AIML writint to work well. This solution would not be able to scale to other languages, which is something I want to do.&lt;/li&gt;&#xA;&lt;li&gt;Another idea I have is to try and cluster the answers given by the customer support staff (they are more likely to be aligned compared to the questions asked by customers). This would give me a sense of what questions are similar. Then I can use something like LSA on the questions and fall back to the first approach.&lt;/li&gt;&#xA;&lt;li&gt;Yet another way is to build an ontology of the domain specific knowledge and given a question decide which category the question falls into. The questions from the chat logs can then be mapped to the ontology and I could train classifiers for mapping questions to different knowledge areas. Once I reach the leaf nodes, I can give back pre-defined answers. However, my reservation is that mapping chat logs to the ontology would be very tedious. Is there a good way to map the existing QA to the ontology?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have 2 data files: the first one is a database, potentially very large; the second one contains queries I want to answer. My program pipeline is processing the database to get some information first, and then use that same information to answer the queries. Although the number of queries is not big, processing each query takes a long time. So I want to give each worker some queries to answer, then combine all the answers together into one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sounds like a MapReduce job. But to answer each query, the worker also needs to use the processed information from the database, and I'm not sure how to do this with MapReduce. I'm new to parallel programming and just heard of MPI and Spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you help me to choose an appropriate one?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please ask if the description is not clear.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1.00 0.88 0.96 0.99 &#xA;0.88 1.00 0.99 0.96 &#xA;0.96 0.99 1.00 0.86 &#xA;0.99 0.96 0.86 1.00 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to implement recommender which, for a set of items, recommends a new set of items.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about using SVD to reduce the items to n-dimensional space, let's say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a base set of items (which can get quite big), I hope I could calculate an average of their vectors, and use it for search.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this a good idea? What is this procedure called? And can it be done in Mahout?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my code so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ItemSimilarity similarity = new LogLikelihoodSimilarity(model);&#xA;Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);&#xA;// copy similarities to a matrix&#xA;for (int i = 0; i &amp;lt; NUM_ITEMS; i++) {&#xA;        double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));&#xA;        for (int j = 0; j &amp;lt; NUM_ITEMS; j++) {&#xA;            m.setQuick(i, j, similar[j]);&#xA;    }&#xA;}&#xA;Matrix v = new SingularValueDecomposition(m).getV();&#xA;Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is, SVD is taking forever for NUM_ITEMS &gt; 30. I don't know if there is an issue with data, or with SVD implementation I'm using. The matrix m is symmetrical, could that be an issue? I tried googling &quot;demean matrix mahout&quot; with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT 2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem was the matrix contained a few NaN values, that's why SVD was taking infinite time. After replacing these with 0.0 it works fine for 1000 x 1000 matrix. And my recommendations are working like a charm. I'll still need compute SVD of 20x more rows and columns. If anyone knows what's the easiest way to compute (approximate) SVD of 20.000 x 20.000 dense matrix, probably through some cloud parallel service (?), please let me know.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS. Thanks for help!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Hello I work as data scientist for a private company.&#xA;I am interested in working for a nonprofit company, such as a research institute (public or private) or a company that takes care of  issues such as  environment, public health,  social improvements. Even an internet company like Wikipedia can be interesting.&#xA;Does anybody know if nonprofit companies hire data scientists?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;If I have virtually endless training data (it's synthesized) is there still purpose in having epochs? I.e. training on the same samples multiple times?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to calculate maximum values for different groups in a relation in Pig. The relation has three columns patientid, featureid and featurevalue (all int). &#xA;I group the relation based on featureid and want to calculate the max feature value of each group, heres the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grpd = GROUP features BY featureid;&#xA;DUMP grpd;&#xA;temp = FOREACH grpd GENERATE $0 as featureid, MAX($1.featurevalue) as val;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its  giving me  &lt;strong&gt;Invalid scalar projection: grpd&lt;/strong&gt; Exception. I read on different forums that MAX takes in a &quot;bag&quot; format for such functions, but when I take the dump of grpd, it shows me a bag format. Here's a small part of the output from the dump:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(5662,{(22579,5662,1)})&#xA;(5663,{(28331,5663,1),(2624,5663,1)})&#xA;(5664,{(27591,5664,1)})&#xA;(5665,{(30217,5665,1),(31526,5665,1)})&#xA;(5666,{(27783,5666,1),(30983,5666,1),(32424,5666,1),(28064,5666,1),(28932,5666,1)})&#xA;(5667,{(31257,5667,1),(27281,5667,1)})&#xA;(5669,{(31041,5669,1)})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Whats the issue ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a complete Hadoop platform with &lt;em&gt;HDFS&lt;/em&gt;, &lt;em&gt;MR&lt;/em&gt;, &lt;em&gt;Hive&lt;/em&gt;, &lt;em&gt;PIG&lt;/em&gt;, &lt;em&gt;Hbase&lt;/em&gt;, etc., &lt;em&gt;Python&lt;/em&gt;, &lt;em&gt;R&lt;/em&gt;, &lt;em&gt;Java&lt;/em&gt;. All data sets have a large size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data set A, describing the jobs of people working in a company, is composed of the following fields:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Id Person: a unique alphanumeric identifier per person.&lt;/li&gt;&#xA;&lt;li&gt;Start Date: a date format iso entry in the post  &lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;End Date: iso size release date of the position. If the date is not given, it is the current position&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with &lt;strong&gt;Google&lt;/strong&gt;, Commercial Manager at [missing text] , Manager at &lt;strong&gt;googole&lt;/strong&gt; ...&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking for a design pattern that is relevant to a module that extracts features.&#xA;I want to define a certain number of features over my data points, and then according to the performance and the feature selection, I may want to remove some of them and add others, and also I may want to consider any subsets of them to test.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is a good design pattern to do that? Did I miss something obvious? I am neither an engineer nor a developer, so I never study such things but I understand that it could help me a lot!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help,&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm very new to this community, so please overlook my noobness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a data set with 2948 instances and I tried to remove outliers using InterquartileRange filter in Weka. The issue is that the number of 'YES' instances in ExtremeValues and Outliers takes up to 2947 and 2946 respectively. In other words, all my data are considered outliers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this say about my data set? Or am I not meant to perform IQR on this data, if so, is there other algorithms to identify outliers other than IQR?  And how would one perform regression on such a data set?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are some data analytic package &amp;amp; feature in python which helps do data analytic?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I was not sure about posting this question with mentioning the name of the company, which I quite respect and admire. However, I've figured that a wider exposure might help the team to fix this and similar problems faster as well as increase the quality of the &lt;em&gt;machine learning (ML)&lt;/em&gt; engine of their website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem exposes itself by too many occurrences of a quite trivial &lt;em&gt;misclassification error&lt;/em&gt; on Amazon's book categories &lt;strong&gt;classification&lt;/strong&gt; (which I'm a frequent visitor of). In the following example, the underlying reason of such behavior is quite clear, but in other cases the reasons might be different. I am curious about what could be other potential &lt;em&gt;reasons&lt;/em&gt; for misclassification and what are the &lt;em&gt;strategies/approaches&lt;/em&gt; to avoiding such problems. Without much further ado, here's how the problem appears in real life.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was reviewing some books, related to transitioning from graduate programs (Ph.D., in particular) to work environment in academia. Among several other books, I ran across the following one:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4LSzV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, so good. However, let's scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out &lt;em&gt;categories&lt;/em&gt;, &lt;strong&gt;relevant&lt;/strong&gt; to the book's &lt;em&gt;discipline, topic and contents&lt;/em&gt;. How surprised was I (and that's an understatement!) to see the following result of Amazon.com's sophisticated ML engine and algorithms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/KXrFh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Clearly, the only fuzzy fact that connects this book with the subject &quot;Audiology and Speech Pathology&quot; (!) is IMHO the author's last name (Boice), which, is &lt;strong&gt;close to&lt;/strong&gt; the word &quot;voice&quot;. If my guess is correct, Amazon's ML engine, for some reason, decided to take into account the book's lexicographical attribute instead of the book's most important and &lt;strong&gt;most relevant attributes&lt;/strong&gt;, such as title, topic and contents. I've seen multiple occurrences of similar absolutely incorrect ML-based decision making on Amazon.com and some other websites. So, hopefully my question makes sense as well as interesting and important enough to spark a discussion: &lt;strong&gt;What could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems?&lt;/strong&gt; (Any related thoughts will also be appreciated.)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;em&gt;Disclaimer:&lt;/em&gt; although I know some things about big data and am currently learning some other things about machine learning, the specific area that I wish to study is vague, or at least appears vague to me now. I'll do my best to describe it, but this question could still be categorised as too vague or not really a question. Hopefully, I'll be able to reword it more precisely once I get a reaction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some experience with Hadoop and the Hadoop stack (gained via using CDH), and I'm reading a book about Mahout, which is a collection of machine learning libraries. I also think I know enough statistics to be able to comprehend the math behind the machine learning algorithms, and I have some experience with R.&#xA;My ultimate goal is making a setup that would make trading predictions and deal with financial data in real time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if there're any materials that I can further read to help me understand ways of managing that problem; books, video tutorials and exercises with example datasets are all welcome.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am looking for a suitable graph representation where the nodes/vertices are molecules with 2 main variables: structural characteristic and a real number property (for example biological potency). The network is build up by molecules by matching structural properties and it can be rather complex with many clusters of high degree vertices (ie molecules that are structurally similar to each other). However the representation should give a clear picture of potencies, although high detail accuracy is not required: some sort of classification is adequate. I played with JUNG with a graph having edge weights as the change in potency and the ISOM layout thinking that it reserves (as much as possible) the edge weights but with not good results. Something like radial layout could be ideal, as it gives a clear picture of potency distribution, but this obviously would suffer from crowding of vertices that are closer to the center.  As, obviously, my knowledge is restricted, is there some layout that you would suggest and is practical for a non expert to implement? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;A book I'm now reading, &quot;Apache Mahout Cookbook&quot; by Pierro Giacomelli, states that &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;To avoid [this], you need to divide the vector files into two sets called the 80-20 split &amp;lt;...&gt;&#xA;  A good dividing percentage is shown to be 80% and 20%.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is there a strict statistical proof of this being the best percentage, or is it a euristic result?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Can you please show the step by step calculation of Entropy(Ssun)?  I do not understand how 0.918 is arrived at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried but I get the values as 0.521089678, 0.528771238, 0.521089678 for Sunny, Windy, Rainy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was able to calculate the target entropy (Decision) correctly as = -(6/10)*log(6/10) + -(2/10)log(2/10) + -(1/10)log(1/10) + -(1/10)log(1/10) = 1.570950594&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am totally stuck at the next step.  Request your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Reference: &lt;a href=&quot;http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.html&quot; rel=&quot;nofollow&quot;&gt;http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.html&lt;/a&gt;&#xA;Please search for &quot;The first thing we need to do is work out which attribute will be put into the node at the top of our tree:&quot; to reach the line I am referring to.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Context of question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to find semantically similar documents in corpora. For that, I'm first trying Latent Dirichlet Allocation (LDA) with divergences (Hellinger, Kullback-Leibler, Jensen-Shannon) on the per document topic distributions. However, to find # of topics for my corpus ( a 948 document dataset, extracted from larger collection, where docs about the same story are humanly annotated), it was suggested to use HDP. Unfortunately, after MANY tries, I'm still unsure I'm using the packages correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More detailed:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) HDPFASTER continually increases the number of topics (in train.log file).  What is the stopping criterion (e.g. difference in avg. likelihood between successive iterations smaller than x? which x?) and/or how many cycles should I let it run? Afterwords, do I take as the correct # of topics the last line of &quot;train.log&quot; OR should I also test for minimum perplexity (It has also been suggested that Bayesian Information Criterion (BIC) might be a better measure) and choose the smallest? If the latter, should I test for ALL iterations?!  Is HDPFASTER's test feature the appropriate tool for this? Last, should I --sample_hyper? Do I just use a(alpha) as input to LDA's prior alpha, perhaps divided by a number (e.g. # of topics)? What about LDA's prior beta? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) HCA continually decreases  the number of topics (in *.log file). When to stop? Do I accept the final # of topics in .log ( exp.ent =number0) OR do I search for the minimum perplexity of test set (as I assume it appears as number2 in &quot;log_2(perp)=number1,number2&quot; in .log throughout iterations), which ALWAYS appears VERY close to my initial max number of topics? Hyperparameters alpha,beta: do I sample using -D , -E? Which do I put as input to LDA's priors? Is it generally worth it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generally: Is it possible that my dataset is just too small and/or 'biased/fragmented/incomplete', in the sense that each story has only 1-3 examples, while the diversity of the topics these stories discuss can be quite large? Should I just try and augment it with a larger 'homogeneous' dataset? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;is the survival table classification method on the Kaggle Titanic dataset an example of an implementation of Naive Bayes ? I am asking because I am reading up on Naive Bayes and the basic idea is as follows:&#xA;&quot;Find out the probability of the previously unseen instance&#xA;belonging to each class, then simply pick the most probable class&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The survival table &#xA;(&lt;a href=&quot;http://www.markhneedham.com/blog/tag/kaggle/&quot; rel=&quot;nofollow&quot;&gt;http://www.markhneedham.com/blog/tag/kaggle/&lt;/a&gt;)&#xA;seems like an evaluation of the possibilities of survival given possible combinations of values of the chosen features and I'm wondering if it could be an example of Naive Bayes in another name. Can someone shed light on this ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am working on a problem(non competition) from hacker rank &lt;a href=&quot;https://www.hackerrank.com/challenges/predict-missing-grade&quot; rel=&quot;nofollow&quot;&gt;https://www.hackerrank.com/challenges/predict-missing-grade&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically you're given test data of a bunch of students of their scores in other subjects but math and you are to predict their score in math based off all their other test scores. Say you were passed data of&lt;/p&gt;&#xA;&#xA;&lt;p&gt;{&quot;SerialNumber&quot;:1,&quot;English&quot;:1,&quot;Physics&quot;:2,&quot;Chemistry&quot;:3,&quot;ComputerScience&quot;:2}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you go about generating that student's score in mathematics or coming up with a prediction engine to generate the math score? I know that's the whole point of this question but can someone give me a hint or a resource to go to so I can have a chance of figuring this out and actually get started? I really want to learn.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;strong&gt;Jaccard similarity&lt;/strong&gt; and &lt;strong&gt;cosine similarity&lt;/strong&gt; are two very common measurements while comparing item similarities. However, I am not very clear in what situation which one should be preferable than another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can somebody help clarify the differences of these two measurements (the difference in concept or principle, not the definition or computation) and their preferable applications?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm curious if anyone has Python library suggestions for inferential statistics. I'm currently reading &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL/&quot; rel=&quot;nofollow&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, which uses R for the example code, but ideally I'd like to use Python as well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most of my data experience is with Pandas, Matplotlib, and Sklearn doing predictive modeling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I've found &lt;a href=&quot;https://pypi.python.org/pypi/statsmodels&quot; rel=&quot;nofollow&quot;&gt;statsmodels&lt;/a&gt;. Is this what is recommended or is there something else?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Hi I am new to Data analytics.I am planning to learn R by doing some real time projects. How should I stream line (set goals) my time in learning R and also I have not learnt statistics till data. I am planning to learn both side by side. I am mid level data warehouse engineer who has experience in DBMS Data-Integration. I am planning to learn R so that I can bring out useful analysis from the Integrated data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I be specific, I am beginning in R, so what are the basic statistical concepts I should know and implement it in R. If I want to be an expert or above average person in R how should I plan strategically to become one. Say if I can spend 2 hrs a day for 1 year what level I should reach. FYI am working for a SaaS company. What are the way s in which I can utilize R knowledge in a SaaS environment &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am pursing Data Analyst course at Udacity. I came across this video : &lt;a href=&quot;https://www.youtube.com/watch?v=3Z73Wd2T1xE&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=3Z73Wd2T1xE&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Watching it lead me to wonder if &lt;a href=&quot;http://www.ayasdi.com/&quot; rel=&quot;nofollow&quot;&gt;Ayasdi&lt;/a&gt; products would reduce the demand for data scientists. I wish to compete in Kaggle contests but after watching the video, I feel that many of those problems can be easily solved using their platform and that I will be at disadvantage since I do not have access to their tools. Also I feel such tools would reduce the need of experts in data sciences. Now I am worried if I should continue with Nano Degree in Data Analyst at Udacity.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am confused by the definition of the likelihood function in different contexts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of linear and logistic regression, it is defined as y given x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case naive bayes and LDA, it is defined over x and y&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of the EM algorithm (with observed variable x and unobserved variable z), it is defined over x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we know over what it should be defined?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering how other developers are setting up their local environments for working on Spark projects. Do you configure a 'local' cluster using a tool like Vagrant? Or, is it most common to SSH into a cloud environment, such as a cluster on AWS? Perhaps there are many tasks where a single-node cluster is adequate, and can be run locally more easily.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Hi I would appreciate it if someone can point me in the right direction.  I'm looking for an algorithm or mathematical theory which I would use to compute the similarity between two ordered lists, where each list element can have n sub-elements.  I will explain with an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I go to a baseball game and I record the sequence of strikes and balls for each of the first 30 players at bat.  My list looks like this, where P is a player, S is a strike and B is a ball. Order matters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;L1: {P1=(S,S,S)}, {P2=(B,B,S)}, {P3=(B,B,S,S)}, ... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My friend goes to a baseball game and does the same thing.  Later, we meet up and compare our lists.  We find that our lists are almost identical except that I recorded a strike for player 16 where my friend recorded a ball.  What are the chances we were at the same game and one of us made a mistake at player 16?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance...&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the best known Data Science Methodologies today? By methodology I mean a step-by-step phased process that can be used for framing guidance, although I will be grateful for something close too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Given some clusters created from similarity measures between items, is there a recommended way to assign a new item to an existing cluster based on similarity alone? (i.e. avoiding re-clustering)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Measuring the similarity of a new item to all other items is fairly cheap, so I'm looking for a way of using this to assign it to the cluster it's most likely to belong to. It's also important for it to take cluster size into account (i.e. doesn't unfairly weight towards or against larger clusters).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I'm trying to sacrifice some clustering accuracy in exchange for avoiding a complete re-clustering when the occasional new item is added.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;This is in continuation to a previos question. I would like to know where in practice will the following M-ary tree-structured social network propagation model arise. I am looking for some concrete examples in social media (twitter and youtube etc.). Eventually, I want to do some statistical analysis on such social networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/x14St.jpg&quot; alt=&quot;Example of a binary tree-structured social network&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'd like to see the top N results for a RandomForestClassifier prediction, ordered by descending probability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The answer may be predict_proba, but I have no idea how to interpret the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Help appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;&lt;em&gt;Hadoop&lt;/em&gt; is a buzzword now. A lot of start-ups use it (or just say, that they use it), a lot of widely known companies use it. But when and what is the border? When person can say: &quot;Better to solve it without Hadoop&quot;?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have the following data&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^1_i$, $y^1_i$) for $i=1,2,...N_1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^2_i$, $y^2_i$) for $i=1,2,...N_2$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^m_i$, $y^m_i$) for $i=1,2,...N_m$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to train a neural net to produce some $y_k$ where $k&amp;lt;=min(N)$ given a input ${x_1, x_2, ..., x_{k-1}}$?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so any suggestion of documentation/ library I can look at (preferably python)?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have 22 classes of objects but they have very skewed distributions where max class has 100.000 images and the min class has 1600 images. In that setting I would like to hear some possible solutions to this balance problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried followings so far;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;multiply number of instances in the lower classes up to the max class by replicating the instances, possibly adding some noise as well.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;changing the learning regarding the class distribution in the given minibatch of the next epoch. (no implemented but in my mind)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What are your suggestions?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have used the same methods/parameters to create two decision trees. The trees classify the presence or absence of a medical condition using the presence or absence of various symptoms. There is a tree for Medical Condition #1 and another tree for Medical Condition #2. Both trees are based on the same set of symptoms, rated by patients. If Medical Condition #1 resulted in a much simpler tree than Medical Condition #2, can that suggest Medical Condition #2 is a more complex disease? If so, can anyone point me to a reference that states the complexity/depth of a tree can be representative of a complex condition?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The goal of my analysis is separate from this question. I am trying to figure out all the conclusions I can draw or suggest from my analysis. Yes, I am interested in saying something about the complexity of condition A to B. &#xA;When is a condition complex ? When there are many symptoms as the disease is diagnosed by the symptoms.&#xA;If it's hard to diagnose ? YES&#xA;if the symptoms are severe ? NO&#xA;can s.o. have both conditions ? NO&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently trying to predict a continuous variable using KNN. Instead of treating each neighbor equally I would like to use the weights to create a weighted average. The weights by themselves are not ideal, as the closer a neighbor the more I would like that neighbor to influence the final results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This lead me to consider the inverse of each of the distances, but this doesn't handle the case where an instance is the exact same -&gt; with a distance of 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any recommendations on how to properly set the weights of each neighbor relative to their distance? Similar to how the inverse would handle this, but one that allows for 0 values.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have code to generate feature vectors in the style of the bag-of-words model, you can see it on my &lt;a href=&quot;https://github.com/h1395010/perceptron/blob/master/src/file_dict_createur/FileDictCreateur.java&quot; rel=&quot;nofollow noreferrer&quot;&gt;github page&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It renders output of the form:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/data/train/politics/p_0.txt, [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]&#xA;/data/train/science/s_0.txt, [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]&#xA;/data/train/atheism/a_0.txt, [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]&#xA;/data/train/sports/s_1.txt, [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to move on to the next step of the perceptron but I'm confused about what to do next. I guess the next thing I need to do is generate a feature vector for the 'test' data, isn't it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, in &lt;a href=&quot;http://en.literateprograms.org/index.php?title=Special:DownloadCode/Perceptron_(Java)&amp;amp;oldid=19184&quot; rel=&quot;nofollow noreferrer&quot;&gt;this example&lt;/a&gt; what is 'teachingOutput' supposed to be, training data vectors? Such as the ones I have above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried poking around in &lt;a href=&quot;https://stackoverflow.com/questions/21738277/perceptron-learning-most-important-feature&quot;&gt;this implementation&lt;/a&gt; in an attempt to figure out what to do but, alas, to no avail. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I often get the problem when this or that alias name is already used somewhere, and I can't easily find that variable or aggregation to release the name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/kK9mM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there some place in Tableau where I can view/edit/reset full list of aliases?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am a PhD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know &lt;code&gt;svn&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find &lt;code&gt;git&lt;/code&gt; also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn't fun, and cost me a lot of time and effort.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that others in science seem to have similar problems, yet I couldn't find a good solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use the storage facilities of my institute, so I need something that can use a &quot;dumb&quot; server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So, I need a tool that can handle more than one remote location.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have evaluated a lot of different solutions, but none seem to fit the bill:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://subversion.apache.org/&quot;&gt;svn&lt;/a&gt; is somewhat inefficient and needs a smart server&lt;/li&gt;&#xA;&lt;li&gt;hg &lt;a href=&quot;http://mercurial.selenic.com/wiki/BigfilesExtension&quot;&gt;bigfile&lt;/a&gt;/&lt;a href=&quot;http://mercurial.selenic.com/wiki/LargefilesExtension&quot;&gt;largefile&lt;/a&gt; can only use one remote&lt;/li&gt;&#xA;&lt;li&gt;git &lt;a href=&quot;https://github.com/beenje/git-bigfile&quot;&gt;bigfile&lt;/a&gt;/&lt;a href=&quot;https://github.com/alebedev/git-media&quot;&gt;media&lt;/a&gt; can also use only one remote, but is also not very efficient&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://attic-backup.org/&quot;&gt;attic&lt;/a&gt; doesn't seem to have a log, or diffing capabilities&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://bup.github.io/&quot;&gt;bup&lt;/a&gt; looks really good, but needs a &quot;smart&quot; server to work&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've tried &lt;code&gt;git-annex&lt;/code&gt;, which does everything I need it to do (and much more), but it is very difficult to use and not well documented. I've used it for several days and couldn't get my head around it, so I doubt any other coworker would be interested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do researchers deal with large datasets, and what are other research groups using?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me that almost everyone should have this problem, yet I don't know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I need to draw a decision tree about this subject :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The research and development manager in an old oil company, which is&#xA;  considering making some changes, lists the following courses of action&#xA;  for the company:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(i) Adopt a process developed by another oil company. This would cost&#xA;  €7 million in royalties and yield a net €20 million profit (before&#xA;  paying the royalty).&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(ii) Carry out one or two (not simultaneously) alternative research&#xA;  projects :&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(R1) the more expensive one has a 0.8 chance of success; net profit&#xA;  €16 million and a further €6 million in royalties. If it fails there&#xA;  will be a net loss of €10 million. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(R2) the alternative research programme is less expensive but only has&#xA;  a 0.7 chance of success with a net profit of €15 million and a further&#xA;  €5 million in royalties. If it fails a net loss of €6 million will be&#xA;  incurred.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(iii) Make no changes. After meeting current operating costs the&#xA;  company expects to make a net €15 million profit from its existing&#xA;  process. Failure of one research program would still leave open all&#xA;  remaining courses of action (including the other research programme).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I need also to indicate the different payoffs. This is what I've done so far :&#xA;&lt;img src=&quot;https://i.stack.imgur.com/BLTJ6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to be sure that I'm going in the right direction since I'm a beginner with decision trees. And then I need to decide the best course of action using Bayes, Maximax and Maximin rules.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm quite new to Data Science, but I would like to do a project to learn more about it.&#xA;My subject will be Data Understanding in Public Health.&#xA;So I want to do some introductory research to public health.&#xA;I would like to visualize some data with the use of a tool like Tableau.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which path would you take to develop a good understanding of Data Science? I imagine taking some online courses, eg. Udacity courses on data science, but which courses would you recommend?&#xA;Where can I get real data (secondary Dummy Data) to work with?&#xA;And are there any good resources on research papers done in Data Science area with the subject of Public Health?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions and comments are welcome.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was wondering which language can I use: R or Python, for my internship in fraud detection in an online banking system: I have to build machine learning algorithms (NN, etc.) that predict transaction frauds.&#xA;Thank you very much for your answer.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Hello and thanks in advance!  I'd like some advice on a scalability issue and the best way to resolve.  I'm writing an algorithm in R to produce forecasts for several thousand entities.  One entity takes about 43 seconds to generate a forecast and upload the data to my database.  That equates to about 80+ hours for the entire set of entities and that's much too long.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought about running several R processes in parallel, possibly many on a few different servers, each performing forecasts for a portion of total entities.  Though that would work, is there a better way?  Can Hadoop help at all?  I have little experience with Hadoop so don't really know if it can apply.  Thanks again!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;A usual way to find association rules in R is the &quot;arules&quot; package, which easily let's use calculate some rules based on the apriori algorithm. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, for the data i'm using, I have a lot of NULL cases (baskets where no product A o B is present). This means that I need to calculate some null-invariant measures (kulcynski, for instance). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of any package or workable code that let's me implement this as opposed to writing from scratch the entire algorithm?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Are there commonly accepted ways to visualize the results of a multivariate regression for a non-quantitative audience? In particular, I'm asking how one should present data on coefficients and T statistics (or p-values) for a regression with around 5 independent variables.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;This question is likely somewhat naive.  I know I (and my colleagues) can install and use Python on local machines. But is that really a best practice?  I have no idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there value in setting up a Python &quot;server&quot;?  A box on the network where we develop our data science related Python code.  If so, what are the hardware requirements for such a box?  Do I need to be concerned about any specific packages or conflicts between projects?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Most literature focus on either explicit rating data or implicit (like/unknown) data. Are there any good publications to handle like/dislike/unknown data? That is, in the data matrix there are three values, and I'd like to recommend from unknown entries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And are there any good open source implementations on this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a pandas dataframe with a salary column which contains values like:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;£36,000 - £40,000 per year plus excellent bene...,&#xA;  £26,658 to £32,547 etc&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I isolated this column and split it with the view to recombining into the data frame later via a column bind in pandas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I now have an object with columns like the below. The columns I split the original data frame column I think are blank because I didn't specify them (I called &lt;code&gt;df['salary']=df['salary'].astype(str).str.split()&lt;/code&gt;&#xA;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my new object contains this type of information:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[£26,658, to, £32,547],&#xA;[Competitive, with, Excellent, Benefits]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to do is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create three columns called minvalue and maxvalue and realvalue&lt;/li&gt;&#xA;&lt;li&gt;List items starting with £ (something to do with &lt;code&gt;&quot;^£&quot;&lt;/code&gt;?&lt;/li&gt;&#xA;&lt;li&gt;Take till the end of the items found ignoring the £ (get the number out) (something to do with &lt;code&gt;(substr(x,2,nchar(x)))&lt;/code&gt;?&lt;/li&gt;&#xA;&lt;li&gt;If there are two such items found, call the first number &quot;minvalue&quot; and call the second number &quot;maxvalue&quot; and put it below the right column. If there is only one value in the row, put it below the realvalue column.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am very new to pandas and programming in general, but keen on learning, your help would be appreciated. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;As an example. If you are tying to classify humans from dogs. Is it possible to approach this problem by classifying different kinds of animals (birds, fish, reptiles, mammals, ...) or even smaller subsets (dogs, cats, whales, lions, ...) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then when you try to classify a new data set, anything that did not fall into one of those classes can be considered a human.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this is possible, are there any benefits into breaking a binary class problem into several classes (or perhaps labels)? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Benefits I am looking into are: accuracy/precision of the classifier, parallel learning.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am performing Named Entity Recognition using Stanford NER. I have successfully trained and tested my model. Now I want to know:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) What is the general way of measuring accuracy of NER model ?? For example what techniques or approaches are used ??&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is there any built-in method in STANFORD NER for evaluating the accuracy ??&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;We are currently developing a customer relationship management software for SME's. What I'd like to structure for our future CRM is developing CRM with a social-based approach (Social CRM). Therefore we will provide our users (SME's) to integrate their CRM into their social network accounts. Also CRM will be enhance intercorporate communication of owner company.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All these processes I've just indicated above will certainly generate lots of unstructured data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering how can we integrate &lt;i&gt;big data&lt;/i&gt; and &lt;i&gt;data-mining&lt;/i&gt; contepts for our project; especially for the datas generated by social network? I am not the expert of these topics but I really want to start from somewhere.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Basic capabilities of CRM (Modules)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;-Contacts: People who you have a business relationship.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Accounts: Clients who you've done a business before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Leads: Accounts who are your potential customers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Oppurtunites: Any business opportunity for an account or a lead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Sales Orders&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Calendar&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Tasks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What kind of unstructured data or the ways (ideas) could be useful for the modules I've just wrote above? If you need more specific information please write in comments.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am starting to play around in datamining / machine learning and I am stuck on a problem that's probably easy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have a report that lists the url and the number of visits a person did. So a combination of ip and url result in an amount of visits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to run the k-means clustering algorithm on this so I thought I could approach it like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;url      ip    visits&#xA;&#xA;abc.be   123   5&#xA;abc.be/a 123   2&#xA;abc.be/b 123   2&#xA;abc.be/b 321   4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I would turn in into a feature vector/matrix like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc.be  abc.be/a   abc.be/b   impressions&#xA;   1       0          0          5&#xA;   0       1          0          2&#xA;   0       0          1          2&#xA;   0       0          1          4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I am stuck on how to transform my data set to a feature matrix. Any help would be appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I would like to use a neural network for image classification.  I'll start with pre-trained CaffeNet and train it for my application.  &lt;/p&gt;&#xA;&#xA;&lt;h1&gt;How should I prepare the input images?&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don't know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn't be really the same scale (maybe 1.5x variation).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Dataset augmentation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I've heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am new to &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot; rel=&quot;noreferrer&quot;&gt;machine learning&lt;/a&gt;!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now I am doing some problems on an application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, &lt;a href=&quot;http://scikit-learn.org&quot; rel=&quot;noreferrer&quot;&gt;scikit-learn&lt;/a&gt; takes only numbers as parameters, but I want to inject the strings as well as they carry a significant amount of knowledge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I handle such a scenario?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can convert a string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your support!&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am struggling to choose a right data prediction method for the following problem.&#xA;Essentially I am trying to model a scheduler operation, trying to predict its scheduling without knowing the scheduling mechanism and having incomplete data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) There are M available resource blocks that can carry data, N data channels that must be scheduled every time instance i&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Inputs into the scheduler:  &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Matrix $X_i$ size M by N, consisting of N column vectors from each data source.    Each of M elements is index from 1 to 32 carrying information about quality of data channel for particular resource block. 1 - really bad quality, 32 - excellent quality.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Data which contains type of data to be carried (voice/internet etc)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Scheduler prioritizes number of resource blocks occupied by each channel every time instant i.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;I CAN see resource allocation map every time instant&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I DO have access to matrix $X_i$  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I DON'T know the algorithm of scheduler and&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I dont have access to the type of data to be scheduled. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I want to have a best guess (prediction) how the data will be scheduled based on this incomplete information i.e, which resource block will be occupied by which data channel. What is the best choice of prediction/modelling algorithm?&#xA;Any help appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;There are several metrics for the quality of a graph clustering, e.g. Newman modularity. These enable you to compare two candidate clusterings of the same graph. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know a metric that will answer the question &quot;how modular is this graph&quot;? For example the first of these two graphs is more modular than the second:&#xA;    o===o-----o====o    o----o===o-----o&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be possible to choose a clustering algorithm, run it, and compute your preferred modularity metric for the best clustering found. But this is only a lower bound, so it doesn't seem very satisfactory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question matters. For example, the work of life scientists will be easier if the molecular organisation of life is modular than if it is not. It would be good to have a robust test - some of the discussion so far seems to involve wishful thinking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My best attempt at this is:&#xA;- a tree is more modular if the edges near leaves are higher weight&#xA;- the modularity of a graph is the modularity of its min cut spanning tree&#xA;Does anyone know of an established answer to this question?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I produced association rules by using the arules package (apriori). I'm left with +/- 250 rules. I would like to test/validate the rules that I have, like answering the question: How do I know that these association rules are true? How can I validate them? What are common practice to test it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought about cross validation (with training data and test data) as I read that it's not impossible to use it on unsupervised learning methods..but I'm not sure if it makes sense since I don't use labeled data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If someone has a clue, even if it's not specifically about association rules (but testing other unsupervised learning methods), that would also be helpful to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I uploaded an example of the data that I use here in case it's relevant: &lt;a href=&quot;https://www.mediafire.com/?4b1zqpkbjf15iuy&quot; rel=&quot;nofollow&quot;&gt;https://www.mediafire.com/?4b1zqpkbjf15iuy&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Please I want to know if there is any SVM R package that can handle more than one response variable (y) at a time. that is to train one model for predicting more than one response variable. it could be regression or multi class classification problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have been working in the last years with statistics and have gone pretty deep in programming with R. I have however always felt that I wasn't completely grasping what I was doing, still understanding all passages and procedures conceptually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wanted to get a bit deeper into the math behind it all. I've been looking online for texts and tips, but all texts start with a very high level. Any suggestions on where to start?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more precise, I'm not looking for an exaustive list of statistical models and how they work, I kind of get those. I was looking for something like &quot;Basics of statistical modelling&quot;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Many of us are very familiar with using R in reproducible, but very much targeted, ad-hoc analysis. Given that R is currently the best collection of cutting-edge scientific methods from world-class experts in each particular field, and given that plenty of libraries exist for data io in R, it seems very natural to extend its applications into production environments for live decision making.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;did someone of you go into production with pure R (I know of shiny, yhat etc, but would be very interesting to hear of pure R);&lt;/li&gt;&#xA;&lt;li&gt;is there a good book/guide/article on the topic of building R into some serious live decision-making pipelines (such as e.g. credit scoring);&lt;/li&gt;&#xA;&lt;li&gt;I would like to hear also if you think it's not a good idea at all;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I start with a data.frame (or a data_frame) containing my dependent Y variable for analysis, my independent X variables, and some &quot;Z&quot; variables -- extra columns that I don't need for my modeling exercise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I would like to do is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an analysis data set without the Z variables;&lt;/li&gt;&#xA;&lt;li&gt;Break this data set into random training and test sets;&lt;/li&gt;&#xA;&lt;li&gt;Find my best model;&lt;/li&gt;&#xA;&lt;li&gt;Predict on both the training and test sets using this model;&lt;/li&gt;&#xA;&lt;li&gt;Recombine the training and test sets by rows; and finally&lt;/li&gt;&#xA;&lt;li&gt;Recombine these data with the Z variables, by column.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It's the last step, of course, that presents the problem -- how do I make sure that the rows in the recombined training and test sets match the rows in the original data set? We might try to use the row.names variable from the original set, but I agree with Hadley that this is an error-prone kludge (my words, not his) -- why have a special column that's treated differently from all other data columns?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One alternative is to create an ID column that uniquely identifies each row, and then keep this column around when dividing into the train and test sets (but excluding it from all modeling formulas, of course). This seems clumsy as well, and would make all my formulas harder to read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This must be a solved problem -- could people tell me how they deal with this? Especially using the plyr/dplyr/tidyr package framework?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Terms like 'data science' and 'data scientist' are increasingly used these days.&#xA;Many companies are hiring 'data scientist'. But I don't think it's a completely new job.&#xA;Data have existed from the past and someone had to deal with data. &#xA;I guess the term 'data scientist' becomes more popular because it sounds more fancy and 'sexy'&#xA;How were data scientists called in the past?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want to turn reviews of up to 5 stars and the number of reviews into upvotes. What's a good algorithm for doing this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A venue with 10 reviews total with a 5-star average rating should obviously get more upvotes than a venue with 10 reviews total with a 3-star average rating. Also, a venue with 60 ratings and a 4-star rating should probably get more upvotes than the one with 10 reviews and a 5-star rating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need this rating to be based off of the total number of reviews and the average star rating, but I would also like the number to stay below a variable number (for example, say upvotes stay below 100, but I can also plug in 200 and it would stay below 200).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm struggling with for loop in R. I have a following data frame with sentences and two dictionaries with pos and neg words:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(stringr)&#xA;library(plyr)&#xA;library(dplyr)&#xA;library(stringi)&#xA;library(qdap)&#xA;library(qdapRegex)&#xA;library(reshape2)&#xA;library(zoo)&#xA;&#xA;# Create data.frame with sentences&#xA;sent &amp;lt;- data.frame(words = c(&quot;great just great right size and i love this notebook&quot;, &quot;benefits great laptop at the top&quot;,&#xA;                         &quot;wouldnt bad notebook and very good&quot;, &quot;very good quality&quot;, &quot;bad orgtop but great&quot;,&#xA;                         &quot;great improvement for that great improvement bad product but overall is not good&quot;, &quot;notebook is not good but i love batterytop&quot;), user = c(1,2,3,4,5,6,7),&#xA;               number = c(1,1,1,1,1,1,1), stringsAsFactors=F)&#xA;&#xA;# Create pos/negWords&#xA;posWords &amp;lt;- c(&quot;great&quot;,&quot;improvement&quot;,&quot;love&quot;,&quot;great improvement&quot;,&quot;very good&quot;,&quot;good&quot;,&quot;right&quot;,&quot;very&quot;,&quot;benefits&quot;,&#xA;          &quot;extra&quot;,&quot;benefit&quot;,&quot;top&quot;,&quot;extraordinarily&quot;,&quot;extraordinary&quot;,&quot;super&quot;,&quot;benefits super&quot;,&quot;good&quot;,&quot;benefits great&quot;,&#xA;          &quot;wouldnt bad&quot;)&#xA;negWords &amp;lt;- c(&quot;hate&quot;,&quot;bad&quot;,&quot;not good&quot;,&quot;horrible&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And now I'm gonna to create replication of origin data frame for big data simulation:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Replicate original data.frame - big data simulation (700.000 rows of sentences)&#xA;df.expanded &amp;lt;- as.data.frame(replicate(100000,sent$words))&#xA;    sent &amp;lt;- coredata(sent)[rep(seq(nrow(sent)),100000),]&#xA;    sent$words &amp;lt;- paste(c(&quot;&quot;), sent$words, c(&quot;&quot;), collapse = NULL)&#xA;rownames(sent) &amp;lt;- NULL&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For my further approach, I'll have to do descending ordering of words in dictionaries with their sentiment score (pos word = 1 and neg word = -1).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Ordering words in pos/negWords&#xA;wordsDF &amp;lt;- data.frame(words = posWords, value = 1,stringsAsFactors=F)&#xA;wordsDF &amp;lt;- rbind(wordsDF,data.frame(words = negWords, value = -1))&#xA;wordsDF$lengths &amp;lt;- unlist(lapply(wordsDF$words, nchar))&#xA;wordsDF &amp;lt;- wordsDF[order(-wordsDF[,3]),]&#xA;wordsDF$words &amp;lt;- paste(c(&quot;&quot;), wordsDF$words, c(&quot;&quot;), collapse = NULL)&#xA;rownames(wordsDF) &amp;lt;- NULL&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have a following function with for loop. 1) matching exact words 2) count them 3) compute score 4) remove matched words from sentence for another iteration:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scoreSentence_new &amp;lt;- function(sentence){&#xA;  score &amp;lt;- 0&#xA;  for(x in 1:nrow(wordsDF)){&#xA;    sd &amp;lt;- function(text) {stri_count(text, regex=wordsDF[x,1])} # count matched words&#xA;    results &amp;lt;- sapply(sentence, sd, USE.NAMES=F) # count matched words&#xA;    score &amp;lt;- (score + (results * wordsDF[x,2])) # compute score&#xA;    sentence &amp;lt;- str_replace_all(sentence, wordsDF[x,1], &quot; &quot;) # remove matched words from sentence for next iteration&#xA;  }&#xA;  score&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I call that function&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SentimentScore_new &amp;lt;- scoreSentence_new(sent$words)&#xA;    sent_new &amp;lt;- cbind(sent, SentimentScore_new)&#xA;    sent_new$words &amp;lt;- str_trim(sent_new$words, side = &quot;both&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it resulted into desired output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                                                                             words user     SentimentScore_new&#xA;                             great just great right size and i love this notebook    1                  4&#xA;                                                 benefits great laptop at the top    2                  2&#xA;                                               wouldnt bad notebook and very good    3                  2&#xA;                                                                very good quality    4                  1&#xA;                                                             bad orgtop but great    5                  0&#xA; great improvement for that great improvement bad product but overall is not good    6                  0&#xA;                                       notebook is not good but i love batterytop    7                  0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In real I'm using dictionaries with pos/neg words about 7.000 words and I have 200.000 sentences. When I used my approach for 1.000 sentences it takes 45 mins. Please, could you anyone help me with some faster approach using of vectorization or parallel solution. Because of my beginner R programming skills I'm in the end of my efforts :-( Thank you very much in advance for any of your advice or solution&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering about something like that:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 1:nrow(wordsDF)&#xA;score &amp;lt;- 0&#xA;&#xA;try_1 &amp;lt;- function(ttt) {&#xA;sd &amp;lt;- function(text) {stri_count(text, regex=wordsDF[ttt,1])}&#xA;results &amp;lt;- sapply(sent$words, sd, USE.NAMES=F)&#xA;    score &amp;lt;- (score + (results * wordsDF[ttt,2])) # compute score (count * sentValue)&#xA;    sent$words &amp;lt;- str_replace_all(sent$words, wordsDF[ttt,1], &quot; &quot;)&#xA;score&#xA;}&#xA;&#xA;a &amp;lt;- unlist(sapply(n, try_1))&#xA;apply(a,1,sum)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But doesn't work :-(&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Does reinforcement learning always need a grid world problem to be applied to?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me any other example of how reinforcement learning can be applied to something which does not have a grid world scenario?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I took on a project to predict the outcome of soccer matches but it turned out to be a very challenging task.&#xA;I tried out different models but I only got 50-54% accuracy on my test dataset. Some of the models were created in such&#xA;a way that a certain model would predict if a team will win, draw, or loose a match. That same model would also predict if&#xA;the opponent of that team will win, draw, or loose the match. Each model predicting with an accuracy of about 50% on each team distinctively. The second set of models I tried, takes the combination of data from both teams and predicts which class the match belongs to (home win, away win, draw). In the system,&#xA;only 10 matches are given everyday to be predicted. Meaning, if I predict the 10 matches using the second model, I have a chance of predicting 5 &#xA;correctly. In this project, I only need to predict 3 matches correctly out of the 10 matches given in a day. Is there a system&#xA;of knowing the 3 matches which my models have the best chance of predicting correctly? I only need to get 3 correct, I usually&#xA;get 5 correctly but I don't know how to select my 3 best matches. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The first type of models use about 50 features for prediction while the second uses 101. I've tried ensembles, they still give &#xA;me ~50% accuracy. I'm still about to setup a system that selects matches where the prediction for the home team does not &#xA;contradict the prediction for the away team using the first type of model.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The project I am working on allows users to create Stock Screeners based on both technical and fundamental criteria.  Stock Screeners are then &quot;backtested&quot; by simulating the results of applying in over the last 10 years using Point-in-Time data. I get back the list of trades and overall graph of performance.&#xA;(If that is unclear, I have an overview &lt;a href=&quot;https://www.equitieslab.com/features/stock-screener/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.equitieslab.com/wiki/QuickStart/StockScreener&quot; rel=&quot;nofollow&quot;&gt;there&lt;/a&gt; with more details).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now a common problem is that users create overfitted stock screeners. I would love to give them a warning when the screen is likely to be over-fitted.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fields I have to work with&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All trades made by the Stock Screener&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stock, Start Date, Start Price, End Date, End Price&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;S&amp;amp;P 500 performance for the same time frame&lt;/li&gt;&#xA;&lt;li&gt;Market Cap, Sector, and Industry of each Stock&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;The training set contains $p$ papers. Each paper is annotated as &lt;em&gt;research&lt;/em&gt; or &lt;em&gt;non-research&lt;/em&gt;. To develop the research paper filter, we consider the $W$ most frequent phrases in a paper. The research paper filter will use the presence/absence of these $W$ phrases to decide if the paper is indeed a research paper or not.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;How many possible hypothesis are there?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have $2^W$ possible hypothesis, but if we are including conjunctive hypothesis symbols, then I would say $4^W$. Is my logic correct?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How many of them are consistent?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For this question, I have no idea where to begin. Any insight?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We learned normalization last week and it seems like you're supposed to normalize data when they have very different values. For decision trees, is the case the same?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure about this but would normalization affect the resulting decision tree from the same data set? It doesn't seem like it should but...&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was given a target function to design neural network and train: (y = (x1 ∧ x2) ∨ (x3 ∧ x4))&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of input and number of output seems obvious (4 and 1). And the training data can use truth table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in order to train as a multilayer artificial neural network, I need to choose number of hidden units. May I know where can I find some general guideline for this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I want to generate an $N\\\\times N$ matrix $A$ so as to target an $N$ vector of row sums and simultaneously all column sums should sum to 1. In addition to this, I have a prefixed number of elements which are set to zero. &#xA;For example, beginning with:&#xA;$$&#xA;  \\\\left[\\\\begin{array}{rrr}&#xA;    0 &amp;amp; 1 &amp;amp; 1 \\\\\\\\&#xA;    1 &amp;amp; 0 &amp;amp; 0 \\\\\\\\&#xA;    1 &amp;amp; 1 &amp;amp; 0&#xA;  \\\\end{array}\\\\right]&#xA;$$&#xA;and the row sum vector $[1.5, 0.25, 1]^{T}$, I want to end up with&#xA;$$&#xA;  \\\\left[\\\\begin{array}{rrr}&#xA;    0 &amp;amp; a_{12} &amp;amp; a_{13} \\\\\\\\&#xA;    a_{21} &amp;amp; 0 &amp;amp; 0 \\\\\\\\&#xA;    a_{31} &amp;amp; a_{32} &amp;amp; 0&#xA;  \\\\end{array}\\\\right]&#xA;$$&#xA;under the following conditions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{12} + a_{13}  = 1.5$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$ a_{21} = 0.25$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{31}+a_{32} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{21}+a_{31} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{12}+a_{32} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{13} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While this is simplistic, in general, I have $2N$ equations in $N^{2}-Z$ unknowns, where $Z$ is the number of elements fixed to zero. So, this system of equations could be overdetermined or underdetermined, but I would like to be able to generate matrices like this such that all nonzero elements lie in $(0,1]$.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm a Newbie to Hadoop!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By web search and after going through hadoop guides, it is clear that hdfs doesn't allow us to edit the file but to append some data to existing file, it will be creating a new instance temporarily and append the new data to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1.would like to know whether the &lt;em&gt;new file&lt;/em&gt; or &lt;em&gt;temp file&lt;/em&gt; is created in the &lt;em&gt;same block&lt;/em&gt; or in a &lt;em&gt;different block&lt;/em&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2.What will happen if the revised file &lt;strong&gt;exceeds&lt;/strong&gt; the previous  allocated block size?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be greatly appreciated!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Any suggestion on what kind of dataset lets say $n \\\\times d$ ($n$ rows, $d$ columns) would give me same eigenvectors?.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe it should be the one with same absolute values in each cell. Like alternating +1 and -1. But it seems to work otherwise. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am using twitteR package to retrievie timeline data. My request looks as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;tweets &amp;lt;- try(userTimeline(user , n=50),silent=TRUE)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and this worked quite well for a time, but now I receive this error message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in function (type, msg, asError = TRUE)  : easy handle already used in multi handle&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In a related question on Stackoverflow one answer is to use Rcurl directly but this does not seem to work with twitteR package. Anybody got an idea on this?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a huge collection of objects from which only a tiny fraction are in a class of interest. The collection is initially unlabelled, but labels can be added using an expensive operation (for example, by human).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I use the simple generic machine learning strategy:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Use hand-crafted rules to select a smaller subset of objects (thus leaving out a fraction of interesting ones).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Label part of the smaller subset, and use these for training and choosing a classification algorithm and its parameters.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Classify the remaining objects in the smaller set (and also perhaps in the big set).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This has two drawbacks:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The labeller still needs to see a huge number of uninteresting objects, and therefore is able to label only a very small fraction of interesting ones. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The objects not in the smaller set are completely ignored in the learning phase, resulting in a loss of some information (the classification algorithm might not work well on this complement).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It seems that it would be better to use online learning: i.e., select the objects to show to the labeller based on the previous labels. But then it becomes no longer obvious that the result of classification algorithm retains the nice theoretical properties (i.e., statistical consistency).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a general framework for active object detection which works either theoretically or practically (or both)? I could not get the complete picture from the Wikipedia article &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_learning_%28machine_learning%29&quot; rel=&quot;nofollow&quot;&gt;active learning&lt;/a&gt;. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;This is an interview question&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;How are neural nets related to Fourier transforms?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I could find papers that talk about methods to process the Discrete Fourier Transform (DFT) by a single-layer neural network with a linear transfer function. Is there some other correlation that I'm missing?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I need to implement an algorithm for multiple extended string matching in text. Algorithms to match regular expression would be perhaps too slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Extended&lt;/strong&gt; means the presence of wildcards (any number of characters instead of a star), for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc*def //matches abcdef, abcpppppdef etc.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Multiple&lt;/strong&gt; means that the search is going on simultaneously for multiple string patterns (not a separate search for each pattern), for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc*def&#xA;abc&#xA;whatever&#xA;some*string&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QUESTION:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the fast algorithm that can do multiple extended string matching?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Preferably, optimized for SIMD instructions and multicore implementation. Open source implementation (C/C++/Python) would be great as well. I'm interested in 10 Gbps+ performance on a single core of a modern CPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am attempting to work with a very large data-set (~1.5mil lines) for the first time in SAS and I am having some difficulty. The data-set I have is formatted as a &quot;long&quot; .txt file as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;'cat1/: Topic1_Variable1'&#xA;'cat2/: Topic1_Variable2'&#xA;'cat3/: Topic1_Variable3'&#xA;'cat4/: Topic1_Variable4'&#xA;&#xA;'cat1/: Topic2_Variable1'&#xA;'cat2/: Topic2_Variable2'&#xA;'cat3/: Topic2_Variable3'&#xA;'cat4/: Topic2_Variable4'&#xA;&#xA;'cat1/: Topic3_Variable1'&#xA;'cat2/: Topic3_Variable2'&#xA;'cat3/: Topic3_Variable3'&#xA;'cat4/: Topic3_Variable4'&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fro analysis and sharing with others, I really would like to see it formatted as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat1              cat2              cat3              cat4&#xA;Topic1_Variable1  Topic1_Variable2  Topic1_Variable3  Topic1_Variable4&#xA;Topic2_Variable1  Topic2_Variable2  Topic2_Variable3  Topic2_Variable4&#xA;Topic3_Variable1  Topic3_Variable2  Topic3_Variable3  Topic3_Variable4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think that this may be easier in R, but I honestly am drawing a complete blank in SAS. I've even played with MS Access to try to get it to look the way I want but the program crashes every time (due to the size?). At any rate, I have looked into some of the statements in PROC TRANSPOSE and PROC SQL but it seems that most functions within those procedures are utilized to combine duplicate 'Topics'. In the data I have been provided, each &quot;group&quot; represents an individual response to a question with several thousand individuals repeated, I want to retain the independence of each occurrence and not perform a UNION as defined in PROC SQL. At this point, I feel like I am over-thinking this but I just can't get around the mental block and actually do what I am working toward. &#xA;Any help or guidance is much appreciated. I'm open to trying all suggestions or ideas, I think I have access to most statistical computing programs. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying my first 'project' concerning machine learning and I am a bit stuck.&#xA;However, I am not sure if it's even possible but here goes my question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to achieve is clustering user groups based on the amount of visits a user does on a certain website.&#xA;So I started out with this feature matrix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;USER    abc.be  abc.be/a    abc.be/b    xyz.be  xyz.be/a&#xA;123      0        0           0            0      1&#xA;456      1        0           1            0      0&#xA;789      2        3           1            0      0&#xA;321      1        0           1            0      1&#xA;654      1        1           1            1      1&#xA;987      0        1           0            3      0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I got in this example 5 features (my 5 different websites).&#xA;So then I used PCA to come to 2 dimensions, so I could plot it and see how it went.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My feature matrix (in my example) is 5 columns * 6 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My PCA matrix is 2 columns * 6 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I came to this plot (please note that this plot uses different data then the example but the idea is the same)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Tl0Qv.png&quot; alt=&quot;PCA points and k-means centroids&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green points are my PCA points&#xA;The red circles are my K-Means centroids.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the part I am struggling with is this: so I got my clusters (red circles) but how can I use this to say:&quot;Looks like most users go to either site A or site B)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I couple my clusters to a feature label from my feature matrix?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or how does one approach this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated :)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Problem: I want to know methods to perform an effective sampling from a database.&#xA;The size of the database is about &lt;code&gt;250K&lt;/code&gt; text documents and in this case each text document is related to some majors (Electrical Engineering, Medicine and so on). So far I have seen some simple techniques like &lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_random_sample&quot; rel=&quot;nofollow&quot;&gt;Simple random sample&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Stratified_sampling&quot; rel=&quot;nofollow&quot;&gt;Stratified sampling&lt;/a&gt;; however, I don't think it's a good idea to apply them for the following reasons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;In the case of simple random sample, for instance, there are a few documents in the database that talk about majors like Naval Engineering or Arts. Therefore I think they are less likely to be sampled with this method but I would like to have some samples of every major as possible.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In the case of stratified sampling, most of the documents talk about more than one major so I cannot divide the database in subgroups because they will not be mutually exclusive, at least for the case in which each major is a subgroup.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Finally, I cannot use the whole database due to expensive computational cost processing. So I would really appreciate any suggestions on other sampling methods. Thanks for any help in advance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am a newbie. I would like to do visualization on twitter data : top trends based on country (over map) and time variations (for each months in 1 year or for each year) . Can someone tell me where can I get the twitter data set and any advice on how to start proceeding would be really help full.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am training a neural network with 1 sigmoid hidden layer and a linear output layer. The network simply approximates a cosine function. The weights are initiliazed according to Nguyen-Widrow initialization and the biases are initialized to 1. I am using MATLAB as a platform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Running the network a number of times without changing any parameters, I am getting results (mean squared error) which range from 0.5 to 0.5*10^-6. I cannot understand how the results can even vary that much, I'd imagine there would at least be a narrower and more consistent window of errors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What could be causing such a large variance?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;In a assignment we are given macro economic indicators like GDP, Consumer price index, Producer Price index and Industrial production index. Also we are given Crude oil, Sugar prices and FM-CG Sales. We are required to forecast future quarter sales and give a model. As I'm new to this subject, I don't know where to start with it, or what to read. Can anyone provide me with some examples of what to do, or any PDFs which might be helpful. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Given a sentence like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Complimentary gym access for two for the length of stay ($12 value per person per day)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What general approach can I take to identify the word gym or gym access?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;For example, there are 4 objects. Each object has some elements. There are 3 clusters. Each element of each object belongs to some cluster. Clustering was done before, and we know only how many elements of each object belong to some cluster.&#xA;How we can calculate the porbability of similarity of two objects?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;         Cluster1  Cluster2  Cluster3&#xA;Object1    500       300        200&#xA;Object2    200       200        100&#xA;Object3    250       2500       1250       &#xA;Object4    190       210        300&#xA;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am new to python programming. As part of a data analysis project, I am trying to get a scatter plot of Salary vs Wins for each year of 4 consecutive years (so I am trying to get 4 different scatter plots, one for each year). I am using the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;teamName = 'OAK'&#xA;years = np.arange(2000,2004)&#xA;for yr in years:&#xA;    df = joined[joined['yearID']==yr]&#xA;    plt.scatter(df['salary']/1e6,df['W'])&#xA;    plt.title('Wins vs Salaries in year' + str(yr))&#xA;    plt.xlabel('Total Salary (in millions)')&#xA;    plt.ylabel('Wins')&#xA;    plt.xlim(0,180)&#xA;    plt.ylim(30,130)&#xA;    plt.grid()&#xA;    plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I am only getting one plot corresponding to 2003. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone point out the mistake ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;The optimisation problem for support vector regression is (see &lt;a href=&quot;http://alex.smola.org/papers/2003/SmoSch03b.pdf&quot; rel=&quot;nofollow&quot;&gt;http://alex.smola.org/papers/2003/SmoSch03b.pdf&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;minimise:&#xA;\\\\begin{align*}&#xA;C\\\\sum_{i=0}^{l}(\\\\xi_{i} +\\\\xi^{*}_{i})+ \\\\frac{1}{2}\\\\lVert w \\\\rVert^{2}&#xA;\\\\end{align*}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;subject to the constraints:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;\\\\begin{align*}&#xA;&amp;amp; y_{i} - &amp;lt;w,x_{i}&amp;gt; - b \\\\leq \\\\epsilon + \\\\xi_{i} \\\\\\\\&#xA;&amp;amp; &amp;lt;w,x_{i}&amp;gt; + b - y_{i} \\\\leq \\\\epsilon + \\\\xi^{*}_{i} \\\\\\\\&#xA;&amp;amp; \\\\xi_{i}, \\\\xi^{*}_{i} \\\\geq 0&#xA;\\\\end{align*}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand where the $\\\\lVert w \\\\rVert^{2}$ comes from. I understand how the $\\\\lVert w \\\\rVert^{2}$ is derived in the case of support vector classification (by maximizing the margin), but not in the case of regression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper says that the &quot;goal is to find a function $f(x)$ that has at most $\\\\epsilon$ deviation from the actually obtained targets $y_{i}$ for all the training data, and at the same time is as flat as possible. In other words, we do not care about errors as long as they are less than $\\\\epsilon$, but will not accept any deviation larger than this.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I am not sure what &quot;flat&quot; means.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does someone know?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have implemented an interactive visualization with d3.js, javascript to explore the frequency and various combinations of co-occurring item sets. I want to complement the interactive exploration with some automated options.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does someone know an efficient javascript implementation of the association rules mining ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My typical scenario will have just up to 30 different items.&#xA;There are some good web site with implementations of frequent item set mining (improvements from the initial apriori algorithm): &lt;a href=&quot;http://www.borgelt.net/apriori.html&quot; rel=&quot;nofollow&quot;&gt;http://www.borgelt.net/apriori.html&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is greatly appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to read a csv file as input from user in Shiny and assign it to a variable in global.r file.The code I have in ui.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fileInput('file1', 'Choose CSV File',&#xA;                     accept=c('text/csv', 'text/comma-separated-values,text/plain', .csv'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code I have in main Panel of server.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;textOutput('contents')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code I have currently in server.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;output$contents &amp;lt;- renderText({    &#xA;      if(is.null(input$file1))return()&#xA;      inFile &amp;lt;- input$file1&#xA;      data&amp;lt;-read.csv(inFile$datapath)&#xA;  print(summary(data))&#xA;&#xA; })&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to assign this input$file1 to a variable called 'data' in global.r. Please let me know if this is possible. &#xA;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am from information security field, and have some introductory level understanding of machine learning field. My problem is to identify behavioural patterns from network traffic. If I use supervised learning ( classification ) the results are very promising but I encounter of a problem of missing out a pattern with slight change in behaviour. And If I use clustering then i need to do a much work on manual tuning of centroids position to get better clusters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking to ensemble classification and clustering and use classification output to tune cluster centroids. Is this is a good idea to address the problem?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Jaccard_index&quot; rel=&quot;nofollow&quot;&gt;Jaccard coefficient&lt;/a&gt; measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had 100 sets all of same cardinality. By mistake I calculated the similarity measures as the ratio of intersection with total elements in a set (i.e 100).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This gives different similarity values than the original Jaccard formula.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering if the original formula has considered the union of two sets to handle cases where there might be sets with different cardinalities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think though numerically my values are different, they repersent the same idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anybody could verify/disverify what I am trying to do ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am subsetting some data frames and am sure there is a better way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially, I have two data frames.  The first is the actual data.  The second has some meta data and importantly a flag on whether or not the row is in the subset I am interested in.  All I would like to do is pull out the subset and write a file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, here is my subset data frame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     head(temp[,c(1,4:8)])&#xA;     ensembl_gene_id   FGF Secreted  SigP Receptors    TF&#xA;1 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;2 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;3 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;4 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;5 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;6 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is my actual data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Expt1_C1_EXONS_dxd_res_wDesc[1:5,1:5]&#xA;                                   groupID featureID exonBaseMean dispersion         stat&#xA;ENSMUSG00000000001:E001 ENSMUSG00000000001      E001    624.80240 0.04271781  1.255734504&#xA;ENSMUSG00000000001:E002 ENSMUSG00000000001      E002     30.92281 0.02036015  0.514038911&#xA;ENSMUSG00000000001:E003 ENSMUSG00000000001      E003     41.61413 0.01546023 10.105615831&#xA;ENSMUSG00000000001:E004 ENSMUSG00000000001      E004    137.47209 0.03975305  0.281105120&#xA;ENSMUSG00000000001:E005 ENSMUSG00000000001      E005     85.97116 0.05753662  0.005482149`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I was doing is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;write.table(Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,],&#xA;            &quot;Expt1_C1_EXONS_dxd_res_wDesc_FGF.tab&quot;,col.names=T,row.names=F,quote=F)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is taking 20+ min per subset just to subset and write the data.  The data frame is 370,000 rows by 27 variables, so large but not huge.  I have about 30 of these to do.  Is there a more efficient way?  Note, the groupID does NOT equal the first column in my subset data frame.  In some instances the groupID contains a concatenated set of ensembl ids.  So I have preprocessed to get the temp data frame to have what I want in the same row order.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Bob&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to write an ANN in python for handwriting recognition by mouse movements. ( like identify characters we draw in paint app n convert it to text) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question might seem that I haven't done my homework, but I am *not * looking for image datasets for handwritten characters.( at least that's what I've figured out so far)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any database for such (array or co-ordinates corresponding to each character) inputs? If not, how can I create this database from existing ones like MNIST?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, I have written a script to generate my own database because I could not find any. But it seems a very tedious task to generate a good database. &#xA;I am a beginner in ML. Could somebody point me in &lt;em&gt;right direction&lt;/em&gt; upon how else can we do it?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;If I prune a decision tree, does that make the resulting decision tree always more general than the original decision tree?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there examples where this is not the case?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm struggling to find a solution to produce a line chart which shows a trend from a Time Series metric. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I try to make my line chart look similar to the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/hZaa2.png&quot; alt=&quot;Time series&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to make use of relative change rates, but I struggle to find a calculation which makes a visualization as seen above possible. The graph should always be relative to a specific time window, meaning that I can query my data dynamically with a start and end timestamp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My (incomplete) sample data set is the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;timestamp,value,change_rate,change_rate_absolute_sum,change_rate_delta,change_rate_sum&#xA;1426587900000,778;0.00129,0.00129;0.00129,0.00129&#xA;1426588200000,778;0,0.00129;-0.00129,0&#xA;1426588500000,1189;0.52828,0.52957;0.52828,0.52828&#xA;1426588800000,1195;0.00505,0.53462;-0.52323,0.00505&#xA;1426589100000,1195;0,0.53462;-0.00505,0&#xA;1426589400000,1196;0.00084,0.53546;0.00084,0.00084&#xA;1426589700000,1286;0.07525,0.61071;0.07441,0.07525&#xA;1426590000000,1290;0.00311,0.61382;-0.07214,0.00311&#xA;1426590300000,1294;0.0031,0.61692;-0.00001,0.0031&#xA;1426590600000,1296;0.00155,0.61847;-0.00155,0.00155&#xA;1426590900000,1356;0.0463,0.66477;0.04475,0.0463&#xA;1426591200000,1358;0.00147,0.66624;-0.04483,0.00147&#xA;1426591500000,1358;0,0.66624;-0.00147,0&#xA;1426591800000,1360;0.00147,0.66771;0.00147,0.00147&#xA;1426592100000,1408;0.03529,0.703;0.03382,0.03529&#xA;1426592400000,1390;-0.01278,0.69022;-0.04807,-0.01278&#xA;1426592700000,1391;0.00072,0.69094;0.0135,0.00072&#xA;1426593000000,1410;0.01366,0.7046;0.01294,0.01366&#xA;1426593300000,1414;0.00284,0.70744;-0.01082,0.00284&#xA;1426593600000,1410;-0.00283,0.70461;-0.00567,-0.00283&#xA;1426593900000,1414;0.00284,0.70745;0.00567,0.00284&#xA;1426594200000,1420;0.00424,0.71169;0.0014,0.00424&#xA;1426594500000,1417;-0.00211,0.70958;-0.00635,-0.00211&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any ideas warmly appreciated. Thanks a lot in advance!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm going to describe the procedure for gradient descent using the language of English:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; For all of the files (e.g. documents) in our training corpus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;--&gt; commence, now we are considering our first document&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; For all the features in the feature vector associated with that training example (document)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; Take the dot product of the vector of weight values * and the vector of features specific to the document under consideration&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* &lt;em&gt;vector of weight values is applicable to all features, though all features will not be present in all documents, indeed only some features will be present in each individual document&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;4)&lt;/strong&gt; Take the value resulting from step 3 and apply this to our loss function. I'm using the Hinge Loss function, i.e. the formula&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/tVvmW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that equation saying that the hinge loss objective function should be the sum of the misclassification error for every file in our training set?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is that equivalent to the java code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Math.max(0, 1 - y * value_from_step_three)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With &lt;code&gt;y = {-1, 1}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;^ THIS IS MY FIRST POINT OF CONFUSION&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess that is inaccurate but I'm not sure, according to my mental model it should be right. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;5)&lt;/strong&gt; For every weight, update it as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;new weight value&lt;sub&gt;j&lt;/sub&gt; = old weight value&lt;sub&gt;j&lt;/sub&gt; - learning rate * &lt;em&gt;value from step 4&lt;/em&gt; * value of feature&lt;sub&gt;j&lt;/sub&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm a bit confused about what &lt;em&gt;value of feature&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; should be, since I guess this would correspond to a vector, that has values for every weight under consideration, i.e.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Document 1 = [&quot;I&quot;, &quot;am&quot;, &quot;awesome&quot;]&#xA;Document 2 = [&quot;I&quot;, &quot;am&quot;, &quot;great&quot;, &quot;great&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Dictionary is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[&quot;I&quot;, &quot;am&quot;, &quot;awesome&quot;, &quot;great&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the documents as a vector would look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Document 1 = [1, 1, 1, 0]&#xA;Document 2 = [1, 1, 0, 2]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There will be weights:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Weight&lt;sub&gt;&quot;I&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;am&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;awesome&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;great&quot;&lt;/sub&gt;]&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I use RStudio for R programming. I remember about solid IDE-s from other technology stacks, like Visual Studio or Eclipse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What other IDE-s than RStudio are used (please consider providing some brief description on them).&lt;/li&gt;&#xA;&lt;li&gt;Does any of them have noticeable advantages over RStudio?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I mostly mean debug/build/deploy features, besides coding itself (so text editors are probably not a solution).&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm not sure which word to use to differentiate a self-organizing map (SOM) training procedure in which updates for the entire data set are aggregated before they are applied to the network from a training procedure in which the network is updated with each data point individually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of other algorithms I'd say &lt;code&gt;stochastic gradient descent&lt;/code&gt;, but I'm not sure &lt;code&gt;gradient descent&lt;/code&gt; is correct for SOM learning.  To my knowledge, SOM learning does not follow any energy function exactly, so that should mean it doesn't do gradient descent exactly, right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another term would be &lt;code&gt;online learning,&lt;/code&gt; but `online' sort of implies I train my SOM in the real world with data points streaming in, eg. from a set of sensors.  It may also imply that I use every data point only once, which is not what I want to say.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm an &lt;code&gt;R&lt;/code&gt; language programmer. I'm also in the group of people who are considered Data Scientists but who come from academic disciplines other than CS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works out well in my role as a Data Scientist, however, by starting my career in &lt;code&gt;R&lt;/code&gt; and only having basic knowledge of other scripting/web languages, I've felt somewhat inadequate in 2 key areas:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Lack of a solid knowledge of programming theory.&lt;/li&gt;&#xA;&lt;li&gt;Lack of a competitive level of skill in faster and more widely used languages like &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt; and &lt;code&gt;Java&lt;/code&gt;, which could be utilized to increase the speed of the pipeline and Big Data computations as well as to create DS/data products which can be more readily developed into fast back-end scripts or standalone applications.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The solution is simple of course -- go learn about programming, which is what I've been doing by enrolling in some classes (currently C programming). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, now that I'm starting to address problems #1 and #2 above, I'm left asking myself &quot;&lt;em&gt;Just how viable are languages like &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;C++&lt;/code&gt; for Data Science?&lt;/em&gt;&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, I can move data around very quickly and interact with users just fine, but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So. can &lt;code&gt;C&lt;/code&gt; do the job -- what tools are available for advanced statistics, ML, AI, and other areas of Data Science?&lt;/strong&gt; Or must I loose most of the efficiency gained by programming in &lt;code&gt;C&lt;/code&gt; by calling on &lt;code&gt;R&lt;/code&gt; scripts or other languages?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best resource I've found thus far in C is a library called &lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/index.html&quot; rel=&quot;noreferrer&quot;&gt;Shark&lt;/a&gt;, which gives &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;C++&lt;/code&gt; the ability to use Support Vector Machines, linear regression (not non-linear and other advanced regression like multinomial probit, etc) and a shortlist of other (great but) statistical functions.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What approaches are there to not display a search result that a user is not supposed to see?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose we have the following situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;High-level view&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In an enterprise, there are different repositories like websites, databases, filesystem folder etc. And there is a search engine that crawls all those repositories and creates an index. The user can then use the search engine UI to perform searches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say the search engine was &lt;a href=&quot;https://lucene.apache.org/core/&quot; rel=&quot;nofollow&quot;&gt;Apache Lucene&lt;/a&gt; and the encompassing indexing and retrieval system &lt;a href=&quot;http://lucene.apache.org/solr/&quot; rel=&quot;nofollow&quot;&gt;Apache Solr&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;User permissions are managed via &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_Directory&quot; rel=&quot;nofollow&quot;&gt;Microsoft Active Directory&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The content in the different repositories are managed with all kinds of applications like MS Word, custom-made applications, database management tools etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Search-index and user permissions&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The index contains the documents A1, A2, B1 and B2.&lt;/li&gt;&#xA;&lt;li&gt;User A has only permission to see documents of type A*.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Performing a search&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Now, user A performs a search for &lt;code&gt;A* AND B*&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Displaying the results&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A system administrator with permission to see everything should see all four documents in the search results: A1, A2, B1 and B2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;User A however, who only has permissions to see documents of type A*, should/will only see two documents in the search results: A1 and A2. So user A shouldn't even know that documents B1 and B2 exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Elaboration on the question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What ways are there to implement that requirement that users only see documents they are allowed to see?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suppose it would be a bad idea to add the information to the index whether a user may see a document. I think that because of this use case: suppose all of a sudden user A may only see document A1 and not A2 anymore; those permissions are now set for example on the filesystem. Now it takes a while until the permissions in the index are updated and during that time user A would still be able to see document A2 in the search results even if he can't click on the result and access it anymore.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, the above approach probably would break down when there are hundreds of millions of documents and thousands of employees.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where and how would the security aspect be implemented in a content indexing and retrieval system in the above scenario?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm fairly new to machine learning, but I'm doing my best to learn as much as possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious about how predicting athlete performance (runners in particular) in a race of a specific starting lineup. For instance, if RunnerA, RunnerB, RunnerC, and RunnerD are all racing a 400 meter race, I want to best predict whether &lt;strong&gt;RunnerA&lt;/strong&gt; will beat &lt;strong&gt;RunnerB&lt;/strong&gt; based on past race result information (which I have at my disposal). However, I have many cases where &lt;strong&gt;RunnerA&lt;/strong&gt; has never raced against &lt;strong&gt;RunnerB&lt;/strong&gt;; yet I do have data showing &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; in the past, and &lt;strong&gt;RunnerC&lt;/strong&gt; has beat &lt;strong&gt;RunnerB&lt;/strong&gt; in the past. This logic extends deeper as well. So, it would seem that &lt;strong&gt;RunnerA&lt;/strong&gt; &lt;em&gt;should&lt;/em&gt; beat &lt;strong&gt;RunnerB&lt;/strong&gt;, given this information. My real concern is when it gets more complicated than this as I add more features (multiple runners, different distances, etc), and so I'm turing to ML algorithms to help my predictions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am having difficulty figuring out how to include this in my row data that I can train (after all, correctly formatting data is 99% of proper machine learning), and I am hoping that someone here might have thought along the same lines in the past and might be able to shed some light. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently trying to include &lt;strong&gt;RunnerX&lt;/strong&gt;-&lt;strong&gt;RunnerY&lt;/strong&gt; past race data by counting all the races that &lt;strong&gt;RunnerX&lt;/strong&gt; and &lt;strong&gt;RunnerY&lt;/strong&gt; have run together and normalizing them on a scale from &lt;code&gt;-1&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;; &lt;code&gt;-1&lt;/code&gt; indicating &lt;strong&gt;RunnerX&lt;/strong&gt; lost all past races against RunnerY; and &lt;code&gt;+1&lt;/code&gt; indicating that RunnerX has  won all past races against RunnerY; and &lt;code&gt;+1&lt;/code&gt; indicating. And &lt;code&gt;0&lt;/code&gt; indicating an equal number of wins and losses (or no past races against each other).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, if &lt;strong&gt;RunnerA&lt;/strong&gt; is racing &lt;strong&gt;RunnerB&lt;/strong&gt;, and &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerB&lt;/strong&gt; in the past, then I want the algorithm to know that (denoted by a &lt;code&gt;+1&lt;/code&gt; on the &lt;strong&gt;RunnerB&lt;/strong&gt; column of row &lt;strong&gt;RunnerA&lt;/strong&gt;); same for vice versa. Taking it another step further, If &lt;strong&gt;RunnerA&lt;/strong&gt; is racing &lt;strong&gt;RunnerC&lt;/strong&gt; (but the two have never raced each other in the past), and &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerD&lt;/strong&gt; in a past race, and &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; in a past race, then I want the algorithm to learn that &lt;strong&gt;RunnerA&lt;/strong&gt; should beat &lt;strong&gt;RunnerC&lt;/strong&gt;. I say &lt;em&gt;beat&lt;/em&gt; here, but I mean an &quot;average beat&quot; for any &lt;strong&gt;RunnerX&lt;/strong&gt;-&lt;strong&gt;RunnerY&lt;/strong&gt; combinations when data for more than 1 past race is available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have set my data up as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name     track   surface  distance  age    RunnerA   RunnerB   RunnerC   RunnerD&#xA;RunnerA  Home    2        400       11     0         1         0         1&#xA;RunnerC  Away    2        400       12     0         0         0         -1&#xA;RunnerD  Home    2        400       10     0         0         1         0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which shows that RunnerA has beat &lt;strong&gt;RunnerB&lt;/strong&gt; and &lt;strong&gt;RunnerD&lt;/strong&gt; in the past. RunnerC has lost to &lt;strong&gt;RunnerD&lt;/strong&gt;. And &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I don't really think this is a correct display of the information for an ML algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;From what I understand, ML data should be row independent. And this data isn't because row 1 (&lt;strong&gt;RunnerA&lt;/strong&gt;) has beat &lt;strong&gt;RunnerD&lt;/strong&gt;, yet the data indicating &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; is in row 3.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any ideas how I might be able to incorporate this past win-percentage-for-runner-pair-combination data??? I'm totally stuck here. I've read a lot about some algorithms that estimate the win loss by simply totaling win statistics, but those don't say anything about the actual probability of a particular runner to beat another particular runner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers would be super helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!!!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I think there are numerous posts regarding which one to use: R or Python. However, I'm curious about how their architecture differences yield differences in speed performance, &lt;strong&gt;not&lt;/strong&gt; which one to use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This &lt;a href=&quot;http://alstatr.blogspot.com/2014/01/python-and-r-is-python-really-faster.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog post&lt;/a&gt; performs a small test between R and python to show that the (optimized) python code was 2x faster than R code.* And I've read in &lt;a href=&quot;https://datascience.stackexchange.com/questions/59/what-are-rs-memory-constraints&quot;&gt;this post&lt;/a&gt; that R tends to put everything in memory, which is why computations on large datasets is generally slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what makes python's low level memory management so much different than R, which helps it yield these benchmarks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*Though python was 2x faster in this test than R, I'm not saying that python is generally 2x faster than R.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a large dataset with characters and 90000 intances and I have the error &lt;em&gt;ValueError: array is too big&lt;/em&gt; when I have the following code before the plot_kmeans_digits.py code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data2=list(csv.DictReader(open('C:\\\\diabeticdata.csv', 'rU')))&#xA;vec = DictVectorizer()&#xA;data = vec.fit_transform(data2).toarray()&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know how I can solve this error?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am struggling with a conceptual problem related to feature scaling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume I am building a classifier (e.g., a NN) and let's assume I rely on future scaling for the input features of my model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this context I will normalise the training set using its mean and its std and I would do the same with the testing set using the testing mean and std.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let us also assume I succeed in building my classifier and I move to production where I try to classify new inputs.  However for such new inputs the mean and std are unknown! How can I scale them appropriately before processing with my model? May be I could use the mean and std from training+testing.....&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I really don't know which is the correct practice here....any hint?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your help!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am investigating whether to use theano or caffe for convnets. I would like to know which one provides a better debug environment. In caffe, it seems you don't write any code just a config in protobuf - how do you debug the convnet then?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a complex dataset with more than 16M rows coming from pharmaceutical industry. Regarding the data, it is saved in a sql server with multiple (more than 400) relational tables. Data got several levels of hierachies like province, city, postal code, person, and antigens measures, etc. I would like to create many dashboards in order to observe the changes &amp;amp; trends happening. I can use Pentaho, R (shiny) or Tableau for this purpose. But the problem is data is so huge, and it take so long to process it with dashboard softwares. I have a choice of making cube and connect it to dashboard. My question here is whether there are  any other solutions that I can use instead of making a cube? I don't want to go through the hassle of making &amp;amp; maintaining a cube. I would like to use a software where I specify relationships between tables, so the aggregation/amalgamation happens smoothly and output processed tables that can connect  to dashboards. I hear Alteryx is one software that can do it for you (I haven't tried it myself, and it is an expensive one!). I understand this task needs two or more softwares/tools. Please share your input &amp;amp; experience. Please mention what tools do you use, size of your data, and how fast/efficient is the entire system, and other necessary details. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm testing a SVD-based collaborative filter on my data set, in which the label, $r_{ij}$, is a real value from 0 to 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like the many papers suggested, to have a better performance, instead of using $ \\\\hat{R} = U \\\\cdot V^T $ directly, I use $\\\\hat{R} = \\\\mu + B_u + B_v + U \\\\cdot V^T $, where $\\\\mu$ is the average rating, $B_u$ is the bias of user, and $B_v$ is the bias of item.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, this model corresponds to a loss function:&#xA;$\\\\min_{B_u, B_v, U, V}  = ||I\\\\circ(R-\\\\mu-Bu-B_v-U\\\\cdot V^T)||_F^2 + \\\\lambda (||B_u||_F^2 + ||B_v||_F^2 + ||U||_F^2 + ||V||_F^2)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where I is the masking matrix in which $I_{ij} = 1$ if $R_{ij}$ is known, and $||\\\\cdot||_F$ is the frobenius norm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, I solve this by gradient descent, it seems to work fine, and the test RMSE is 0.25. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I investigate the contribution of each part in predict function $\\\\hat{R} = \\\\mu + B_u + B_v + U \\\\cdot V^T $, I notice that, $\\\\mu$ is about 0.5, $b_u$ and $b_i$ are about $\\\\pm0.3$, but the part of $ U \\\\cdot V^T $ is quite small, normally about $\\\\pm 0.01$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why does this part contribute so small? Since this part is the actual part where the collaborative filter works, I expect it to contribute more in prediction.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have one clarification - &lt;/p&gt;&#xA;&#xA;&lt;p&gt;First the definitions- &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;User-based&lt;/strong&gt;: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Item-based&lt;/strong&gt;: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed offline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the question - in item based CF the similarity between items are tracked via user behavior - since user behavior is changing will it not impact the similarity between items?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Why would you want to decorrelated data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I am reading about PCA and whitening on image data for DNN, I wonder what is the purpose of achieving the identity covariance matrix in your data is?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it to remove interaction between variables, thus allow simpler models to express interactions without having to compute x1*x2?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have been looking at Stochastic Single Value Decomposition in Mahout for implementing a distributed LSA algorithm. However, I am having trouble finding the best way to set k and p such that k+p &amp;lt; min(m,n). Is there an optimal way to set k and p? I know that p should not exceed 10% of k and that k is the rank (typically from 20 to 200 according to the documents). Can I relate it to the number of dependent vectors?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am a beginner studying &lt;strong&gt;social network analysis&lt;/strong&gt;.&#xA;I installed python 3 just 2 weeks ago.&#xA;There are a lot of books for python and social network analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I couldn't choose one of them.&#xA;I found one book named &quot;Mining the Social Web (Analyzing Data from Facebook, Twitter, Linkedln, and Other Social Media Sites)&quot; written by Matthhew A. Russell.&#xA;This book looks very interesting and fits in my purpose, but it is based on python 2. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any good books with &lt;strong&gt;python 3&lt;/strong&gt;? I usually use &lt;strong&gt;Twitter, Facebook, or Blog data&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition, could you recommend any good book for &lt;strong&gt;nodeXL and UCINET?&lt;/strong&gt;  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;This may be too broad of a question with heavy opinions, but I really am finding it hard to seek information about running various algorithms using SQL Server Analysis Service Data Mining projects versus using R. This is mainly because all the data science guys I work with don't have any idea about SSAS because no one seems to use it. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Database Guy&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Before I start, let me clarify. I am a database guy and not a data scientist. I work with people who are data scientist who mainly use R. I assist these guys with creating large data sets where they can analyze and crunch data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My objective here is to leverage a tool that came with SQL Server that no one is really leveraging because no one seems to have a clue about how it works in comparison to other methods and tools such as R, SAS, SSPS and so forth in my camp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;SSAS&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have never really used SQL Server Analysis Services (SSAS) outside of creating OLAP cubes. Those who know SSAS, you can also perform data mining tasks on cubes or directly on the data in SQL Server. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;SSAS Data Mining comes with a range of algorithm types:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Classification algorithms&lt;/strong&gt; predict one or more discrete variables,&#xA;based on the other attributes in the dataset.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Regression algorithms&lt;/strong&gt; predict one or more continuous variables, such&#xA;as profit or loss, based on other attributes in the dataset.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Segmentation algorithms&lt;/strong&gt; divide data into groups, or clusters, of&#xA;items that have similar properties.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Association algorithms&lt;/strong&gt; find correlations between different attributes&#xA;in a dataset. The most common application of this kind of algorithm&#xA;is for creating association rules, which can be used in a market&#xA;basket analysis.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sequence analysis algorithms&lt;/strong&gt; summarize frequent sequences or episodes&#xA;in data, such as a Web path flow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Predicting Discrete Columns&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With these different algorithm options, I can start making general predictions from the data such as finding out simply who is going to buy a bike based on a predictable column, Bike Buyers, against an input column, Age. The histogram shows that the age of a person helps distinguish whether that person will purchase a bicycle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/X64Y1.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Predicting Continuous Columns&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When the Microsoft Decision Trees algorithm builds a tree based on a continuous predictable column, each node contains a regression formula. A split occurs at a point of non-linearity in the regression formula. For example, consider the following diagram.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/6MkTt.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With some of that said, it seems I can run a range of algorithms on the data and also have various functions available to me in SSAS to run against the data. It also seems I can develop my own algorithms in Visual Studio and deploy them to SSAS (if I'm not mistaken).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what am I missing here in regards to languages and tools from R? Is it just that they have more flexibility to deploy and edit complex algorithms versus SSAS etc?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using k-means clustering to processes running on machines. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Dataset sample : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;machine name, process&#xA;m1,java&#xA;m2,tomcat&#xA;m1,word&#xA;m3,excel&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Build a matrix of associated counts : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   java,tomcat,word,excel&#xA;m1,1,0,1,0&#xA;m2,0,1,0,0&#xA;m3,0,0,0,1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then run k-means against this dataset (have tried Euclidean and Manhattan distance functions) &#xA;The dataset is extremely sparse which I think is causing the generated clusters to not make much sense as many machines get grouped into the same cluster(as they are very similar) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to achieve clusters where each cluster contains approx equal number of points ? Or perhaps this is not possible due to the sparseness of the data and instead I should try to cluster on a different attributes of dataset ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am doing some research on Logistic regression and SVM using different parameters using HOG features. I am facing a bit of problem while understanding each classifier with combination of different parameters and different HOG features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My findings and confusions are given below, &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(1,1)&#xA;Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=209,Wrong Prediction=7&#xA;    , &#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(1,1)&#xA;Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5&#xA;Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5&#xA;Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6            &#xA;&#xA;-------------------------------------------------------------------------            &#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(9,9), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=213,Wrong Prediction=3&#xA;&#xA;&#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;&#xA;&#xA;For Hog:    orientations=18, pixelsPerCell=(9,9), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=3&#xA;&#xA;&#xA;For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For the above results we can conclude that &quot;C&quot; of SVM and orientation of HOG doesn't matter much for the combinations, so we do not have the issue of highbias or variance and bin size. What matters is the value of gamma and combination of HOG with &quot;cellsPerBlock&quot;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;From my analysis, I found&#xA;1. Using &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(1,1)&quot; doesn't give proper    outcome.&#xA;2. The &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(1,1)&quot; works best with gamma=0.1&#xA;3. The &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(3,3)&quot; and different combination of &quot;pixelspercells&quot; works best with gamma=1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've also tried pixelspercell = (3,3) but all the different combinations of &quot;rbf&quot; kernels fails to classify properly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;&quot; The problem I face is understanding this discrepancy&quot;&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Confusion even eggravates when I use Logistic regression for the Combinations of HOG surprisingly Regularized Logistic Regression works better with cellsPerBlock=(1,1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below are my findings with 2nd order Regularized Logistic Regression: I used 0.9 as my threshold level, i.e below 0.9 would be classifies as 0 and above 0.9 would be classified as 1&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Q:   Why did I put 0.9 as threshold- &#xA;Sol: Because the test data that had the class(labels)=1, 99% of them output probability &amp;gt;0.9&#xA;     However the test data that had the class(labels)=0, many instances outputprobability &amp;gt;0.5&#xA;   So inorder to filter out many test data instances that gave probability &amp;gt;0.5 A threshold of 0.9 was used.&#xA;&#xA;1.  orientations=9,    &#xA;pixelsPerCell=(6,6)&#xA;cellsPerBlock=(3,3)&#xA;Output:     Total dataset=216, Correct prediction=187, Wrong Prediction=29&#xA;&#xA;2.  orientations=9,    &#xA;pixelsPerCell=(9,9)&#xA;    cellsPerBlock=(3,3)&#xA;    Output:     Total dataset=216, Correct prediction=202, Wrong Prediction=14&#xA;&#xA;3.  orientations=9,    &#xA;pixelsPerCell=(9,9)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=211, Wrong Prediction=5&#xA;&#xA;4.  orientations=9,    &#xA;pixelsPerCell=(6,6)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=206, Wrong Prediction=10&#xA;&#xA;5.  orientations=9,    &#xA;pixelsPerCell=(3,3)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=203, Wrong Prediction=13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;E.g 1: &quot;The speed of the car is 35 km/h and speed limit is 50 km/h&quot; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g 2: &quot;The car is crossing the current speed limit by driving at 90 km/h&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g.3: &quot;Driving at 50 km/h your car will hit the speed limit.&quot;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I very new in neural networks and in NLP too. Good in perl and in writing parsers for web pages. NLP is asking me my code to learn english and word meanings. I wish to try differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to train my first neural network classification algo using offsets to words. I can train that to parse speed: in e.g1 the 35 is true and 50 is false. That in e.g2 90 is true. and e.g3 50 is false.&#xA;Suppose I have a collection of lot of such sentences, so I can use my offset based training for my algorithm. There will be very confusing sentences too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way my algo would work is to use offets of all words which are near the text which is to be classified. In sentence one it would output 11 parameters, is = -1, km/h=+1, car=-2, and=+2,the=-3,speed=+3,of=-4,limit=+4,speed=-5,is=+5,The=-6,km/h=+6. Same thing would be done for all sentences. the algo will train with these offsets and the result. There are too many words and they all will become parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please suggest how durable is my proposed learning algorithm to parse out actual car driven speed from sentences? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I've only worked with ML with .csv formats. I've worked with image formats too but only premade imagesets (MNIST,etc). If I were to create an imageset from scratch, how are the class labels typically formated? Would I have to manually title the image of a jpeg? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best,&#xA;Jeremy&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to implement Streaming Naive Bayes in a distributed system. What are the best approach to choose framework. Should I choose:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Storm alone and implement streaming naive bayes on my own in storm topology.&lt;/li&gt;&#xA;&lt;li&gt;Storm + TridentML&lt;/li&gt;&#xA;&lt;li&gt;Storm + SAMOA&lt;/li&gt;&#xA;&lt;li&gt;Spark Streaming + MLlib&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What is the best framework set to choose and start working on. Any suggestion will be of great help.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Generally, the machine learning model is built on datasets. I'd like to know if there is any way to generate synthetic dataset using such trained machine learning model preserving original dataset characteristics ?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;[original data --&gt; build machine learning model --&gt; use ml model to generate synthetic data....!!!]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible ? Please point me to related resource if possible.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm having a having a hard time understanding the difference between an isomorphism in graphs and canonical graphs. I have read through the Wikipedia articles, but it still isn't clicking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can somebody explain the difference, perhaps with an example?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have watched an explanation of Big O Notations from this eloquent YouTube video: &lt;a href=&quot;https://www.youtube.com/watch?v=V6mKVRU1evU&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=V6mKVRU1evU&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, he does not mention algorithms with two asterisk signs. How does the algorithm with the complexity of &lt;code&gt;O(N**3)&lt;/code&gt; work?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am working on a data set where the categorical variables have lots of empty spaces (not &quot;NA&quot; but &quot;&quot;). For example, one variable has 14587 empty spaces out of 14644 observations. There are many such variables where most of the observations are empty.In fact it is a survey dataset where the participant just chose to ignore a particular question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have never handled similar dataset. I am looking for advise as to how best to handle such datasets before any modeling is done. Deleting the rows or the variables with lots of empty spaces doesn't seem feasible. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to see if there is a conventional term for this concept to help me in my literature research and writing.  When a machine learning model causes an action to be taken in the real world that affects future instances, what is that called?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about something like a recommender system that recommends one given product and doesn't recommend another given product.  Then, you've increased the likelihood that someone is going to buy the first product and decreased the likelihood that someone is going to buy the second product.  So then those sales numbers will eventually become training instances, creating a sort of feedback loop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a term for this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Does anyone know, where I can learn about applying data science to win a political campaign? I know the Obama campaign had 12 data scientists in 2008 and 165 data scientists in 2012. In 2012, they ran over 65,000 simulations every night, for 14 months. They correctly predicted every state within 0.5% and Florida within 0.05%. How did they do this? And where can I find the data they used?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I would consider myself a journeyman data scientist.  Like most (I think), I made my first charts and did my first aggregations in high school and college, using Excel.  As I went through college, grad school and ~7 years of work experience, I quickly picked up what I consider to be more advanced tools, like SQL, R, Python, Hadoop, LaTeX, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are interviewing for a data scientist position and one candidate advertises himself as a &quot;senior data scientist&quot; (a very buzzy term these days) with 15+ years experience.  When asked what his preferred toolset was, he responded that it was Excel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I took this as evidence that he was not as experienced as his resume would claim, but wasn't sure.  After all, just because it's not my preferred tool, doesn't mean it's not other people's. &lt;strong&gt;Do experienced data scientists use Excel?  Can you assume a lack of experience from someone who does primarily use Excel?&lt;/strong&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;with regards to the Logistic Regression cost function of:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lBaI2.png&quot; alt=&quot;Logistic Regression Cost Function&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And hypothesis:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ISUJW.png&quot; alt=&quot;Hypothesis&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to tell the +/- of the error for how &quot;confident&quot; the hypothesis is?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g. if the +/- of the error was 0.1, I would know that if my hypothesis predicted 0.4 it could be 0.1 greater (0.5) or 0.1 less (0.3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is for binary classification&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have around 1,000 job ads in the filed of IT (in excel file). I want to find the skills which are mentioned in each of ads. and then find the similar jobs based on skills.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My method: I created 12 categories Such as programming skills,  testing skills,  communication skills, network skills, ... . Each advertisement may belong to 3-4 categories. In this case, some said multi-variate classification or Multi label classification is useful. But I don't know how to do this kind of classification in RapidMiner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1- Does anyone know how to do multi-variate classification or Multi label classification in RapidMiner? or is there another way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2- Do you recommend &quot;classification&quot; in order to analysis required job skills? or another technique? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;3- Is there any better way to classify the skills which are stated in job ads?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm new in the field of text mining. Please let me know if you have any idea. Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How to write &quot;OR&quot; in this example?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DATA a1;&#xA;set a;&#xA;if var1=1 OR 2;&#xA;run;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. va1 is the categorial (with categories: 1, 2, 3)&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;If I parse following sentence:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;He is playing cricket in ground with grass.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with stanford parser, the result is: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(ROOT&#xA;  (S&#xA;    (NP (PRP He))&#xA;    (VP (VBZ is)&#xA;      (VP (VBG playing)&#xA;        (NP&#xA;          (NP (NN cricket))&#xA;          (PP (IN in)&#xA;            (NP (NN ground))))&#xA;        (PP (IN with)&#xA;          (NP (NN grass)))))&#xA;    (. .)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any parser who can correct the result on the basis of probability as the probability of appearing grass with ground is higher than cricket?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or parser which do chunking before parsing like this:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;He is playing cricket |in ground| |with grass|.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and calculate probability with all combinations before generating the parse tree.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;He is playing cricket |in ground| |with grass|.&lt;/li&gt;&#xA;&lt;li&gt;He is playing cricket |with grass| |in ground|.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Obviously most employers, when hiring a data scientist, would prefer experience with big data and/or data science.  But what can one safely assume they will acknowledge as experience?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say someone often launches software on a computing cluster that typically generates an amount of data.  I'm not sure what the best measure of this data is for data science.  I'll call it one or two thousand rows, 200k or 300k points per row...certainly under 500k.  Then for each point, let's call it 25 or 30 values.  This amounts to 30 or 40 gig of data.  300 or 400 times of this and you can call it a study - maybe one or two studies per year.  I'm under the impression that this is much smaller than a data scientist at Google or Facebook would be used to, but it's certainly too big for my home computing systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If someone's been working with this for years (some people at this company have been doing it since before &lt;em&gt;data science&lt;/em&gt; was coined/before social media existed), is it fair for them to claim big data experience?  According to &lt;a href=&quot;https://datascience.stackexchange.com/a/37/8953&quot;&gt;this answer&lt;/a&gt;, it's not the amount of data but what needs to be done with the data that matters - is that a universally accepted opinion?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, working with this data entails manipulation/cleaning of the data with some proprietary languages, shell scripts, and a lot of Python.  A little bit of R but that's a more recent thing.  It involves tons of data visualization, drawing conclusions, and presenting to management/convincing decision makers.  Some of it involves trend determination, extrapolation, and comparisons made between data sets that aren't related in a straightforward way, so it sounds data science-ish to me.  But I will be the first to admit that I have a limited understanding of what data science currently is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;...bonus points if you can tell me whether or not this is an Easter egg or is the actual answer total for this site at the moment:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/w6haD.png&quot; alt=&quot;1337&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll try to clarify.  What do data science employers acknowledge as experience with big data/data science?  Does the size of the data above qualify experience with it?  Or is it universally accepted among those in the field that it's not at all the size of the data, but what you need to do with the data?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to understand the procedure of building a static local website using &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; and &lt;code&gt;Rmarkdown&lt;/code&gt;. I am aware of a &lt;a href=&quot;http://rmarkdown.rstudio.com/html_document_format.html#creating-a-website&quot; rel=&quot;nofollow&quot;&gt;Rmarkdown&lt;/a&gt; website where the procedure is outlined, but unfortunately I do not understand the steps. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody here have some experience in building a static local website and would be so kind as to describe the procedure in more detail?   &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;In recent methods for community detection in dynamic networks, LFR benchmark is used as dynamic dataset generator, but I thought it is for static community based data generation. For example in paper &lt;a href=&quot;http://www.cise.ufl.edu/~mythai/files/mobicom_CS.pdf&quot; rel=&quot;nofollow&quot;&gt;Overlapping Communities in Dynamic Networks: Their Detection and Mobile Applications&lt;/a&gt;, LFR is used. But I don't know how this dataset is generated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://pages.towson.edu/npnguyen/Databases/AFOCS.zip&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the code of this paper and ground truth included is only for one snapshot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question: How is the dataset generated?&lt;/strong&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Is there a recommended approach for storing processed data for testing new data products?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I'd like to have a system where a data scientist or an analyst could think of a new data product to present to users, do the data processing to create it, and then put it in a data store that our application can then access easily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I'm not sure about is what kind of data store would be good for this type of &quot;testing&quot; use case.  Since it would need to be flexible enough to handle different types of data products, like aggregates, windowed data, etc.  And ideally it wouldn't require a huge instrumentation process to try out new things.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;a matrix X with N lines&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;a vector Y&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've computed the Euclidean distance with Y for each line of X.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I get is a vector of distances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want is a vector of scores between 0 and 1, 1 meaning &quot;very&quot; high correlation, 0 meaning &quot;no&quot; correlation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here what I did :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I divided the vector of distances by the max distance inside it.&#xA;I get vector D. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;1 - D&lt;/em&gt; is the final result with values between 0 and 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I get many values (75%) too close to 1.&#xA;Do you think what I did is correct ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you get a better result ?&#xA;(Between 0 and 1 but not everything too close to 1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For now, I tried to take the square of the result. (To stay between 0 and 1 but to minimize the values)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here a picture of the distance values I want to turn in a score&#xA;&lt;img src=&quot;https://i.stack.imgur.com/36LDh.png&quot; alt=&quot;distance values I want to turn in a score&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've made a logistic regression to combine two independent variables in &lt;code&gt;R&lt;/code&gt;, using &lt;code&gt;pROC&lt;/code&gt; package and I obtain this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; summary(fit)&#xA;&#xA;Call: glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;Deviance Residuals: &#xA;  Min       1Q     Median     3Q      Max  &#xA;-1.5751  -0.8277  -0.6095   1.0701   2.3080  &#xA;&#xA;Coefficients:&#xA;             Estimate  Std. Error z value Pr(&amp;gt;|z|)    &#xA;(Intercept) -0.153731   0.538511  -0.285 0.775281    &#xA;X           -0.048843   0.012856  -3.799 0.000145 ***&#xA;Y            0.028364   0.009077   3.125 0.001780 ** &#xA;---&#xA;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xA;&#xA;(Dispersion parameter for binomial family taken to be 1)&#xA;&#xA;Null deviance: 287.44  on 241  degrees of freedom&#xA;Residual deviance: 260.34  on 239  degrees of freedom&#xA;AIC: 266.34&#xA;&#xA;Number of Fisher Scoring iterations: 4&#xA;&#xA;&amp;gt;     fit&#xA;&#xA;Call:  glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;Coefficients:&#xA;  (Intercept)       X            Y  &#xA;   -0.15373     -0.04884      0.02836  &#xA;&#xA;Degrees of Freedom: 241 Total (i.e. Null);  239 Residual&#xA;Null Deviance:      287.4 &#xA;Residual Deviance:  260.3        AIC: 266.3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I need to extract some information from this data and I'm not sure about how to do it. First, I need the model equation: suppose that fit is a combined predictor called &lt;code&gt;CP&lt;/code&gt;; could it be &lt;code&gt;CP=-0.15-0.05X+0.03Y&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, the resulting combined predictor from the regression should present a median value, so that I can compare median from the two groups &lt;code&gt;Case&lt;/code&gt; and &lt;code&gt;Controls&lt;/code&gt; which I used to make the regression (in other words, my &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; variables are N-dimensional with &lt;code&gt;N = N1+N2&lt;/code&gt;, where &lt;code&gt;N1 = Number of Controls&lt;/code&gt;, for which &lt;code&gt;Case=0&lt;/code&gt;, and &lt;code&gt;N2 = Number of Cases&lt;/code&gt;, for which &lt;code&gt;Case=1&lt;/code&gt;).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working for a logistics firm and there are approx. 750+ customers who avail our services. I am in the process of building and generating some insight for the business based on the payments made by these customers for the last 1 year. Some make payment on time whereas some are late &amp;amp; some are extremely late. Could you please advice which modeling technique or statistical approach would be best in this case. &#xA;I can think of creating clusters based on the payment history and highlights &amp;amp; placing all defaulters in one cluster where our company can focus. &#xA;PLease suggest. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I understand that by filtering out the instances with labels that Random Forest trees are uncertain upon with their decisions, and model these with another classifier could give a better overall result. My question is, how can I &quot;combine&quot; two (or more) classifiers' classification on a single unlabeled dataset?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to apply classification algorithms to KDD Cup 2012 track2 data using R&#xA;&lt;a href=&quot;http://www.kddcup2012.org/c/kddcup2012-track2&quot; rel=&quot;nofollow&quot;&gt;http://www.kddcup2012.org/c/kddcup2012-track2&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems not possible to work with this 10GB training data on my local system with 4GB RAM.&#xA;Can anyone work on this data using this kind of a local system ? Or is using a cluster the norm ?&lt;br&gt;&#xA;It would be great if anyone could provide me with any guidance on how to get started with working on a cluster and the normally used type of cluster for such tasks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am interested in any data, publications, etc about what is the smallest neural network that can achieve a certain level of classification performance.  By small I mean &lt;em&gt;few parameters&lt;/em&gt;, not &lt;em&gt;few arithmetic operations&lt;/em&gt; (=fast).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested primarily in convolutional neural networks for vision applications, using something simple like CIFAR-10 without augmentation as the benchmark.  Top-performing networks on CIFAR in recent years have had anywhere between 100 million and 0.7 million parameters (!!), so clearly small size is not (always) a bad thing.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Small networks are also in general faster to train and overfit less.  Moreover, recent work on &lt;a href=&quot;http://arxiv.org/abs/1503.02531&quot; rel=&quot;nofollow&quot;&gt;Knowledge Distillation&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6550&quot; rel=&quot;nofollow&quot;&gt;FitNets&lt;/a&gt;, etc show ways of making smaller networks from large networks while preserving most of the performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another question is, what is the best performance achievable with a network no larger than a fixed size?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples of especially small networks that get good performance (100k parameters with 10% on CIFAR, anyone?) or systematic studies of the size vs performance tradeoff would be appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a project for comparison between clustering techniques using the data set of SSA for birth names from 1910-2013 years for the different states.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have finished applying my clustering techniques on my data set and the output of the clusters were the clusters of the states for each year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I can know from my results; which states are close to each other with the birth names and which were not. By looking &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to have interesting results to make my project report interesting, Which states are similar in birth names are not enough to make the read be excited about my project. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;any ideas of what can be learned from my project?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How can I show the comparison between the clustering techniques? anyway other than seeing how Homogeneous the clusters are?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;PROC means data=d mean; &#xA;var a;&#xA;class b; var a;&#xA;run; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to perform the &quot;PROC means&quot; for continuous &quot;var a&quot;:&#xA;1) in general and&#xA;2) by classes.&#xA;But it performed by the classes only.&#xA;How to make procedure for &quot;var a&quot; here in general too?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. SAS WARNING: Analysis variable &quot;a&quot; was defined in a previous statement, duplicate definition will be ignored.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a new data point and want to classify it into the existing classes.I can calculate pairwise distance for the new point to all existing points(in the existing classes). I know using KNN would be a straightforward to classify this point. Is there a way I could randomly sampling existing classes and then correlated the new point to a potential classes without calculating all pairwise distances? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Assume the training data is &lt;code&gt;fruit&lt;/code&gt;, which I am going to use for prediction in a CART model in R:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fruit = data.frame(color=c(&quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;yellow&quot;,&#xA;                           &quot;orange&quot;, &quot;green&quot;, &quot;pink&quot;, &quot;red&quot;, &quot;red&quot;),&#xA;                   isApple=c(TRUE, TRUE, FALSE, FALSE, FALSE,&#xA;                             FALSE, FALSE, FALSE, TRUE, FALSE))&#xA;mod = rpart(isApple ~ color, data=fruit, method=&quot;class&quot;, minbucket=1)&#xA;prp(mod)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could anyone explain what is exactly the role of &lt;code&gt;minbucket&lt;/code&gt; in plotting CART tree for this example if we are going to use &lt;code&gt;minbucket = 2, 3, 4, 5&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say &lt;code&gt;fruit&lt;/code&gt; is my data frame, I'm finding whether the outcome is apple or not? I have 5 red apples (4 TRUE, 1 FALSE), one FALSE value is tomato here, so what ever is red need not be an apple. But if I give &lt;code&gt;minbucket&lt;/code&gt;=5 or 4 here, there is no split at all. Only for &lt;code&gt;minbucket&lt;/code&gt; 1 to 3 there is a split beyond 3 there is no split. But I have more than 3 observation in my leaf node.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm conjecturing that with &lt;a href=&quot;http://en.wikipedia.org/wiki/Complete-linkage_clustering&quot; rel=&quot;nofollow&quot;&gt;Complete-linkage clustering&lt;/a&gt; two elements from the same cluster will always be closer to each other some other element from another cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In more formal terms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $C$ be a clustering.&#xA; $\\\\not\\\\exists z \\\\in C_j$ s.t. $\\\\bigtriangleup(x, z) &amp;lt; \\\\bigtriangleup(x, y)$ where $x,y \\\\in C_i$, $C_i \\\\neq C_j$ and $C_i, C_j \\\\in C$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't been able to prove the conjecture yet, thus I'm wondering whether I'm right or wrong. If this is indeed the case, I would much appreciate a sketch a proof. I'm pretty sure I can work my way from there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side note (not that I think it makes a difference), I'll be applying the clustering algorithm on a one-dimensinal dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your input is much appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a pandas DataFrame containing a time series column. The years are shifted in the past, so that I have to add a constant number of years to every element of that column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best way I found is to iterate through all the records and use&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;x.replace(year=x.year + years)  # x = current element, years = years to add&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is cythonized as below, but still very slow (proofing)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cdef list _addYearsToTimestamps(list elts, int years):&#xA;    cdef cpdatetime x&#xA;    cdef int i&#xA;    for (i, x) in enumerate(elts):&#xA;        try:&#xA;            elts[i] = x.replace(year=x.year + years)&#xA;        except Exception as e:&#xA;            logError(None, &quot;Cannot replace year of %s - leaving value as this: %s&quot; % (str(x), repr(e)))&#xA;    return elts&#xA;&#xA;def fixYear(data):&#xA;    data.loc[:, 'timestamp'] = _addYearsToTimestamps(list(data.loc[:, 'timestamp']), REAL_YEAR-(list(data[-1:]['timestamp'])[0].year))&#xA;    return data&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm pretty sure that there is a way to change the year without iterating, by using Pandas's Timestamp features. Unfortunately, I don't find how. Could someone elaborate?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I don't know, if I can ask this here, but I'm interested to know what kind of &lt;em&gt;abstract data type (ADT)&lt;/em&gt; does Twitter use to &lt;strong&gt;model relations&lt;/strong&gt; between profiles and why. I'm just starting to learn ADTs, so I would like to learn how they work in the real world as well as their applications.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to learn web scraping using Python by myself as part of an effort to learn data analysis. I am trying to scrape imdb webpage whose url is the following: &lt;a href=&quot;http://www.imdb.com/search/title?sort=num_votes,desc&amp;amp;start=1&amp;amp;title_type=feature&amp;amp;year=1950,2012&quot; rel=&quot;noreferrer&quot;&gt;http://www.imdb.com/search/title?sort=num_votes,desc&amp;amp;start=1&amp;amp;title_type=feature&amp;amp;year=1950,2012&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using BeautifulSoup module. Following is the code I am using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;r = requests.get(url) # where url is the above url    &#xA;bs = BeautifulSoup(r.text)&#xA;for movie in bs.findAll('td','title'):&#xA;    title = movie.find('a').contents[0]&#xA;    genres = movie.find('span','genre').findAll('a')&#xA;    genres = [g.contents[0] for g in genres]&#xA;    runtime = movie.find('span','runtime').contents[0]&#xA;    year = movie.find('span','year_type').contents[0]&#xA;    print title, genres,runtime, rating, year&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the following outputs:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;The Shawshank Redemption [u'Crime', u'Drama'] 142 mins. (1994)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using this code, I could scrape title, genre, runtime,and year but I couldn't scrape the imdb movie id,nor the rating. After inspecting the elements (in chrome browser), I am not being able to find a pattern which will let me use similar code as above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody help me write the piece of code that will let me scrape the movie id and ratings ? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using the &lt;a href=&quot;http://cran.r-project.org/web/packages/biglm/biglm.pdf&quot; rel=&quot;nofollow&quot;&gt;biglm&lt;/a&gt; R package for linear regression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;click/impression is the output required. But the test data does not contain click and impression. &#xA;The &lt;code&gt;predict&lt;/code&gt; function of &lt;code&gt;biglm&lt;/code&gt; gives the error&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error in eval(expr, envir, enclos) : object 'click' not found&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I assume this is because &lt;code&gt;predict&lt;/code&gt; tries to compute the standard errors also.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a method to just obtain the predictions ? I tried assigning values to &lt;code&gt;se.fit&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt; attributes, but I get the same error. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I noticed a difference between R and Excel when it comes to trend lines.  Basically I am automating a simple process that someone does.  He/she takes web traffic and finds the best trend line by looking at the R squared to predict what the web traffic will be a few months from now to make sure we will have the servers to handle it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In R, one can do: lm(log(y)~x,data) but you must &quot;remember&quot; to do &lt;b&gt;exp&lt;/b&gt;(predict(...)).  Is there a better predict function that remembers the left hand operations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also the R-squared reported for log(y)~x is the r squared for x vs that y's log.  The R-squared against the original y was better.  Is there a nifty function that calculates that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see from this output, what summary reports as the rsquared is not &quot;correct&quot; while my rsq data accurately shows the best rmse. (See source code below.)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;[1] &quot;rmse is 305221.416535481 rsq is 0.749525713697497 summary$r.squared 0.705953722113025&quot;&#xA;[1] &quot;rmse is 304961.752311906 rsq is 0.749815047530537 summary$r.squared 0.706163612870978&quot;&#xA;[1] &quot;rmse is 318083.254498832 rsq is 0.723564406971294 summary$r.squared 0.723564406971402&quot;&#xA;[1] &quot;rmse is 317352.614485029 rsq is 0.724832898371528 summary$r.squared 0.724832898371531&quot;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to learn the best way to do this in R...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;&#xA;rmse=function(x,y){&#xA;  return( sqrt(sum((x-y)^2)/length(x)));&#xA;}&#xA;rsq=function(actual,pred){    &#xA;  return( cor(actual,pred,method=c(&quot;pearson&quot;))^2);&#xA;}&#xA;&#xA;&#xA;findBestFormula=function(data,yvar,xvar){&#xA;  #This figures out the best Excel-like trend function based on lowest rmse&#xA;  data$y=data[,yvar];&#xA;  data$x=data[,xvar];  &#xA;  models=list(lm(log(y)~log(x),data),&#xA;              lm(log(y)~x,data),&#xA;             lm(y~log(x),data),&#xA;              lm(y~x,data));&#xA;  preds=list(exp(predict(models[[1]],data)),&#xA;    exp(predict(models[[2]],data)),&#xA;    (predict(models[[3]],data)),&#xA;    (predict(models[[4]],data)));&#xA;  rs=c();&#xA;  for(i in (1:4)){&#xA;    #say(names(models[[i]]))&#xA;    rs[i]=rmse(data$y,preds[[i]]);&#xA;    print(paste(&quot;rmse is&quot;,rs[i],&quot;rsq is&quot;,rsq(data$y,preds[[i]]),&#xA;                &quot;summary$r.squared&quot;,summary(models[[i]])$r.squared));&#xA;  }&#xA;  best=min(rs)&#xA;  #say(best);&#xA;  for(i in (1:4)){&#xA;    if(best==rs[i]){&#xA;      return(list(model=models[[i]],pred=preds[[i]],modelNum=i,rmse=best));  &#xA;    }&#xA;  }&#xA;&#xA;}&#xA;&#xA;&#xA;summary(findBestFormula(weekDayDf,'daily','d')$model)&#xA;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any tips on niftier code to write the above?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a question about two different methods from different libraries which seems doing same job. I am trying to make linear regression model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the code which I using statsmodel library with OLS :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.3, random_state=1)&#xA;&#xA;x_train = sm.add_constant(X_train)&#xA;model = sm.OLS(y_train, x_train)&#xA;results = model.fit()&#xA;&#xA;print &quot;GFT + Wiki / GT  R-squared&quot;, results.rsquared&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This print out &lt;strong&gt;GFT + Wiki / GT  R-squared 0.981434611923&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the second one is scikit learn library Linear model method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model = LinearRegression()&#xA;model.fit(X_train, y_train)&#xA;&#xA;predictions = model.predict(X_test)&#xA;&#xA;print 'GFT + Wiki / GT R-squared: %.4f' % model.score(X_test, y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This print out &lt;strong&gt;GFT + Wiki / GT R-squared: 0.8543&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is the both method prints our R^2 result but one is print out 0.98 and the other one is 0.85. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my understanding, OLS works with training dataset. So my questions, &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is there a way that work with test data set with OLS ?&lt;/li&gt;&#xA;&lt;li&gt;Is the traning data set score gives us any meaning(In OLS we didn't use test data set)? From my past knowledge we have to work with test data.&lt;/li&gt;&#xA;&lt;li&gt;What is the difference between OLS and scikit linear regression. Which one we use for calculating the score of the model ? &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am working on image classification tasks and decided to use Lasagne + Nolearn for neural networks prototype.&#xA;All standard examples like MNIST numbers classification run well, but problems appear when I try to work with my own images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use 3-channel images, not grayscale.&#xA;And there is the code where I'm trying to get arrays from images:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; img = Image.open(item)&#xA; img = ImageOps.fit(img, (256, 256), Image.ANTIALIAS)&#xA; img = np.asarray(img, dtype = 'float64') / 255.&#xA; img = img.transpose(2,0,1).reshape(3, 256, 256)   &#xA; X.append(img)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the code of NN and its fitting:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X, y = simple_load(&quot;new&quot;)&#xA;&#xA;X = np.array(X)&#xA;y = np.array(y)&#xA;&#xA;&#xA;net1 = NeuralNet(&#xA;    layers=[  # three layers: one hidden layer&#xA;        ('input', layers.InputLayer),&#xA;        ('hidden', layers.DenseLayer),&#xA;        ('output', layers.DenseLayer),&#xA;        ],&#xA;    # layer parameters:&#xA;    input_shape=(None, 65536),  # 96x96 input pixels per batch&#xA;    hidden_num_units=100,  # number of units in hidden layer&#xA;    output_nonlinearity=None,  # output layer uses identity function&#xA;    output_num_units=len(y),  # 30 target values&#xA;&#xA;    # optimization method:&#xA;    update=nesterov_momentum,&#xA;    update_learning_rate=0.01,&#xA;    update_momentum=0.9,&#xA;&#xA;    regression=True,  # flag to indicate we're dealing with regression problem&#xA;&#xA;&#xA;       max_epochs=400,  # we want to train this many epochs&#xA;        verbose=1,&#xA;        )&#xA;&#xA;  net1.fit(X, y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I recieve exceptions like this one:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;las_mnist.py&quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    net1.fit(X[i], y[i])&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 266, in fit&#xA;    self.train_loop(X, y)&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 273, in train_loop&#xA;    X, y, self.eval_size)&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 377, in train_test_split&#xA;    kf = KFold(y.shape[0], round(1. / eval_size))&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So, in which format do you &quot;feed&quot; your networks with image data?&lt;/strong&gt;&#xA;Thanks for answers or any tips!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm having trouble figuring out a way to analyze some simple data. When graphed, the data I have make a somewhat sinusoidal curve. What I want to do is to find the x-values of the maximum peaks of the sinusoidal curve. I then want to subtract each of these x-values from the last peak found and average these differences to obtain an average distance between peaks. Is there an easy way to do this with Excel, Mathematica, or MatLab (the programs I have available to me?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!!!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I would like to know more on fraud/anomaly detection. I am looking for good source or survey article/book etc out there which will give me some preliminary idea of the area. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestion is greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have an application that tracks people making mentions of various topics. We've used a Bayes algorithm to do some simple classification (users give a thumbs up/thumbs down) to pick the people that they believe are the best fit for their project. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our intention was to use this data to help us sort and order the influencers based on &quot;fit&quot; to the customer's needs.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, we've got some trained data, and now all we can do is say are they similar to the &quot;thumbs up&quot; group, or the &quot;thumbs down&quot; group.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What algorithm should we have used for this instead? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, the ideal is to have a score.. and the biggest, smallest based on the trained data is the one that gets shown first. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thoughts? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even better if its in Ruby. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am looking towards a solution where classification algorithms produce output with some confidence value. but I am confused whether classification algorithms are able to produce results with percentage of confidence? &#xA;Thanks&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm working on a problem where frequency analysis applies (decomposition of a signal into frequencies, that is), but it's noisy and the samples are unevenly spaced.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically: given a list of items purchased at a bar/restaurant, try to estimate the number of guests on the check based on distinct &quot;frequencies&quot; of purchase. The logic is that if there are N guests on a check, then it's reasonable to see N frequencies of drinks being purchased, one person buying every 10 minutes, another every 15, etc. (Plenty of other properties of the check should be included, but here I'm focusing specifically on estimating distinct frequencies).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So more formally: given a noisy, unevenly spaced time series, find the smallest number of frequencies which reproduce the signal while minimizing the error (... for some sensible definition of how to minimize both the error and the number of frequencies simultaneously).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is more a machine learning problem than signal processing. I realize it's also an open question, but can anyone point me in the right direction? Is there a particular method or algorithm that applies here?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am analyzing a dataset in Python for strictly learning purpose.&#xA;In the code below that I wrote, I am getting some errors which I cannot get rid off. Here is the code first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')&#xA;plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)&#xA;plt.xlabel('Year')&#xA;plt.ylabel('Rating')&#xA;remove_border()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the following errors:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. TypeError: 'str' object is not callable&#xA;2. NameError: name 'remove_border' is not defined&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, the label='Decade Average' is not showing up in the plot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What confuses me most is the fact that in a separate  code snippet for plots (see below), I didn't get the 1st error above, although &lt;code&gt;remove_border&lt;/code&gt; was still a problem.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;plt.hist(movieDF.rating, bins = 5, color = 'blue', alpha = 0.3)&#xA;plt.xlabel('Rating')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any explanations of all or some of the errors would be greatly appreciated. Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following the comments, I am posting the data and the traceback below:&#xA;decade_mean is given below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;year&#xA;1970    8.925000&#xA;1980    8.650000&#xA;1990    8.615789&#xA;2000    8.378947&#xA;2010    8.233333&#xA;Name: rating, dtype: float64&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;traceback:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;TypeError                                 Traceback (most recent call last)&#xA;&amp;lt;ipython-input-361-a6efc7e46c45&amp;gt; in &amp;lt;module&amp;gt;()&#xA;      1 plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')&#xA;      2 plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)&#xA;----&amp;gt; 3 plt.xlabel('Year')&#xA;      4 plt.ylabel('Rating')&#xA;      5 remove_border()&#xA;&#xA;TypeError: 'str' object is not callable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have solved remove_border problem. It was a stupid mistake I made. But I couldn't figure out the problem with the 'str'.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is it possible to learn the weights for a logistic regression classifier using EM (Expectation Maximization)algorithm? Is there any instance reference?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Undergraduate researcher here. I worked at many traditional scientific research labs, ranging from cancer biology, to radiation medicine, and to supercapacitors. I'm thinking of switching to Statistics and Computer Science from Computational Biology to join the exciting field of Data Science. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What ways can I contribute to Humanity without a technical background in basic science? I don't know how I will feel about making better clickbaits. I've thought about continuing in scientific research but I will lack the science background. I've thought about working for the United Nations. What other ways can I contribute? Inspire me!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I understand how recurrent neural networks work, however I'm trying to build a deep intuitive understanding of their behavior which is difficult for me because they exhibit such complex behaviors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most difficult thing for me to understand is how a Reccurrent Neural Network can exhibit an oscillatory behavior along with the notion of exploding weights. For one, I'm guessing that oscillatory behaviors are only possible for certain activation functions and configurations. Here are my following questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Can sigmoid activation functions exhibit oscillatory behavior? I've convinced myself that they don't since they have a positive range between 0 and 1 which doesn't allow for negative derivatives, but maybe I'm wrong. Is there a formal proof out there for whether it can or cannot?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Which activation functions can exhibit oscillatory behaviors and are there proofs out there for them? I believe tanh might have this behavior but I'm not sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) What are exploding weight derivatives and how do they occur? There is polar opposite of exploding weight's which seems to be where the weights do not learn and stay stagnate. What causes these issues? I imagine this is dependent upon the activation function as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I began reading this article:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CCsQFjAB&amp;amp;url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FChristian_OReilly%2Fpublication%2F52004955_Permanent_oscillations_in_a_3-node_recurrent_neural_network_model%2Flinks%2F0c96052464be171d68000000.pdf&amp;amp;ei=X_E1Vf7ULNGsogSo9YHIBQ&amp;amp;usg=AFQjCNE0fex0s7hY2w_upMNkmaxIseUKww&amp;amp;bvm=bv.91386359,d.cGU&quot; rel=&quot;nofollow&quot;&gt;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CCsQFjAB&amp;amp;url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FChristian_OReilly%2Fpublication%2F52004955_Permanent_oscillations_in_a_3-node_recurrent_neural_network_model%2Flinks%2F0c96052464be171d68000000.pdf&amp;amp;ei=X_E1Vf7ULNGsogSo9YHIBQ&amp;amp;usg=AFQjCNE0fex0s7hY2w_upMNkmaxIseUKww&amp;amp;bvm=bv.91386359,d.cGU&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but got confused around page 10 because the author omit's explaining opaque assumptions such as why we are defining the matrix measure as is and so forth(it's a good paper nonetheless). Is there a more transparent paper or lecture out there that can shed light on this subject, or maybe simple explanation for what the author is trying to get at?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am currently working with the forest cover type prediction from Kaggle, using classification models with scikit-learn. My main purpose is learning about the different models, so I don't pretend to discuss about which one is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When working with logistic regression, I wonder if I need the 'penalty' parameter (where I can choose L1 or L2 regularization). Based on what I found, these regularization terms are useful to avoid over-fitting, specially when the parameter values are extreme (by extreme I understand the range of some parameter values are very large compared to other parameters, Correct me if I am wrong. In this case, wouldn't it be enough to apply log-scale or normalization to these values?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main questions are: as the number of parameters is large, are there visualization techniques and tools in scikit-learn which can help me to find parameters with extreme values? is there any statistical function/tool which returns how extreme the values of parameters are?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Has there been a successful implementation of Nash-Equilibrium in big data problems like suggesting a best buy in a stock market, in traffic monitoring systems or crowd control systems.&#xA;All the above mentioned scenarios have competitive environments and one needs to get the best possible solution in them, which should suit well for Nash-Equilibrium cases.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have scraped the rating that a customer gave in the following categories namely overall rating score, value for money, seat comfort,staff service, catering and entertainment from an airline forum. I would like to know what quality information I can deduce from such ratings about an airline. I have rating of nearly 300 customers who traveled in an airline.  &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I had this basic query on ML and would like to get basic ideas on modelling prediction models using ML and Python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say I have a training data of 1000 items as&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Item_name, Attrib_1, Attrib_2, Attrib_3,....Attrib_N, Cost&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And my aim is to create a model to predict cost for a new item given the attributes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So where should I start and what are the different ways to prediction and solve this problem ? Also how to evaluate different methods ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;First of all, I hope I'm in the right StackExchange here. If not, apologies!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently working with huge amounts of feature-value vectors. There are millions of these vectors (up to 20 million possibly). They contain some linguistic/syntactic features and their values are all strings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because most classifiers do not handle string data as values, I convert them to binary frequency values, so an attribute looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@attribute 'feature#value' numeric&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And per row, the value is either 1 or it is absent (so note it's a &lt;em&gt;sparse&lt;/em&gt; ARFF file). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The thing is, with 250K rows, there are over 500K attributes and so, most algorithms have a hard time with this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a lot of algorithms. I'm really curious as to what you would consider a suitable one (preferably unsupervised, but anything works), and if you even have some ideas how I could improve performance. I can train on small subsets of data, but the results only get better when using large amounts of data (at least 7 million events). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For now, I've been using NaiveBayes variations (Multinomial and also DMNBText) and those are really the only ones that are able to chew up data with acceptable speed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot. If you need more information, please let me know.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cheers.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Does anyone know if it's possible to import a large dataset into Amazon S3 from a URL?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to avoid downloading a huge file and then reuploading it to S3 through the web portal. I just want to supply the download URL to S3 and wait for them to download it to their filesystem. It seems like an easy thing to do, but I just can't find the documentation on it.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a dataset with differents attributes which don't have the same range on their values which is a problem when we need to compute distance beetween objects. After some research i found that i can do the regularisation job with this formula : (value-min)/(max-min)  where min and max are respectively the minimum and maximum value in the domain of val attribute.&#xA;The question is that one, does it exist other ways ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your help.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Using scikit-learn, why would you use bfgs optimization which is non-linear for a linear classifier as logistic regression? I am confused. Does the optimization method finds the optimum of the chosen score function? if so, which one? I can't choose it when defining the estimator. does the linearity or non-linearity of the score function depend on the model (whether it is linear or non-linear)?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I used clustering on my dataset. Now when I'm trying to use a LASSO with cv to predict a response, one of the variables it takes into consideration is which cluster a new point is classified into.(I included the cluster variable as a predictor to see if being in a particular group affects the response) &#xA;Since the information on all variables is already captured by the cluster variable,using it again in the Lasso model with some other variables,does it become redundant/biased?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a text classification problem in which i need to classify an answer to a message as either relevant or not. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the first phase of my calculations, I have already used a SVM to determine if the original message was relevant or not, deciding whether a message contains a hint or question if somebody's twitter account has been hacked.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;example:&#xA;&quot;Hey @foobar, have you been hacked?&quot;   &amp;lt;-- relevant&#xA;&quot;My bank account has just been hacked&quot; &amp;lt;-- not relevant&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, when I want to classify whether the answer is relevant, I would want to have both the original message and the answer as input, right? An answer is relevant in my case if it, in any way, responds to the original message. Is this approach possible using a SVM or any other machine learning tool? I'm using python with the scikit-learn library.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;example:&#xA;&quot;Hey @foobar, have you been hacked?&quot;&#xA;&quot;@barfoo it seems so, thx for suggesting&quot; &amp;lt;-- relevant&#xA;&#xA;&quot;Hey @foobar, have you been hacked?&quot;&#xA;&quot;Lose 20 pounds quickly! http://blabla.com&quot; &amp;lt;-- not relevant&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not very experienced in this field, so any input would be very appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am currently trying to develop a classifier in python using Naive Bayes technique. I need a dataset so that I can train it. My classifier would classify a new document given to it into one of the four categories : Science and technology, Sports, politics, Entertainment. Can anybody please help me find a dataset for this. I've been stuck on this problem for quite some time now. Any help would be greatly appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm currently studying Chapter 7 (&quot;Modeling with Decision Trees&quot;) of the book &quot;Programming Collective intelligence&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find the output of the function mdclassify() p.157 confusing. The function deals with missing data. The explanation provided is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In the basic decision tree, everything has an implied weight of 1,&#xA;  meaning that the observations count fully for the probability that an&#xA;  item fits into a certain category. If you are following multiple&#xA;  branches instead, you can give each branch a weight equal to the&#xA;  fraction of all the other rows that are on that side.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;From what I understand, an instance is then split between branches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, I simply don't understand how we can obtain:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{'None': 0.125, 'Premium': 2.25, 'Basic': 0.125}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;as &lt;code&gt;0.125+0.125+2.25&lt;/code&gt; does not sum to 1 nor even an integer. How was the new observation split?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/arthur-e/Programming-Collective-Intelligence/blob/master/chapter7/treepredict.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/arthur-e/Programming-Collective-Intelligence/blob/master/chapter7/treepredict.py&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the original dataset, I obtain the tree shown here: &#xA;&lt;a href=&quot;http://mattscodecave.com/media/tree3.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://mattscodecave.com/media/tree3.jpg&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone please explain me precisely what the numbers precisely mean and how they were exactly obtained?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS : The 1st example of the book is wrong as described on their errata page but just explaining the second example (mentioned above) would be nice.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Assume that &quot;1,2,3&quot; are the ids of users, active means that person visited the stackoverflow in last one month (0=passive, 1=active), and there are positive and negative votes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id  question       votes                 active&#xA; 1     1        -1, +1, -1, -1, -1         0&#xA; 1     2        -1, +1, -1, -1, +1         0&#xA; 2     1        +1, +1, -1, -1             0&#xA; 3     1        +1, +1, +1, -1, +1         1&#xA; 3     2        +1, +1, -1, +1, +1, +1     1&#xA; 3     3        -1, +1                     1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to know what makes the users stop using stackoverflow. Think that, I have already calculate the how many times did they get negative votes, total vote, average vote for each question...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder what kind of information could I get from these sequences. I want to find something like this: these users who are passive have two negative votes sequentially. For example, one positive vote after two negative votes in the second question of user 1, doesn't prevent the user churn. User 3 doesn't have any 2 negative votes sequentially in any of his questions. Hence he is still active.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for something like PrefixSpan Algorithm but order is important for me. I mean, I can't write the sequences like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;(-1 +1 -1 -1 -1) (-1 +1 -1 -1 +1 )&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;(-1) (+1) (-1) (-1) (-1) (-1) (+1) (-1) (-1) (+1 )&amp;gt;. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Because the first one loses the order, and the second one jumbled the questions together. Is there any algorithm to find these sequences which is common for churners?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are Hybrid classifiers used for sentiment analysis? How are they built? Please suggest good tutorial/book/link for reference. Also how are they different from other classifiers like SVM and Naive Bayes? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm doing some cluster analysis on the &lt;code&gt;MLTobs&lt;/code&gt; from the &lt;code&gt;LifeTables&lt;/code&gt; package and have come across a tricky problem plotting frequency of the &lt;code&gt;Year&lt;/code&gt; variable in the &lt;code&gt;mlt.mx.info&lt;/code&gt; dataframe. &lt;code&gt;Year&lt;/code&gt; contains the period that the life table was measured over, in intervals. Here's a table of the data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    1751-1754 1755-1759 1760-1764 1765-1769 1770-1774 1775-1779 1780-1784 1785-1789 1790-1794 &#xA;        1         1         1         1         1         1         1         1         1 &#xA;1795-1799 1800-1804 1805-1809 1810-1814 1815-1819 1816-1819 1820-1824 1825-1829 1830-1834 &#xA;        1         1         1         1         1         2         3         3         3 &#xA;1835-1839 1838-1839 1840-1844 1841-1844 1845-1849 1846-1849 1850-1854 1855-1859 1860-1864 &#xA;        4         1         5         3         8         1        10        11        11 &#xA;1865-1869 1870-1874 1872-1874 1875-1879 1876-1879 1878-1879 1880-1884 1885-1889 1890-1894 &#xA;       11        11         1        12         2         1        15        15        15 &#xA;1895-1899 1900-1904 1905-1909 1908-1909 1910-1914 1915-1919 1920-1924 1921-1924 1922-1924 &#xA;       15        15        15         1        16        16        16         2         1 &#xA;1925-1929 1930-1934 1933-1934 1935-1939 1937-1939 1940-1944 1945-1949 1947-1949 1948-1949 &#xA;       19        19         1        20         1        22        22         3         1 &#xA;1950-1954 1955-1959 1956-1959 1958-1959 1960-1964 1965-1969 1970-1974 1975-1979 1980-1984 &#xA;       30        30         2         1        40        40        41        41        41 &#xA;1983-1984 1985-1989 1990-1994 1991-1994 1992-1994 1995-1999 2000-2003 2000-2004 2005-2006 &#xA;        1        42        42         1         1        44         3        41        22 &#xA;2005-2007 &#xA;       14 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, some of the intervals sit within other intervals. Thankfully none of them overlap. I want to simplify the intervals so intervals such as &lt;code&gt;1992-1994&lt;/code&gt; and &lt;code&gt;1991-1994&lt;/code&gt; all go into &lt;code&gt;1990-1994&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An idea might be to get the modulo of each interval and sort them into their new intervals that way but I'm unsure how to do this with the interval data type. If anyone has any ideas I'd really appreciate the help. Ultimately I want to create a histogram or barplot to illustrate the nicely.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;For word2vec with negative sampling, the cost function for a single word is the following according to &lt;a href=&quot;http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf&quot; rel=&quot;nofollow&quot;&gt;word2vec&lt;/a&gt;:&#xA;$$&#xA;E = - log(\\\\sigma(v_{w_{O}}^{'}.u_{w_{I}})) - \\\\sum_{k=1}^K log(\\\\sigma(-v_{w_{k}}^{'}.u_{w_{I}}))&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$v_{w_{O}}^{'}$ = hidden-&gt;output word vector of the output word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$u_{w_{I}}$ = input-&gt;hidden word vector of the output word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$v_{w_{k}}^{'}$ = hidden-&gt;output word vector of the negative sampled word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\sigma$ is the sigmoid function&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And taking the derivative with respect to $v_{w_{O}}^{'}.u_{w_{j}}$ is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$&#xA;\\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} = \\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) * (\\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) - 1)&#xA;$ $ if w_j = w_O $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$&#xA;\\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} = \\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) * \\\\sigma(-v_{w_{j}}^{'}.u_{w_{I}})&#xA;$ $ if w_j = w_k \\\\ for \\\\ k = 1...K$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we can use chain rule to get &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$ \\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}} = \\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} * \\\\frac{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}}{\\\\partial v_{w_{j}}^{'}} $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is my reasoning and derivative correct? I am still new to ML so any help would be great! &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want to measure the correlation between the survival time which is a time to event data and the patient's activity count which is measured on continuous scale. What type of correlation coefficient is available to measure the strength of these two variables?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Suppose classifier trained with 5 class, and input query content does not belong to any of the trained class data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Naive bayes provides and random class as a result here. Which classifier deals best in such scenario?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have setup R-Studio Server on an Ubuntu EC2 instance for the first time and successfully started r-studio server in my browser. I also have putty ssh client. How do I set path in r-studio server to my mounted EBS volume and why do I not see the contents of my EBS volume in the r-studio files area (bottom right side? ) . Also, I had a file in an s3 bucket. I passed this command to bring it from s3 to my ebs volume: &lt;code&gt;s3cmd get s3://data-analysis/input-data/filename.csv&lt;/code&gt; . I assume this command downloads the file from s3 into the ebs volume. But I can't find it in RStudio Server! I have scoured the internet looking for help on this but not able to solve my problem.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;It is said that before an earthquake happens, a viewer experiences disturbances in DTH TV transmission in the form of distorted images on the screen which automatically correct after a few seconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to identify patterns of such disturbances by continuously monitoring TV images so that earthquakes can potentially be predicted at least few minutes in advance and many lives could be saved?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have one small list of entities, such as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Russia&#xA;Vladimir&#xA;Moscow&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have a massive database of JSON indices. For each entry there are multiple alpha-numeric identifiers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for instance, for &lt;code&gt;Russia&lt;/code&gt; there might only be one. But for &lt;code&gt;Vladimir&lt;/code&gt; maybe there will be 100. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;They're stored in JSON but I read them into my java program like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        // GET JSON DATA&#xA;        File f = new File(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json&quot;);&#xA;        String jsonTxt = null;&#xA;&#xA;        if (f.exists())&#xA;        {&#xA;            InputStream is = new FileInputStream(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json&quot;);&#xA;            jsonTxt = IOUtils.toString(is);&#xA;        }&#xA;        //reformat&#xA;        jsonTxt = ( jsonTxt.substring(1, jsonTxt.length()-1) ).replace(&quot;\\\\\\\\&quot;,&quot;&quot;);&#xA;&#xA;        Gson json = new Gson();&#xA;        Map&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt; mast_Q_storage_map = new HashMap&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt;();&#xA;        mast_Q_storage_map = (Map&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt;) json.fromJson(jsonTxt, mast_Q_storage_map.getClass());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to get all of the values associated with the entities from the small list. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I need to search the big list and retrieve their values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I want to try to determine if there is a relationship between any of the entities in the sentence, as in I want to check if &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Russia   X Vladimir&#xA;Russia   X Moscow&#xA;Moscow   X Vladimir&#xA;Moscow   X Russia&#xA;Vladimir X Russia&#xA;Vladimir X Moscow&lt;/p&gt;&#xA;&#xA;&lt;p&gt;will result in a relationship, I've been trying to do it like this, but I'm running into big problems:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        // Read in all the sentences, that are in files, in this folder&#xA;        final File folder = new File(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/&quot;);&#xA;&#xA;        for (final File fileEntry : folder.listFiles()) &#xA;        {&#xA;            BufferedReader br = new BufferedReader(new FileReader(fileEntry));&#xA;            try &#xA;            {&#xA;                //Store the filename&#xA;                //System.out.println(fileEntry.getName());&#xA;                StringBuilder sb = new StringBuilder();&#xA;                String line = br.readLine();&#xA;&#xA;                while (line != null) &#xA;                {   &#xA;                    sb.append(line);&#xA;                    sb.append(System.lineSeparator());&#xA;                    line = br.readLine();&#xA;                }&#xA;                String everything = sb.toString();&#xA;                //System.out.println(everything);&#xA;&#xA;                Document doc = Jsoup.parse(everything);&#xA;&#xA;                Elements contents = doc.getElementsByTag(&quot;sentence&quot;);&#xA;                for (Element content : contents) &#xA;                {&#xA;                    //store the sentence number&#xA;                    String number = content.select(&quot;sentence&quot;).text();&#xA;                    number = number.substring(0, number.indexOf(&quot; &quot;)); &#xA;                    System.out.println(number);&#xA;&#xA;                    //get all the entities in this sentence&#xA;                    Elements pers = content.select(&quot;PERSON&quot;);&#xA;                    Elements locs = content.select(&quot;LOCATION&quot;);&#xA;                    Elements orgs = content.select(&quot;ORGANIZATION&quot;);&#xA;&#xA;                    //collect all the elements to a list, all the elements of one sentence&#xA;                    List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;String&amp;gt;();&#xA;&#xA;                    for (Element per : pers) &#xA;                    {&#xA;                        list.add(per.text().trim());&#xA;                    }&#xA;                    for (Element loc : locs) &#xA;                    {&#xA;                        list.add(loc.text().trim());&#xA;                    }&#xA;                    for (Element org : orgs) &#xA;                    {&#xA;                        list.add(org.text().trim());&#xA;                    }&#xA;&#xA;&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println(&quot;This is list of sentence elements:&quot;);&#xA;                    for (String s : list)&#xA;                        System.out.println(s);&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;&#xA;&#xA;                    List&amp;lt;String&amp;gt; Q_value_list = new ArrayList&amp;lt;String&amp;gt;();&#xA;&#xA;                    // for the list of Q values to keys&#xA;                    for (Entry&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt; e : mast_Q_storage_map.entrySet()) &#xA;                    {&#xA;                        for (String s : list)&#xA;                        {&#xA;                            if (e.getKey().contains(s)) &#xA;                            {&#xA;                                //List&amp;lt;String&amp;gt; Q_value_list = new ArrayList&amp;lt;String&amp;gt;(e.getValue());&#xA;&#xA;                                System.out.println(e.getKey() + &quot; :: &quot; + Q_value_list.toString());&#xA;                            }&#xA;                        }&#xA;                    }&#xA;&#xA;&#xA;&#xA;                                //czeher&#xA;                                for (String home:Q_value_list) &#xA;                                {&#xA;                                  for (String away:Q_value_list) &#xA;                                  {&#xA;&#xA;&#xA;                                    String URL_czech = &quot;http://milenio.dcc.uchile.cl/sparql?default-graph-uri=&amp;amp;query=PREFIX+%3A+%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0D%0ASELECT+*+WHERE+%7B%0D%0A+++%3A&quot; &#xA;                                                       + home + &quot;+%3FsimpleProperty+%3A&quot; &#xA;                                                       + away + &quot;%0D%0A%7D%0D%0A&amp;amp;format=text%2Fhtml&amp;amp;timeout=0&amp;amp;debug=on&quot;;&#xA;&#xA;&#xA;                                    URL wikidata_page = new URL(URL_czech);&#xA;                                    HttpURLConnection wiki_connection = (HttpURLConnection)wikidata_page.openConnection();&#xA;                                    InputStream wikiInputStream = null;&#xA;&#xA;&#xA;                                        try &#xA;                                        {&#xA;                                            // try to connect and use the input stream&#xA;                                            wiki_connection.connect();&#xA;                                            wikiInputStream = wiki_connection.getInputStream();&#xA;                                        } &#xA;                                        catch(IOException error) &#xA;                                        {&#xA;                                            // failed, try using the error stream&#xA;                                            wikiInputStream = wiki_connection.getErrorStream();&#xA;                                        }&#xA;                                    // parse the input stream using Jsoup&#xA;                                    Document docx = Jsoup.parse(wikiInputStream, null, wikidata_page.getProtocol()+&quot;://&quot;+wikidata_page.getHost()+&quot;/&quot;);&#xA;&#xA;&#xA;&#xA;                                    Elements link_text = docx.select(&quot;table.sparql &amp;gt; tbody &amp;gt; tr:nth-child(2) &amp;gt; td &amp;gt; a&quot;);&#xA;                                    //link_text.text();&#xA;                                    for (Element l : link_text) &#xA;                                    {&#xA;                                        String output = l.text();&#xA;                                        System.out.println( output );&#xA;                                    }&#xA;&#xA;&#xA;                                  }&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;                    }&#xA;&#xA;                }&#xA;            }&#xA;            finally &#xA;            {&#xA;                br.close();&#xA;            }&#xA;&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I would like to find different patterns recognition algorithm to detect different type of fraud. I have 1 million unstructured text documents about the clients' information with metadata about the client name, viewers, location in the cloud. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the patterns I was thinking of and the related fraud that i would like to detect:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Numeric Patterns - fictitious invoice numbers, fictitiously-generated transaction amounts. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Time Patterns - transactions occurring too regularly, activity at unusual times or dates. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Name Patterns - similar and alerted name and addresses. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Geographic Patterns - Proximity relationships between apparently unrelated entities. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What technique can I use ? any keywords? thx.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Does anyone know where I might be able to find a list of the most common typing errors and their corrections? This is separate from more complicated considerations concerning general spelling checking (which can have very many candidates in relation to the correct spelling of the word in question); rather I am looking for a similar list as used by Microsoft Word (for instance correcting &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;teh&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;the&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;becaise&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;because&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Not only can the manner in which these sort of errors are fixed be hard-coded, their frequent occurrence in text provides significant dividends in textual mining (provided that such a list of errors and corrections can be obtained, of course).&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;What is the difference between data mining approaches: frequent itemsets and item-based collaborative filtering in the area of recommender systems?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to analyze the movie database using python, downloaded from imdb. While trying to generate some plots, I am running into errors which confuses me.&#xA;I am trying to generate a matrix of small figures which can show me any hidden pattern etc. Here is the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fig, axes = plt.subplots(nrows=4, ncols=6, figsize=(12, 8), &#xA;                         tight_layout=True)&#xA;&#xA;bins = np.arange(1950,2012,3)&#xA;for ax, genre in zip(axes.ravel(), movieGenre):&#xA;    ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')&#xA;    ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')&#xA;    ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)&#xA;    ax.xaxis.set_ticks(np.arange(1950, 2013, 30))&#xA;    ax.set_yticks([])&#xA;    ax.set_xlabel('Year')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The first &lt;code&gt;hist&lt;/code&gt; isn't working, but the second one is working when I am commenting out the first one. Here is the traceback:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; KeyError                                  Traceback (most recent call last)&#xA;&amp;lt;ipython-input-158-c2e7c2737372&amp;gt; in &amp;lt;module&amp;gt;()&#xA;      4 bins = np.arange(1950,2012,3)&#xA;      5 for ax, genre in zip(axes.ravel(), movieGenre):&#xA;----&amp;gt; 6     ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')&#xA;      7     ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')&#xA;      8     ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/axes.pyc in hist(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)&#xA;   8247         # Massage 'x' for processing.&#xA;   8248         # NOTE: Be sure any changes here is also done below to 'weights'&#xA;-&amp;gt; 8249         if isinstance(x, np.ndarray) or not iterable(x[0]):&#xA;   8250             # TODO: support masked arrays;&#xA;   8251             x = np.asarray(x)&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)&#xA;    477     def __getitem__(self, key):&#xA;    478         try:&#xA;--&amp;gt; 479             result = self.index.get_value(self, key)&#xA;    480 &#xA;    481             if not np.isscalar(result):&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)&#xA;   1169 &#xA;   1170         try:&#xA;-&amp;gt; 1171             return self._engine.get_value(s, k)&#xA;   1172         except KeyError as e1:&#xA;   1173             if len(self) &amp;gt; 0 and self.inferred_type == 'integer':&#xA;&#xA;&#xA;&#xA;KeyError: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the first few columns of the data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     imdbID     title                      rating    vote      runtime  year    genre&#xA;0   tt0111161   The Shawshank Redemption    9.3      1,439,277  142    1994 [Crime, Drama]&#xA;1   tt0468569   The Dark Knight             9.0      1,410,124  152    2008 [Action, Crime, Drama]&#xA;2   tt1375666   Inception                   8.8      1,209,159  148    2010 [Action, Mystery, Sci-Fi, Thriller]&#xA;3   tt0137523   Fight Club                  8.9      1,123,462  139    1999 [Drama]&#xA;4   tt0110912   Pulp Fiction                8.9      1,117,193  154    1994 [Crime, Drama]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;movieGenre is basically collecting all the different genres from 'genre' column with duplicates removed: &lt;code&gt;movieGenre = set(movieDF.genre.sum())&lt;/code&gt; . I then added a single column to movieDF data frame for each genre such that if a particular movie belong to that genre, then that cell is &lt;code&gt;True&lt;/code&gt; otherwise it is &lt;code&gt;False&lt;/code&gt;.  So for example, for the movie Inception, the &lt;code&gt;Action&lt;/code&gt; column is marked &lt;code&gt;True&lt;/code&gt; but &lt;code&gt;Crime&lt;/code&gt; column is marked &lt;code&gt;False&lt;/code&gt; and so forth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I was going through a paper comparing glove and word2vec. I came across the pound notation shown below. What does it mean when used like this?&#xA;&lt;img src=&quot;https://i.imgur.com/WQoNTdG.png&quot; alt=&quot;equation&quot;&gt;&#xA;The link for paper is &lt;a href=&quot;http://arxiv.org/pdf/1411.5595v2.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am looking for a package that does gradient descent parameter estimation in R, maybe with some bootstrapping to get confidence intervals. I wonder if people call it something different here as I get almost nothing on my searches, and the one article I found was from someone who rolled their own. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is not that hard to implement, but I would prefer to use something standard.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I would like to post a paper in International Conference on Soft Computing. I want to know whether the journal is a reputed journal. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Something that I often see in papers (&lt;a href=&quot;http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt;) about large-scale learning is that click-through rate (CTR) problems can have up to a billion of features for each example. In &lt;a href=&quot;http://research.google.com/pubs/pub41159.html&quot; rel=&quot;nofollow&quot;&gt;this Google paper&lt;/a&gt; the authors mention:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The features used in our system are drawn from a variety of sources,&#xA;  including the query, the text of the ad creative, and various&#xA;  ad-related metadata.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I can imagine a few thousands of features coming from this type of source, I guess through some form of feature hashing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: how does one get to a billion features? How do companies translate user behavior into features in order to reach that scale of features?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;When a random initialization of centroids is used, different runs of K-means produce different total SSEs. And it is crucial in the performance of the algorithm. &#xA;What are some effective approaches toward solving this problem? Recent approaches are appreciated.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am not in the data science field, but I would like to examine in depth this field and, particularly, I would like to start from the analysis of the social networks data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to find some good references, both paper, websites and books, in order to start learning about the topic. Browsing on the internet, one can find a lot of sites, forum, papers about the topic, but I'm not able to discriminate among good and bad readings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am an R, Matlab, SAS user and I know a little bit of python language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you suggest any references from which I could start studying and deepen the industry?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've got survey data that resembles:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-------------| Q1a | Q1b | Q1c | Q2a | Q2b | Q2c | Classification&#xA;| Respondent  | 1   | 0   | 0   | 1   | 0   | 0   | Red&#xA;| Respondent  | 0   | 0   | 1   | 1   | 0   | 0   | Green&#xA;| Respondent  | 0   | 1   | 0   | 0   | 0   | 1   | Yellow&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to predict the classification for new respondents. Currently I'm using a Naive Bayes, and getting pretty bad accuracy (~20%). I don't have much training data, and the training data is hand scraped from non-standard sources (internal company procedures are a mess here).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for other ways to predict the classification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about assigning weights to each question, and magically predicting the result based on those, somehow. Although I don't really know where to start learning about how to do that, and whether it's appropriate for this data. I have very little background in this :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or tips on predicting the classification column with no training data?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a large dataset that I need to split into groups according to specific parameters.  I want the job to process as efficiently as possible.  I can envision two ways of doing so&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt; - Create map from original RDD and filter&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def customMapper(record):&#xA;    if passesSomeTest(record):&#xA;        return (1,record)&#xA;    else:&#xA;        return (0,record)&#xA;&#xA;mappedRdd = rddIn.map(lambda x: customMapper(x))&#xA;rdd0 = mappedRdd.filter(lambda x: x[0]==0).cache()&#xA;rdd1 = mappedRdd.filter(lambda x: x[1]==1).cache()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt; - Filter original RDD directly&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def customFilter(record):&#xA;    return passesSomeTest(record)&#xA;&#xA;rdd0 = rddIn.filter(lambda x: customFilter(x)==False).cache()&#xA;rdd1 = rddIn.filter(customFilter).cache()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The fist method has to itterate over all the records of the original data set 3 times, where the second only has to do so twice, under normal circumstances, however, spark does some behind the scenes graph building, so I could imagine that they are effectively done in the same way.  My questions are:&#xA;a.) Is one method more efficient than the other, or does the spark graph building make them equivalent&#xA;b.) Is it possible to do this split in a single pass&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have two series of values, a and b as inputs and I want to create a score, c, which reflects both of them equally.  The distribution of a and b are below&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/TSKfA.png&quot; alt=&quot;Distribution of a&quot;&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/WivxT.png&quot; alt=&quot;Distribution of b&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In both cases, the x-axis is just an index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I go about creating an equation c = f(a,b) such that a and b are (on average) represented equally in c?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: c = (a+b)/2 or c = ab will not work because c will be too heavily weighted by a or b.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need a function, f, where c = f(a,b) and c' = f(a + stdev(a),b) = f(a, b + stdev(b))&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;In Support Vector Machines, when used for sentiment analysis, text gets converted into a set of data points. How does this happen, usually?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a question about MLlib in Spark.(with Scala)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to understand how LogisticRegressionWithLBFGS and LogisticRegressionWithSGD work. I usually use SAS or R to do logistic regressions but I now have to do it on Spark to be able to analyze Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How is the variable selection done? Is there any try of different variable combinations in LogisticRegressionWithLBFGS or LogisticRegressionWithSGD? Something like a test of significance of variable one by one? Or a correlation calculation with the variable of interest? Is there any calculation of BIC, AIC to choose the best model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because the model only returns weights and intercept...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I understand those Spark functions and compare to what I'm used to with SAS or R ?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;What are the tools, practices and algorithms used in automated text writing?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, lets assume that I have access to wikipedia/wikinews and similar websites API and I would like to produce article about &quot;Data Science with Python&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe that this task should be divided into two segments. First would be &lt;strong&gt;text mining&lt;/strong&gt; and second would be &lt;strong&gt;text building&lt;/strong&gt;. I'm more or less aware how text mining is performed and there are lots of materials about it in Internet. However, amount of materials related to automated text building seems to be lower. There are plenty of articles which says that some companies are using it, but there is lack of details. Are there any common ideas about such text building?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've used &lt;code&gt;scikit-learn&lt;/code&gt; in Python to compare results of naive Bayes and SVM. I've found that naive Bayes is quicker than SVM. Could anyone shed some light on reasons for such finding?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'd like to classify the data on coordinate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are 2 example data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data1 = [(1,1), (2,2), &lt;strong&gt;(3, 3), (4, 2), (5, 3)&lt;/strong&gt;, (6, 0)]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data2 = [(1,1), (2,2), &lt;strong&gt;(3, 10), (4, 9), (5, 10)&lt;/strong&gt;, (6, 0)]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bold part have the same wave in above data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The length are all different in my data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way that I can find the similar wave in many data like this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;First of all, I am new in this field we call &lt;em&gt;big data&lt;/em&gt;, so my questions may be naive. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to build an application, which deals with geolocation data, which could be : &lt;em&gt;latitude and longitude coordinates&lt;/em&gt; and &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/cc280766.aspx&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Geography SQL Server&lt;/em&gt;&lt;/a&gt; column types.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to have the following elements made easy:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scalability : be prepared to receive huge amount of data, adding servers to the system have to be easy&lt;/li&gt;&#xA;&lt;li&gt;Proximity requests : in example, how much points are in a circle (at meter scale). &lt;/li&gt;&#xA;&lt;li&gt;Data must be accessible rapidly after being written. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've been looking around for existing solutions, which are &quot;Hadoop friendly&quot; (Hortonworks, Cloudera) and available DBMS, like Cassandra. &#xA;I have found some interesting information, but I still think it's hard to decide, which one to choose. &#xA;It also need drivers for &lt;em&gt;NodeJS&lt;/em&gt; &amp;amp; &lt;em&gt;.NET&lt;/em&gt; (Hadoop with Cassandra seem to be OK with that). &#xA;I've also looked around the MongoDB ecosystem, but, again, I feel that it is hard to know where to look at. By (little) experience with Mongoose, MongoDB can be disqualified by the third point because data writes are slow. But my model could certainly be improved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do any of you have some recent experiences, manipulating massive amount of &lt;em&gt;geolocation data&lt;/em&gt;? I would appreciate sharing them here as well as any quality and recent literature on the subject.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;A commonly heard sentence in unsupervised Machine learning is&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;High dimensional inputs typically live on or near a low dimensional&#xA;  manifold&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is a dimension? What is a manifold? What is the difference?&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Can you give an example to describe both?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Manifold&lt;/strong&gt; from Google/Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In mathematics, a manifold is a topological space that resembles&#xA;  Euclidean space near each point. More precisely, each point of an&#xA;  n-dimensional manifold has a neighbourhood that is homeomorphic to the&#xA;  Euclidean space of dimension n.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dimesion&lt;/strong&gt; from Google/Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In physics and mathematics, the dimension of a mathematical space (or&#xA;  object) is informally defined as the minimum number of coordinates&#xA;  needed to specify any point within it.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What does the Google/Wikipedia even mean in layman terms? It sounds like some bizarre definition like most machine learning definition?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;They are both spaces, so what's the difference between a Euclidean space (i.e. Manifold) and a dimension space (i.e. feature-based)?&lt;/strong&gt; &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Could you tell me, are there any techniques for building neural networks with non-negative weights?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;One of the problems I often encounter is that of poor data provenance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I do research I continuously make modifications to my code and rerun experiments. Each time I'm faced with a number of questions, such as: do I save the old results somewhere, just in case? Should I include the parameter settings in the output filenames or perhaps save them in a different file? How do I know which version of the script was used to produce the results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've recently stumbled upon &lt;a href=&quot;https://pythonhosted.org/Sumatra&quot; rel=&quot;nofollow&quot;&gt;Sumatra&lt;/a&gt;, a pretty lightweight Python package that can capture Code, Data, Environment (CDE) information that can be used to track data provenance. I like the fact that it can be used both from the command line and from within my Python scripts and requiring no GUI. The downside is that the project seems inactive and perhaps there's something better out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: what is a good lightweight data provenance solution for my research? I'm coding small projects mostly in Python in the terminal on a remote server over SSH, so a command line solution would be perfect for me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I have stuck with Sumatra. When I posted this question I didn't look into the web interface yet, but that turns out to be a unique selling point. It displays a very detailed overview of the experiments, capturing not only the state of the data and code, but also the Python environment (package dependencies and versions!) and platform information (architecture and kernel version!).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I've updated the subject of my question to emphasize that I'm mostly concerned about  provenance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Is the Semantic Web dead? Are ontologies dead?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am developing a work plan for my thesis about &lt;em&gt;&quot;A knowledge base through a set ontology for interest groups around wetlands&quot;&lt;/em&gt;. I have been researching and developing ontologies for it but I am still unclear about many things. What is the modeling language for ontologies?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which methodology for ontologies is better? &lt;a href=&quot;http://semanticweb.org/wiki/OTK_methodology&quot; rel=&quot;nofollow&quot;&gt;OTK&lt;/a&gt; or &lt;a href=&quot;http://semanticweb.org/wiki/METHONTOLOGY&quot; rel=&quot;nofollow&quot;&gt;METHONTOLOGY&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any program that does  as does &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.hd.uib.no/AcoHum/abs/Mejia.htm&quot; rel=&quot;nofollow&quot;&gt;Cratilo&lt;/a&gt; is a software for analyzing of textual corpora and for extraction of specific terms of the domain of study (it is developed by professors Jorge Antonio Mejia, Francisco Javier Alvarez and John Albeiro Sánchez, Institute of Philosophy the University of Antioquia). It enables lexical analysis of texts, identifying the words that appear their frequency and location in the text. Through a process of recognition, Cratylus identifies all the words in the text and builds a database becomes the draft analysis of the work. Are there other similar tools?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can the terms found by Cratilo be used to create a knowledge base?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the existing open semantic frameworks that can be used for such things? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there software that automatically creates RDF, OWL, and XML? How does Tails work? Jena? Sesame? &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;Referring to the Stanford course notes on &lt;a href=&quot;http://cs231n.github.io/neural-networks-1/#actfun&quot; rel=&quot;noreferrer&quot;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;, a paragraph says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Unfortunately, ReLU units can be fragile during training and can&#xA;  &quot;die&quot;. For example, a large gradient flowing through a ReLU neuron&#xA;  could cause the weights to update in such a way that the neuron will&#xA;  never activate on any datapoint again. If this happens, then the&#xA;  gradient flowing through the unit will forever be zero from that point&#xA;  on. That is, the ReLU units can irreversibly die during training since&#xA;  they can get knocked off the data manifold. For example, you may find&#xA;  that as much as 40% of your network can be &quot;dead&quot; (i.e. neurons that&#xA;  never activate across the entire training dataset) if the learning&#xA;  rate is set too high. With a proper setting of the learning rate this&#xA;  is less frequently an issue.&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What does dying of neurons here mean? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you please provide an intuitive explanation in simpler terms.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am redesigning some of the classical algorithms for Hadoop/MapReduce framework. I was wondering if there any established approach for denoting Big(O) kind of expressions to measure time complexity?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, hypothetically, a simple average calculation of n (=1 billion) numbers is O(n) + C operation using simple for loop, or O(log) I am assuming division to be a constant time operation for the sake for simplicity. If i break this massively parallelizable algorithm for MapReduce, by dividing data over k nodes, my time complexity would simply become O(n/k) + C + C'. Here, C' can be assumed as the job planning time overhead. Note that there was no shuffling involved, and reducer's job was nearly trivial.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested in a more complete analysis of algorithm with iterative loops over data and involve heavy shuffling and reducer operations. I want to incorporate, if possible, the I/O operations and network transfers of data.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have 40000 rows of text data of health care domain. Data has one column for text (2-5 sentences) and one column for its category.&#xA;I want to classify that into 300 categories. Some categories are independent while some are somewhat related. Distribution of data among categories is not uniform either i.e some of the categories(around 40 of them) have less data about 2-3 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am attaching log probablity of each class/categories. (OR distribution of classes) here.&#xA;&lt;img src=&quot;https://i.stack.imgur.com/nko61.png&quot; alt=&quot;Class prior logarithm of probabilities (log class distribution of data)&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I've compared the logistic regression models on R (&lt;code&gt;glm&lt;/code&gt;) and on Spark (&lt;code&gt;LogisticRegressionWithLBFGS&lt;/code&gt;) on a dataset of 390 obs. of 14 variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The results are completely different in the intercept and the weights.&#xA;How to explain this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the results of Spark (LogisticRegressionWithLBFGS) :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model.intercept  : &#xA; 1.119830027739959&#xA;model.weights :&#xA; GEST    0.30798496002530473&#xA; DILATE  0.28121771009716895&#xA; EFFACE  0.01780105068588628&#xA; CONSIS -0.22782058111362183&#xA; CONTR  -0.8094592237248102&#xA; MEMBRAN-1.788173534959893&#xA; AGE    -0.05285751197750732&#xA; STRAT  -1.6650305527536942&#xA; GRAVID  0.38324952943210994&#xA; PARIT  -0.9463956993328745&#xA; DIAB   0.18151162744507293&#xA; TRANSF -0.7413500749909346&#xA; GEMEL  1.5953124037323745&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the result of R :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;             Estimate Std. Error z value Pr(&amp;gt;|z|)   &#xA;(Intercept)  3.0682091  3.3944407   0.904 0.366052    &#xA;GEST         0.0086545  0.1494487   0.058 0.953821    &#xA;DILATE       0.4898586  0.2049361   2.390 0.016835 *  &#xA;EFFACE       0.0131834  0.0059331   2.222 0.026283 *  &#xA;CONSIS       0.1598426  0.2332670   0.685 0.493196    &#xA;CONTR        0.0008504  0.5788959   0.001 0.998828    &#xA;MEMBRAN     -1.5497870  0.4215416  -3.676 0.000236 ***   &#xA;AGE         -0.0420145  0.0326184  -1.288 0.197725    &#xA;STRAT       -0.3781365  0.5860476  -0.645 0.518777    &#xA;GRAVID       0.1866430  0.1522925   1.226 0.220366    &#xA;PARIT       -0.6493312  0.2357530  -2.754 0.005882 **  &#xA;DIAB         0.0335458  0.2163165   0.155 0.876760    &#xA;TRANSF      -0.6239330  0.3396592  -1.837 0.066219 .  &#xA;GEMEL        2.2767331  1.0995245   2.071 0.038391 *  &#xA;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Beyond the immediate suspects defined in the &lt;a href=&quot;https://spark.apache.org/docs/latest/tuning.html#memory-tuning&quot; rel=&quot;nofollow&quot;&gt;spark documentation&lt;/a&gt;, what are some ways to profile, tune and boost performance of an Apache Spark application? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;RBF kernel using SVM depends on two parameters C and gamma. If the equation of the kernel RBF as the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$K(X,X')= \\\\exp(\\\\gamma||X-X'||^2)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the equation I can see where can I use gamma, but I can't find the C parameter.&#xA;So, can enybody tell me please?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance,&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on an information extractor specifically purposed with parsing relationships between entities such as movies, directors, and actors. NLTK appears to provide the necessary tools to construct such a system. However, it is not clear how one would go about adding custom labels (e.g. actor, director, movie title).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, &lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow&quot;&gt;Chapter 7 of the NLTK Book&lt;/a&gt; discusses information extraction using a named entity recognizer, but it glosses over labeling details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have two questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would I add custom labels?&#xA;If I have bare lists of relevant named entities (e.g. movies, actors, etc.), how can I include them as features? It appears that I would need to use IOB format, but I am unsure about how to do this when I only have lists of named entities.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;This is just for finding overfitting gap.&lt;br&gt;&#xA;After initial research, I can only find method to draw learning curve using evaluation of test set.  However, I could not evaluate on training set and over the two learning curves.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to work using Amazon machine learning, but the data set that I have is small. The model I want to build is for regression based predictions and the domain I am aiming for the data set to belong is financial, say product price prediction, price and demand prediction based on macro/micro economic factor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for a data set that contains factors that lead to variations in value of a product or commodity. For example, I would like to predict the value of 1 unit of polyester yarn after 1 year.  The factors which influence the yarn price are say - prices of crude oil, GDP of country,figures of IIP, inflation etc. So I would like a data set that contains the quotes of these factors on which the final price depends.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find difficulty assembling this data myself because I don't know all the factors that contribute to a certain predictive price. Does anyone know of a dataset I can start with that sounds like it might contain these factors?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm new to all this and am putting together a learning project. I've decided on finding similarities between users in a data set such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Enron_Corpus&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://en.wikipedia.org/wiki/Enron_Corpus&lt;/a&gt;. After doing a bit of research, I also came across &lt;a href=&quot;https://datascience.stackexchange.com/questions/641/dataset-for-named-entity-recognition-on-informal-text/5397#5397&quot;&gt;Dataset for Named Entity Recognition on Informal Text&lt;/a&gt;. So I'm not short of data or a goal, I need to understand high-level techniques to get there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One valuable comment noted that this question appears too broad. What I was hoping to find with this question was the breadth of techniques I should focus research on, not answers that are immediately implementable. Please consider vague answers as entirely appropriate!!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Expanding on the goal, I am hoping to discover which authors might have affinity toward each other, or conversely do not care much for each other. So I will definitely need to start with Named Entity Recognition and build a means to organize the documents against those entities. Beyond that, I am not so sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What high level concepts should I be looking at? Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are some Python libraries which can convert a (X,Y) tuple to strings? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1.23,4.56) yields strings “1_4”, “12_45”, “123_456”.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am looking for a good free existing tool which visualizes geographical data (let's say in the form of coordinates) by plotting them on a map. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be a library (see &lt;a href=&quot;https://stackoverflow.com/questions/9018607/library-for-map-visualization&quot;&gt;this question on StackOverflow&lt;/a&gt;, which suggests a Python library called basemap, which is interesting but not dynamic enough, namely it does not allow for interactivity) or a complete toolkit. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Existing things I found are oriented towards realizing web pages, which are not my ultimate goal (see &lt;a href=&quot;http://www.simile-widgets.org/exhibit/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exhibit&lt;/a&gt; or &lt;a href=&quot;http://modestmaps.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Modest Maps&lt;/a&gt;). I'd like something to feed with data which spits out an interactive map where you can click on places and it displays the related data. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I found a data set called &lt;a href=&quot;http://www.cs.cmu.edu/~./enron/&quot;&gt;Enron Email Dataset&lt;/a&gt;. It is possibly the only substantial collection of &quot;real&quot; email that is public. I found some prior analysis of this work:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A paper describing the Enron data was presented at the 2004 CEAS conference.&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Some experiments associated with this data are described on Ron Bekkerman's home page&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.parakweet.com/&quot;&gt;Parakweet&lt;/a&gt; has released an open source set of Enron sentence data, labeled for speech acts.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Work at the University of Pennsylvania includes a query dataset for email search as well as a tool for generating spelling errors based on the Enron corpus. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm looking for some interesting current trend topics to work with.please give me some suggestions.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;When do we feel need to go through non-linear transformation like kernel PCA ? Please share an example&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am looking to do k-means clustering on a set of 10-dimensional points.  The catch: &lt;strong&gt;there are 10^10 points&lt;/strong&gt;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for just the center and size of the largest clusters (let's say 10 to 100 clusters); I don't care about what cluster each point ends up in.  Using k-means specifically is not important; I am just looking for a similar effect, any approximate k-means or related algorithm would be great (minibatch-SGD means, ...).  Since GMM is in a sense the same problem as k-means, doing GMM on the same size data is also interesting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this scale, subsampling the data probably doesn't change the result significantly: the odds of finding the same top 10 clusters using a 1/10000th sample of the data are very good.  But even then, that is a 10^6 point problem which is on/beyond the edge of tractable.  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an Excel sheet with &lt;code&gt;n&lt;/code&gt; columns, these columns contain info about the students. For admissions we have the score of test scores in multiple subject areas, scores from an interview, and scores of a written test and comprehension test. There is a column which contains student's academic level (High, M.High, Middle, M.Low, Low). I want to compare the last column with the others variables and see whether there are common features that passing students have in common.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there software for this? If this can be done with excel, how can I do it? Does SPSS provide this kind of analysis?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'll set the question up with an example. You are analysing news coverage text data from 2014, and find that a term appears less often in the third quarter of 2014 than the final quarter (let's imagine it's the term &quot;Christmas&quot;). Unfortunately, there are also far less news articles in the third quarter than in the second (due to the lack of news in the summer). So how do we accurately compare the counts in each quarter? We assume that there will be a greater number of occurrences in the fourth quarter, but how much does the magnitude of this difference depend on the change in size of the underlying text?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Heaps%27_law&quot; rel=&quot;nofollow&quot;&gt;Heap's law&lt;/a&gt; shows the relationship between text size and number of unique terms. It's non-linearity implies that the rate of new, unique words introduced by the text decreases as you increase the size of the text, and the proportion of the text taken up by each existing word therefore increases. This applies given documents taken from the same 'distribution' of text, in other words the underlying zipfian distribution of word ranks is identical (see &lt;a href=&quot;http://en.wikipedia.org/wiki/Zipf%27s_law&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my example above this is obviously not the case, since the underlying topics, and resultant term distribution, will change between summer and winter, especially with regards to the term &quot;Christmas&quot;. But take the same term count but over the whole of 2013 and 2014; you would reasonably expect the general underlying term distribution to be the same in each period, so Heap's law applies, but what if the volume of text has changed? Simply normalising by the size of the text, or the number of documents, does not, as far as I can tell, account for the relative change in expected value of the term count.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a hunch that this might be a simple application of Heap's or Zipf's laws, but I can't see how to apply them to this particular question. Appreciate any insight.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I know that there are a number of predictive models (generized linear ones, trees, neural network, support vector machines, knn, Naive Bayes, ...) that have been proposed to perform various analytical tasks. Now I am striving to find appropriate references about their performance when the data becames &quot;Big&quot;. In other words, how is their performance when the data becames really big. Does the training time increase more than linear? Is there any comparative benchmark between computational time and precision when the data becames high (for the various predictive models).&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm a new bee for data analysis . i need to work on a research project in big data analysis. first of all i search for a dataset and i found interest in s&lt;a href=&quot;https://archive.org/details/stackexchange&quot; rel=&quot;nofollow&quot;&gt;tack exchange  data dump&lt;/a&gt;. however i browse for researches i found a lot .And whatever i thought about a idea based on this dataset , its already done by someone else . &#xA;please help me out with a new and useful idea for my research .&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Users tend to click on results ranked highly by search engines much more often than those ranked lower. How do you train a search engine using click data / search logs without this bias? I.e. you don't want to teach the search engine that the results that are currently ranked highly should necessarily continue to be ranked highly just because they were frequently clicked.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Building out a system that tries to apply zero or more predefined labels to text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each label, we've:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;built out a reasonably good vocabulary of high-value words/features&lt;/li&gt;&#xA;&lt;li&gt;developed a corpus containing thousands of labeled entries&lt;/li&gt;&#xA;&lt;li&gt;trained a NaiveBayesClassifier for each topic that does a good job of classifying valid vs noisy content&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The problem seems to be that the individual classifier is great at differentiating between valid &amp;amp; noisy content &lt;strong&gt;WITHIN&lt;/strong&gt; a topic:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&quot;the green energy bill will revolutionize...&quot; (green = &quot;green energy&quot;)&lt;/li&gt;&#xA;&lt;li&gt;&quot;the green bay packers went on to lose their...&quot; (green != &quot;green energy&quot;) &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;...but when classifying content that shouldn't match ANY topic it has a very high rate of false positives. There's no &quot;everything else&quot; label!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;tl;dr it's good at subtle, in-topic differentiation, but terrible at broad topic labeling&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any algorithms that help you classify into N categories, but allow for &quot;everything else&quot; which might not fit into ANY of the categories?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Could somebody please recommend a good R package for doing logit and probit regression? I have tried to find an answer by searching on Google but all the links I find go into lengthy explanations about what logit regression is, which I already know, but nobody seems to recommend an R package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Jerome Smith&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.ensemble import RandomForestClassifier&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; clf = RandomForestClassifier(n_estimators=10, random_state=1)&#xA;&amp;gt;&amp;gt;&amp;gt; Y=[0,1]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,2,1,0], [7,6,5,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[0, 1, 2,3], [4,5,6,7]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,2,1,0], [7,6,5,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,1,2,0], [7,5,6,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Assume the features have names.&#xA;When I shuffle/change the listed order of features specified in the training data set, the importance for each feature changes.&lt;br&gt;&#xA;That means the resulted random forest classifier also changes.&#xA;Note that I have rule out the effect of randomness, by fixing the random seed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why does the listed order of features specified in the data set matter to the random forest classifier, given that the random seed is fixed? Thanks.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I dropped out of college but am interested in a career in data analysis. Now I am self-studying approximately 10 hours per day.  Browsing through job postings on Linkedin has allowed me to compose a rough curriculum.  It would be of great help to me if you would either add a subject I have omitted or eliminate a subject that is not necessary for success in the market place.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Curriculum (in 3-subject groupings):&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 1&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Single-variable calculus&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Intro to python&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;SQL&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 2&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Multi-variable calculus/linear algebra&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Discrete math&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Data structures and algorithms&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 3&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Calculus-based statistics and probability&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Hadoop stack&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Differential equations&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 4&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Statistical learning/predictive modelling&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Python data analysis techniques/Statistical programming in R&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Fundamentals of machine learning&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;All the while I plan to practice using any data sets I can find online.  Will this be sufficient to land a job in data analysis?  Of course I plan to learn far more than just this, but is this foundation solid enough to land an entry level data engineering/science position?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I recently discovered a &lt;a href=&quot;http://www.r-bloggers.com/analyze-linkedin-with-r/&quot;&gt;new R package&lt;/a&gt; for connecting to the LinkedIn API. Unfortunately the LinkedIn API seems pretty limited to begin with; for example, you can only get basic data on companies, and this is detached from data on individuals. I'd like to get data on all employees of a given company, which you can do &lt;a href=&quot;https://www.linkedin.com/vsearch/p?keywords=stack%20exchange&amp;amp;f_CC=974353&amp;amp;sb=People%20who%20work%20at%20Stack%20Exchange&amp;amp;trk=tyah&amp;amp;trkInfo=clickedVertical%3Asuggestion%2Cidx%3A1-1-1%2CtarId%3A1431584515143%2Ctas%3Astack%20exchange&quot;&gt;manually on the site&lt;/a&gt; but is not possible through the API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://import.io/&quot;&gt;import.io&lt;/a&gt; would be perfect if it &lt;a href=&quot;http://blog.import.io/post/tips-tricks&quot;&gt;recognised the LinkedIn pagination&lt;/a&gt; (see end of page).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know any web scraping tools or techniques applicable to the current format of the LinkedIn site, or ways of bending the API to carry out more flexible analysis? Preferably in R or web based, but certainly open to other approaches.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a data set of tweets regarding vaccines.  They have been collected from an API because they have keywords like &quot;flu, measles, MMR, vaccine&quot; etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to find tweets specifically about measles and the outbreak that occurred in California this past February.  It isn't enough to search the data set for words like &quot;California&quot; and &quot;Measles&quot; because tweets like &quot;MMR vaccination rates in Palo Alto on the rise&quot; are about measles and California, but wont be captured by a naive search.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any unsupervised algorithms that could help me out?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm not quite sure what &quot;latent&quot; refers to in this context. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://ijcai.org/papers07/Papers/IJCAI07-259.pdf&quot; rel=&quot;nofollow&quot;&gt;Computing Semantic Relatedness using&#xA;Wikipedia-based Explicit Semantic Analysis&lt;/a&gt; they say &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;''Our semantic analysis is explicit in the sense that we manipulate &#xA; manifest concepts grounded in human cognition, rather than &#xA; 'latent concepts' used by Latent Semantic Analysis''.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What does that mean in simple terms?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had a look at the related Wikipedia articles, ( &lt;code&gt;Latent&lt;/code&gt; (wikipedia.org/wiki/Latent_semantic_analysis), &lt;code&gt;Explicit&lt;/code&gt; wikipedia.org/wiki/Explicit_semantic_analysis) ), and I wasn't able to make heads or tails of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Perhaps someone with a better appreciate of the nuance involved here might be able to provide me with a clear indication of the similarities and differences, the pros and cons between these two methods for accessing document/text fragment relatedness to a particular concept.  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a training data set distributed in two files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;File 1&lt;/strong&gt;: This contains actual classification for each X1. X1 is unique in this file. X1 has one-to-one relationship with X2, i.e. X2 is also unique. Y is binary.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X2 | Y  | &#xA;| 1  | 4  | 0  | &#xA;| 3  | 5  | 1  | &#xA;...&#xA;| 8  | 9  | 1  | &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;File 2&lt;/strong&gt;: This contains the real 'observations' of the experiment. X1 can appear multiple times. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X3 | X4 | &#xA;| 3  | 4  | 5  | &#xA;| 3  | 1  | 2  | &#xA;...&#xA;| 1  | 4  | 8  | &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here I can combine the two tables to have a structure like below and use them as observations:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X2 | X3 | X4 | Y |&#xA;| 3  | 5  | 4  | 5  | 1 |&#xA;| 3  | 5  | 1  | 2  | 1 |&#xA;...&#xA;| 1  | 4  | 4  | 8  | 0 |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For test data I have similar structure, just the Y column is missing in File 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have multiple concerns here:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;X1 and X2 has one-to-one dependency in the data, i.e. X1 = f(X2) and X2 = f(X1)&lt;/li&gt;&#xA;&lt;li&gt;Y = f'(X1) or f'(X2)&lt;/li&gt;&#xA;&lt;li&gt;Frequency distribution of X1,X2 and Y changes dramatically in the new joined data set.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Does this kind of transformation of data leads to any insights?&lt;/li&gt;&#xA;&lt;li&gt;Does regression and ensemble learning techniques are capable of capturing these internal relationships?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I found in many sources that Hidden Markov Models are linear-chain networks(e.g. in Predicting Structured Data book by MIT). However, as I understand it, HMMs can have any edges in its graph. Even simple example of HMM in wikipedia has non-linear graph: &lt;img src=&quot;https://i.stack.imgur.com/JU1YV.png&quot; alt=&quot;enter image description here&quot;&gt; .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, the question is: what is the formal definition linear-chain structure and in which case forward-backward and Viterbi algorithms can give precise results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also taken into consideration this picture, taken from CRF tutorial, which says, that linear-chain CRF is &quot;generative-discriminative pair&quot; to HMM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/9CA8H.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;HI I am currently trying to apply various algorithms to a classification problem to assess which could be better and then try to fine tune the bests of the first approach. I am a beginner so I use Weka for now. I have basic ML concept understanding but am not in the details of algorithms yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I observed that on my problem, RBF networks performed vastly worse than IBK and other K methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I read about RBF networks, &quot;it implements a normalized Gaussian radial basisbasis function network. It uses the k-means clustering algorithm to provide the basis functions and learns either a logistic regression (discrete class problems) or linear regression (numeric class problems) on top of that. Symmetric multivariate Gaussians are fit to the data from each cluster. If the class is nominal it uses the given number of clusters per class.It standardizes all numeric attributes to zero mean and unit variance.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So basically, it also use k means to classify at first. But for some reason, I get the worst results with it using my metrics (ROC), while K methods are among the bests. Can I deduce from that fact something important about my data, like the fact that it has not a gaussian distribution, or is not fitted for logistic regression, or whatever I can't figure out?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also observed that random forests get similar results to K methods, and that adding a filter to reduce dimensionality improved these RF, random projection being better than PCA?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can this last point means that there is much randomness in my data so random dimension reduction is better than &quot;ruled&quot; dimension reduction like PCA? What can I deduce from the fact that RF perform equally to K methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel there is some signification here, but I am not skilled enough to understand what, and I would be very glad for any insights. Thanks by advance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have location data of taxis moving around the city sourced from: &lt;a href=&quot;http://research.microsoft.com/apps/pubs/?id=152883&quot; rel=&quot;nofollow&quot;&gt;Microsoft Research&lt;/a&gt;&lt;br&gt;&#xA;Overall it has around 17million data points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have converted the data to JSON and filled up mongo. A sample looks like this:&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84349, 'timestamp': '2008-02-08 17:38:10', 'lon': 116.33986, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84441, 'timestamp': '2008-02-08 17:38:15', 'lon': 116.33995, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.8453, 'timestamp': '2008-02-08 17:38:20', 'lon': 116.34004, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84615, 'timestamp': '2008-02-08 17:38:25', 'lon': 116.34012, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84705, 'timestamp': '2008-02-08 17:38:30', 'lon': 116.34022, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84891, 'timestamp': '2008-02-08 17:38:40', 'lon': 116.34039, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.85083, 'timestamp': '2008-02-08 17:38:50', 'lon': 116.3406, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;br&gt;&#xA;It consists of a taxiID - ID field, timestamp of its latitude and longitude combination.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: I want to use this data to calculate estimated time of arrival(ETA)  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I am doing it a crude way by querying mongoDB with aggregation. It is totally inefficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking at some sort of learning algorithm where the historical data can be used to train it. In the end, given two points, the algorithm should traverse the possible route by referring historical data and give an estimate of time.&#xA;Calculating time estimate is not a problem at all if I get the array of JSON documents between the points. But, getting those right arrays is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers in this direction will be very helpful. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Apologies if this isn't the correct place to ask - I'm not sure if this fits best with Stats or Data Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using analytics to help marketers identify attributes of their users correspond to successful conversions (such as someone buying a product, signing up for a newsletter, or subscribing to a service). Attributes could be things like which site they came from (referrer), their location, time/day of week, device type, browser, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I'd like to say (although I'm not certain it's possible) is to isolate differences in conversion rate to an individual attribute, something like, '11% of your users from Facebook converted whereas only 3% of non-Facebook users converted', which would mean that the attribute 'referrer' and the level of the attribute 'Facebook' are responsible for driving conversions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that I may have 100s of quasi-independent variables, is it even possible to isolate the effect to one variable and one level of that variable? As opposed to a combination of them that is more likely to be driving the difference? If so, what technique or conceptual paradigm do I use to identify which variable-level is responsible for the greatest lift in my dependent variable, conversion rate?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to figure out how to use NLTK's cascading chunker as per &lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Chapter 7 of the NLTK book&lt;/a&gt;. Unfortunately, I'm running into a few issues when performing non-trivial chunking measures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's start with this phrase:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&quot;adventure movies between 2000 and 2015 featuring performances by daniel craig&quot;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to find all the relevant NPs when I use the following grammar:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;grammar = &quot;NP: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&quot;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am not sure how to build nested structures with NLTK. The book gives the following format, but there are clearly a few things missing (e.g. How does one actually specify multiple rules?):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grammar = r&quot;&quot;&quot;&#xA;  NP: {&amp;lt;DT|JJ|NN.*&amp;gt;+}          # Chunk sequences of DT, JJ, NN&#xA;  PP: {&amp;lt;IN&amp;gt;&amp;lt;NP&amp;gt;}               # Chunk prepositions followed by NP&#xA;  VP: {&amp;lt;VB.*&amp;gt;&amp;lt;NP|PP|CLAUSE&amp;gt;+$} # Chunk verbs and their arguments&#xA;  CLAUSE: {&amp;lt;NP&amp;gt;&amp;lt;VP&amp;gt;}           # Chunk NP, VP&#xA;  &quot;&quot;&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my case, I'd like to do something like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grammar = &quot;MEDIA: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&#xA;           RELATION: {&amp;lt;V.*&amp;gt;}{&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&#xA;           ENTITY: {&amp;lt;NN.*&amp;gt;}&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It occurs to me that a CFG might be a better fit for this, but I only became aware of NLTK's support for this function about 5 minutes ago (from &lt;a href=&quot;https://stackoverflow.com/questions/14692489/chunking-with-nltk&quot;&gt;this question&lt;/a&gt;), and it does not appear that much documentation for the feature exists.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, assuming that I'd like to use a cascaded chunker for my task, what syntax would I need to use? Additionally, is it possible for me to specify specific words (e.g. &quot;directed&quot; or &quot;acted&quot;) when using a chunker?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I prefer this model in R&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are capturing sales data by time series (Month by month). Some of items have commissions and some have Discounts and others have both commissions and discounts. Is it Commissions or Discounts or commissions + Discounts have impact on my sales growth? Or is it my sales are growing because of those commissions or discounts or discounts +commissions Can you suggest me best model to solve my use case? I am thinking multiple regression. But I want to double check with experts like you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your all your help&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Data set: (5 variables)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Year-Month -Product -Sales  -Commission -Discounts&#xA;2013-01 Milk    300 No  Yes&#xA;2013-02 Milk    400 No  Yes&#xA;2013-03 Milk    200 No  Yes&#xA;2013-04 Milk    150 No  Yes&#xA;2013-05 Milk    500 No  Yes&#xA;2013-01 Bread   800 Yes No&#xA;2013-02 Bread   879 Yes No&#xA;2013-03 Bread   790 Yes No&#xA;2013-04 Bread   459 Yes No&#xA;2013-05 Bread   600 Yes No&#xA;2013-01 Cheese  400 Yes Yes&#xA;2013-02 Cheese  350 Yes Yes&#xA;2013-03 Cheese  600 Yes Yes&#xA;2013-04 Cheese  590 Yes Yes&#xA;2013-05 Cheese  720 Yes Yes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a data frame (a csv file) with dimensions 100x6 and I need only the columns c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;) and the rows in which the value of &quot;X1&quot; is greater than 30. So I did:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  data_frame &amp;lt;- read.csv (&quot;data_frame&quot;)&#xA;  data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The column subset problem is solved but now I need to subset rows from data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)] where the values of &quot;X1&quot; is greater than 30. I tried:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  data_frame [c(&quot;X1&quot; &amp;gt; 30), c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)] &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it returned the same data frame as data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)].&#xA;Also tried using the function subset() with the same approach but got the same results.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have recently completed an MSc in Control Systems from a top university. It seems to me that control theory must have an application within data science. I would like to apply my degree within this domain, but I want to be sure that it is relevant to the role of a data scientist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The topics which I have particular interest and experience in are State Space Control, Systems Identification, Model Predictive Control and Optimal Control. I imagine that effective management of any large dataset must involve modelling of the system in terms of transfer functions/state space models (based on large sets of historical input/output data). These models could then be used to predict the evolution of a market/variable over time, and therefore optimise a given cost function such as profit, risk etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this kind of role exists within data science/ other areas, can you please give me more information/ ideas of job roles/ industries to research.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to investigate price-setting behavior of airlines -- specifically how airlines react to competitors pricing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I would say my knowledge about more complex analysis is quite limited I've done mostly all basic methods to gather a overall view of the data. This includes simple graphs which already help to identify similar patterns. I'am also using SAS Enterprise 9.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I'am looking for a more number based approach.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Data Set&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The (self) collected data set I'am using contain around ~54.000 fares.&#xA;All fares were collected within a 60 day time window, on a daily basis (every night at 00:00).&lt;img src=&quot;https://i.stack.imgur.com/q5bcT.png&quot; alt=&quot;Collection Method&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, every fare within that time window occurs $n$ times subject to the availability of the fare as well as the departure date of the flight, when it is passed by the collection date of the fare.&#xA;&lt;em&gt;(You cant collect a fare for a flight when the departure date of the flight is in the past)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The unformatted that looks basically like this: (fake data)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| requestDate        | price| tripStartDeparture | tripDestinationDeparture | flightCarrier |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 725.32    | 16APR2015:10:50:02 | 23APR2015:21:55:04       | XA            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 966.32    | 16APR2015:13:20:02 | 23APR2015:19:00:04       | XY            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 915.32    | 16APR2015:13:20:02 | 23APR2015:21:55:04       | XH            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&quot;DaysBeforeDeparture&quot; is calculated via $I=s-c$ where&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I &amp;amp; interval (days before departure)&lt;/li&gt;&#xA;&lt;li&gt;s &amp;amp; date of the fare (flight departure)&lt;/li&gt;&#xA;&lt;li&gt;c &amp;amp; date of which the fare was collected&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here is a example of grouped data set by I (DaysBeforeDep.) (fake data!):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----------------+------------------+------------------+------------------+------------------+&#xA;| DaysBefDeparture | AVG_of_sale | MIN_of_sale | MAX_of_sale | operatingCarrier |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 880.68           | 477.99           | 2,245.23         | DL           |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 904.89           | 477.99           | 2,534.55         | DL           |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 1,044.39         | 920.99           | 2,119.09         | LH               |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;What I came up with so far&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Looking at the line graphs I can already estimate that several lines will have a high correlation factor. Hence, I tried to use correlation analysis first on the grouped data. But is that the correct way? Basically I try now to make correlations on the averages rather then on the individual prices?&#xA;Is there an other way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'am unsure wich regression model fits here, as the prices do not move in any linear form and appear non-linear. Would I need to fit a model to each of price developments of an airline&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: This is a long text-wall. If I need to clarify anything let me know. I'am new to this sub.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone a clue? :-)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have several sets of text files on hdfs that are exports from relations.  Unfortunately I do not know the structure of the table is, but I do know that each has a multi-part key that defines a row uniquely.  I know through domain knowledge that the key is multi-part (e.g. reporting-date, and item number) and I can identify some columns that are clearly not in the key (e.g. revenue from sale).  What is an effective way to identify potential sets of columns that are natural keys, because they are not duplicated in the observed data?  I can get several days of logs in a few Gig, so python or sql could work.  This seems like an great application for a dictionary, but I am not sure how to approach this.     &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have some basic features which I encoded in a &lt;strong&gt;one-hot vector&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Length of the feature vector equals to 400.&#xA;It is &lt;strong&gt;sparse&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw that conv nets is applied to a dense feature vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any problems to apply conv nets to a sparse feature vectors?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering if anyone might have some novel insights as to the best way to analyze the following data.  It's a problem I've been thinking about in the back of my mind for a while, so I thought that I'd ask here. I have data that look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     day event actor recipient&#xA;995    8   128     G         J&#xA;996    8   129     G         K&#xA;997    8   130     G         B&#xA;998    8   131     B         G&#xA;999    8   132     H         G&#xA;1000   8   133     G         H&#xA;1001   8   134     E         G&#xA;1002   8   135     G         J&#xA;1003   8   136     B         H&#xA;1004   8   137     G         H&#xA;1005   8   138     G         H&#xA;1006   8   139     B         J&#xA;1007   9     1     D         J&#xA;1008   9     2     A         J&#xA;1009   9     3     A         J&#xA;1010   9     4     H         J&#xA;1011   9     5     A         J&#xA;1012   9     6     D         H&#xA;1013   9     7     A         F&#xA;1014   9     8     D         J&#xA;1015   9     9     A         H&#xA;1016   9    10     D         J&#xA;1017   9    11     A         J&#xA;1018   9    12     F         J&#xA;1019   9    13     F         J&#xA;1020   9    14     F         H&#xA;1021   9    15     F         G&#xA;1022   9    16     F         H&#xA;1023   9    17     C         F&#xA;1024   9    18     C         G&#xA;1025   9    19     D         H&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What you see here is an extract of a R dataframe.  The first column being the rownumber of the df, then the four variables.    These data start at day1 and end at day 22.  There are between 13 and 215 'events' on each day - each event is a separate behavioral event. Higher number events occur later in time than earlier numbered events.  Individuals are in the 'actor' and 'recipient' variables.   The data are available in csv format &lt;a href=&quot;https://github.com/jalapic/exampledata/blob/master/demodf.csv&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are 11 individuals (A - K). One thing you'll notice is that recipients tend to be lower down the alphabet, and actors tend to be higher up the alphabet.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A key question I'm interested in working out a methodology to address is to see if the likelihood of becoming an actor increases if an individual has recently been a recipient. You can see this on line 997 that G-B and then B-G occur followed by H-G and G-H.    An individual recipient doesn't have to appear on the next line to count as having an increased likelihood of appearing as an actor - I'm interested in the decay in the probability of this occurring over events (but not continuing to the next day).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further, I don't think this will be true for all individuals, so I am keen to test which individuals it is true for.     &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I am interested to know if an individual who has recently been a recipient but is now an actor is paired with more often an individual of a higher or lower letter than themselves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that these questions are clear and make sense.   I obviously don't expect a full analysis.  But I would be keen to hear of ideas for this type of analysis.  I believe that examining some Markov processes may be useful but I am interested in hearing about other ideas.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a set of strings, each also has soem categorical information associated with it. The categorical information isn't always great though, so I need to cluster the messages based on the text content &amp;amp; the categories. What is the best way to do this? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am looking to change careers and would appreciate some advice. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an undergraduate degree in English Literature and a JD. Needless to say, these were not the best decisions and I would like to change my career. I have always enjoyed math and science, and after months of research and self study, I have decided that I would like to pursue statistics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is basically this : would it be better to get another Bach. Degree in statistics or should I take calculus 1-3, linear algebra, probably and statistics and some computer science courses at a community college then try to get into a grad program instead? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am afraid that without the bachelors degree I will not have the required knowledge for a masters even after taking the courses at my local c.c. However, another bachelors degree may be a worthless waste of time. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please share your thoughts. Also, sorry if this is not the right place for this question. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I was informed of 5 java NLP libraries.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Apache cTAKES™&lt;/li&gt;&#xA;&lt;li&gt;MetaMap&lt;/li&gt;&#xA;&lt;li&gt;LexEVS (&lt;a href=&quot;http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/web/download.html&quot; rel=&quot;nofollow&quot;&gt;http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/web/download.html&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Apache OpenNLP&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I also plan to parallelize an NLP library via map-reduce with hadoop.&#xA;However, I'm new to natural language processing, so I don't know how to approach the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The goal is to download a set of clinical trials on cancer from www.clinicaltrials.gov and parse eligibility criteria (both inclusion and exclusion criteria), identify the ECOG scores and annotate each CTA with the allowed ECOG scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, in the following document, if ECOG is specified in inclusion criteria, it is not negated. If ECOG is specified in exclusion criteria, it is negated. If both exclusion criteria and inclusion criteria are not mentioned, then ECOG is not negated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Document: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Eligibility criteria of CTA &quot;NCT01572038&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Inclusion Criteria:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;Patients must have histological proof of a primary non-small cell lung cancer&#xA;  (bronchoalveolar carcinomas presenting as a discrete solitary radiological mass or&#xA;  nodule are eligible)&lt;/li&gt;&#xA;  &lt;li&gt;Patients must be classified post-operatively as stage IB, II or IIIA on the basis of&#xA;  pathologic criteria&lt;/li&gt;&#xA;  &lt;li&gt;At the time of resection a complete mediastinal lymph node resection or at least&#xA;  lymph node sampling should have been attempted; if a complete mediastinal lymph node&#xA;  resection or lymph node sampling was not undertaken, any mediastinal lymph node which&#xA;  measured 1.5 cm or more on the pre-surgical computed tomography (CT)/magnetic&#xA;  resonance imaging (MRI) scan or any area of increased uptake in the mediastinum on a&#xA;  pre-surgical positron emission tomography (PET) scan must have been biopsied; note: a&#xA;  pre-surgical PET scan is not mandatory&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;The nodal tissue must be labelled according to the recommendations of the&#xA;  American Thoracic Society; surgeons are encouraged to dissect or sample all&#xA;  accessible nodal levels; the desirable levels for biopsy are:&lt;/li&gt;&#xA;  &lt;li&gt;Right upper lobe: 4, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Right middle lobe: 4, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Right lower lobe: 4, 7, 9, 10&lt;/li&gt;&#xA;  &lt;li&gt;Left upper lobe: 5, 6, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Left lower lobe: 7, 9, 10&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Surgery may consist of lobectomy, sleeve resection, bilobectomy or pneumonectomy as&#xA;  determined by the attending surgeon based on the intraoperative findings; patients&#xA;  who have had only segmentectomies or wedge resections are not eligible for this&#xA;  study; all gross disease must have been removed at the end of surgery; all surgical&#xA;  margins of resection must be negative for tumor&lt;/li&gt;&#xA;  &lt;li&gt;No more than 16 weeks may have elapsed between surgery and randomization; for&#xA;  patients who received post-operative adjuvant platinum-based chemotherapy, no more&#xA;  than 26 weeks may have elapsed between surgery and randomization&lt;/li&gt;&#xA;  &lt;li&gt;Patient must consent to provision of and investigator(s) must agree to submit a&#xA;  representative formalin fixed paraffin block of tumor tissue at the request of the&#xA;  Central Tumor Bank in order that the specific EGFR correlative marker assays may be&#xA;  conducted&lt;/li&gt;&#xA;  &lt;li&gt;The patient must have an Eastern Cooperative Oncology Group (ECOG) performance status&#xA;  of 0, 1 or 2&lt;/li&gt;&#xA;  &lt;li&gt;Leukocytes &amp;gt;= 3.0 x 10^9/L or &amp;gt;= 3000/ul&lt;/li&gt;&#xA;  &lt;li&gt;Absolute granulocyte count &amp;gt;= 1.5 x 10^9/L or &amp;gt;= 1,500/ul&lt;/li&gt;&#xA;  &lt;li&gt;Platelets &amp;gt;= 100 x 10^9/L or &amp;gt;= 100,000/ul&lt;/li&gt;&#xA;  &lt;li&gt;Total bilirubin within normal institutional limits&lt;/li&gt;&#xA;  &lt;li&gt;Alkaline phosphatase =&amp;lt; 2.5 x institutional upper limit of normal; if alkaline&#xA;  phosphatase is greater than the institutional upper limit of normal (UNL) but less&#xA;  than the maximum allowed, an abdominal (including liver) ultrasound, CT or MRI scan&#xA;  and a radionuclide bone scan must be performed prior to randomization to rule out&#xA;  metastatic disease; if the values are greater than the maximum allowed, patients will&#xA;  not be considered eligible regardless of findings on any supplementary imaging&lt;/li&gt;&#xA;  &lt;li&gt;Aspartate aminotransferase (AST) (serum glutamic oxaloacetic transaminase [SGOT])&#xA;  and/or alanine aminotransferase (ALT) (serum glutamate pyruvate transaminase [SGPT])&#xA;  =&amp;lt; 2.5 x institutional upper limit of normal; if AST (SGOT) or ALT (SGPT) are greater&#xA;  than the institutional upper limit of normal (UNL) but less than the maximum allowed,&#xA;  an abdominal (including liver) ultrasound, CT or MRI scan must be performed prior to&#xA;  randomization to rule out metastatic disease; if the values are greater than the&#xA;  maximum allowed, patients will not be considered eligible regardless of findings on&#xA;  any supplementary imaging&lt;/li&gt;&#xA;  &lt;li&gt;Patient must have a chest x-ray done within 14 days prior to randomization; patient&#xA;  must have a CT or MRI scan of the chest done within 90 days prior to surgical&#xA;  resection if at least one of the following was undertaken:&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;A complete mediastinal lymph node resection; or&lt;/li&gt;&#xA;  &lt;li&gt;Biopsy of all desired levels of lymph nodes - as specified above; or&lt;/li&gt;&#xA;  &lt;li&gt;A pre-surgical PET scan within 60 days prior to surgical resection If none of&#xA;  the above was undertaken then the CT or MRI scan of the chest must have been&#xA;  performed within 60 days prior to surgical resection Note: a pre-surgical PET&#xA;  scan is not mandatory&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Patient must have an electrocardiogram (EKG) done within 14 days prior to&#xA;  randomization&lt;/li&gt;&#xA;  &lt;li&gt;Women of childbearing age and men must agree to use adequate contraception (hormonal&#xA;  or barrier method of birth control) prior to study entry and while taking study&#xA;  medication and for a period of three months after final dose; should a woman become&#xA;  pregnant or suspect she is pregnant while she or her male partner are participating&#xA;  in this study, she should inform her treating physician immediately&lt;/li&gt;&#xA;  &lt;li&gt;Patients may receive post-operative radiation therapy; patients must have completed&#xA;  radiation at least 3 weeks prior to randomization and have recovered from all&#xA;  radiation-induced toxicity; patients who have received radiation therapy should also&#xA;  be randomized within 16 weeks of surgery&lt;/li&gt;&#xA;  &lt;li&gt;Patient consent must be obtained according to local institutional and/or University&#xA;  Human Experimentation Committee requirements; it will be the responsibility of the&#xA;  local participating investigators to obtain the necessary local clearance, and to&#xA;  indicate in writing to either the National Cancer Institute of Canada (NCIC) Clinical&#xA;  Trials Group (CTG) study coordinator (for NCIC CTG centers) or the Cancer Trials&#xA;  Support Unit (CTSU) (for all other investigators), that such clearance has been&#xA;  obtained, before the trial can commence in that center; a standard consent form for&#xA;  the trial will not be provided, but a sample form is given; this sample consent form&#xA;  has been approved by the National Cancer Institute (NCI) Central Institutional Review&#xA;  Board (IRB) and must be used unaltered by those CTSI centers which operate under CIRB&#xA;  authority; for NCIC CTG centers, a copy of the initial full board Research Ethics&#xA;  Board (REB) approval and approved consent form must be sent to the NCIC CTG central&#xA;  office; please note that the consent form for this study must contain a statement&#xA;  which gives permission for the government agencies, NCI, NCIC CTG and monitoring&#xA;  agencies to review patient records&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;NCIC-CTG Centers: the patient must have the ability to understand and the&#xA;  willingness to sign a written informed consent document; the patient must sign&#xA;  the consent form prior to randomization&lt;/li&gt;&#xA;  &lt;li&gt;CTSU Centers: the patient, or in the case of a mentally incompetent patient his&#xA;  or her legally authorized and qualified representative, must have the ability to&#xA;  understand and the willingness to sign a written informed consent document; the&#xA;  consent form must be signed prior to randomization&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Patients must be accessible for treatment and follow-up; investigators must assure&#xA;  themselves that patients registered on this trial will be available for complete&#xA;  documentation of the treatment administered, toxicity and follow-up&lt;/li&gt;&#xA;  &lt;li&gt;Initiation of protocol treatment must begin within 10 working days of patient&#xA;  randomization&lt;/li&gt;&#xA;  &lt;li&gt;Patients may have received post-operative adjuvant platinum-based chemotherapy;&#xA;  patients must have completed chemotherapy at least 3 weeks prior to randomization and&#xA;  have recovered from all chemotherapy-induced toxicity; patients who have received&#xA;  adjuvant chemotherapy should also be randomized within 26 weeks of surgery&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;Exclusion Criteria:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;Prior or concurrent malignancies; patients who have had a previous diagnosis of&#xA;  cancer, if they remain free of recurrence and metastases five years or more following&#xA;  the end of treatment and, in the opinion of the treating physician do not have a&#xA;  substantial risk of recurrence of the prior malignancy, are eligible for the study;&#xA;  patients who have been adequately treated for non-melanomatous skin cancer or&#xA;  carcinoma in situ of the cervix are eligible irrespective of when that treatment was&#xA;  given&lt;/li&gt;&#xA;  &lt;li&gt;A combination of small cell and non-small cell carcinomas or a pulmonary carcinoid&#xA;  tumor&lt;/li&gt;&#xA;  &lt;li&gt;More than one discrete area of apparent primary cancer (even if within the same lobe,&#xA;  T4, IIIB)&lt;/li&gt;&#xA;  &lt;li&gt;Clinically significant or untreated ophthalmologic (e.g. Sjogren's etc.) or&#xA;  gastrointestinal conditions (e.g. Crohn's disease, ulcerative colitis)&lt;/li&gt;&#xA;  &lt;li&gt;Any active pathological condition that would render the protocol treatment dangerous&#xA;  such as: uncontrolled congestive heart failure, angina, or arrhythmias, active&#xA;  uncontrolled infection, or others&lt;/li&gt;&#xA;  &lt;li&gt;A history of psychiatric or neurological disorder that would make the obtainment of&#xA;  informed consent problematic or that would limit compliance with study requirements&lt;/li&gt;&#xA;  &lt;li&gt;Patient, if female, is pregnant or breast-feeding&lt;/li&gt;&#xA;  &lt;li&gt;Neoadjuvant chemotherapy or immunotherapy for NSCLC; however, patients may have&#xA;  received pre-operative limited field, low dose (less than 1000 cGy) external beam&#xA;  radiation therapy or endobronchial brachytherapy or laser therapy for short term&#xA;  control of hemoptysis or lobar obstruction; full dose pre-operative radiotherapy of&#xA;  curative intent is a cause for exclusion; patients may have received post-operative&#xA;  adjuvant platinum-based chemotherapy however non-platinum-based chemotherapy is a&#xA;  cause for exclusion&lt;/li&gt;&#xA;  &lt;li&gt;History of allergic reactions attributed to compounds of similar chemical or biologic&#xA;  composition to the agents used on this trial; patients with ongoing use of phenytoin,&#xA;  carbamazepine, barbiturates, rifampicin, or St John's Wort are excluded&lt;/li&gt;&#xA;  &lt;li&gt;Incomplete healing from previous oncologic or other major surgery&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I want to find documents with a query as below.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;'Patients with Eastern cooperative Oncology Group (ECOG) performance status &gt; 2' will be excluded&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Output &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CTA: NCT01572038 &#xA;Labels: ECOG 0, ECOG 1, ECOG 2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the simplest way to approach this problem?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a problem I would like to solve using machine learning. I would like to use some sort of classification to know if a just added change in a tree data structure is &quot;good&quot; or is &quot;bad&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say I have this tree:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        (A) &#xA;        / \\\\&#xA;       /   \\\\&#xA;     (B)   (C)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I apply a change to it (a &quot;good&quot; change, so the algorithm should associate this change with the &quot;good&quot; changes). The updated tree would be like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;       (A)&#xA;       / \\\\&#xA;      /   \\\\&#xA;    (D)   (C)&#xA;    /&#xA;   /&#xA; (B)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Added a certain node (D) above another node (B) would be classified as a &quot;good&quot; change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So when I have the learner with the correct data, the algorithm should be able to know that if I add a node of type D above a node of type B, it is a &quot;good&quot; change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to work with XML files that keeps the tree structure, a simple classifier like a naive bayes would not work, because it wouldn't be able to recognise if a node is added above another one, it only would be able to know that a node has been added.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know how which algorithm/technique I should use and I don't know how I should pass the data to the learner, because the context in this scenario is important.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best technique/algorithm to compare trees changes?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a dataset about users purchasing product from website. The attributes I have are  user id, region(state) of the user, the categories id of product, keywords id of product, keywords id of website, sales amount spent of the product. The goal is to use the information of product and website to identity who the users are, such as &quot;male young gamer&quot;;&quot;stay at home mom&quot;. I attached a sample picture as below.&lt;img src=&quot;https://i.stack.imgur.com/S7OSa.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;There are totally 1940 unique categories and 13845 unique keywords for products. For the website, there are 13063 unique keywords. The whole dataset is huge as that's the daily logging data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking of clustering, as those are unsupervised, but those id are ordered number having no numeric meaning, then I don't know how to apply the algorithm. I also think of classification if I add a column of class based on the sales amount of product purchased. I think clustering is more preferred. I don't know what algorithm I should use for this case as the dimensions of the keywords id could be more than 10000 (each product could have many keywords, so does website). I need to use Spark for this project. Can anyone help me out with some ideas,suggestions? Thank you so much!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm looking for a spatial index that can efficiently find the most extreme n points in a certain direction, i.e. for a given w, find x[0:n] in the dataset where x0 gives the largest value of w.x and x1 the second largest value of w.x, etc... . Is there a name for this type of query? What would be an efficient data structure to use? x might have around 20 dimensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thankyou!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am interested in modeling startup companies failure and success rates to describe what is the representative startup. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 40 companies in a dataset. Each company is represented as a list of all the investment financing rounds it has gone through. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Company 1: Seed Round, Series A Round&#xA;Company 2: Seed Round, Series A Round, Series B Round&#xA;Company 3: Seed Round, Series A Round, Series B Round&#xA;Company 4: Seed Round, Series A Round, Series B Round, Series C Round&#xA;Company 5: Seed Round&#xA;Company 6: Series A Round, Series B Round&#xA;Company 6: Series A Round&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can see that each round can be represented as a state in a Markov Chain, and transitions are only allowed from earlier stages to later stages. I can go from Seed to Series A, and from Series A to Series B, but not from Series B to Series A.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we have N-order markov chains (production data has N &amp;lt;= 4).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output I'm looking for is a binary tree chart showing each stage as a node and each node can either transition to the next node or to a final state meaning the company has failed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This problem can also be seen as a real options model...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas on how to implement this model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can code in Python or Ruby (but I am no expert).&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have completely followed the machine learning course on &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot; rel=&quot;nofollow&quot;&gt;coursera Machine Learning by professor Andrew Ng&lt;/a&gt; &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to put my knowledge to action. Some ideas that I have include : &lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Voice synthesis&#xA; -Voice recognition&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But since the course did not focus specifically on application of machine learning in these domains, could some one point me to some other course or books that can get me started.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am looking for a practical guide/tutorial preferably in R to show how to do gerrymandering. (I was looking for it also in CRAN but didn't find such package) Gerrymandering is the manipulation of the boundaries of electorial districts in order to gain political advantage for one party. If there is an analogy/similar process in another area it would be also interesting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have detailed historical  election results. I can see the results from each &quot;voting office&quot; and also the geographical area they cover. These results are aggregated to electorial district level. I'd like to see how moving voting offices to different electorial districts can influence the results of the election and how this process can be optimized for one party. Certainly there should be some constraints not to create really weird shaped districts at the end.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I went through &lt;a href=&quot;http://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared&quot; rel=&quot;nofollow&quot;&gt;this comparison of analytic disciplines&lt;/a&gt; and &lt;a href=&quot;http://www.win-vector.com/blog/2010/10/a-personal-perspective-on-machine-learning/&quot; rel=&quot;nofollow&quot;&gt;this perspective of machine learning&lt;/a&gt;, but I am not finding any answers on the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How is Data Science related to Machine learning? &lt;/li&gt;&#xA;&lt;li&gt;How is it &lt;strong&gt;not&lt;/strong&gt; related to Machine Learning? &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What's a good book to start learning Artificial Intelligence? &#xA;What field to learn first? What are the prerequisites? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Training a basic multilayer perceptron neural network boils down to minimizing some kind of error function. Often the sum of squared errors is chosen as a this error function, but where does this function come from?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I always thought this function was chosen because it makes sense intuitively. However, recently I learned that this is only partly true and there is more behind it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bishop wrote in one of his &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/downloads/bishop%20-%20nna%20-%20rsi94.pdf&quot; rel=&quot;nofollow&quot;&gt;papers&lt;/a&gt; that the sum of squared errors function can be derived from the principle of &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;maximum likelihood&lt;/a&gt;. Furthermore he wrote that the squared error therefore makes the assumption that the noise on the target value has a Gaussian distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not sure what he means with that. How does the sum of squared errors relate to the maximum likelihood principle in the context of neural networks?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;All distributions in the &lt;code&gt;gbm&lt;/code&gt; package in R are associated with a loss function. For example, when we set &lt;code&gt;distribution = 'binomial'&lt;/code&gt;, the loss function chosen internally is the logistic loss function. Can anyone explain how multi-class classification works with &lt;code&gt;gbm&lt;/code&gt; and the loss function that is being used for it i.e. when we set &lt;code&gt;distribution='multinomial'&lt;/code&gt;? Is it using one-vs-all or all-vs-all under the hood for doing its multi-class classification? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have customer data since 2013 and there is a file which has the customer unique id, a timestamp, and the reason for the call (a drop down from the person who handled the call).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some cumulative counts based on customer ID and the timestamp and I saw that one customer called in over 1000 times alone. What's the best way to make sense of the call driver data when I'm looking at millions of rows and around 200 categories of call types?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a broader topic which looks into 'downstream'  issues or predicting the probability of future calls or events?  The end goal would be to visualize these calling patterns and focus on reducing the call backs. This is a specific problem but it seems like it should be common and I can learn about addressing it in a bigger picture manner. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am working on a  classification problem. I have 1000+ features in this dataset. I don't know how to select the right variables/ features that can actually contribute to predicting the output. What are the different methods through which I can identify the important variables that can be used out of these 1000+ variables.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;While I was studying, few years ago , one of the most interesting topic was evolution, genetic algorithms and neural networks. Many of the problems I faced could be solved by using that knowledge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume that since that time world has found more interesting algorithms, do you recommend some books worthy of reading in that domain ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mainly I am looking a way to find patterns in huge amount of data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ex. Having energy consumption for few years for one building.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lets have an algorithm that is trying to find all possible repeatings in many variations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously at the beginning it should find that at the weekend energy consumption is less then average between Mon and Fri, but is that possible that an algorithm would tell me sth like this ? :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every Friday at third week in even months user sleeps for 3 hours and in uneven months 5 hours ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or the algorithm finds it self that user like to save energy so if previous month he sees that he spent more then usual , next month he is trying to spent less, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;or lets assume that user eats breakfast at work but if he eats it at home then he will stay whole day at home, then lets assume that he has off day, then check energy usage after breakfast if is high, which means user is preparing to leave or is small which means that he wants to sleep and basically stays home. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I was wondering if this possible to auto detect this kind of patterns ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am enthusiast of c# and interested in R.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm using &lt;a href=&quot;https://www.npmjs.com/package/brain&quot; rel=&quot;nofollow noreferrer&quot;&gt;Brain&lt;/a&gt; to train a neural network on a feature set that includes both positive and negative values. But Brain requires inputs that are between 0 and 1. What's the best way to normalize my data?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Consider a talent pool in which each member has some set of skills. Some of these talent are submitted to orders as potential candidates of which one is selected. It is reasonable to assume that the submitted talent have some dominant thing in common in their skill sets (let's call it a segment) that qualifies them for the order. Example segments are &quot;front end web-designer&quot; or &quot;brochure / sprint designer&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the total set of skills over all of the talent submitted to an order (like 2-5 with say 10 skills each, so 20 - 50 skills total), I am looking for the dominant segment. Then, I am looking for the dominant segment for each individual talent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My plan is to use latent Dirichlet allocation (LDA) such that the skills of all the talent submitted for an order are a &quot;document&quot; that contains some segments or &quot;topics&quot; with some probability. Likely, there will be one or two dominant topics depending on the total topic number. I will then use this model to predict the dominant segment for each talent where the individual talent skill set is a &quot;document&quot; with some segments or &quot;topics&quot; within.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious if anyone has feedback about my use of LDA or other ideas about how I might go about discovering these segments?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am looking for some hints on how to curate a list of stopwords. Does someone know / can someone recommend a good method to extract stopword lists from the dataset itself for preprocessing and filtering?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;a huge amount of human text input of variable length (searchterms and whole sentences (up to 200 characters) ) over several years. The text contains a lot of spam (like machine input from bots, single words, stupid searches, product searches ... ) and only a few % of seems to be useful. I realised that sometimes (only very rarely) people search my side by asking really cool questions. These questions are so cool, that i think it is worth to have a deeper look into them to see how people search over time and what topics people have been interested in using my website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My problem:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;is that i am really struggling with the preprocessing (i.e. dropping the spam). I already tried some stopword list from the web (NLTK etc.), but these don't really help my needs regarding this dataset. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your ideas and discussion folks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How important is linear algebra to being a data scientist? Are we talking college postgraduate level?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have data of the form :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Id1      A_Id2      B_Id2       C_Id2       D_Id2      E_Id2      F_Id2&#xA;1         6           3           9           23         20         5&#xA;1         4           7           8           9          11         56                                                   &#xA;1         2           36          98          73         2          4     &#xA;1         9           5           2           7          32         24           &#xA;1         14          7           5           9          12         5                                                   &#xA;2         34          4           7           10          7         12                                                       &#xA;2         5           57          23          91          4         6                                                    &#xA;2         7           .           .           .           .         .&#xA;2         3           .           .           .           .         .&#xA;2         .           .           .           .           .         .&#xA;.&#xA;.&#xA;100      .            .           .           .           .         .            &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to build a model such that I can get a best match of Id1, given top 5 Id2 matches of each attribute(A_Id2, B_Id2,...,F_Id2). Now every match should be computed keeping in mind A has highest priority, followed by B and C, followed by D and least priority to E and F. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the output will look like this :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Id1    Match_Id2&#xA;1        9&#xA;2        7&#xA;3        .&#xA;4        .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I hope the problem is clear, if not please ask.&#xA;How should I go about building a Machine Learning model for this? I was wondering if ranking algorithm will help ?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am not sure if &quot;minimize correlation&quot; is the right title for this issue but I could not find a better sentence to describe what I would like to achieve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say that I have a black box with multiple inputs and a single output. I know one of the inputs and the output and I have multiple example recordings of both. This known input modifies the output in a way that it is not desired, therefore, I would like to get rid of this &quot;noise&quot; caused by the known input. The transfer function for this input can be safely assumed as linear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I am doing right now, it is to loop through the example recordings, creating a linear regression model to predict the unwanted outcome and subtracting it from the real measured output signal, for each example. Afterwards, I compute the average of all the fixed output signals to reveal meaningful data beyond noise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This strategy seems to work according to the following plot:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/oAdyb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;X axis is the known input signal, Y axis is the output signal, blue and green dots represent the averaged data before and after applying the linear regression algorithm, respectively. Lines are the best fit for each data set. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see that the green line (&quot;cleaned&quot; dataset) has the smallest slope, meaning that the output variable is considerably less linearly correlated with the input than it was previously. Therefore, I assume that the regression technique explained before is working as expected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question, looking at the plot, is there any mathematical procedure to directly &quot;project&quot; the original dataset in a way that the correlation between the input and output variables is minimized? Is there any math trick to avoid the use of the regression technique on all the example datasets to obtain a similar result? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My written expression is not the best so please feel free to comment the question if you need further explanations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any code is welcomed but python (pandas, numpy, etc.) and Matlab are preferred. Theoretical explanations are also very welcomed. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;How do you assess the quality of your data? In data scientists' world, we come across several data. We often crunch numbers without formally assessing its quality due to various reasons. One such reason is we need to meet deadlines for reports &amp;amp; publications. I am wondering if anyone has adopted or come across a method/guidelines that help to find issues within the data (time-saving tips), so we can analyze the data efficiently. Please share your experience, tips, etc. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm using Google Analytics on my mobile app to see how different users use the app. I draw a path based on the pages they move to. Given a list of paths for say a 100 users, how do I go about clustering the users. Which algorithm to use? By the way, I'm thinking of using sckit learn package for the implementation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dataset (in csv) would look like this :  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DeviceID,Pageid,Time_spent_on_Page,Transition.&amp;lt;br&amp;gt; &#xA;ABC,Page1, 3s, 1-&amp;gt;2.&amp;lt;br&amp;gt;&#xA;ABC,Page2, 2s, 2-&amp;gt;4.&amp;lt;br&amp;gt;&#xA;ABC,Page4,1s,4-&amp;gt;1.&amp;lt;br&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the path, here is 1-&gt;2-&gt;4-&gt;1, where 1,2,4 are Pageids.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;While running rattle in my system I am getting this error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rattle()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error: attempt to apply non-function&#xA;In addition: Warning message:&#xA;In method(obj, ...) : Unknown internal child: selection&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using R version 3.1.0 (2014-04-10)&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am searching for pointers to algorithms for feature detection. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: all the answers helped me a lot, I cannot decide which one I should accept. THX guys!&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What I did:&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;For discrete variables (i.e. $D_i, E$ are finite sets) $X_i : \\\\Omega \\\\to D_i$ and a given data table &#xA;$$ &#xA;\\\\begin{pmatrix}{}&#xA;  X_1 &amp;amp; ... &amp;amp; X_n &amp;amp; X_{n+1} \\\\\\\\&#xA;  x_1^{(1)} &amp;amp; ... &amp;amp; x_n^{(1)} &amp;amp; x_{n+1}^{(1)} \\\\\\\\&#xA;  ... \\\\\\\\&#xA;  x_1^{(m)} &amp;amp; ... &amp;amp; x_n^{(m)} &amp;amp; x_{n+1}^{(m)} \\\\\\\\&#xA;\\\\end{pmatrix}&#xA;$$&#xA;(the last variable will be the 'outcome', thats why I stress it with a special index) and $X, Y$ being some of the $X_1, ..., X_{n+1}$ (so if $X=X_a, Y=X_b$ then $D=D_a, E=D_b$) compute&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$H(X) = - \\\\sum_{d \\\\in D} P[X=d] * log(P[X=d])$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$H(Y|X) = - \\\\sum_{d \\\\in D} {&#xA;             P[X=d] * \\\\sum_{e \\\\in E} {&#xA;               P[Y=e|X=d] * log(P[Y=e|X=d])&#xA;             }&#xA;           }$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where we estimate&#xA;  $$P[X_a=d] = |\\\\{j \\\\in \\\\{1, ..., m\\\\} : x_a^{(j)} = d\\\\}|$$&#xA;and analogously&#xA;  $$P[X_a=d \\\\cap X_b=e] = |\\\\{j \\\\in \\\\{1, ..., m\\\\} : x_a^{(j)} = d ~\\\\text{and}~ x_b^{(j)}=e\\\\}|$$&#xA;and then&#xA;  $$I(Y;X) = \\\\frac{H(Y) - H(Y|X)}{\\\\text{log}(\\\\text{min}(|D|, |E|))}$$&#xA; which is to be interpreted as the influence of $Y$ on $X$ (or vice versa, its symmetric).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: A little late now but still:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is wrong:&#xA;&lt;s&gt;Exercise for you: show that if $X=Y$ then $I(X,Y)=1$.&lt;/s&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is correct:&#xA;Exercise for you: show that if $X=Y$ then $I(X,X)=H(X)/log(|D|)$ and if $X$ is additionally equally distributed then $I(X,X)=1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For selecting features start with the available set $\\\\{X_1, ..., X_n\\\\}$ and a set 'already selected'$ = ()$ [this is an ordered list!]. We select them step by step, always taking the one that maximizes   $$\\\\text{goodness}(X) = I(X, X_{n+1}) - \\\\beta \\\\sum_{X_i ~\\\\text{already selected}} I(X, X_i)$$ for a value $\\\\beta$ to be determined (authors suggest $\\\\beta = 0.5$). I.e. goodness = influence on outcome - redundancy introduced by selecting this variable. After doing this procedure, take the first 'few' of them and throw away the ones with lower rank (whatever that means, I have to play with it a little bit). This is what is described in &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.7629&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For computing the $I$ for continuous variables one needs to bin them in some way. More concretely, the inventors of 'I' suggest to take the maximal value over binning $X$ into $n_x$ bins, $Y$ into $n_y$ bins and $n_x \\\\cdot n_y &amp;lt;= m^{0.6}$, i.e.  compute&#xA;  $$ \\\\text{MIC}(X;Y) = \\\\text{max}_{n_X \\\\cdot n_Y \\\\leq m^{0.6}} \\\\left( \\\\frac{I_{n_X, n_Y}(X;Y)}{log(\\\\text{min}(n_X, n_Y)} \\\\right)$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $I_{n_X, n_Y}(X;Y)$ means: compute the $I$ precisely as you did for discrete variables by treating $X$ as a discrete random variable after binning it into $n_X$ bins and analogously with $Y$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;===&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;ORIGINAL QUESTION&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;More precisely: I have a classification problem for one boolean variable, let's call this variable &lt;code&gt;outcome&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have lots of data and lots of features (~150 or so) but these features are not totally 'meaningless' as in image prediction (where every x and y coordinate is a feature) but they are of the form gender, age, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I did until now: from these 150 features, I guessed the ones that 'seem' to have some importance for the outcome. Still, I am unsure which features to select and also how to measure their importance before starting the actual learning algorithm (that involves yet more selection like PCA and stuff). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, for a feature &lt;code&gt;f&lt;/code&gt; taking only finitely many values &lt;code&gt;x_1, ..., x_n&lt;/code&gt; my very naive approach would be to compute some relation between&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;P(outcome==TRUE | f==x_1)&lt;/code&gt;, ..., &lt;code&gt;P(outcome==TRUE | f==x_n)&lt;/code&gt; and &lt;code&gt;P(outcome==TRUE)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(i.e. the feature is important when I can deduce more information about the coutcome from it than without any knowledge about the feature).&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;em&gt;Concrete question(s): Is that a good idea? Which relation to take? What to do with continuous variables?&lt;/em&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I'm sure that I'm not the first one ever wondering about this. I've read about (parts of) algorithms that do this selection in a sort-of automated way. Can somebody point me into the right direction (references, names of algorithms to look for, ...)?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a multi year dataset. Each time frame  of the data has different predictor importance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say for example, I am slicing the data into two partions as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;a dataset for the year 2014 (whole year)&lt;/li&gt;&#xA;&lt;li&gt;a 2015 Jan. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;When i look for the predictor importance, the predictor variables are different for both the partions. (1) Hence i am not able to arrive at one  unique decision tree which can explain the model better. (2). I am not able to train a model which can predict the new data correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there anything I am going wrong here.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I tried finding about exception lists in wordnet lemmatizers. &quot;Morphy() uses inflectional ending rules and exception lists to handle different possibilities&quot; which I read from &lt;a href=&quot;http://www.nltk.org/howto/wordnet.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nltk.org/howto/wordnet.html&lt;/a&gt; . Can you explain what is an exception list. Thank you.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to connect to hive from java but getting error. I searched in google but not got any helpfull solution. I have added all jars also.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is:-&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package mypackage;&#xA;import java.sql.SQLException;&#xA;import java.sql.Connection;&#xA;import java.sql.ResultSet;&#xA;import java.sql.Statement;&#xA;import java.sql.DriverManager;&#xA;&#xA;&#xA;public class HiveJdbcClient {&#xA;&#xA;&#xA;private static String driver = &quot;org.apache.hadoop.hive.jdbc.HiveDriver&quot;;&#xA;&#xA;public static void main(String[] args) throws SQLException, &#xA;       ClassNotFoundException {&#xA; Class.forName(&quot;org.apache.hadoop.hive.jdbc.HiveDriver&quot;);&#xA; try {&#xA; Class.forName(driver);&#xA; } catch (ClassNotFoundException e) {&#xA; e.printStackTrace();&#xA;  System.exit(1);&#xA;}&#xA;&#xA;Connection connect = DriverManager.getConnection(&quot;jdbc:hive://master:10000 /default&quot;, &quot;&quot;, &quot;&quot;);&#xA;Statement state = connect.createStatement();&#xA;String tableName = &quot;mytable&quot;;&#xA;state.executeQuery(&quot;drop table &quot; + tableName);&#xA;ResultSet res=state.executeQuery(&quot;ADD JAR /home/hadoop_home/hive/lib /hive-serdes-1.0-SNAPSHOT.jar&quot;);&#xA;res = state.executeQuery(&quot;create table tweets (id BIGINT,created_at     STRING,source STRING,favorited BOOLEAN,retweet_count INT,retweeted_status STRUCT&amp;lt;text:STRING,user:STRUCT&amp;lt;screen_name:STRING,name:STRING&amp;gt;&amp;gt;,entities STRUCT&amp;lt;urls:ARRAY&amp;lt;STRUCT&amp;lt;expanded_url:STRING&amp;gt;&amp;gt;,user_mentions:ARRAY&amp;lt;STRUCT&amp;lt;screen_name:STRING,name:STRING&amp;gt;&amp;gt;,hashtags:ARRAY&amp;lt;STRUCT&amp;lt;text:STRING&amp;gt;&amp;gt;&amp;gt;,text STRING,user  STRUCT&amp;lt;screen_name:STRING,name:STRING,friends_count:INT,followers_count:INT,statuses_count:INT,verified:BOOLEAN,utc_offset:INT,time_zone:STRING&amp;gt;,in_reply_to_screen_name STRING) ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe' LOCATION '/user/flume/tweets'&quot;);&#xA;&#xA;&#xA;String show = &quot;show tables&quot;;&#xA;System.out.println(&quot;Running show&quot;);&#xA;res = state.executeQuery(show);&#xA;if (res.next()) {&#xA;  System.out.println(res.getString(1));&#xA;}&#xA;&#xA;&#xA;String describe = &quot;describe &quot; + tableName;&#xA;System.out.println(&quot;Running describe&quot;);&#xA;res = state.executeQuery(describe);&#xA;while (res.next()) {&#xA;  System.out.println(res.getString(1) + &quot;\\\\t&quot; + res.getString(2));&#xA;}&#xA;&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting these errors:-&lt;/p&gt;&#xA;&#xA;&lt;p&gt;run:&#xA;SLF4J: Class path contains multiple SLF4J bindings.&#xA;SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: Found binding in [jar:file:/home/hadoop/lib/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: Found binding in [jar:file:/home/GlassFish_Server/glassfish/modules/weld-osgi-bundle.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: See &lt;a href=&quot;http://www.slf4j.org/codes.html#multiple_bindings&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.slf4j.org/codes.html#multiple_bindings&lt;/a&gt; for an explanation.&#xA;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/io/Writable&#xA;    at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:198)&#xA;    at org.apache.hadoop.hive.jdbc.HiveStatement.execute(HiveStatement.java:132)&#xA;    at org.apache.hadoop.hive.jdbc.HiveConnection.configureConnection(HiveConnection.java:133)&#xA;    at org.apache.hadoop.hive.jdbc.HiveConnection.(HiveConnection.java:122)&#xA;    at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:106)&#xA;    at java.sql.DriverManager.getConnection(DriverManager.java:571)&#xA;    at java.sql.DriverManager.getConnection(DriverManager.java:215)&#xA;    at dp.HiveJdbcClient.main(HiveJdbcClient.java:35)&#xA;Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Writable&#xA;    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)&#xA;\\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)&#xA;    at java.security.AccessController.doPrivileged(Native Method)&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)&#xA;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)&#xA;    ... 8 more&#xA;Java Result: 1&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a dataset which contains information about when do people enter and leave a premise. I have the following information in the dataset : &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Person Id&lt;/li&gt;&#xA;&lt;li&gt;Time of Entry&lt;/li&gt;&#xA;&lt;li&gt;Time of Leaving&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The dataset has around 50 unique persons. Each person will have multiple entries corresponding to multiple visits. The data spans over a year so I have quite a lot of entries (around 1 million).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These people can be classified on the basis of the department they work under (2 departments - mutually exclusive) or on basis of  their role (4 possible roles - all mutually exclusive)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering what kind of data analysis can be done with this kind of dataset. I am not looking for straight-forward things like &quot;who spent the most time in building&quot;. However things like finding correlation between visits of 2 people would be interesting. So if person A visits the premise, what is the probability that person B would also visit. Since I have only around 50 unique visitors, I think such an analysis is feasible. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another line of thought was to apply some interval-pattern mining techniques but I am not much familiar with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone give me some pointers/ideas about what kind of data products can be build using this or what kind of techniques can be used with such data.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit : As discussed in comments, I call it a product in the sense that I do not want some simple or trivial analysis. And I am not looking for any commercially viable idea - just some cool fun idea :)&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am doing load forecasting using SVR(kernel='rbf').How can I understand which is the best value for parameters C, epsilon and gamma?Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am currently selecting features of products by using LDA to group 6000 keywords of product into topics. &#xA;Here is the sample of my dataset after being organized into list of keywords for each product id.&#xA;&lt;img src=&quot;https://i.stack.imgur.com/H6Fhh.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;I consider each id as a &quot;document&quot; and each keywords as the &quot;word&quot; in a &quot;document&quot; for the case of LDA.&#xA;It didn't work out as I expected as each topic have many identical keywords with different weight. I removed 100 most common keywords but there are still some identical keywords in the topics. Here is the sample output:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/kOR8W.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I deal with the identical keywords in my topics? and also how to deal with the 100 most common keywords I removed?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the graph showing the frequency of the words in all &quot;documents&quot;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/gbMUC.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;Each word only presents once in each document, but may present in different documents. I updated a new graph of the frequency of the words&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you so much. Any suggestion is appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Problem:&#xA;Lets say we have an irreducible Markov chain.&#xA;Given a failure state or non desirable state F and a current state S&#xA;Is it possible to find how far we are from the failure state F.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: if we try to model an engine failure using Markov process and using historic data we have a state transition matrix P, one of which is a state where the engine was failed .&#xA;Now , in a dynamic system, if the current state S and a state transition matrix are given, can we calculate how far we are from failure state F.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;x(n+1) = (x)P^n&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right hand side of the equation gives us a vector v which consists of probabilities of transitioning to different states at stage n.&#xA;With different value of n, the values in this vector v will change&#xA;one of the values in v will be the probability of going to state F.&#xA;I want to find for what n , transitioning probability to F is maximum.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have no idea whether this is the right StackExchange flavor to post this question in, but here goes:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an agent (an IRC bot that listens to an event stream) that I would like to add &quot;intelligent notifications&quot; to. Let me give a specific example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bot monitors the results from our continuous integration system, so it basically sees a stream of test results associated with changes (and the changes are associated with users.) If I ask it to notify me of interesting events, then for each event it sees, it decides whether to tell me about it (or rather, about the current results so far.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like it to decide based on how interesting something is for me. Something is interesting if (1) it is a result of a change that I pushed to the continuous integration system, (2) it is a test failure, and (3) the information conveyed by that failure is a significant indicator of whether my change was bad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tests have a baseline probability of failing for no good reason (&quot;intermittent failures&quot;). If one test fails, it does not necessarily mean my change was bad. It might just be a flaky test. (Any given push tends to result in a couple bogus failures, so this isn't some obscure edge case.) But the baseline probability of an intermittent failure varies according to the test suite running (we have several dozen different test suites that fire off for every change pushed.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I don't want to be bothered with reports of failing tests that are probably just noise. And if my push &lt;em&gt;is&lt;/em&gt; bad, I don't want to be flooded with notifications for every failed test. (If I break something, it'll probably show up in multiple test suites. So if the agent sees two failures and 20 successes, maybe it won't bother me, but if it then sees another failure or two it should.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am imagining I could look at the change between the prior and posterior probabilities and use that to estimate the entropy of that new test result, and perhaps also compute my personal entropy (if that makes any sense -- as in, the bot should assume I don't know what any of the results are until it notifies me, at which point it should assume I am aware of the current full set of results and not bother me again until enough additional results have come in to substantially change the probability estimate of my change being bad.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or something like that. I'm really looking for the right mathematical framework to compute things like this. I know next to nothing about machine learning or... well, mathematics in general.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(The above is simplified; I would additionally like to do crazy things like take into account &quot;labeling&quot;, where a third party looks at the test failures and decides whether they are real failures or not. But the time between the failure coming in and when that person labels it for me can also be modeled by a distribution, and I'd like to hold off notifications for interesting results if there's a good chance that this other person may tell me that the result is not interesting after all. But only if that's going to happen &quot;soon&quot;. Also, we have an automated system for guessing whether something &lt;em&gt;might&lt;/em&gt; be an intermittent vs real failure, and it'd be nice to take its opinion into account as well.)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Suppose I have a set of data(with  2 diemensional feature space), and I want to obtain clusters from them. But I do not know how many clusters will be formed.&lt;br&gt;&lt;br&gt; Yet I want separate clusters(The number of clusters is more than 2). &lt;br&gt;&lt;br&gt;I figured that k means of k medoid cannot be used in this case. Nor can I use hierarchical clustering. Also since there is no training set hence cannot use Knn classifier to any others(supervised learning cannot be used as no training set). I cannot use OPTICS algorithm as I do not want to specify the radius(I dont know the radius)&lt;br&gt;&lt;br&gt; Is there any machine learning technique that would give me multiple clusters(distance based clustering) that deals well with outlier points too? &lt;img src=&quot;https://i.stack.imgur.com/dAimG.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;This should be the output&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Each observation in my data was collected with a difference of 0.1 seconds. I don't call it a time series because it don't have a date and time stamp. In the examples of clustering algorithms (I found online) and PCA the sample data have 1 observation per case and are not timed. But my data have hundreds of observations collected every 0.1 seconds per vehicle and there are many vehicles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note: I have asked this question on quora as well.&lt;/em&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;How widely is Theano used in deep learning research? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is Theano a good start to learn the implementation of machine learning algorithms?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Will learning the implementation of something like a feed forward network really help? Do graduate students implement neural networks or other algorithms at least once during their college days?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a reasonable idea about feed forward and recurrent networks, backpropagation, the general pipeline for a machine learning problem and the necessary mathematics. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I need to do some experimenting with Brown clustering, graph partitioning, agglomerative clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Are there Python/Matlab libraries for that? I know sklearn.cluster but it doesn't have algorithms I need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is it possible to install graphical interface for Cluto on Mac OS? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) Overall, are there useful tutorials on using Cluto?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) Other software for clustering that I could learn within a couple of hours?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to paste a &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;int&lt;/code&gt; from &lt;code&gt;map&lt;/code&gt; in &lt;code&gt;Hive&lt;/code&gt; to an &lt;code&gt;array&lt;/code&gt;.&#xA;For now, record looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;string1&quot;:1,&quot;string2&quot;:1,&quot;string3&quot;:15}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a way to convert it to an array like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[&quot;string1:1&quot;,&quot;string2:1&quot;,&quot;string3:15&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;We are developing a classification system, where the categories are fixed, but many of them are inter-related. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, we have a category called, &quot;&lt;code&gt;roads&lt;/code&gt;&quot; and another one called &quot;&lt;code&gt;traffic&lt;/code&gt;&quot;. We believe that the model will be confused by the text samples, which could be in &lt;code&gt;roads&lt;/code&gt; category and also in &lt;code&gt;traffic&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of our text samples are suitable for multi class labelling too.  For example, &quot;There is a garbage dump near the footpath. The footpath is broken completely&quot;. This text could be categorized into &lt;code&gt;garbage&lt;/code&gt; bucket or &lt;code&gt;footpath&lt;/code&gt; bucket. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are going to build a training set for this classifier, by manually annotating the text. So, can we put multiple labels for one issue? How should we deal with  text with multiple labels for it? Should they be added into all categories to which it is tagged to, as training sample ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, &quot;There is a garbage dump near the footpath. The footpath is broken completely&quot;. This text could be categorized into &lt;code&gt;garbage&lt;/code&gt; bucket or &lt;code&gt;footpath&lt;/code&gt; bucket. So, should this text be added as a training sample for &lt;code&gt;garbage&lt;/code&gt; and &lt;code&gt;footpath&lt;/code&gt;? How should we consider the labels?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please give your insights?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am solving a rare event cum classification problem. I have come across  a package called &lt;strong&gt;SMOTEBoost&lt;/strong&gt; which oversamples the rare event and boost the results. But I'm not sure is that supported in R. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you please help me how can I use SMOTEBoost in R? Any examples?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I want to learn how a spam email detector is done. I'm not trying to build a commercial product, it'll be a serious learning exercise for me. Therefore, I'm looking for resources, such as existing projects, source code, articles, papers etc that I can follow. I want to learn by examples, I don't think I am good enough to do it from scratch. Ideally, I'd like to get my hand dirty in Bayesian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there anything like that that? Programming language isn't a problem for me.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Given some dataset for prediction, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;for eg say I have different housing price prediction dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  1 : 100 training and 100 testing sample, 50 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  2 : 100 training and 100 testing sample, 120 feature &lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  3 : 1000 training and 1000 testing sample, 50 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  4 : 1000 training and 1000 testing sample, 5000 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;how should I choose the best methods for estimating the unknown parameters ( predict price) in a linear regression model from the following for each of these dataset?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Ordinary least squares&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Stepwise regression&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Principal component regression &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Partial least squares regression&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Should I experiment with each of these one by one and compare the results or is there any rule of thump on when to use each of them based on the dataset ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;sorry if this question is out of place. I'm a begginer to machine learning, and I have use for a technique, and I don't even know where to look. The problem is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I have 5 features which are real valued (Parameters in a deterministic simulation). &lt;/li&gt;&#xA;&lt;li&gt;This features determine two aspects of the instance (model solution). Its feasibility (binary) and some measure of likelihood given certain experimental data (only for the instances that achieved feasibility). &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Since I want to avoid generating &quot;infeasible&quot; combination of features, what I devised was an algorithm that iteratively does the following:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generate Nc candidate feature vectors&lt;/li&gt;&#xA;&lt;li&gt;Evaluate Feasibility and Likelihood for each&lt;/li&gt;&#xA;&lt;li&gt;Find linear combination of features that involves a compromise between least amount of features / holds largest cluster of feasibility. Add this as constraints to the feature vector generation. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In short, it detects and iteratively refines &quot;simple&quot; constraints that once added to the feature vector generation &quot;guarantee&quot; its feasibility to save computational time evaluating combination of parameters that lead to infeasible models. Afterwards, they could be tested by inverting them and looking for other &quot;regions&quot; (if any) of the feature vector where the model is feasible.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any name of techniques and references I might look for ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am trying to use a Random Forest Model (Regression Type) as a substitute of logistic regression model. I am using R - randomForest Package. I want to understand the meaning of Importance of Variables (%IncMSE and IncNodePurity) by example. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I have a population of 100 employees out of which 30 left the company. &#xA;Suppose in a particular decision tree, population is split by an attribute (say location) into two nodes. One node contains 50 employees out of which 10 left the company and other contains 50 employees from which 20 left the company. Can someone demonstrate me a calculation of %IncMSE and IncNodePurity. (if Required for averages etc., please consider another decision tree)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may look like a repeated question but I could not find a worked out example.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to determine which site in our organization is in greater need of upgrades to SEP 12, so when I run a query to count, I get these type of numbers&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Group       Windows_SEP_11  Mac_SEP_11  Windows_SEP_12  Mac_SEP_12&#xA;Arizona\\\\A   417                  29              219         6&#xA;Arizona\\\\B   380                  20              282        15&#xA;Arizona\\\\C   340                  30              383        507&#xA;Arizona\\\\D   310                  104             186        857&#xA;Arizona\\\\E   307                  74              403        243&#xA;Arizona\\\\F   285                  171             522        14&#xA;Arizona\\\\G   269                  1               559        41&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, when I find percentages, I get these numbers&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Group          Win_Sep_11_%   Mac_SEP_11_%  Windows_SEP_12_%    Mac_SEP_12_%&#xA;Boston/Site 1   100               0                0               0&#xA;Boston/Site 2   100               0                0               0&#xA;Boston/Site 3   94                0                0               5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And obviously, percentage isn't good indicator because Boston/Site 1 has only 3 computers, Boston/Site 2 only has 4 computers, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best way to analyze data? I ultimately need a visual of sites that have&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;many computers, and&lt;/li&gt;&#xA;&lt;li&gt;a great need for upgrades to SEP 12, i.e. if there are more computers with SEP 11 than SEP 12.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Please point me in the right direction.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am trying to train an artificial neural network with two convolutional layers (c1, c2) and two hidden layers (c1, c2). I am using the standard backpropagation approach. In the backward pass I calculate the error term of a layer (delta) based on the error of the previous layer, the weights of the previous layer and the gradient of the activation in respect to the activation function of the current layer. More specifically the delta of layer l looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;delta(l) = (w(l+1)' * delta(l+1)) * grad_f_a(l)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am able to compute the gradient of c2, which connects into a regular layer. I just multiply the weights of h1 with it's delta. Then I reshape that matrix into the form of the output of c2, multiply it with the gradient of the activation function and am done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I have a the delta term of c2 - Which is a 4D matrix of size (featureMapSize, featureMapSize, filterNum, patternNum). Furthermore I have the weights of c2, which are a 3D matrix of size (filterSize, filterSize, filterNum). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;With these two terms and the gradient of the activation of c1 I want to calculate the delta of c1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Long story short:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the delta term of a previous convolutional layer and the weights of that layer, how do I compute the delta term of a convolutional layer?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am having 'hour' field as my attribute, but it takes a cyclic values. How could I transform the feature to preserve the information like '23' and '0' hour are close not far. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way I could think is to do transformation: &lt;code&gt;min(h, 23-h)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Input: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]&#xA;&#xA;Output: [0 1 2 3 4 5 6 7 8 9 10 11 11 10 9 8 7 6 5 4 3 2 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any standard to handle such attributes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: I will be using superviseed learning, to train random forest classifier!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to create and train a model which classifies a new text content into finance, programming, analytics, design etc. Where can I get a relevant dataset to train my models? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;TIA. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am new to practicing NLP and most topics related, but I want to make a program that can gather and extract data for me on its own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more specific, I want to tell the program &quot;I want more information on this topic(i.e heart attacks)&quot;, and then the program shall find, gather and extract meaningful texts on the topic from around the www.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I happen to live in Norway, which means that most interesting data will be in English, but I also want to fetch interesting data found in Norwegian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One challenge is the differences in stop words. For instance, &quot;are&quot; and &quot;and&quot; are both stop words in English and subjects in Norwegian.&#xA;Other challenges are also likely to occur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So my question is: Would I need to create separate algorithms for every natural language to be interpreted?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a large dataset with 9m JSON objects at ~300 bytes each. They are posts from a link aggregator: basically links (a URL, title and author id) and comments (text and author ID) + metadata.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They could very well be relational records in a table, except for the fact that they have one array field with IDs pointing to child records.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What implementation looks more solid?&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;JSON objects on a PostgreSQL database (just one large table with one column, namely the JSON object)&lt;/li&gt;&#xA;&lt;li&gt;JSON objects on a MongoDB&lt;/li&gt;&#xA;&lt;li&gt;Explode the JSON objects into columns and use arrays on PostgreSQL&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I want to maximize performance in joins, so I can massage the data and explore it until I find interesting analyses, at which point I think it will be better to transform the data into a form specific to each analysis.&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a data set of questions belonging to ten different categories namely (definitions, factoids, abbreviations, fill in the blanks, verbs, numerals, dates, puzzle, etymology and category relation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The categories are briefly described as follows: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Definition – A question that contains a definition of the answer. &lt;/li&gt;&#xA;&lt;li&gt;Category Relation – The answer has a semantic relation to the question where the relation is specified in the category.&lt;/li&gt;&#xA;&lt;li&gt;FITB – These are generic fill in the blank questions – some of them ask for the completion of a phrase. &lt;/li&gt;&#xA;&lt;li&gt;Abbreviation – The answer is an expansion of an abbreviation in the question. &lt;/li&gt;&#xA;&lt;li&gt;Puzzle – These require derivation or synthesis for the answer. &lt;/li&gt;&#xA;&lt;li&gt;Etymology – The answer is an English word derived from a foreign word. &lt;/li&gt;&#xA;&lt;li&gt;Verb – The answer is a verb. &lt;/li&gt;&#xA;&lt;li&gt;Number – The answer is a numeral. &lt;/li&gt;&#xA;&lt;li&gt;Date – The question asks for a date or a year. &lt;/li&gt;&#xA;&lt;li&gt;Factoid – A question is a factoid if its answer can be found on Wikipedia. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I used the Stanford core NLP package called shiftreducer to find out the Part-Of-Speech (POS) values for each question in a category. I thought of using this POS pattern as a discriminant among the classes but it turned out to be generalized since: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All the classes follow a similar pattern&lt;/li&gt;&#xA;&lt;li&gt;Nouns top the POS count followed by Determinants, Prepositions, Adjectives, Plural nouns and finally verbs.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What could be the other ways in which I could differentiate among the question categories? Or as my question was in its first place, &quot;What kind of features do I select for efficient categorization?&quot;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I want to make a 3D scatter plot of multiple data selections on a single plot (i.e same axes). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that in 2D this is possible by using par() function like so: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;plot(6:25,rnorm(20),type=&quot;b&quot;,xlim=c(1,30),ylim=c(-2.5,2.5),col=2)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;par(new=T)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;plot(rnorm(30),type=&quot;b&quot;,axes=F,col=3)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;par(new=F)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;(source: &lt;a href=&quot;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I do something like that on a 3D plot, preferably an interactive 3D plot, like the ones created using plot3D from 'rgl' package? &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am new to R Programming and just learned basics through codeschool.com&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our network spans the city, and it is divided into districts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to create a map that assigns a value (based on ratio of outdated software and new software) to each district.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This website has sample of 3-D maps that were created by R Programming, and I see one I am very interested in replicating, but for our city only. &lt;img src=&quot;https://i.stack.imgur.com/Q1HTK.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.rforscience.com/portfolio/koppen-geiger/&quot; rel=&quot;nofollow noreferrer&quot;&gt;But when I see the source code&lt;/a&gt;, I don't see any mention of latitude or longitude. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My head is spinning, trying to figure out how I will input this, i.e latitude and longitude of a district in our city, versus an assigned ratio, which I believe will be read from a spreadsheet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any guidance.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am looking into creating a model to predict whether an item is &quot;Very Good&quot;, &quot;Good&quot;, &quot;Bad&quot; or &quot;Very Bad&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I fit the training data to the models, comparing the accuracy of the models during test stump me: should it matter if a model misclassified a G to VG while the other G to VB? What about a model that has two misclassifications of one level away versus another model with only one misclassification but three levels away (eg VG to VB)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any guideline on what is the common approach? Also, my thinking at the moment is that this should be a regression problem, but I'm happy to be corrected if I should approach this labeling of datasets more as a classification problem.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;What are the general assumptions of a Random Forest Model? I could not find by searching online. For example, in a linear regression model, limitations/assumptions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;It may not work well when there are non-linear relationship between dependent and independent variables. &lt;/li&gt;&#xA;&lt;li&gt;It may not work if the dependent variables considered in the model are linearly related. Therefore one has to remove correlated variable by some other technique. &lt;/li&gt;&#xA;&lt;li&gt;It assumes that model errors are uncorrelated and uniform (No hetroscedasticity). &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Are there any assumptions/limitations on similar lines.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I just started using machine learning and was wondering if anybody have cool ideas for a Startup project, I've seen this website &lt;a href=&quot;http://treato.com&quot; rel=&quot;nofollow&quot;&gt;Treato&lt;/a&gt;  and was amazed by it. &#xA;Thanks.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have an algorithm which have as an input about 20-25 numbers. Then in every step it uses some of these numbers with a random function to calculate the local result which will lead to the final output of A, B or C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since every step has a random function, the formula is not deterministic. This means that with the same input, I could have either A, B or C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first thought was to take step by step the algorithm and calculating mathematically the probability of each output. However, it is really difficult due to the size of the core.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My next thought was to use machine learning with supervised algorithm. I can have as many labeled entries as I want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have the following questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How many labeled inputs should I need for a decent approach of the probabilities? Yes, I can have as many as I want, but it needs time to run the algorithm and I want to estimate the cost of the simulations to gather the labeled data.&lt;/li&gt;&#xA;&lt;li&gt;Which technique do you suggest that works with so many inputs that can give the probability of the three possible outputs?&lt;/li&gt;&#xA;&lt;li&gt;As an extra question, the algorithm run in 10 steps and there is a possibility that some of the inputs will change in one of the steps. My simple approach is to not include this option on the prediction formula, since I have to set different inputs for some of the steps. If I try the advanced methods, is there any other technique I could use?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;&quot;Knowledge&quot; is crucial within several fields like Knowledge Discovery, Knowledge Distraction, Natural Language Processing, Data Mining, Big Data, etc etc etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is the definition of knowledge within these fields?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there 1 common definition, or does it depend on the exact context?&lt;/strong&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am working on a stock market decision system. I have currently centered on gradient boosting as the likely best machine learning solution for the problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have 2 fundamental issues with my data owing to it being from the stock market having to do with it not being IID. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, because of the duration of average in some indicators use, some data-points are highly correlated. For example, the 2-year trailing return of a stock is not very different if measured a month ago. My understanding is that this requires a sampling (for ensembles) where I choose datapoints that are &quot;far away&quot; in time to make trees more independent. From what I can tell so far, Matlab does not have functionality to pick a random subspace with this criteria. When I was previously thinking of using simple bagging, I figured I would just build the trees myself from custom subspaces and aggregate them into an ensemble, but this won’t work if I want to do gradient boosting. Now, on this point I am not totally sure that it is so critical to have samples “far away.” My intuition is that it is better if they are, but even if they are not perhaps by right-sizing the percent of data sampled and having enough trees it gives the same result. I would love any insight on that issue and how I might be able to use LSboost in matlab on custom samples.  (One idea I have is to just to create a small number, like 5-10 custom samples, use LS-Boost on each, and then average them.) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second fundamental problem is that data from a given stock is correlated/related to itself. I realized after thinking about it that this is of critical importance. Consider, it would likely be better, if there is enough data, to make a prediction for stock A from training data only or mostly from stock A than to use the entire market. Thus, I had been thinking of a “system” where I train on stock-specific data, stock-group data (where I use a special algorithm to group stocks), and the entire market, and then use a calculation (I can elaborate if interested) that determines which of these models is more likely to give the better result. If the input looks very different from the stock-specific training data, for example, then it will use the group or entire market. I am pretty convicted that some form of taking into account which stock the system is looking at is important to optimizing performance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, on the second issue the question is what is the best way to organize this. Thinking naively, it would be great to simply feed categories to the predictor that indicate what stock it is looking at. However, my belief here from what I know about these algorithms is that this will have poor results on new data, because this predictor will assume that it has seen the full universe of potential outcomes for each stock, when many times this isn’t the case. (Say there is a stock with only a one year history with a big rally – the system will think the rally will continue regardless of how different the new data looks). So I feel like I have to do something like in the previous paragraph. I don’t know if there is some way for the system to “automatically” recognize when new data is sufficiently similar to stock-specific data to focus on a stock-specific prediction vs. when it is different and it should go to the default system with multiple stocks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have any insights on these issues and/or how to address them in Matlab or otherwise, I would very much appreciate. Thanks in advance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best,&#xA;Mike&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have few points in $S^n$, i.e., the $n$-dimensional unit sphere embedded in $\\\\mathbb{R}^{n+1}$, and I would like to project them down to $S^2$, i.e., the 2-dimensional unit sphere (embedded in $\\\\mathbb{R}^3$) to visualize it with the constraint that neighboring points should be close by. I spent some time playing with t-sne but of course, the points no longer lie on $S^2$. I normalized the projections but that introduces weird distortions, for instance, if the variance of one dataset is very small in $S^n$ as compared to other, I expect the same to hold in their $S^2$ projections; that is not the case upon normalizing t-sne. Any ideas? I would really like something that makes the previous statement hold.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;The Vowpal Wabbit (VW) apparently supports sequence tagging functionality via &lt;a href=&quot;http://www.umiacs.umd.edu/~hal/searn/&quot; rel=&quot;nofollow noreferrer&quot;&gt;SEARN&lt;/a&gt;. The problem is that I cannot find anywhere detailed parameter list with explanations and with some examples. The best I could find is &lt;a href=&quot;http://zinkov.com/posts/2013-08-13-vowpal-tutorial/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Zinkov's blog entry&lt;/a&gt; with a very short example. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Vowpal_Wabbit&quot; rel=&quot;nofollow noreferrer&quot;&gt;main wiki page&lt;/a&gt; barely mentions SEARN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the checked out source code I found demo folder with some NER sample data. Unfortunately, the script running all the tests does not show how to run on this data. At least it was informative enough to see what is the expected format: almost the same as standard VW data format, except that entries are separated by blank lines (this is important).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current understanding is to run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat train.txt | vw -c --passes 10 --searn 25 --searn_task sequence \\\\&#xA;--searn_passes_per_policy 2 -b 30 -f twpos.vw&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn 25&lt;/code&gt; - the total number of NER labels (?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn_task sequence&lt;/code&gt; - sequence tagging task (?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn_passes_per_policy 2&lt;/code&gt; - not clear what it does&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other parameters are standard to VW and need no additional explanation. Perhaps there are more parameters specific to SEARN? What is their importance and impact? How to tune them? Any rules of thumb?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers to examples will be appreciated.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a number of weather stations, and I know their positions. I would like to interpolate measurements from them for various other positions as a weighted average of these stations, but of course I need weights for this. I am thinking the most logical choice here from a physics point of view will be weighting by the inverse of the distance to the station, but I haven't quite convinced myself that that is right, and am not sure if I should maybe be using the distance squared, or maybe something in between.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any comments? Are there any other reasonable alternatives I should be considering?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm not sure if this is more appropriate for SO or DS in Stack Exchange since technically it's not about coding: in &lt;code&gt;caret&lt;/code&gt; package for training in R, it's possible to train the model using &lt;code&gt;rpart&lt;/code&gt; or &lt;code&gt;rpart2&lt;/code&gt; as the method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that &lt;code&gt;rpart&lt;/code&gt; is an implementation of CART. What is &lt;code&gt;rpart2&lt;/code&gt; and how is it different from &lt;code&gt;rpart&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My eventual aim is actually to compare the difference between the tree generated by &lt;code&gt;rpart&lt;/code&gt; and &lt;code&gt;rpart2&lt;/code&gt;, because my result seems to imply &lt;code&gt;rpart2&lt;/code&gt; has better accuracy for my dataset, but I have no clue how to view the &lt;code&gt;rpart2&lt;/code&gt; tree.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have a dataset of clients (their city, name, age, gender, number of children) and another dataset about the products that they have bought. i have been asked to do:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;extract knowledge about client profiles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know what knowledge should I extract.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;we are studying clustering and classification so they should connect to the question. what i thought about is to make clusters of clients. but i don't know what criterisas should I depend on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;should I just use a clustering algorithm like k means and let it give me the clusters ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;any suggestion would be appreciated&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Update&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;These are the dataset that I have:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Card: CardID, City, Region, PostalCode, CardStartDate, Gender, DateOfBirth, MaritalStatus, HasChildren, NumChildren, YoungestChild&lt;/li&gt;&#xA;&lt;li&gt;Item: ItemCode, ItemDescription, CategoryCode, SubCategoryCode, BrandCode, UpmarketFlag&lt;/li&gt;&#xA;&lt;li&gt;Transaction: Store, Date, Time, TransactionID, CardID, PaymentMethod&lt;/li&gt;&#xA;&lt;li&gt;Category: CategoryCode, CategDescription&lt;/li&gt;&#xA;&lt;li&gt;Transaction_Item: Store, Date, Time, TransactionID, ItemNumber, ItemCode, Amount&lt;/li&gt;&#xA;&lt;li&gt;SubCategory: SubCategoryCode, SubCategDescrip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The teacher said that we &lt;strong&gt;should&lt;/strong&gt; categories the clients and then indicate which products best suit for each customer.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a Rare event classification problem. I Have 95% of the data as a majority class and 5% of the data as the minority class. I use classification trees algorithm. I am measuring the goodness of the model using confusion matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the i have the minority class just 5% of the total data, even though my prediction performance of minority class is close to 70%, the total number of errors are high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, here is my confusion matrix.&#xA;             0           1&#xA;     0     213812      7008&#xA;     1     29083       16877&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though the Minority class(class 1) has predicted 16877 times correctly(70% and the misclassifcation is just 30%, but the absolute value of the misclassifcation is very high(29083) comparing to the correctly predicted minotriy class (16877). Which makes the solution less usable for the business. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any idea on handling these kind of  issues in such rare event modelling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Kind note: I have  balanced the target variable using the SMOTE algorithm before applying Classification tree.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;What are xml datasets? Is it possible to convert them to csv files?&#xA;I'm working on a Java program and I sometimes download datasets wich are in a binary format, are those xml? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a classification problem. I have a dataset containing equal number of categorical variables and continuous variables. How will i know what technique to use? between a decision tree and a logistic regression?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it right to assume that logistic regression will be more suitable for continuous variable and decision tree will be more suitable for continuous + categorical variable?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a qualitative variable, e.g. &lt;code&gt;userId&lt;/code&gt;, which could take around 30,000 different coded values ($k$). I would like to represent this variable as a dummy variable. Coding this into a vector of size $k$ doesn't seem to be a good approach. Is there a more compact method for coding for this variable?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm wondering if programmers tend to use AI APIs. And if so, what are they like? And where can I find a nice one for Java?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm not a NLP guy and I have this question. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a text dataset containing terms which go like, &quot;big data&quot; and &quot;bigdata&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my purpose both of them are the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I detect them in NLTK (Python)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or any other NLP module in Python?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am a beginner on Machine Learning.&#xA;In SVM, the separating hyperplane is defined as $y = w^T x + b$.&#xA;Why we say vector $w$ orthogonal to the separating hyperplane?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;New to R. In my example, my customers have restricted allocation of budget for Milk. I have more than 5 brands of milk in my store. Here my objective is how I know my customer is shifting from one brand to other? (Example: Customer is replacing Brand 1 with Brand 2 in my time series data). I would like to compute that shifting pattern every quarter and observe the trend quarter by quarter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Quarter Data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Date    Milk-Brand1 Milk-Brand2 Milk-Brand3 Milk-Brand4 Milk-Brand5&#xA;&#xA;1/1/2015    200 140 190 220 150&#xA;1/2/2015    204 138 195 226 144&#xA;1/3/2015    208 136 200 232 126&#xA;1/4/2015    212 134 205 238 108&#xA;2/2/2015    216 132 210 244 90&#xA;1/6/2015    220 130 215 250 72&#xA;1/7/2015    224 128 220 256 54&#xA;1/8/2015    228 126 225 262 36&#xA;1/9/2015    232 124 230 268 18&#xA;3/1/2015    236 122 235 274 0&#xA;3/2/2015    240 120 240 280 13&#xA;3/3/2015    244 118 245 286 33&#xA;3/4/2015    248 116 250 292 15&#xA;20/3/2015   252 114 255 298 33&#xA;20/3/2015   256 112 260 304 15&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you suggest compute correlation between each 'brand of milk' and compare those correlations from one quarter to other quarter? Or Cross-correlation? Or others? I am open.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your advice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the data via &lt;code&gt;dput&lt;/code&gt; for anyone wanting it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;structure(list(Date = structure(c(1420070400, 1422748800, 1425168000, &#xA;1427846400, 1422835200, 1433116800, 1435708800, 1438387200, 1441065600, &#xA;1420243200, 1422921600, 1425340800, 1428019200, 1426809600, 1426809600&#xA;), tzone = &quot;UTC&quot;, class = c(&quot;POSIXct&quot;, &quot;POSIXt&quot;)), Milk.Brand1 = c(200L, &#xA;204L, 208L, 212L, 216L, 220L, 224L, 228L, 232L, 236L, 240L, 244L, &#xA;248L, 252L, 256L), Milk.Brand2 = c(140L, 138L, 136L, 134L, 132L, &#xA;130L, 128L, 126L, 124L, 122L, 120L, 118L, 116L, 114L, 112L), &#xA;    Milk.Brand3 = c(190L, 195L, 200L, 205L, 210L, 215L, 220L, &#xA;    225L, 230L, 235L, 240L, 245L, 250L, 255L, 260L), Milk.Brand4 = c(220L, &#xA;    226L, 232L, 238L, 244L, 250L, 256L, 262L, 268L, 274L, 280L, &#xA;    286L, 292L, 298L, 304L), Milk.Brand5 = c(150L, 144L, 126L, &#xA;    108L, 90L, 72L, 54L, 36L, 18L, 0L, 13L, 33L, 15L, 33L, 15L&#xA;    )), .Names = c(&quot;Date&quot;, &quot;Milk.Brand1&quot;, &quot;Milk.Brand2&quot;, &quot;Milk.Brand3&quot;, &#xA;&quot;Milk.Brand4&quot;, &quot;Milk.Brand5&quot;), row.names = c(NA, -15L), class = &quot;data.frame&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am very new to machine learning.&#xA;I have a text classification problem in hand. I have a tagged dataset of around 750 documents( short texts), categorized manually into 16 buckets. I want to train a classifier on this data. I know that there should be a training set and a test set (an option could be 80-20 ). In my understanding, this should be for the complete set( 80% of my 750 documents- training, 20% of 750 documents - testing ). &#xA;1. They should be randomly generated or is there some condition for category? ie. if category A constitutes 60%,category B 5%, C 7% etc. how to choose the training set?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have some data of form showed below to squezze through some neural network.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Customer Step Var0  ... VarN&#xA;0        0    some  ... stuff&#xA;0        1    again ... stuff&#xA;0        2    foo   ... bar&#xA;0        3    bla   ... blub&#xA;1        0    other ... stuff&#xA;1        1    and   ... ongoing&#xA;2        0    and   ... so on&#xA;.        .    .     ... .&#xA;.        .    .     ... .&#xA;.        .    .     ... .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As the neural network has a fixed input layer size, but the customers step count can differ at each customer, I have following question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best practise (if exists) to rearange the given data for using as input of a NN? &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am interested in clustering $N$ time series of $T$ 'values' each. These values are distributions (which can be represented by their cumulative distribution functions (cdf), or their probability density functions (pdf), or more convenient forms &lt;a href=&quot;http://projects.csail.mit.edu/atemuri/wiki/images/f/fe/SrivastavaJermynJoshiCVPR2007.pdf&quot; rel=&quot;nofollow&quot;&gt;such as square-root pdfs yielding a simple spheric geometry&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For comparing given distributions, there is an extensive literature on statistical distances (KL, Hellinger, Wasserstein, and so on), but for comparing given time series of distributions, I am not sure whether there is any literature at all?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Such distances should somehow take into account dynamics information besides the distribution proximity at time t. Ideally, I wish I could have a kind of &lt;a href=&quot;http://arxiv.org/pdf/1506.00976v1.pdf&quot; rel=&quot;nofollow&quot;&gt;information factorization similar to this result&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering if such distances already exist and whether this kind of problem has already been formulated in the literature?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;--&#xA;edit for further precisions and answer to comments:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your answer, but dynamic time warping does not suit to my need. This dp technique only captures a rough similarity of shapes by allowing non-linear time distortion. But, it does not amount for the whole information in these time series, e.g. what about the distribution of distortions? Do the distributions of a given time series vary smoothly through time or violently? DTW is not always the solution, for instance, when working with random walks, it does not make sense to use a DTW since there are no time patterns! In this case, the only information is &quot;correlation&quot; and &quot;distribution&quot; (cf. Sklar's theorem in Copula Theory), and the paper cited above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- edit 2 Here are the papers that are somehow related to my question:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://pub.ist.ac.at/~chl/erc/papers/lampert-cvpr2015.pdf&quot; rel=&quot;nofollow&quot;&gt;Predicting the Future Behavior of a Time-Varying Probability Distribution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/BanerjeeDGS05.pdf&quot; rel=&quot;nofollow&quot;&gt;Clustering on the unit hypersphere using von Mises-Fisher distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://users.cis.fiu.edu/~lzhen001/activities/KDD2011Program/docs/p636.pdf&quot; rel=&quot;nofollow&quot;&gt;Unsupervised clustering of multidimensional distributions using earth mover distance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.machinelearning.org/archive/icml2009/papers/538.pdf&quot; rel=&quot;nofollow&quot;&gt;Hilbert space embeddings of conditional distributions with applications to dynamical systems&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am thinking of preprocessing techniques for the input data to a convolutional neural network (CNN) using sparse datasets and trained with SGD. In Andrew Ng's coursera course, &lt;a href=&quot;https://www.coursera.org/course/ml&quot; rel=&quot;nofollow&quot;&gt;Machine Learning&lt;/a&gt;, he states that it is important to preprocess the data so it fits into the interval $ \\\\left[ 3, 3 \\\\right] $ when using SGD. However, the most common preprocessing technique is to standardize each feature so $ \\\\mu = 0 $ and $ \\\\sigma = 1 $. When standardizing a highly sparse dataset many of the values will not end up in the interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am therefore curious - would it be better to aim for e.g. $ \\\\mu = 0 $ and $ \\\\sigma = 0.5 $ in order for the values be closer to the interval $ \\\\left[ 3, 3 \\\\right] $? Could anyone argue based on a knowledge of SGD on whether it is most important to aim for $ \\\\mu = 0 $ and $ \\\\sigma = 1 $ or $ \\\\left[ 3, 3 \\\\right] $?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Suppose I have data in the form of  Query/Document Pairs, along with corresponding relevance scores (or class labels). Is there a way to use topic modeling to devise a model so that later given a query and a document, we can predict its relevance score?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I am interested in parsing semi-structured text.&#xA;Assume that I have a text with labels of the kind: year_field, year_value, identity_field, identity_value, ..., address_field, address_value, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These fields and their associated values can be everywhere in the text, but usually they are near to each other, and more generally the text in organized in a (very) rough matrix, but rather often the value is just after the associated field with eventually some non-interesting information in between. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of different format can be up to several dozens, and is not that rigid (do not count on spacing, moreover some information can be added and removed).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking toward machine learning techniques to extract all those (field,value) of interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think metric learning and/or conditional random fields (CRF) could be of a great help, but I have not practical experience with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have already encounter a similar problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestion or literature on this topic?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have a dataset like this. The data has been collected through a questionnaire and I am going to do some exploratory data analysis.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;windows &amp;lt;- c(&quot;yes&quot;, &quot;no&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;)&#xA;sql     &amp;lt;- c(&quot;no&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;no&quot;,&quot;no&quot;)&#xA;excel  &amp;lt;- c(&quot;yes&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;)&#xA;salary &amp;lt;- c(100,200,300,400,500 )&#xA;&#xA;test&amp;lt;- as.data.frame (cbind(windows,sql,excel,salary),stringsAsFactors=TRUE)&#xA;test[,&quot;salary&quot;] &amp;lt;- as.numeric(as.character(test[,&quot;salary&quot;] ))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have an outcome variable (salary) in my dataset and a couple of input variables (tools). How can I visualize a horizontal box plot like this:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/CNLC1.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;So, I want to create a Player Profile &lt;strong&gt;Radar Chart&lt;/strong&gt; something like this:&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/neeVU.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/neeVU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Not only the scale of each variable different, but also I want a reversed scale for some statistics like the 'dispossessed' stat, where less actually means good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One solution for the variable scale for each statistic maybe is setting a benchmark and then calculating a score on a scale of 100? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But, How do I display the actual numbers on the chart then? Also, how do I get the reversed scale for some of the statistics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently working in Excel. What is the most powerful tool to create a complex chart like this?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Suppose I have five sets I'd like to cluster. I understand that the SimHashing technique described here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://moultano.wordpress.com/2010/01/21/simple-simhashing-3kbzhsxyg4467-6/&quot;&gt;https://moultano.wordpress.com/2010/01/21/simple-simhashing-3kbzhsxyg4467-6/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;could yield three clusters (&lt;code&gt;{A}&lt;/code&gt;, &lt;code&gt;{B,C,D}&lt;/code&gt; and &lt;code&gt;{E}&lt;/code&gt;), for instance, if its results were:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A -&amp;gt; h01&#xA;B -&amp;gt; h02&#xA;C -&amp;gt; h02&#xA;D -&amp;gt; h02&#xA;E -&amp;gt; h03&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Similarly, the MinHashing technique described in the Chapter 3 of the MMDS book:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://infolab.stanford.edu/~ullman/mmds/ch3.pdf&quot;&gt;http://infolab.stanford.edu/~ullman/mmds/ch3.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;could also yield the same three clusters if its results were:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A -&amp;gt; h01 - h02 - h03&#xA;&#xA;B -&amp;gt; h04 - h05 - h06&#xA;      |&#xA;C -&amp;gt; h04 - h07 - h08&#xA;                  |&#xA;D -&amp;gt; h09 - h10 - h08&#xA;&#xA;E -&amp;gt; h11 - h12 - h13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(Each set corresponds to a MH signature composed of three &quot;bands&quot;, and two sets are grouped if at least one of their signature bands is matching. More bands would mean more matching chances.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I have several questions related to these:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) Can SH be understood as a &lt;em&gt;single band&lt;/em&gt; version of MH?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Does MH necessarily imply the use of a data structure like Union-Find to build the clusters?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(3) Am I right in thinking that the clusters, in both techniques, are actually &quot;pre-clusters&quot;, in the sense that they are just sets of &quot;candidate pairs&quot;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(4) If (3) is true, does it imply that I still have to do an $O(n^2)$ search inside each &quot;pre-cluster&quot;, to partition them further into &quot;real&quot; clusters? (which might be reasonable if I have a lot of small and fairly balanced pre-clusters, not so much otherwise)&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a dataset that I want to classify as fraud/not fraud and I have many weak learners. My concern is that there is much more fraud than not fraud, so my weak learners perform better than average, but none perform better than 50% accuracy in the complete set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is whether I should set up testing and training sets that are half fraud and half not fraud or if I should just use a representative sample.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Let's say there is a function $f$ such that $y = f(x)$. However, if $f$ is a piecewise function such that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$y = \\\\begin{cases} 0 \\\\quad x \\\\leq 0 \\\\\\\\ 1 \\\\quad x &amp;gt;0\\\\end{cases} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I fit $f$ in that case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many thanks, guys.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am working on generating restaurant ratings automatically and I &lt;strong&gt;have various feature values&lt;/strong&gt; like delivery time, cost estimate, etc. I want to generate a rating for each restaurant between 0 to 5. But &lt;strong&gt;I don't have any training data or ground truth to validate&lt;/strong&gt;. This rating might vary with user. Most of the related work, mostly related to the Yelp data challenge, have some relevance score as training data. I though of using &lt;em&gt;reinforcement learning&lt;/em&gt; to learn the rating with user feedback, but not sure how to do that. Can anyone please suggest a relevant technique or algorithm for this problem?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I want to find the variables (and its values) used to build a  classification model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I came to know that the following code does the same&lt;/p&gt;&#xA;&#xA;&lt;p&gt;get_all_vars(model_built, dataset_used_for_building_a_model)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I would not be able to use the above code as I am going to use the &quot;model_built&quot; alone in the R code and i will not be having the &quot;dataset_used_for_building_a_mode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simply put i want to use this &quot;Model_built&quot; alone and fetch the variables used along with the values. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am trying to write the code of a Bernoulli block mixture model in matlab, but am facing an error every time I run the function. In particular, I'm having a problem with how to relate the distribution parameter $\\\\alpha$ to the latent variables $Z$ and $W$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, \\\\mathbf{Z} follows a multinomial distribution of parameter $\\\\pi$, such that $\\\\dim(\\\\pi)=(1, g)$, where $g$ is the number of clusters of rows and $\\\\sum \\\\pi=1$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$W$ follows a multinomial distribution of parameter $\\\\rho$ such that $\\\\dim(\\\\rho)=(1,m)$, where $m$ is the number of column clusters and $\\\\sum \\\\rho=1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\mathbf{Z}$ is a matrix of $\\\\dim(N,g)$, and $\\\\mathbf{W}$ is a matrix of $\\\\dim(d,m)$, where $N$ is the number of observations, $p$ is the number of variables, $g$ is the number of row clusters, and $m$ is the number of column clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$X$ follows a bernoulli distribution of parameter $\\\\alpha_{ZW}$, where the notation $\\\\alpha_{ZW}$ denotes the values of $\\\\alpha$ depends on $\\\\mathbf{Z}$ and $\\\\mathbf{W}$, and $\\\\alpha$ is a $\\\\dim(g,m)$ matrix. Please find attached a graphical representation of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I write the code of $\\\\alpha_{ZW}$ in order to get generate the model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/btXXp.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/btXXp.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I am looking for a simple way to sample from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution&quot; rel=&quot;nofollow&quot;&gt;multivariate von Mises-Fisher&lt;/a&gt; distribution in Python. I have looked in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.vonmises.html&quot; rel=&quot;nofollow&quot;&gt;the stats module in scipy&lt;/a&gt; and the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.vonmises.html#numpy.random.vonmises&quot; rel=&quot;nofollow&quot;&gt;numpy module&lt;/a&gt; but only found the univariate von Mises distribution. Is there any code available? I have not found yet.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- edit. Apparently, Wood (1994) has designed an algorithm for sampling from the vMF distribution according to &lt;a href=&quot;http://idg.pl/mirrors/CRAN/web/packages/movMF/movMF.pdf&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;, but I can't find the paper.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm quite new to the NLTK package of Python and to NLP too (I usually work in R but for NLP purposes and scraping maybe Python is more able). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I scrap articles from Hungarian newsportals and want to make a wordcloud out of it to show what are the current trending news topics. First I filter out stopwords and then stem the remaining words. (nltk has Hungarian stemmer) So I'm able to make a frequency table which can be the base of the wordcloud. My problem comes afterwards because stems are usually meaningless chunks (and not lemmas) of real words. I want to somehow complete the stem to a real word.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first idea was to assign the most common word or the shortest one (or some combination of this 2 rules) to the stem and represent that in the wordcloud.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better solution for stem completion or should I follow a different workflow?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;Assuming I can collect the demand of the purchase of a certain product that are of different market tiers. Example: Product A is low end goods. Product B is another low end goods. Product C and D are middle-tier goods and product E and F are high-tier goods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have collected data the last year on the following&#xA;1. Which time period (season - festive? non-festive?) does the different tier product reacts based on the price set? Reacts refer to how many % of the product is sold at certain price range&#xA;2. How fast the reaction from the market after marketing is done? Marketing is done on 10 June and the products are all sold by 18 June for festive season that slated to happened in July (took 8 days at that price to finish selling)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can data science benefit in terms of recommending&#xA;1. If we should push the marketing earlier or later?&#xA;2. If we can higher or lower the price? (Based on demand and sealing rate?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I understanding it right that data science can help a marketer in this aspect? Which direction should I be looking into if I am interested to learn about it.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I've trained an AWS Machine Learning model with the training data from here : &lt;a href=&quot;https://www.kaggle.com/c/titanic/data&quot; rel=&quot;nofollow&quot;&gt;https://www.kaggle.com/c/titanic/data&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm now trying to run a batch prediction with the test data from the same source but I get the following error when I try to load the data : &quot;  The schema in this data file must match the datasource used to create the ML model ml-xxxxxxxxx. Ensure that the data file you are using matches the schema structure.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The schema, as far as I can see, is identical. I have tried it with and without the 'survived' column which is the value I'm trying to predict. I even tried it with the same training set which obviously has an identical schema and got the same error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I recently read &lt;a href=&quot;http://arxiv.org/abs/1411.4038&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt; by Jonathan Long, Evan Shelhamer, Trevor Darrell. I don't understand what &quot;deconvolutional layers&quot; do / how they work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The relevant part is&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;3.3. Upsampling is backwards strided convolution&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Another way to connect coarse outputs to dense pixels&#xA;  is interpolation. For instance, simple bilinear interpolation&#xA;  computes each output $y_{ij}$ from the nearest four inputs by a&#xA;  linear map that depends only on the relative positions of the&#xA;  input and output cells.&lt;br/&gt;&#xA;  In a sense, upsampling with factor $f$ is convolution with&#xA;  a fractional input stride of 1/f. So long as $f$ is integral, a&#xA;  natural way to upsample is therefore backwards convolution&#xA;  (sometimes called deconvolution) with an output stride of&#xA;  $f$. Such an operation is trivial to implement, since it simply&#xA;  reverses the forward and backward passes of convolution.&lt;br/&gt;&#xA;  Thus upsampling is performed in-network for end-to-end&#xA;  learning by backpropagation from the pixelwise loss.&lt;br/&gt;&#xA;  Note that the deconvolution filter in such a layer need not&#xA;  be fixed (e.g., to bilinear upsampling), but can be learned.&#xA;  A stack of deconvolution layers and activation functions can&#xA;  even learn a nonlinear upsampling.&lt;br/&gt;&#xA;  In our experiments, we find that in-network upsampling&#xA;  is fast and effective for learning dense prediction. Our best&#xA;  segmentation architecture uses these layers to learn to upsample&#xA;  for refined prediction in Section 4.2.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I don't think I really understood how convolutional layers are trained. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I think I've understood is that convolutional layers with a kernel size $k$ learn filters of size $k \\\\times k$. The output of a convolutional layer with kernel size $k$, stride $s \\\\in \\\\mathbb{N}$ and $n$ filters is of dimension $\\\\frac{\\\\text{Input dim}}{s^2} \\\\cdot n$. However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if my understanding of convolutional layers is correct, I have no clue how this can be reversed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anybody please help me to understand deconvolutional layers?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I used pretrained GoogleNet from &lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&quot; rel=&quot;nofollow&quot;&gt;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&lt;/a&gt; and finetuned it with my own data (~ 100k images, 101 classes). After one day training I achieved 62% in top-1 and 85% in top-5 classification and try to use this network to predict several images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I just followed example from &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/classification.ipynb&quot; rel=&quot;nofollow&quot;&gt;https://github.com/BVLC/caffe/blob/master/examples/classification.ipynb&lt;/a&gt;,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my Python code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import caffe&#xA;import numpy as np&#xA;&#xA;&#xA;caffe_root = './caffe'&#xA;&#xA;&#xA;MODEL_FILE = 'caffe/models/bvlc_googlenet/deploy.prototxt'&#xA;PRETRAINED = 'caffe/models/bvlc_googlenet/bvlc_googlenet_iter_200000.caffemodel'&#xA;&#xA;caffe.set_mode_gpu()&#xA;&#xA;net = caffe.Classifier(MODEL_FILE, PRETRAINED,&#xA;               mean=np.load('ilsvrc_2012_mean.npy').mean(1).mean(1),&#xA;               channel_swap=(2,1,0),&#xA;               raw_scale=255,&#xA;               image_dims=(224, 224))&#xA;&#xA;def caffe_predict(path):&#xA;        input_image = caffe.io.load_image(path)&#xA;        print path&#xA;        print input_image&#xA;        prediction = net.predict([input_image])&#xA;&#xA;&#xA;        print prediction&#xA;        print &quot;----------&quot;&#xA;&#xA;        print 'prediction shape:', prediction[0].shape&#xA;        print 'predicted class:', prediction[0].argmax()&#xA;&#xA;&#xA;        proba = prediction[0][prediction[0].argmax()]&#xA;        ind = prediction[0].argsort()[-5:][::-1] # top-5 predictions&#xA;&#xA;&#xA;        return prediction[0].argmax(), proba, ind&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my deploy.prototxt I changed the last layer only to predict my 101 classes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;layer {&#xA;  name: &quot;loss3/classifier&quot;&#xA;  type: &quot;InnerProduct&quot;&#xA;  bottom: &quot;pool5/7x7_s1&quot;&#xA;  top: &quot;loss3/classifier&quot;&#xA;  param {&#xA;    lr_mult: 1&#xA;    decay_mult: 1&#xA;  }&#xA;  param {&#xA;    lr_mult: 2&#xA;    decay_mult: 0&#xA;  }&#xA;  inner_product_param {&#xA;    num_output: 101&#xA;    weight_filler {&#xA;      type: &quot;xavier&quot;&#xA;    }&#xA;    bias_filler {&#xA;      type: &quot;constant&quot;&#xA;      value: 0&#xA;    }&#xA;  }&#xA;}&#xA;layer {&#xA;  name: &quot;prob&quot;&#xA;  type: &quot;Softmax&quot;&#xA;  bottom: &quot;loss3/classifier&quot;&#xA;  top: &quot;prob&quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the distribution of softmax output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[ 0.01106235  0.00343131  0.00807581  0.01530041  0.01077161  0.0081002&#xA;   0.00989228  0.00972753  0.00429183  0.01377776  0.02028225  0.01209726&#xA;   0.01318955  0.00669979  0.00720005  0.00838189  0.00335461  0.01461464&#xA;   0.01485041  0.00543212  0.00400191  0.0084842   0.02134697  0.02500303&#xA;   0.00561895  0.00776423  0.02176422  0.00752334  0.0116104   0.01328687&#xA;   0.00517187  0.02234021  0.00727272  0.02380056  0.01210031  0.00582192&#xA;   0.00729601  0.00832637  0.00819836  0.00520551  0.00625274  0.00426603&#xA;   0.01210176  0.00571806  0.00646495  0.01589645  0.00642173  0.00805364&#xA;   0.00364388  0.01553882  0.01549598  0.01824486  0.00483241  0.01231962&#xA;   0.00545738  0.0101487   0.0040346   0.01066607  0.01328133  0.01027429&#xA;   0.01581303  0.01199994  0.00371804  0.01241552  0.00831448  0.00789811&#xA;   0.00456275  0.00504562  0.00424598  0.01309276  0.0079432   0.0140427&#xA;   0.00487625  0.02614347  0.00603372  0.00892296  0.00924052  0.00712763&#xA;   0.01101298  0.00716757  0.01019373  0.01234141  0.00905332  0.0040798&#xA;   0.00846442  0.00924353  0.00709366  0.01535406  0.00653238  0.01083806&#xA;   0.01168014  0.02076091  0.00542234  0.01246306  0.00704035  0.00529556&#xA;   0.00751443  0.00797437  0.00408798  0.00891858  0.00444583]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It seems just like random distribution with no sense.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for any help or hint and best regards, Alex&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm studying machine learning and I feel there is a strong relationship between the concept of VC dimension and the more classical (statistical) concept of degrees of freedom.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain such a connection?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;Similar questions have been asked in this context but this one seems a bit different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Aim]&lt;/strong&gt;&lt;br&gt;&#xA;We would like to find out what the probability is of a person purchasing a 4USD column D (see below) price given that he has purchased C-2, B-2, A&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/mLlaR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Context]&lt;/strong&gt;&lt;br&gt;&#xA;We have a sizeable dataset of about 500,000 observations of this pattern. E.g. customer decides to buy A), bought B-1, then C-2, D-3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Issue]&lt;/strong&gt;&lt;br&gt;&#xA;We don't know how deep the events reach. E.g. it can be to column E, F, G. That's why we would like to auto-generate a probability tree in preferably SPSS but are open to other solutions as well.&#xA;How does this look like mathmatically? Can we just extend bayes conditional probability? If so, how does it look? Any potential issues?&#xA;Any other comments/ ideas are always welcome :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a large sequence of vectors of length N. I need some unsupervised learning algorithm to divide these vectors into M segments. E.g.:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/O1hyC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;k-means is not suitable, because it puts similar elements from different locations into a single cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;real data looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/VyVCU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see here 3 clusters: [0..50], [50..200], [200..250]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD 2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use modified k-means and get this acceptable result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/WBlZn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;borders of clusters: [0, 38, 195, 246]&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am using the below R code to convert text to lower case:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;movie_Clean &amp;lt;- tm_map(movie_Clean, content_transformer(tolower))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I end up getting the below error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error in FUN(content(x), ...) :    invalid input 'I just wanna watch&#xA;  Jurassic World í ½í¸«' in 'utf8towcs'.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Please help how to overcome this error.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;With all the hoopla around Data Science, Machine Learning, and all the success stories around, there are a lot of both justified, as well as overinflated, expectations from Data Scientists and their predictive models.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question to practicing Statisticians, Machine Learning experts, and Data Scientists is - how do you manage expectations from the businesspeople in you company, particularly with regards to predictive accuracy of models? To put it trivially, if your best model can only achieve 90% accuracy, and upper management expects nothing less than 99%, how do you handle situations like these?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;[Apologies if this post sounds naive, I'm fairly new to the world of data science/big data and very unsure where I'm heading career-wise]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently an undergraduate MMath [integrated master's] Mathematics student in the UK who has finished the third year of the course [out of four years].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I have been considering the possibility of doing further research in Mathematics/Statistics/Operational Research/Data Science, I have decided to stay on and complete the Master's component of the course [as it is the only Master's course I can get funding for at this stage]. After the Master's I may continue on and do a PhD.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are currently two projects that appeal to me that seem to have relevant applications. The first one is on improved MCMC [Markov Chain Monte Carlo] methods, in particular MCMC using Hamiltonian Dynamics. There is scope for some big data applications here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other project that I could take part in is one on the centrality/communities detection of networks within network science. This could possibly be useful with applications in Operational Research.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have an idea as to which project will be more relevant to data science/analytics?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I have an excel file that contains details related to determining the quality of a wine and I want to implement the linear model concept using the function &lt;strong&gt;sklearn.linear_model.SGDClassifier(SVM =&gt; Hinge loss) and (Logarithmic regression =&gt;log loss)&lt;/strong&gt; using python. I learned the basics about these function through the &lt;em&gt;scikit learn&lt;/em&gt; website and I am not able to implement the model using excel file. I am very new to python and machine learning and I finding it hard to implement the model. I opened the excel file in python and tried to take two columns [randomly] from the file and use that as an input to call the &lt;strong&gt;fit&lt;/strong&gt; function available in the model. But, I got an error stating &lt;strong&gt;Unknown label type: array&lt;/strong&gt;. I tried a couple of other methods too, but, nothing worked. Can someone guide me with the implementation process? &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from xlrd import open_workbook&#xA;from sklearn import linear_model&#xA;i = 0&#xA;fa = []&#xA;ph = []&#xA;&#xA;book = open_workbook('F:/BIG DATA/winequality.xlsx')&#xA;sheet = book.sheet_by_name('Sheet1')&#xA;num_rows = sheet.nrows - 1&#xA;num_cols = sheet.ncols - 1&#xA;curr_row = 0&#xA;while curr_row &amp;lt;num_rows:&#xA;    curr_row += 1&#xA;    cell_val = sheet.cell_value(curr_row,0)&#xA;    cell_val1 = sheet.cell_value(curr_row,10)&#xA;&#xA;    fa.append([float(cell_val),float(cell_val1)])&#xA;    cell_val2 = sheet.cell_value(curr_row,8)&#xA;    ph.append(float(cell_val2))&#xA;&#xA;model = linear_model.SGDClassifier()&#xA;print(model.fit(fa,ph))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/I9J8u.png&quot; alt=&quot;Screenshot&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The error message screenshot:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lCJTu.png&quot; alt=&quot;ERROR&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a rare event (unbalanced target variable) classification problem using decision trees. My dataset comprises of 95% non-event and 5% minority (events) class. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I used decision tree over logistic regression because I had many categorical variables comparing to continuous variables. I get a  good performance for training data with the decision tree C5.0. However I get poor results for the new data. I use the confusion matrix as a measure of performance. Training model is over-fitting. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did pruning to reduce the over-fitting caused by the decision tree.  I used the following code to build the model&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Classifi_C5.0 &amp;lt;- C5.0(TARGET ~., , data = training_data_SMOTED, trails = 500,&#xA;                      control = C5.0Control(minCases = mincases_count,&#xA;                                            noGlobalPruning = FALSE))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I balanced the minority and majority class using the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;training_data_SMOTED &amp;lt;- SMOTE(TARGET ~ ., training_data,&#xA;                              perc.over = 100, k = 5, perc.under = 200)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any sort of advice will be helpful. &lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have several samples (C2, C4, C5) and want to check if they are at a certain stage. I included some known samples (D0 - D77) which were generated at different stages by another lab. In the PCA plot, my samples cluster together on the left and the known samples are dispersed on the right. I think the major difference among all the samples is different experimental protocols (PC1) and the second is different stages (PC2). So my samples are at the same stage. Is that right? And can we say my samples are at a stage between D12 and D19 (when projected to known samples, my samples are located between D12 and D19)? I have no strong mathematical background. Hope someone with math background can give some explanation. Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[ &lt;strong&gt;UPDATE1&lt;/strong&gt; ]&#xA;I did this analysis using &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;prcomp&lt;/a&gt; in R. The input is a 2D numeric matrix with 17436 rows and 42 columns. Each row represents a gene and each column represents a sample. The number is the gene expression level for a gene in a sample. The gene expression level is normalized using DESeq2 and thus the numbers are comparable across genes (rows) and samples (columns). For the 42 columns, 18 are from my experiments and the rest from published datasets. Besides different protocols, it is possible there are other differences. Generally, the table is a combination of two sources of data. In my data , C2, C4 and C5 are three cell lines which we processed in parallel in experiments. In published datasets, they sampled the cells at different time points (Day 0 to Day 77).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[ &lt;strong&gt;UPDATE2&lt;/strong&gt; ] R Code for PCA and plotting&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# normCounts: normalized count from DESeq2, with 17436 rows and 42 columns&#xA;normCounts0 &amp;lt;- normCounts[ rowSums(normCounts) &amp;gt; 0, ]&#xA;tab &amp;lt;- t(normCounts0)&#xA;pca &amp;lt;- prcomp(tab, scale = TRUE)&#xA;tmp.x &amp;lt;- as.data.frame(pca$x)&#xA;tmp.x$sample&amp;lt;-c(rep(&quot;C2_Con&quot;,3),rep(&quot;C2_KD&quot;,3),rep(&quot;C5_Con&quot;,3),rep(&quot;C5_KD&quot;,3),rep(&quot;C4_Con&quot;,3),rep(&quot;C4_KD&quot;,3), rep(&quot;D0&quot;,4),rep(&quot;D7&quot;,4),rep(&quot;D12&quot;,2),rep(&quot;D19&quot;,4),rep(&quot;D26&quot;,2),rep(&quot;D33&quot;,2),rep(&quot;D49&quot;,2),rep(&quot;D63&quot;,2),rep(&quot;D77&quot;,2))&#xA;&#xA;require(&quot;ggplot2&quot;)&#xA;p &amp;lt;- ggplot(tmp.x, aes(x=PC1, y=PC2, color=sample))&#xA;p + geom_point() + scale_color_discrete(breaks=c(&quot;C2_Con&quot;,&quot;C2_KD&quot;,&quot;C5_Con&quot;,&quot;C5_KD&quot;,&quot;C4_Con&quot;,&quot;C4_KD&quot;, &quot;D0&quot;,&quot;D7&quot;, &quot;D12&quot;,&quot;D19&quot;,&quot;D26&quot;,&quot;D33&quot;,&quot;D49&quot;,&quot;D63&quot;,&quot;D77&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/LuU0y.jpg&quot; alt=&quot;PCA plot&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;Detailed Question Explanation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose say our application X is processing huge logs (size varying from MBs to GBs) and giving insight results in these logs(NOT A Social Data logs or Security Logs)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;now this logs are in format say log.y with different variety, using C++ as Engine to process these huge logs.(It generates imp. insights about data but need to be processed using our application X only and we don't want to change core way processing of application X)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this processing happens on some server it under or over utilizes resources (That I already know).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use cloud computing for this processing we get that processing power with optimum usage. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we see help of BIG data analytics in this particular sort of usage?&#xA;Any help or suggestion is very deeply appreciated  &lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a csv file with characters in Persian and I cannot view them in R correctly. Also, I cannot subset based on Persian characters values. Here is a sample code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;list1 &amp;lt;- c(&quot;x&quot;,&quot;y&quot;)&#xA;list2 &amp;lt;- c(&quot;ب&quot;,&quot;الف&quot;)&#xA;&#xA;list1 &#xA;list2 ##OK-readable&#xA;&#xA;writedf &amp;lt;- as.data.frame(cbind(list1,list2),encoding=&quot;UTF-8&quot;)&#xA;write.csv(writedf,&quot;test.csv&quot;)&#xA;testdf &amp;lt;- read.csv(&quot;test.csv&quot;,encoding=&quot;UTF-8&quot;)&#xA;&#xA;testdf  &#xA;## not readable&#xA;&#xA;testdf[,testdf$list2==&quot;ب&quot;]&#xA;## data frame with 0 columns and 2 rows  ???&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;I have an excel file containing a long text in column A. I am looking for the words starting by &quot;popul&quot; such as popular and populate . I can find these cells by the formula:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    =SEARCH(&quot;popul&quot;,A1,1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want a function that returns the whole words starting by popul such as popular and populate.&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm using the HMMLearn python package for hidden markov models.  That implementation is build on multivariate gaussian distributions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have a string of features.  How sensitive are gaussians to vastly different feature scales?  Will it be really skewed if one feature is scaled between 0 and 1, and another is scaled between 0 and 1e8?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on a project that aims to retrieve a large data-set (i.e., tweet data which is a couple of days old) from Twitter using the twitteR library on R.  have difficulty storing tweets because my machine has only 8 GB of memory. It ran out of memory even before I set it to retrieve for one day. Is there a way where I can store the tweets straight to my disk without storing into RAM? I am not using the streaming API as I need to get old tweets. &lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I am really new to data science. Please don't mark me down as this website is my only hope of progress.&#xA;I have set of data I obtained from NASA website. When I saved it, it saved as &quot;tsv' file. (Tab separated values). I want to open it on Matlab as a Matrix as I have a code to run on that matrix. &#xA;Basically I want to import that file to matlab and start running the code on it. &#xA;Can someone please help me or guide me in the right direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried various things such as import data, tdfread but so far nothing has worked for me. I was first trying to export the tsv file to MS Excel and then go from Excel to Matlab. That too I don't know how to do. I will give you the link of my data which I want to import on to Matlab. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The link for my data is the following.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please take a look at &lt;a href=&quot;http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=J%2FApJS%2F209%2F31&quot; rel=&quot;nofollow&quot;&gt;http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=J%2FApJS%2F209%2F31&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you just click on submit at the lower right corner, one will see the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;a = importdata('J_ApJS_209_31_table3-150618.tsv') [This the command I used].&#xA;Error message on matlab is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ??? Error using ==&amp;gt; importdata at 136&#xA;    Unable to open file.&#xA;    Error in ==&amp;gt; data at 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I wrote a script.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    tdfread(J_ApJS_209_31_table3-150618,'\\\\t')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error message I get is &#xA;        ??? Undefined function or variable 'J_ApJS_209_31_table3'.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Error in ==&amp;gt; data at 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;What are the best method/library/data available to extract named entities [Names and Location] from Twitter data ? [Other than dictionary lookup]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried with Python-Stanford NER, But it seems to fail when named entities is not capitalized. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried to predict NER after converting text to upper case &#xA;eg : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; text = &quot;david beckham played for england&quot;&#xA;&#xA; stanford.NERTagger.tag(text)&#xA; [(u'david', u'PERSON'), (u'beckham', u'PERSON'), (u'played', u'O'), (u'for', u'O'), (u'england', u'O')]&#xA;&#xA; stanford.NERTagger.tag(text.upper())&#xA; output : [(u'DAVID', u'PERSON'), (u'BECKHAM', u'PERSON'), (u'PLAYED', u'O'), (u'FOR', u'O'), (u'ENGLAND', u'LOCATION')]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;A question crossed my mind not so long ago: I am doing experiments on Language Model with RNN (always with the same network topology: 50 hidden units, and 10M &quot;directs connections&quot; that are emulating N_grams models) and different fraction of corpus (10,25,50,75,100%) (9M words).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I noticed that while perplexity seems to decrease when the training data become more abundant, certain times it does not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last example : 143 118 109 106 112&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first thought was network initialization, so I began testing with a smaller corpus and 20 hidden units (for technical reasons. Even with 10% corpus, learning can take up to 30h, which is problematic for me), and I found after 50 tries that all nets converged on values within 3% of each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I thought that maybe the importance of this initialization is a function of the number of hidden units? I mean the more hidden units the more parameters to tune.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also maybe my stop criterion is too sensitive (It stops if evolution of perplexity between two iterations is inferior to a certain number).&#xA;Do you think it would make an impact to allow it to run one of two iterations after the criterion was met to see if it was just a local thing ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Marc&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;h3&gt;Problem&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I have tried using Naive bayes on a labeled data set of crime data but got really poor results (7% accuracy). Naive Bayes runs much faster than other alogorithms I've been using so I wanted to try finding out why the score was so low. &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Research&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;After reading I found that Naive bayes should be used with balanced datasets because it has a bias for classes with higher frequency. Since my data is unbalanced I wanted to try using the Complementary Naive Bayes since it is specifically made for dealing with data skews. In the paper that describes the process, the application is for text classification but I don't see why the technique wouldn't work in other situations. You can find the paper I'm referring to &lt;a href=&quot;http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. In short the idea is to use weights based on the occurences where a class doesn't show up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After doing some research I was able to find an implementation in Java but unfortunately I don't know any Java and I just don't understand the algorithm well enough to implement myself.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Question&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;where I can find an implementation in python? If that doesn't exist how should I go about implementing it myself?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called &lt;strong&gt;spark-submit&lt;/strong&gt; command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/bin&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After that, in cmd prompt: spark-submit script.py&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Run3y.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;It's well known that science has given us large amount of free accessible data, such as &lt;a href=&quot;http://www.1000genomes.org&quot; rel=&quot;nofollow&quot;&gt;http://www.1000genomes.org&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/genbank&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/genbank&lt;/a&gt;. How can we play around with the data and apply data science/machine learning to it? What could be some ideas?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My own ideas:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Biological data visualisation&lt;/li&gt;&#xA;&lt;li&gt;Gene prediction using hidden-markov-model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any more?&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am working on Rstudio on a server which has 250GB ram. But its taking too much time to handle a 2GB data file. how should i speed up my work?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I have a 7 giga confidential dataset which I want to use for a machine learning application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every package recommanded for efficient dataset management in R like :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;data.table, &lt;/li&gt;&#xA;&lt;li&gt;ff &lt;/li&gt;&#xA;&lt;li&gt;and sqldf with no success. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Data.table needs to load all the data in the memory from what I read, so it's obvious that it will not work since my computer has only 4g RAM. Ff leads to a memory error too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I decided to turn to sgdb and I tried :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Mysql which managed to load my dataset in 2 hours and 21'. Then I began my requests (I have a few requests to do to prepare my data before I export a smaller set in R for machine learning application), and then I had to wait for hours before I got the following message &quot;The total number of locks exceeds the lock table size&quot; (my request was just an update to extract the month from a date for each tuple). &lt;/li&gt;&#xA;&lt;li&gt;I read that postgre was similar to mysql in performance so I didn't try&lt;/li&gt;&#xA;&lt;li&gt;I read that redis was really performant but not at all adapted to massive importation like I want to do here so I didn't try&lt;/li&gt;&#xA;&lt;li&gt;I tried mongoDb, the nosql upraising solution that I heard everywhere about. Not only I find rather disturbing that mongoimport is so limited in options (I had to change all semi-colon in commas using sed before I can import the data), but It seems to be less performant that mysql since I launched the loading yesterday and it is still running. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What I can't try : data are confidential so I don't really want to rent some space on Azure or Amazon clouding solution. I am not sure that it is that big that I have to turn to Hadoop solution but maybe I am wrong about that. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an open-source performant solution that I didn't try that you would recommend to perform some sql-like requests on a biggish dataset ?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Edit : &#xA;Some more details about what I want to do with these data for you to visualize. These are events with a timestamp and a geolocalisation. I have 8 billions of lines. One example of what I want to do : &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;standardize series identified by geolocalisation (I need to compute mean grouping by geolocalisation for example), &lt;/li&gt;&#xA;&lt;li&gt;compute average count of events by type of season, day... (usual group by sql request)... &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Edit &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a beginning of answer for those who have limited hardware like me, rSQLite seems to be a possibility. I am still interested in other people experiences. &lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I've been coding a Neural Network for recognizing and tagging parts of speech in English (written in Java). The code itself has no 'errors' or apparent flaws. Nevertheless, it is not learning -- the more I train it does not change its ability to predict the testing data. The following is information about what I've done, please ask me to update this post if I left something important out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wrote the neural network and tested it on several different problems to make sure that the network itself worked. I trained it to learn how to double numbers, XOR, cube numbers, and learn the sin function to a decent accuracy. So, I'm fairly confident that the actual algorithm is working.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The network using using the sigmoid activation function. The learning rate is .3, Momentum is .6. The weights are initialized to rng.nextFloat() -.5) * 4&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then got the Brown Corpus data-set and simplified the tagset to 'universal' with NLTK. I used NLTK for generating and saving all the corpus and dictionary data. I cut the last 15,000 sentences out of the corpus for testing purposes. I used the rest of the corpus (about 40,000 sentences of tagged words) for training. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The neural network layout is as follows: There is an input neuron for each Tag. Output Layer: There is one output neuron for each tag. The network is taking inputs for 3 words: first: the word coming before the word we want to tag, second: the word that needs to be tagged, third: the word that follows the second word. So, total number of inputs are 3x(total number of possible tags). The input values are numbers between 0 and 1. Each of the 3 words being fed into the input layer is searched for in a dictionary (made up by the 40,000 corpus, the same corpus that is used for training). The dictionary holds the number of times that each word has been tagged in the corpus as what part of speech. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For instance, the word 'cover' is tagged as a noun 1 time and a verb 3&#xA;  times.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Percentages of being tagged are computed for each part of speech that the word is associated as, and this is what is fed into the network for that particular word. So, the input neuron designated as NOUN would receive .33 and VERB would receive .66. The other input neurons that hold tags for that word receive an input of 0.0. This is done for each of the 3 words to be inputted. If a word is the first word of a sentence, the first group of tags are all 0. If a word is the last word of a sentence, the final group of input neurons that hold the tag probabilities for the following word are left as 0s.&#xA;I've been using 10 hidden nodes (I've read a number of papers and this seems to be a good place to start testing with)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;None of the 15,000 testing sentences were used to make the 'dictionary.' So, when testing the network with this partial corpus there will be some words the network has never seen. Words that are not recognized have their suffix stripped, and their suffix is searched for in another 'dictionary.' Whatever is most probable for that word is then used as inputs for it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my set-up, and I started trying to train the network. I've been training the network with all 40,000 sentences. 1 epoch = 1 forward and backpropagation of every word in each sentence of the 40,000 training-set. So, just doing 1 epoch takes quite a few seconds. Just by knowing the word probabilities the network did pretty well, but the more I train it, nothing happens. The numbers that follow the epochs are the number of correctly tagged words divided by the total number of words.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First run 50 epochs: 0.928218786&lt;/p&gt;&#xA;&#xA;&lt;p&gt;100 epochs:        0.933130661&lt;/p&gt;&#xA;&#xA;&lt;p&gt;500 epochs:       0.928614499 took around 30 minutes to train this                   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tried 10 epochs:         0.928953683 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using only 1 epoch had results that pretty much varied between .92 and .93&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, it doesn't appear to be working...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then took 55 sentences from the corpus and used the same dictionary that had probabilities for all 40,000 words. For this one, I trained it in the same way I trained my XOR -- I only used those 55 sentences and I only tested the trained network weights on those 55 sentences. The network was able to learn those 55 sentences quite easily. With 120 epochs (taking a couple seconds) the network went from tagging 3768 incorrectly and 56 correctly (on the first few epochs) to tagging 3772 correctly and 52 incorrectly on the 120th epoch. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is where I'm at, I've been trying to debug this for over a day now, and haven't figured anything out.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body=\"&lt;p&gt;I'm trying to think of a data set that is essentially topologically spherical. It's easier to think of cylindrical datasets (two dimensions, one periodic) or toroidal datasets (two dimensions, both periodic). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obvious candidates are geographical and astronomical, ground and sky; but I think the only thing spherical about the sky is its projection onto the ground, so it really just comes back to Earth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find it helpful to think about in terms of a circle's &lt;a href=&quot;https://en.wikipedia.org/wiki/Fundamental_polygon#Examples&quot; rel=&quot;nofollow noreferrer&quot;&gt;fundamental polygon&lt;/a&gt;:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/vxubU.png&quot; alt=&quot;Fundamental polygons&quot;&gt;&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am using rattle in R for predictive models and am trying to see whether there is a difference in different sampling methods. The split function at the start of rattle for splitting into training and testing (optional validation) I take it is split validation. Is there a way to do bootstrap validation and cross validation directly in rattle?&lt;/p&gt;&#xA;'),\n",
       " Row(Body=\"&lt;p&gt;I'm writing my thesis at the moment, and for some time - due to a lack of a proper alternative - I've stuck with &quot;unstructured data&quot; for referring to natural, free flowing text, e.g. Wikipedia articles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This nomenclature has bothered me from the very beginning, since it opens a debate that I don't want to get into. Namely, that &quot;unstructured&quot; implies that natural language lacks structure, which it does not - the most obvious being syntax. It also gives a negative impression, since it is the opposite of &quot;structured&quot;, which is accepted as being positive. This is not the focus of my thesis, though the &quot;unstructured&quot; part itself plays an important role.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I completely agree with the writer of &lt;a href=&quot;https://gigaom.com/2013/03/17/can-we-please-stop-saying-unstructured-data/&quot;&gt;this article&lt;/a&gt;, but he proposes no alternative except for &quot;rich data&quot;, which doesn't cover my point. The point I'm trying to make that the text lacks a traditional database-like (e.g. tabular) structure of the data, with every piece of data having a clear data type and semantics that is easy to interpret using computer programs. Of course I'd like to condense this definition into a term, but so far I've been unsuccessful coming up with, or discovering an acceptable taxonomy in literature.&lt;/p&gt;&#xA;\"),\n",
       " Row(Body='&lt;p&gt;I am new to data science. I have a dataset of around 200,000 records, having 5 columns. There is a field called, &lt;code&gt;class&lt;/code&gt;. For each &lt;code&gt;class&lt;/code&gt;, there are one or many &lt;code&gt;divisions&lt;/code&gt;. I have to do this:&#xA;1. Filter the dataset, such that only those &lt;code&gt;classes&lt;/code&gt; with at least 5 divisions turn up.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;&lt;p&gt;For each division, I have to calculate &lt;code&gt;attendance&lt;/code&gt; from another column.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;There is a minimum &lt;code&gt;attendance&lt;/code&gt; value for each &lt;code&gt;class&lt;/code&gt;. I have to find the &lt;code&gt;percentage of divisions in each class with the minimum attendance&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I started with importing the data in python using Pandas and started writing loops for processing this. But I am sure this is not the right way to do. Can you please give some idea.Can I do this in Excel pivot table?&lt;/p&gt;&#xA;'),\n",
       " Row(Body='&lt;p&gt;What I mean is the following: Instead of processing all the training data at once and calculating a model, we process one data point at a time and update the model directly afterwards. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have seen the terms &quot;on-line (or online) learning&quot; and &quot;incremental learning&quot; for this. Is there a subtle difference? Is one term used more frequently? Or does it depend on the research community?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: The Bishop book (Pattern Recognition and Machine Learning) uses the terms on-line learning and sequential learning as synonyms but does not mention incremental learning. &lt;/p&gt;&#xA;'),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samTekst.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"&lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;Libsvm&lt;/a&gt; to train data and predict classification on &lt;strong&gt;semantic analysis&lt;/strong&gt; problem. But it has a &lt;strong&gt;performance&lt;/strong&gt; issue on large-scale data, because semantic analysis concerns &lt;strong&gt;&lt;em&gt;n-dimension&lt;/em&gt;&lt;/strong&gt; problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last year, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;Liblinear&lt;/a&gt; was release, and it can solve performance bottleneck.&#xA;But it cost too much &lt;strong&gt;memory&lt;/strong&gt;. Is &lt;strong&gt;MapReduce&lt;/strong&gt; the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on &lt;strong&gt;Liblinear&lt;/strong&gt;?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Lots of people use the term &lt;em&gt;big data&lt;/em&gt; in a rather &lt;em&gt;commercial&lt;/em&gt; way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, &lt;em&gt;big data&lt;/em&gt; always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a &lt;em&gt;big data&lt;/em&gt; problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled &lt;em&gt;big data&lt;/em&gt; if the dataset was &lt;em&gt;big enough&lt;/em&gt;? Also, how &lt;em&gt;big&lt;/em&gt; is &lt;em&gt;big enough&lt;/em&gt; (if this is possible to define)?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it's a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;My data set contains a number of numeric attributes and one categorical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say, &lt;code&gt;NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr&lt;/code&gt;, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;CategoricalAttr&lt;/code&gt; takes one of three possible values: &lt;code&gt;CategoricalAttrValue1&lt;/code&gt;, &lt;code&gt;CategoricalAttrValue2&lt;/code&gt; or &lt;code&gt;CategoricalAttrValue3&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using default k-means clustering algorithm implementation for Octave &lt;a href=&quot;https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/&quot;&gt;https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/&lt;/a&gt;.&#xA;It works with numeric data only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question: is it correct to split the categorical attribute &lt;code&gt;CategoricalAttr&lt;/code&gt; into three numeric (binary) variables, like &lt;code&gt;IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3&lt;/code&gt; ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a bunch of customer profiles stored in a &lt;a href=&quot;/questions/tagged/elasticsearch&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;elasticsearch&amp;#39;&quot; rel=&quot;tag&quot;&gt;elasticsearch&lt;/a&gt; cluster. These profiles are now used for creation of target groups for our email subscriptions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could I search for interesting groups &lt;strong&gt;automatically&lt;/strong&gt; - using data science, machine learning, clustering or something else?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;/questions/tagged/r&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;r&amp;#39;&quot; rel=&quot;tag&quot;&gt;r&lt;/a&gt; programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can the algorithm development itself be sped up and made more scalable?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some specific challenges: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can algorithm parameters be tuned faster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I heard about many tools / frameworks for helping people to process their data (big data environment). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are they complementary?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, &quot;Machine Learning with R&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've seen a guideline of 5TB for a dataset to be considered as Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: Is R suitable for the amount of Data typically seen in Big Data problems? &#xA;Are there strategies to be employed when using R with this size of dataset?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I'm on Windows, but I could easily put this script on my Linux machine if that would simplify the process. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;In reviewing “&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1461468485&quot; rel=&quot;nofollow&quot;&gt;Applied Predictive Modeling&lt;/a&gt;&quot; a &lt;a href=&quot;http://www.information-management.com/blogs/applied-predictive-modeling-10024771-1.html&quot; rel=&quot;nofollow&quot;&gt;reviewer states&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;One critique I have of statistical learning (SL) pedagogy is the&#xA;  absence of computation performance considerations in the evaluation of&#xA;  different modeling techniques. With its emphases on bootstrapping and&#xA;  cross-validation to tune/test models, SL is quite compute-intensive.&#xA;  Add to that the re-sampling that's embedded in techniques like bagging&#xA;  and boosting, and you have the specter of computation hell for&#xA;  supervised learning of large data sets. &lt;strong&gt;In fact, R's memory&#xA;  constraints impose pretty severe limits on the size of models that can&#xA;  be fit by top-performing methods like random forests.&lt;/strong&gt; Though SL does a&#xA;  good job calibrating model performance against small data sets, it'd&#xA;  sure be nice to understand performance versus computational cost for&#xA;  larger data.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What are R's memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_forest&quot; rel=&quot;nofollow&quot;&gt;random forests&lt;/a&gt;?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Related:&lt;/strong&gt; Followup to the question above, &quot;&lt;a href=&quot;https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted&quot;&gt;When is a Model Underfitted?&lt;/a&gt;&quot;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;First, think it's worth me stating what I mean by replication &amp;amp; reproducibility:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.&lt;/li&gt;&#xA;&lt;li&gt;Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closet comparison I'm able to think of is &lt;a href=&quot;http://en.wikipedia.org/wiki/Documentation_generator&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation generators&lt;/a&gt; that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the &quot;reproducibility documentation generators&quot; the documentation should match.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, whole intent of this is to understand if it's possible to &quot;bake-in&quot; reproducibility documentation as you build out a stack, not after the stack is built.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;UPDATE:&lt;/strong&gt; Please note that this is the second draft of this question and that &lt;a href=&quot;https://datascience.stackexchange.com/users/178/christopher-louden&quot;&gt;Christopher Louden&lt;/a&gt; was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!&lt;/em&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;&lt;em&gt;(Note: Pulled this question from the &lt;a href=&quot;http://area51.stackexchange.com/proposals/55053/data-science/57398#57398&quot;&gt;list of questions in Area51&lt;/a&gt;, but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.)&lt;/em&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; Following is from the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1449356265&quot;&gt;Graph Databases&lt;/a&gt;, which covers a performance test mentioned in the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1617290769&quot;&gt;Neo4j in Action&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Relationships in a graph naturally form paths. Querying, or&#xA;  traversing, the graph involves following paths. Because of the&#xA;  fundamentally path-oriented nature of the datamodel, the majority of&#xA;  path-based graph database operations are highly aligned with the way&#xA;  in which the data is laid out, making them extremely efficient. In&#xA;  their book Neo4j in Action, Partner and Vukotic perform an experiment&#xA;  using a relational store and Neo4j.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The comparison shows that the graph database is substantially quicker&#xA;  for connected data than a relational store.Partner and Vukotic’s&#xA;  experiment seeks to find friends-of-friends in a social network, to a&#xA;  maximum depth of five. Given any two persons chosen at random, is&#xA;  there a path that connects them which is at most five relationships&#xA;  long? For a social network containing 1,000,000 people, each with&#xA;  approximately 50 friends, the results strongly suggest that graph&#xA;  databases are the best choice for connected data, as we see in Table&#xA;  2-1.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4j&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Depth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned&#xA;2       0.016                       0.01                         ~2500    &#xA;3       30.267                      0.168                        ~110,000 &#xA;4       1543.505                    1.359                        ~600,000 &#xA;5       Unfinished                  2.132                        ~800,000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, it’s clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4j’s response time remains relatively flat: just a fraction of a second to perform the query—definitely quick enough for an online system.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;At depth four the relational database exhibits crippling latency,&#xA;  making it practically useless for an online system. Neo4j’s timings&#xA;  have deteriorated a little too, but the latency here is at the&#xA;  periphery of being acceptable for a responsive online system. Finally,&#xA;  at depth five, the relational database simply takes too long to&#xA;  complete the query. Neo4j, in contrast, returns a result in around two&#xA;  seconds. At depth five, it transpires almost the entire network is our&#xA;  friend: for many real-world use cases, we’d likely trim the results,&#xA;  and the timings.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions are:&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is this a reasonable test to emulate what one might except to find in a social network? &lt;em&gt;(Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/The_rich_get_richer_%28statistics%29&quot;&gt;rich get richer&lt;/a&gt;&quot; model would be more natural for social networks, though might be wrong.)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible? &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is &lt;em&gt;parallel AND distributed&lt;/em&gt;?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Given website access data in the form &lt;code&gt;session_id, ip, user_agent&lt;/code&gt;, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;session_id&lt;/code&gt;: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;IP&lt;/code&gt; can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;User_agent&lt;/code&gt; is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data might look as the fiddle here:&#xA;&lt;a href=&quot;http://sqlfiddle.com/#!2/c4de40/1&quot;&gt;http://sqlfiddle.com/#!2/c4de40/1&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id&#xA;from &#xA;    (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr&#xA;    from&#xA;        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a&#xA;    join&#xA;        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b&#xA;        order by 1&#xA;    )d&#xA;inner join&#xA;    (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5&#xA;    union all select 6 union all select 7 union all select 8 union all select 9 )e&#xA;    on d.nr&amp;gt;=e.nr&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;For example, when searching something in Google, results return nigh-instantly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does MapReduce help solve this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Assume that we have a set of elements &lt;em&gt;E&lt;/em&gt; and a similarity (&lt;strong&gt;not distance&lt;/strong&gt;) function &lt;em&gt;sim(ei, ej)&lt;/em&gt; between two elements &lt;em&gt;ei,ej ∈ E&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could we (efficiently) cluster the elements of &lt;em&gt;E&lt;/em&gt;, using &lt;em&gt;sim&lt;/em&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;k&lt;/em&gt;-means, for example, requires a given &lt;em&gt;k&lt;/em&gt;, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note, that &lt;em&gt;sim&lt;/em&gt; is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of &lt;em&gt;E&lt;/em&gt;).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Consider a stream containing &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Tuple&quot; rel=&quot;nofollow&quot;&gt;tuples&lt;/a&gt; &lt;code&gt;(user, new_score)&lt;/code&gt; representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to have some standing queries like: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Which players posted more than x scores in a sliding window of one hour&lt;/li&gt;&#xA;&lt;li&gt;Which players gained x% score in a sliding window of one hour&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My question is which open source tools can I employ to jumpstart this project? I am considering &lt;a href=&quot;http://esper.codehaus.org/&quot; rel=&quot;nofollow&quot;&gt;Esper&lt;/a&gt; at the moment. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I have just completed reading &quot;Mining Data Streams&quot; (chapter 4 of &lt;a href=&quot;http://infolab.stanford.edu/~ullman/mmds.html&quot; rel=&quot;nofollow&quot;&gt;Mining of Massive Datasets&lt;/a&gt;) and I am quite new to mining data streams.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;When a relational database, like MySQL, has better performance than a no relational, like MongoDB?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw a question on Quora other day, about why Quora still uses MySQL as their backend, and that their performance is still good.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper names are like &quot;Assessment of Utility in Web Mining for the Domain of Public Health&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three crucial characteristics of my database:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;there's many more features than samples.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I mainly use Python, so Python-specific hints would be greatly appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I'm asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Also, your opinions on NoSQL databases, and any recommendations for learning about this topic are welcome.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process&quot;&gt;Hierarchical Dirichlet Process (HDP)&lt;/a&gt; are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn't. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm&quot;&gt;This question&lt;/a&gt; asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? &lt;strong&gt;How are generative and discriminative models used in NLP?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;From wikipedia, &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;dimensionality reduction or dimension reduction is the process of&#xA;  reducing the number of random variables under consideration, and&#xA;  can be divided into feature selection and feature extraction.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is the difference between feature selection and feature extraction?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is an example of dimensionality reduction in a Natural Language Processing task?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB's map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, people tend to reach out for C/C++/low-level languages. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course such set of benefits would not come without a cost: writing the code, and sometimes even &lt;em&gt;reinventing the wheel&lt;/em&gt;, can be quite expensive/tiresome. Although there are lots of libraries available, people are inclined to write the code by themselves whenever they need to &lt;em&gt;grant&lt;/em&gt; performance. What &lt;em&gt;disables&lt;/em&gt; performance assertions from using libraries while processing large databases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider an entreprise that continuously crawls webpages and parses the data collected. For each sliding-window, different data mining algorithms are run upon the data extracted. Why would the developers ditch off using available libraries/frameworks (be it for crawling, text processing, and data mining)? Using stuff already implemented would not only ease the burden of coding the whole process, but also would save a lot of time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In a single shot&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;what makes writing the code by oneself a &lt;em&gt;guarantee&lt;/em&gt; of performance?&lt;/li&gt;&#xA;&lt;li&gt;why is it &lt;em&gt;risky&lt;/em&gt; to rely on a frameworks/libraries when you must &lt;strong&gt;assure&lt;/strong&gt; high performance?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " ('&lt;p&gt;As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metrics from various sources for making a higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somewhat repetitive work. For example analyzing tweets, facebook posts, Wikipedia articles etc. is a part of a lot of big data problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of these data sets are accessible using public APIs provided by the provider site, but usually, some valuable information or metrics are missing from these APIs and everyone has to do the same analyses again and again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API nor available publicly in independent data sets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other big data problems? I mean something like GitHub (or a group of sites/public datasets or at least a comprehensive listing) for the data science. If not, what are the reasons for not having such a platform for data science? The commercial value of data, need to frequently update data sets, ...? Can we not have an open-source model for sharing data sets devised for data scientists?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I see a lot of courses in Data Science emerging in the last 2 years. Even big universities like Stanford and Columbia offers MS specifically in Data Science. But as long as I see, it looks like data science is just a mix of computer science and statistics techniques.&#xA;So I always think about this. If it is just a trend and if in 10 years from now, someone will still mention Data Science as an entire field or just a subject/topic inside CS or stats.&#xA;What do you think?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. &quot;nationality&quot; vs &quot;bornIn&quot;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to find some &quot;important&quot; attributes that somehow &quot;define&quot; the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the attribute &quot;lastName&quot; is more discriminative than the attribute &quot;nationality&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How could I (statistically) find such attributes that are more important than others?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A naive solution would be to take the average IDF of the values of each attribute and make this the &quot;importance&quot; factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have seen the term feature, or attribute selection in machine learning, but I don't want to discard the remaining attributes, I just want to put higher weights to the most important ones.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a modeling and scoring program that makes heavy use of the &lt;code&gt;DataFrame.isin&lt;/code&gt; function of pandas, searching through lists of facebook &quot;like&quot; records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have data coming from a source system that is pipe delimited. Pipe was selected over comma since it was believed no pipes appeared in field, while it was known that commas do occur. After ingesting this data into Hive however it has been discovered that rarely a field does in fact contain a pipe character.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to a constraint we are unable to regenerate from source to escape the delimiter or change delimiters in the usual way. However we have the metadata used to create the Hive table. Could we use knowledge of the fields around the problem field to reprocess the file on our side to escape it or to change the file delimiter prior to reloading the data into Hive?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the existing solutions is &lt;a href=&quot;http://www.stanford.edu/group/sonia/&quot;&gt;SoNIA: Social Network Image Animator&lt;/a&gt;. It let's you make movies like &lt;a href=&quot;https://www.youtube.com/watch?v=yGSNCED6mDc&quot;&gt;this one&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right after posting this question I'll dig into &lt;a href=&quot;http://sigmajs.org/&quot;&gt;sigma.js&lt;/a&gt;, so please consider this library covered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, my input data would be something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time_elapsed; node1; node2&#xA;1; A; B&#xA;2; A; C&#xA;3; B; C&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The details of the Google Prediction API are on this &lt;a href=&quot;https://developers.google.com/prediction/&quot;&gt;page&lt;/a&gt;, but I am not able to find any details about the prediction algorithms running behind the API. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have gathered that they let you provide your preprocessing steps in PMML format.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm learning &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;Support Vector Machines&lt;/a&gt;, and I'm unable to understand how a class label is chosen for a data point in a binary classifier. Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently using &lt;a href=&quot;http://en.wikipedia.org/wiki/General_Algebraic_Modeling_System&quot; rel=&quot;nofollow&quot;&gt;General Algebraic Modeling System&lt;/a&gt; (GAMS), and more specifically CPLEX within GAMS, to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores), and it finds an optimal solution in a relatively short amount of time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I'm open to suggestions of all kinds, and other users may be interested in different solutions.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Can someone explain me, how to classify a data like MNIST with MLBP-Neural network if I make more than one output (e.g 8), I mean if I just use one output I can easily classify the data, but if I use more than one, which output should I choose ?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;The most popular use case seem to be recommender systems of different kinds (such as recommending shopping items, users in social networks etc.).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what are other typical data science applications, which may be used in a different verticals?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: customer churn prediction with machine learning, evaluating customer lifetime value, sales forecasting.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a &quot;bucket&quot; that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Item A { 4-door, small, steel } =&amp;gt; { sedan }&#xA;Item B { 2-door, big,   steel } =&amp;gt; { truck }&#xA;Item C { 2-door, small, steel } =&amp;gt; { coupe }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I just want the rules that say &quot;if it's big and a 2-door, it's a truck,&quot; not the rules that say &quot;if it's a 4-door it's also small.&quot; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;LDA has two hyperparameters, tuning them changes the induced topics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does the alpha and beta hyperparameters contribute to LDA? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does the topic change if one or the other hyperparameters increase or decrease? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why are they hyperparamters and not just parameters?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Working on what could often be called &quot;medium data&quot; projects, I've been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I'm looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bandwidth and RAM are non-trivial concerns in my projects, but it's easy to spot when those are the bottlenecks and readjust. It's much harder, I'd imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I've just failed to google this one properly, feel free to point me to the right answer somewhere else!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It's like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;user_id item_id result&#xA;1       4       1&#xA;1       7       -1&#xA;5       19      1&#xA;5       80      1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 1&lt;/strong&gt;&#xA;Let us think about it in the context of a matrix factorization approach. If we treat -1 and 1 as ratings, there will be some problem. For example, user 1 likes movie A which scores high in one factor (e.g. having glorious background music) in the latent factor space. The system recommends movie B which also scores high in &quot;glorious background music&quot;, but for some reason user 1 is too busy to look into the recommendation, and we have a -1 rating movie B. If we just treat 1 or -1 equally, then the system might be discouraged to recommend movie with glorious BGM to user 1 while user 1 still loves movie with glorious BGM. I think this situation is to be avoided.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm new to this community and hopefully my question will well fit in here.&#xA;As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I'm concern this topic relates to Machine Learning and Support Vector Machines. I'm not well familiar with this technologies yet so I will need some help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have decided to follow this project idea &lt;a href=&quot;http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html&quot;&gt;http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html&lt;/a&gt; (first project on the top)&#xA;The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject's waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the data set is given in one folder with some description and feature labels. The data is divided for 'test' and 'train' files in which data is represented in this format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And that's only a very small sample of what the file contain. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use? &#xA;Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any hints/tips would be much appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm building a workflow for creating machine learning models (in my case, using Python's &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;sklearn&lt;/code&gt; packages) from data pulled from a very large database (here, Vertica by way of SQL and &lt;code&gt;pyodbc&lt;/code&gt;), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I'm curious where best to locate this step in a multi-platform workflow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's simple enough to do this in Python, either with the &lt;a href=&quot;http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values&quot;&gt;&lt;code&gt;sklearn.preprocessing.Imputer&lt;/code&gt;&lt;/a&gt; class, using the &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.fillna.html&quot;&gt;&lt;code&gt;pandas.DataFrame.fillna&lt;/code&gt;&lt;/a&gt; method, or by hand (depending upon the complexity of the imputation method used). But since I'm going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there's a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building &quot;complete&quot; versions of tables, so we don't need to fill in a new set of missing values from scratch every time we want to run a model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't been able to find much guidance about this, but I imagine that we could:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column&lt;/li&gt;&#xA;&lt;li&gt;join the substitute value table with the original table to assign a substitute value for each row and incomplete column&lt;/li&gt;&#xA;&lt;li&gt;use a series of case statements to take the original value if available and the substitute value otherwise&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;So, I have a dataset with 39.949 variables and 180 rows. dataset is successfully &#xA;saved in DataFrame but when I try to find cov() it result an error.&#xA;here is the code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  import pandas as pd&#xA;  cov_data=pd.DataFrame(dataset).cov()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;/home/syahdeini/Desktop/FP/pca_2.py&quot;, line 44, in find_eagen&#xA;cov_data=pd.DataFrame(data_mat).cov()&#xA;File &quot;/usr/lib/python2.7/dist-packages/pandas/core/frame.py&quot;, line 3716, in cov&#xA;baseCov = np.cov(mat.T)&#xA;File &quot;/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py&quot;, line 1766, in cov&#xA;return (dot(X, X.T.conj()) / fact).squeeze()&#xA;ValueError: array is too big.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I saw the company's manager last day.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;Person name=&quot;John&quot;&amp;gt;I&amp;lt;/Person&amp;gt; saw the &amp;lt;Organization name=&quot;ACME&quot;&amp;gt;company&amp;lt;/Organization&amp;gt;'s&#xA;manager &amp;lt;Time value=&quot;2014-5-29&quot;&amp;gt;last day&amp;lt;/Time&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The output of my word alignment file looks as such:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it . In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen .   0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28&#xA;It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it .    Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen .   0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I produce the phrase tables that are used by MOSES from this output?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Going through the presentation and material of Summingbird by Twitter, one of the reasons that is mentioned for using Storm and Hadoop clusters together in Summingbird is that processing through Storm results in cascading of error. In order to avoid this cascading of error and accumulation of it, Hadoop cluster is used to batch process the data and discard the Storm results after the same data is processed by Hadoop. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the reasons for generation of this accumulation of error? and why is it not present in Hadoop? Since I have not worked with Storm, I do not know the reasons for it. Is it because Storm uses some approximate algorithm to process the data in order to process them in real time? or is the cause something else?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I want to test the accuracy of a methodology. I ran it ~400 times, and I got a different classification for each run. I also have the ground truth, i.e., the real classification to test against.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;May I sum all confusion matrices in order to obtain the overall one?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;As Yann LeCun &lt;a href=&quot;http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/chisdw1&quot; rel=&quot;noreferrer&quot;&gt;mentioned&lt;/a&gt;, a number of PhD programs in data science will be popping up in the next few years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://datascience.nyu.edu/academics/programs/&quot; rel=&quot;noreferrer&quot;&gt;NYU&lt;/a&gt; already have one, where Prof.LeCun is at right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A statistics or cs PhD in machine learning is probably more rigorous than a data science one.  Is data science PhD for the less mathy people like myself? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are these cash cow programs?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a huge industry demand for big data, but what is the academic value of these programs, as you probably can't be a professor or publish any paper.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Data visualization is an important sub-field in data science and python programmers would need to have available toolkits for them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there a Python API to Tableau?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there any Python-based data visualization toolkits?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;There seem to be at least 2 ways to connect to HBase from external application, with language other then Java (i.e. Python):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;HBase Thrift API&lt;/li&gt;&#xA;&lt;li&gt;HBase Stargate (REST API) &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Does anyone know which one should be used in which circumstances?&#xA;I.e. what are their main differences, and pros/cons?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;An aspiring data scientist here. I don't know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back &lt;em&gt;theoretically should&lt;/em&gt; have a tendency to run the ball in that situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Scoring might be more difficult to predict, but the winning team might be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Being new to machine-learning in general, I'd like to start playing around and see what the possibilities are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am developing a system that is intended to capture the &quot;context&quot; of user activity within an application; it is a framework that web applications can use to tag user activity based on requests made to the system.  It is hoped that this data can then power ML features such as context aware information retrieval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm having trouble deciding on what features to select in addition to these user tags - the URL being requested, approximate time spent with any given resource, estimating the current &quot;activity&quot; within the system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested to know if there are good examples of this kind of technology or any prior research on the subject - a cursory search of the ACM DL revealed some related papers but nothing really spot-on.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Yann LeCun mentioned in his &lt;a href=&quot;http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/&quot; rel=&quot;nofollow&quot;&gt;AMA&lt;/a&gt; that he considers having a PhD very important in order to get a job at a top company.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a masters in statistics and my undergrad was in economics and applied math, but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;There is plenty of hype surrounding Hadoop and its eco-system.  However, in practice, where many data sets are in the terabyte range, is it not more reasonable to use &lt;a href=&quot;http://aws.amazon.com/redshift/&quot;&gt;Amazon RedShift&lt;/a&gt; for querying large data sets, rather than spending time and effort building a Hadoop cluster? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have read lot of blogs\\\\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What kinda data these companies used. What was the size of the data&lt;/li&gt;&#xA;&lt;li&gt;What kinda of tools technologies they used to process the data&lt;/li&gt;&#xA;&lt;li&gt;What was the problem they were facing and how the insight they got the data helped them to resolve the issue.&lt;/li&gt;&#xA;&lt;li&gt;How they selected the tool\\\\technology to suit their need.&lt;/li&gt;&#xA;&lt;li&gt;What kinda pattern they identified from the data &amp;amp; what kind of patterns they were looking from the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be great if someone share how finance industry is making use of Big Data Analytic.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are about 1,150 known &quot;positives&quot;, against a background of about 13 million protein sequences (&quot;Unknown/poorly annotated background&quot;), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly &quot;negative&quot; way). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My previous implementation looked at this as a binary classification problem: &#xA;Positive set = Proteins marked as Neuropeptides.&#xA;Negative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it's at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts were to:&#xA;1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set.&#xA; (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) . &#xA;2) One class learning - Would be nice, but as I understand it, it's meant just for anomaly detection, and has poorer performance than discriminatory approaches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*) I've heard of P-U learning, which sounds neat, but I'm a programming N00b, and I don't know of any existing implementations for it. (In Python/sci-kit learn).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the &quot;negative&quot; proteins, but they're all very very different from each other, so I don't know how well the classifier would handle them as one big , unbalanced mix).&#xA;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;What are the books about the science and mathematics behind data science? It feels like so many &quot;data science&quot; books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples: Agresti's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470463635&quot;&gt;Categorical Data Analysis&lt;/a&gt;, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441902996&quot;&gt;Linear Mixed Models for Longitudinal Data&lt;/a&gt;, etc... etc...&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've implemented gradient checking, double checked everything etc and I'm pretty certain it's working correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've run it a few times and it reaches 'Optimization terminated successfully' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be interested to understand what's going on here, or if I've implemented neural net incorrectly?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;The setup is simple: binary classification using a simple decision tree, each node of the tree has a single threshold applied on a single feature. In general, building a ROC curve requires moving a decision threshold over different values and computing the effect of that change on the true positive rate and the false positives rate of predictions. What's that decision threshold in the case of a simple fixed decision tree? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to extract news about a company from online news by using RODBC package in R. And I want to use the extracted data for sentiment analysis. I want to accomplish this in such a way that the positive news is assigned a value of +1, the negative news is assigned a value of -1 and the neutral news is assigned a value of 0.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm just starting to develop a &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; application for academic purposes. I'm currently using &lt;strong&gt;R&lt;/strong&gt; and training myself in it. However, in a lot of places, I saw people using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are people using in academia and industry, and what is the recommendation?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've now seen two data science certification programs - the &lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage&quot;&gt;John Hopkins one available at Coursera&lt;/a&gt; and the &lt;a href=&quot;http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html&quot;&gt;Cloudera one&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure there are others out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;R Programming&lt;/li&gt;&#xA;&lt;li&gt;cleaning and obtaining data&lt;/li&gt;&#xA;&lt;li&gt;Data Analysis&lt;/li&gt;&#xA;&lt;li&gt;Reproducible Research&lt;/li&gt;&#xA;&lt;li&gt;Statistical Inference&lt;/li&gt;&#xA;&lt;li&gt;Regression Models&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning&lt;/li&gt;&#xA;&lt;li&gt;Developing Data Products&lt;/li&gt;&#xA;&lt;li&gt;And what looks to be a Project based completion task similar to Cloudera's Data Science Challenge&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The Cloudera program looks thin on the surface, but looks to answer the two important questions - &quot;Do you know the tools&quot;, &quot;Can you apply the tools in the real world&quot;.  Their program consists of:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Introduction to Data Science&lt;/li&gt;&#xA;&lt;li&gt;Data Science Essentials Exam&lt;/li&gt;&#xA;&lt;li&gt;Data Science Challenge (a real world data science project scenario)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am not looking for a recommendation on a program or a quality comparison.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: These are all great answers.  I'm choosing the correct answer by votes.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Could you give some examples of typical tasks that a data scientist does in his daily job, and the must-know minimum for each of the levels (like junior, senior, etc. if there are any)? If possible, something like a &lt;a href=&quot;http://www.starling-software.com/employment/programmer-competency-matrix.html&quot; rel=&quot;nofollow&quot;&gt;Programmer competency matrix&lt;/a&gt;.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;In some cases, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20975147&quot;&gt;it may be impossible&lt;/a&gt; to draw Euler diagrams with overlapping circles to represent all the overlapping subsets in the correct proportions. This type of data then requires using polygons or other figures to represent each set. When dealing with data that describes overlapping subsets, how can I figure out whether a simple Euler diagram is possible?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;as I am very interested in programming and statistics, Data Science seems like a great career path to me - I like both fields and would like to combine them. Unfortunately, I have studied political science with a non-statistical sounding Master. I focused on statistics in this Master, visiting optional courses and writing a statistical thesis on a rather large dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since almost all job adds are requiring  a degree in informatics, physics or some other techy-field, I am wondering if there is a chance to become a data scientist or if I should drop that idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am lacking knowledge in machine learning, sql and hadoop, while having a rather strong informatics and statistics background. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So can somebody tell me how feasible my goal of becoming a data scientist is? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I attack this problem frequently with inefficiency because it's always pretty low on the priority list and my clients are resistant to change until things break.  I would like some input on how to speed things up.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have multiple datasets of information in a SQL database.  The database is vendor-designed, so I have little control over the structure.  It's a sql representation of a class-based structure.  It looks a little bit like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Main-class table&#xA; -sub-class table 1&#xA; -sub-class table 2&#xA;  -sub-sub-class table&#xA; ...&#xA; -sub-class table n&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Each table contains fields for each attribute of the class.  A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class' table, joined by a unique identifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are hundreds of classes. which means thousands of views and tens of thousands of columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Beyond that, there are multiple datasets, indicated by a field value in the Main-class table.  There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again.  The rules are in place because we might trust one source of data more than another for a particular value of a particular class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row.  The common changes are limited to a handful of fields in each row, but since anything can change we compare each value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours.  We resolve that problem generally, by throwing more hardware at it, but this isn't a hadoop environment currently so there's a pretty finite limit to what can be done in that regard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you go about scaling a solution to this problem such that there were no limitations?  And how would you go about accomplishing the most efficient data-merge?  (currently it is field by field comparisons... painfully slow).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Logic often states that by underfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is a followup to my question, &quot;&lt;a href=&quot;https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/&quot;&gt;Why Is Overfitting Bad?&lt;/a&gt;&quot;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What kind of error measures do RMSE and nDCG give while evaluating a recommender system, and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'd like to explore 'data science'. The term seems a little vague to me, but I expect it to require:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;machine learning (rather than traditional statistics);&lt;/li&gt;&#xA;&lt;li&gt;a large enough dataset that you have to run analyses on clusters.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What are some good datasets and problems, accessible to a statistician with some programming background, that I can use to explore the field of data science?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm curious about natural language querying.  Stanford has what looks to be a strong set of &lt;a href=&quot;http://nlp.stanford.edu/software/index.shtml&quot; rel=&quot;noreferrer&quot;&gt;software for processing natural language&lt;/a&gt;.  I've also seen the &lt;a href=&quot;http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html&quot; rel=&quot;noreferrer&quot;&gt;Apache OpenNLP library&lt;/a&gt;, and the &lt;a href=&quot;http://gate.ac.uk/science.html&quot; rel=&quot;noreferrer&quot;&gt;General Architecture for Text Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first rectangle on my flow chart is a bit of a mystery.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/wJPx9.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I might want to know:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;How many books were sold last month?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I'd want that translated into&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Select count(*) &#xA;  from sales &#xA;  where &#xA;   item_type='book' and &#xA;   sales_date &amp;gt;= '5/1/2014' and &#xA;   sales_date &amp;lt;= '5/31/2014'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The majority of people use S3. However, Google Drive seems a promising alternative solution for storing large amounts of data. Are there specific reasons why one is better than the other?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we're able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), and depending on what is being evaluated, it may raise &lt;em&gt;false/unfair&lt;/em&gt; comparisons. For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, how can one honestly compare different experiments to point out efficiency gains? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've came across the following problem, that I recon is rather typical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some large data, say, a few million rows. I run some non-trivial analysis on it, e.g. an SQL query consisting of several sub-queries. I get some result, stating, for example, that property X is increasing over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, there are two possible things that could lead to that:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;X is indeed increasing over time&lt;/li&gt;&#xA;&lt;li&gt;I have a bug in my analysis&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;How can I test that the first happened, rather than the second? A step-wise debugger, even if one exists, won't help, since intermediate results can still consist of millions of lines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only thing I could think of was to somehow generate a small, synthetic data set with the property that I want to test and run the analysis on it as a unit test. Are there tools to do this? Particularly, but not limited to, SQL.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a binary classification problem:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Approximately 1000 samples in training set&lt;/li&gt;&#xA;&lt;li&gt;10 attributes, including binary, numeric and categorical&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which algorithm is the best choice for this type of problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default I'm going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it'd become a burden to continuously validate the &lt;em&gt;current state&lt;/em&gt; of each spammer blocked, checking if the site/domain still disseminate spam data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: if possible, any example of strategy, even if just the intuition behind it, would be very welcome along with the answer.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In view of this I face the following dilemma: Should I learn Python to be able to use its extensive scientific libraries (Pandas, Numpy, Scipy, ...), or should I just dive into similar packages of a language I'm already acquainted with (Racket, or to a lesser extent Scala)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Ideally I would learn Python in parallel with using statistical libraries in Racket, but I'm not sure I'll have time for both)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not looking for an answer to this dilemma, but rather for feedback on my different considerations:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current position is as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In favour of Python:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Extensively used libraries&lt;/li&gt;&#xA;&lt;li&gt;Widely used (may be decisive in case of collaboration with others)&lt;/li&gt;&#xA;&lt;li&gt;A lot of online material to start learning it&lt;/li&gt;&#xA;&lt;li&gt;Conferences that are specifically dedicated to Scientific Computing with Python&lt;/li&gt;&#xA;&lt;li&gt;Learning Python won't be a waste of time anyway&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In favour of a language I already know:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It's a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)&lt;/li&gt;&#xA;&lt;li&gt;It is feasible. Both Racket and Scala have good mathematics and statistics libraries&lt;/li&gt;&#xA;&lt;li&gt;I can start right away with learning what I need to know rather than first having to learn the basics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Two concrete questions:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What am I forgetting?&lt;/li&gt;&#xA;&lt;li&gt;How big of a nuisance could the Python 2 vs 3 issue be?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4Ih6o.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to do Logistic Regression using SAS Enterprise Miner. &#xA;My Independent variables are &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CPR/Inc (Categorical 1 to 7)&#xA;OD/Inc (Categorical 1 to 4)&#xA;Insurance (Binary 0 or 1)&#xA;Income Loss (Binary 0 or 1)&#xA;Living Arrangement (Categorical 1 to 7)&#xA;Employment Status (categorical 1 to 8)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My Dependent Variable is Default (Binary 0 or 1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following is the output from running Regression Model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Analysis of Maximum Likelihood Estimates&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                                  Standard          Wald&#xA;Parameter       DF    Estimate       Error    Chi-Square    Pr &amp;gt; ChiSq    Exp(Est)&#xA;&#xA;Intercept        1     -0.4148      0.0645         41.30        &amp;lt;.0001       0.660&#xA;CPR___Inc  1     1     -0.8022      0.1051         58.26        &amp;lt;.0001       0.448&#xA;CPR___Inc  2     1     -0.4380      0.0966         20.57        &amp;lt;.0001       0.645&#xA;CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363&#xA;CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997&#xA;CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142&#xA;CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185&#xA;Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795&#xA;Emp_Status 2     1      0.4061      0.0940         18.66        &amp;lt;.0001       1.501&#xA;Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809&#xA;Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116&#xA;Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796&#xA;Emp_Status 6     1      0.3761      0.0943         15.91        &amp;lt;.0001       1.457&#xA;Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716&#xA;Inc_Loss   0     1     -0.1996      0.0449         19.76        &amp;lt;.0001       0.819&#xA;Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134&#xA;Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893&#xA;Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294&#xA;Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024&#xA;Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100&#xA;Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952&#xA;Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689&#xA;OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808&#xA;OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973&#xA;OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I used this Model to Score a new set of data. An example row of my new data is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CPR - 7&#xA;OD - 4&#xA;Living Arrangement - 4&#xA;Employment Status - 4&#xA;Insurance - 0&#xA;Income Loss - 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For this sample row, the model predicted output (Probability of default = 1) as 0.7335 &#xA;To check this manually, I added the estimates&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0&#xA;-0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Odds ratio = Exponential(-0.0839) = 0.9195&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am unable to understand why there is such a mismatch between the Model's predicted probability and theoretical probability. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be much appreciated .&#xA;Thanks&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked into machine learning algorithms, but I'm not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN (with what I think are sensible parameters and distance function).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently working on implementing Stochastic Gradient Descent, &lt;code&gt;SGD&lt;/code&gt;, for neural nets using back-propagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?&lt;/li&gt;&#xA;&lt;li&gt;If so, how do you use this information to inform your decision about a value?&lt;/li&gt;&#xA;&lt;li&gt;If it's not what sort of values should I choose, and how should I choose them?&lt;/li&gt;&#xA;&lt;li&gt;It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?&lt;/li&gt;&#xA;&lt;li&gt;Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In short: How do I choose the learning rate for SGD?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;It seems as though most languages have some number of scientific computing libraries available. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Python has &lt;code&gt;Scipy&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Rust&lt;/code&gt; has &lt;code&gt;SciRust&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;C++&lt;/code&gt; has several including &lt;code&gt;ViennaCL&lt;/code&gt; and &lt;code&gt;Armadillo&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Java&lt;/code&gt; has &lt;code&gt;Java Numerics&lt;/code&gt; and &lt;code&gt;Colt&lt;/code&gt; as well as several other&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Not to mention languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Julia&lt;/code&gt; designed explicitly for scientific computing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. And will anything ever outperform &lt;code&gt;Fortran&lt;/code&gt;? Additionally compiled languages tend to have GPU acceleration, while interpreted languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; don't. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I've missed?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;h1&gt;Motivation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Anonymizing Confidential Data&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What Doesn't Work&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &amp;lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &amp;lt;= 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the only method I am familiar with that does this is &lt;a href=&quot;http://www.techrepublic.com/blog/it-security/cryptographys-running-gag-rot13/&quot; rel=&quot;noreferrer&quot;&gt;ROT13&lt;/a&gt; (more generally, any &lt;a href=&quot;https://en.wikipedia.org/wiki/Caesar_cipher&quot; rel=&quot;noreferrer&quot;&gt;shift cipher&lt;/a&gt;), which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;Promise you won't flip the paper over?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another &lt;strong&gt;bad&lt;/strong&gt; solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+----+-------------------+-----------+--------+&#xA;| Row | ID | Name              | WordChars | Origin |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;| 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |&#xA;+-----+----+-------------------+-----------+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp;amp; 3, or about the distance between rows 2 &amp;amp; 4 (i.e., how close they are to matching).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found a couple papers that might be relevant, but they're a bit over my head:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.merl.com/publications/docs/TR2010-109.pdf&quot; rel=&quot;noreferrer&quot;&gt;Privacy Preserving String Comparisons Based on Levenshtein Distance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf&quot; rel=&quot;noreferrer&quot;&gt;An Empirical Comparison of Approaches to Approximate String &#xA;Matching in Private Record Linkage&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Does anyone know some good tutorials on online machine learning technics?&#xA;I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD: Thank you everyone for answers, by &quot;online&quot; I mean methods which can be trained in a real-time mode, based on a new inputs one by one.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;As an extension to our great list of &lt;a href=&quot;https://datascience.stackexchange.com/questions/155/publicly-available-datasets&quot;&gt;publicly available datasets&lt;/a&gt;, I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the name of the social network;&lt;/li&gt;&#xA;&lt;li&gt;what kind of user information it provides (posts, profile, friendship network, ...);&lt;/li&gt;&#xA;&lt;li&gt;whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);&lt;/li&gt;&#xA;&lt;li&gt;whether it simply provides a snapshot of the whole dataset.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any suggestions and further characteristics to be added are very welcome.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm planning to run experiments with large datasets on distributed system in order to evaluate efficiency gains in comparison with previous proposals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have limited number of machines nearly ten machines having 200 GB of free space on hard disk on each. On the contrary, I wished to perform experiments on more than available nodes in order to measure scalability, &lt;strong&gt;more precisely&lt;/strong&gt;. Since I don't have any, I thought about using a commodity cluster. However, I'm not sure about the policies of usage, and I need to reliably measure execution times. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there commodity services which will grant me that only my application would be running at a given time?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I recently saw a cool feature that &lt;a href=&quot;https://support.google.com/docs/answer/3543688?hl=en&quot;&gt;was once available&lt;/a&gt; in Google Sheets: you start by writing a few related keywords in consecutive cells, say: &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, and it automatically generates similar keywords (in this case, other colors). See more examples in &lt;a href=&quot;http://youtu.be/dlslNhfrQmw&quot;&gt;this YouTube video&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to reproduce this in my own program. I'm thinking of using Freebase, and it would work like this intuitively: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Retrieve the list of given words in Freebase;&lt;/li&gt;&#xA;&lt;li&gt;Find their &quot;common denominator(s)&quot; and construct a distance metric based on this;&lt;/li&gt;&#xA;&lt;li&gt;Rank other concepts based on their &quot;distance&quot; to the original keywords;&lt;/li&gt;&#xA;&lt;li&gt;Display the next closest concepts.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;As I'm not familiar with this area, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is there a better way to do this?&lt;/li&gt;&#xA;&lt;li&gt;What tools are available for each step?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is anyone using &lt;code&gt;Julia&lt;/code&gt; (&lt;a href=&quot;http://julialang.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://julialang.org/&lt;/a&gt;) for professional jobs?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or using it instead of R, Matlab, or Mathematica? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it a good language?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have to predict next 5-10 years: Do you think it grow up enough to became such a standard in data science like R or similar?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to understand how all the &quot;big data&quot; components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can't run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use &lt;a href=&quot;http://en.wikipedia.org/wiki/Inductive_logic_programming&quot;&gt;Inductive Logic Programming&lt;/a&gt; (ILP), but due to special circumstances I can't do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other way to tackle this would be just to try to aggregate the values when I have a foreign relation. However, I have thousands of important and distinct rows for some nominal attributes (e.g.: A patient with a relation to several distinct drug prescriptions). So, I just can't do that without creating a new attribute for each distinct row of that nominal attribute, and furthermore most of the new columns would have NULL values if I do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any non-ILP algorithm that allows me to data mine relational databases without resorting to techniques like pivoting, which would create thousands of new columns?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I think that Bootstrap can be useful in my work, where we have a lot a variables that we don't know the distribution of it. So, simulations could help.&#xA;What are good sources to learn about Bootstrap/other useful simulation methods?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I've read through the introduction documentation for Spark, but I'm curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a large number of samples which represent Manchester encoded bit streams as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have manually decoded these streams, but I was wondering if I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? &lt;a href=&quot;http://en.wikipedia.org/wiki/Independent_component_analysis&quot; rel=&quot;nofollow&quot;&gt;Independent Component Analysis&lt;/a&gt; (ICA) seems like could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm developing a distributed application, and as it's been designed, there'll be a great load of communication during the processing. Since the communication is already as much &lt;em&gt;spread&lt;/em&gt; along the entire process as possible, I'm wondering if there any standard solutions to improve the performance of the message passing layer of my application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What changes/improvements could I apply to my code to reduce the time spent sending messages? For what it's worth, I'm communicating up to 10GB between 9 computing nodes, and the framework I'm using is implemented with OpenMPI.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I understand that compression methods may be split into two main sets: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;global&lt;/li&gt;&#xA;&lt;li&gt;local&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing on any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting information that usually improves the compression rate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While reading about some of these methods, I noticed that &lt;a href=&quot;http://en.wikipedia.org/wiki/Universal_code_%28data_compression%29#Universal_and_non-universal_codes&quot; rel=&quot;nofollow noreferrer&quot;&gt;the unary method is not universal&lt;/a&gt;, which surprised me since I thought &quot;globality&quot; and &quot;universality&quot; referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My primary questions:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is the difference between universal and global methods? &lt;/li&gt;&#xA;&lt;li&gt;Aren't these classifications synonyms?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it's difficult to create training datasets that contain enough examples of the positive class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Which freely available datasets can I use to train a text classifier?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are trying to enhance our users engagement by recommending the most related content for him, so we thought If we classified our content based on a predefined bag of words we can recommend to him engaging content by getting his feedback on random number of posts already classified before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can use this info to recommend for him pulses labeled with those classes. But we found If we used a predefined bag of words not related to our content the feature vector will be full of zeros, also categories may be not relevant to our content. so for those reasons we tried another solution that will be clustering our content not classifying it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks :)&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;K-means&lt;/a&gt; is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;There's this side project I'm working on where I need to structure a solution to the following problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two groups of people (clients). Group &lt;code&gt;A&lt;/code&gt; intends to buy and group &lt;code&gt;B&lt;/code&gt; intends to sell a determined product &lt;code&gt;X&lt;/code&gt;. The product has a series of attributes &lt;code&gt;x_i&lt;/code&gt;, and my objective is to facilitate the transaction between &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; by matching their preferences. The main idea is to point out to each member of &lt;code&gt;A&lt;/code&gt; a corresponding in &lt;code&gt;B&lt;/code&gt; whose product better suits his needs, and vice versa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some complicating aspects of the problem:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Attributes might be continuous, binary, or non-quantifiable (ex: price, functionality, design);&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any suggestion on how to approach this problem and solve it in an automated way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also appreciate some references to other similar problems if possible.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Great suggestions! Many similarities in to the way I'm thinking of approaching the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Let’s take an example of a car. The product “car” has lots and lots of attributes that range from its performance, mechanical structure, price etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I just want a cheap car, or an electric car. Ok, that's easy to map because they represent main features of this product. But let’s say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn't ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them. Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I'm thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next step would be a “refined search”. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot “guess” what the buyer needs. But might get me close.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The weighting criteria suggested is great. It allows me to quantify the level to which the seller matches the buyer’s needs. The scaling part might be a problem though, because the importance of each attribute varies from client to client. I'm thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I can't find the reason. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not an expert so I would like to ask you what I can improve and what I'm doing wrong. This is the complete description: &lt;a href=&quot;https://github.com/denadai2/Gas-consumption-outliers&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/denadai2/Gas-consumption-outliers&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The neural network is a FeedFoward Network with Back Propagation. As described &lt;a href=&quot;http://nbviewer.ipython.org/github/denadai2/Gas-consumption-outliers/blob/master/3-%20Regression_NN.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; I splitted the dataset in a &quot;small&quot; dataset of 41'000 rows, 9 features and I tried to add more features. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I trained the networks but the results have 14.14 RMSE, so it can't predict so well the gas consumptions, consecutively I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I improve? What am I doing wrong? Can you have a look of my description?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working on a project and need resources to get me up to speed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The dataset is around 35000 observations on 30 or so variables.  About half the variables are categorical with some having many different possible values, i.e. if you split the categorical variables into dummy variables you would have a lot more than 30 variables.  But still probably on the order of a couple of hundred max.  (n&gt;p).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The response we want to predict is ordinal with 5 levels (1,2,3,4,5).  Predictors are a mix of continuous and categorical, about half of each.  These are my thoughts/plans so far:&#xA;1.  Treat the response as continuous and run vanilla linear regression.&#xA;2.  Run nominal and ordinal logistic and probit regression&#xA;3.  Use MARS and/or another flavor of non-linear regression&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm familiar with linear regression.  MARS is well enough described by Hastie and Tibshirani.  But I'm at a loss when it comes to ordinal logit/probit, especially with so many variables and a big data set.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The r package &lt;a href=&quot;http://cran.r-project.org/web/packages/glmnetcr/index.html&quot;&gt;glmnetcr&lt;/a&gt; seems to be my best bet so far, but the documentation hardly suffices to get me where I need to be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where can I go to learn more?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The usual definition of regression (as far as I am aware) is &lt;em&gt;predicting a continuous output variable from a given set of input variables&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Logistic regression is a binary classification algorithm, so it produces a categorical output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it really a regression algorithm? If so, why?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;In network structure, what is the difference between &lt;strong&gt;k-cliques&lt;/strong&gt; and &lt;strong&gt;p-cliques&lt;/strong&gt;, can anyone give a brief explaination with examples? Thanks in advanced!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;============================&#xA;&lt;br&gt;EDIT:&#xA;I found an online &lt;a href=&quot;http://open.umich.edu/sites/default/files/SI508-F08-Week7-Lab6.ppt&quot; rel=&quot;nofollow&quot;&gt;ppt&lt;/a&gt; while I am googling, please take a look on &lt;strong&gt;p.37&lt;/strong&gt; and &lt;strong&gt;p.39&lt;/strong&gt;, can you comment on them?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;There's this side project I'm working on where I need to structure a solution to the following problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two groups of people (clients). Group &quot;A&quot; intends to buy and group &quot;B&quot; intends to sell a determined product &quot;X&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The product has a series of attributes x_i and my objective is to facilitate the transaction between &quot;A&quot; e &quot;B&quot; by matching their preferences. The main idea is to point out to each member of &quot;A&quot; a corresponding in &quot;B&quot; who’s product better suits his needs, and vice versa. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some complicating aspects of the problem:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any suggestion on how to approach this problem and solve it in an automated way?&lt;br&gt;&#xA;The idea is to really think out of the box here so feel free to &quot;go wild&quot; on your suggestions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also appreciate some references to other similar problems if possible. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;My data set is formatted like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;User-id | Threat_score&#xA;aaa       45&#xA;bbb       32&#xA;ccc       20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The list contains the top 100 users with the highest threat scores. I generate such a list monthly and store each month's list in its own file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three things I would like to get from this data:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;1. Users who are consistently showing up in this list&lt;/ul&gt;&#xA;&#xA;&lt;ul&gt;2. Users who are consistently showing up in this list with high threat scores&lt;/ul&gt;&#xA;&#xA;&lt;ul&gt;3. Users whose threat scores are increasing very quickly&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am thinking a visual summary would be nice; each month (somehow) decide which users I want to plot on a graph of historic threat scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any known visualization techniques that deal with similar requirements?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I be transforming my current data to achieve what I am looking for?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example scenario:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business's daily walk in client's following a specific event?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am attempting to determine the impact that a particular marketing agency has on their client's website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently working with a dataset with a wide range of document lengths -- anywhere from a single word to a full page of text.  In addition, the grammatical structure and use of punctuation varies wildly from document to document.  The goal is to classify those documents into one of about 10-15 categories.  I'm currently using ridge regression and logistic regression for the task, and CV for the alpha values of ridge.  The feature vectors are tf-idf ngrams.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Recently I've noticed that longer documents are much less likely to be categorized. Why might this be the case, and how can one &quot;normalize&quot; for this kind of variation?  As a more general question, how does one typically deal with diverse data sets?  Should documents be grouped based off of metrics like document length, use of punctuation, grammatical rigor, etc. and then fed through different classifiers?  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Classification algorithms used: random forest &amp;amp; logistic regression.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working on multiclass logistic regression model with a large number of features (numFeatures &gt; 100). Using a Maximum Likelihood Estimation based on the cost function and gradient, the fmincg algorithm solves the problem quickly. However, I'm also experimenting with a different cost function and do not have a gradient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good way to speed up the calculation process? E.g., is there a different algorithm or fmincg setting that I can use?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Please, could someone recommend a paper or blog post that describes the online k-means algorithm.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm&quot; rel=&quot;nofollow noreferrer&quot;&gt;Smith-Waterman Algorithm&lt;/a&gt; to compare the similarity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I've drawn a picture of how I'm thinking about representing the data - &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/VC4em.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Image that this matrix represents a key type of &quot;things&quot; I then need to add the &quot;things&quot; similarity score into a vector of 0 or 1. Thats ok.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;There is a general recommendation that algorithms in ensemble learning combinations should be different in nature. Is there a classification table, a scale or some rules that allow to evaluate how far away are the algorithms from each other? What are the best combinations? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a dataset with following specifications:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Training dataset with 193176 samples with 2821 positives&lt;/li&gt;&#xA;&lt;li&gt;Test Dataset with 82887 samples with 673 positives&lt;/li&gt;&#xA;&lt;li&gt;There are 10 features.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mean square error : 0.00804710026904&#xA;Confusion matrix : [[82214   667]&#xA;                   [    0     6]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Different algorithms like RandomForest, DecisionTree, SVM&lt;/li&gt;&#xA;&lt;li&gt;Changing parameters value to call the function&lt;/li&gt;&#xA;&lt;li&gt;Some intuition based feature engineering to include compounded features&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Now, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What can I do to improve the number of positive hits ? &lt;/li&gt;&#xA;&lt;li&gt;How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )&lt;/li&gt;&#xA;&lt;li&gt;At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )&lt;/li&gt;&#xA;&lt;li&gt;Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?&lt;/li&gt;&#xA;&lt;li&gt;Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am using the scikit-learn library with Python and all implementations are library functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the results with a few other algorithms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Random Forest Classifier(n_estimators=100)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[82211   667]&#xA;[    3     6]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Decision Trees:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[78611   635]&#xA;[ 3603    38]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;More often than not, data I am working with is not 100% clean. Even if it is reasonably clean, still there are portions that need to be fixed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When a fraction of data needs it, I write a script and incorporate it in data processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;,  such as that in CSV files or a relational database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The practical problems I encountered:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).&lt;/li&gt;&#xA;&lt;li&gt;Copying and modifying data may make a mess, as:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generating it again will destroy all fixes.&lt;/li&gt;&#xA;&lt;li&gt;When there are more errors of different kinds, too many copies of the same file result, and it is hard to keep track of them all.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, &lt;em&gt;seems&lt;/em&gt; to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What are the best practices in such a case as this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: The question is on the workflow, not whether to use it or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(In my particular case I don't want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Data Science and Machine Learning include a lot of different topics and it´s hard to stay up-to-date about all the news about papers, researches or new tutorials and tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What sources do you use to get all the information?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use mostly Reddit as my first source and the subreddits Machine Learning and R&#xA;&lt;a href=&quot;http://www.reddit.com/r/MachineLearning&quot; rel=&quot;nofollow&quot;&gt;http://www.reddit.com/r/MachineLearning&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.reddit.com/r/rstats&quot; rel=&quot;nofollow&quot;&gt;http://www.reddit.com/r/rstats&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But also datatau.com and of course the great KDNuggets page &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;What is the very first thing you do when you get your hands on a new data set (assuming it is cleaned and well structured)? Please share sample code snippets as I am sure this would be extremely helpful for both beginners and experienced. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;ADD:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As mentioned in a response below, I also considered (but forgot to add here) using Dynamic Time Warping (DTW) and then hierarchical clustering on the resulting distance matrix - creating some number of clusters and then using the cluster membership as a feature. Scoring test data would likely have to follow a process where the DTW was done on new cases and the cluster centroids - matching the new data series to their closest centroids... &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Practices to make sure the dataset is clean?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Practices to make sure the dataset doesn't have wrong values or so?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The task is the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there's a cat present so that I may go chase it off using my super soaker. For simplicity's sake, say that I only care about a cat with black and white coloring.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/cNqus.jpg&quot; alt=&quot;Sample garden image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xA;a,b,.2&#xA;a,c,.3&#xA;a,d,.4&#xA;a,e,.1&#xA;e,b,.3&#xA;e,c,.5&#xA;e,d,.8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've looked at &lt;a href=&quot;https://github.com/mbostock/d3/wiki/Gallery&quot; rel=&quot;nofollow noreferrer&quot;&gt;some graphs&lt;/a&gt;, but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business that for these users the recommendation system is useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;@Steve Kallestad do you mean something like this : &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4zyQR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;As I increase the number of trees in &lt;a href=&quot;http://scikit-learn.org/stable/&quot; rel=&quot;nofollow&quot;&gt;scikit learn&lt;/a&gt;'s &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features, most of which are binary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the parameters that I was tuning were:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the number of trees/iterations;&lt;/li&gt;&#xA;&lt;li&gt;learning depth;&lt;/li&gt;&#xA;&lt;li&gt;and learning rate.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code is something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;X_train, X_test, y_train, y_test = train_test_split(X, y)&#xA;&#xA;reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = 'ls', learning_rate = .01)&#xA;&#xA;reg.fit(X_train, y_train)&#xA;&#xA;ypred = reg.predict(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Many times &lt;a href=&quot;http://en.wikipedia.org/wiki/Named-entity_recognition&quot;&gt;Named Entity Recognition&lt;/a&gt; (NER) doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider the following input:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Barack Obama is a great person.&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And the output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]),&#xA;    ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where as for the input: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;'Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he &quot;was honored&quot; to be compared to Darth Vader while in office.'&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;the output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'),&#xA;    Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'),&#xA;    ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]),&#xA;    ('that', 'IN'), ('he', 'PRP'), ('``', '``'), ('was', 'VBD'), ('honored', 'VBN'),&#xA;    (&quot;''&quot;, &quot;''&quot;), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'),&#xA;    Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'),&#xA;    ('office', 'NN'), ('.', '.')])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here &lt;code&gt;Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP)&lt;/code&gt; is correctly extracted. So, I think if &lt;code&gt;nltk.ne_chunk&lt;/code&gt; is used first, and then if two consecutive trees are NNP, there are higher chances that both refer to one entity. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been playing with NLTK toolkit, and I came across this problem a lot, but couldn't find a satisfying answer. Any suggestion will be really appreciated. I'm looking for flaws in my approach.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to rank some percentages. I have numerators and denominators for each ratio. To give a concrete example, consider ratio as &lt;code&gt;total graduates / total students&lt;/code&gt; in a school.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the issue is that &lt;code&gt;total students&lt;/code&gt; vary over a long range (1000-20000). Smaller schools seem to have higher percentage of students graduating, but I want to standardize it, and not let the size of the school affect the ranking. Is there a way to do it?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm new to the world of text mining and have been reading up on annotators at places like the &lt;a href=&quot;http://uima.apache.org/&quot; rel=&quot;nofollow&quot;&gt;UIMA website&lt;/a&gt;. I'm encountering many new terms like named entity recognition, tokenizer, lemmatizer, gazetteer, etc. Coming from a layman background, this is all very confusing so can anyone tell me or link to resources that can explain what the main categories of annotators are and what they do?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they're restricted to US stocks. Yahoo Finance provides this data online but there's no option to download it ( or none I am aware of ). &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?&lt;/li&gt;&#xA;&lt;li&gt;Is there some way to download it via Yahoo Finance or Google Finance ?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I need data for the last decade or so and hence need some script or API which would do this.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;While doing a Google image search, the page displays some figured out categories for the images of the topic being searched for. I'm interested in learning how this works, and how it chooses and creates categories.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, I couldn't find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this, and what basis these categories are created from?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I search for &quot;animals&quot; I get the categories:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;cute&quot;, &quot;baby&quot;, &quot;wild&quot;, &quot;farm&quot;, &quot;zoo&quot;, &quot;clipart&quot;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If I go into &quot;wild&quot;, I then have subcategories:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;forest&quot;, &quot;baby&quot;, &quot;africa&quot;, &quot;clipart&quot;, &quot;rainforest&quot;, &quot;domestic&quot;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm new to machine learning, but I have an interesting problem. I have a large sample of people and visited sites. Some people have indicated gender, age, and other parameters. Now I want to restore these parameters to each user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which way do I look for? Which algorithm is suitable to solve this problem? I'm familiar with Neural Networks (supervised learning), but it seems they don't fit.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;There are several classic datasets for machine learning classification/regression tasks. The most popular are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html&quot;&gt;Iris Flower Data Set&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.kaggle.com/c/titanic-gettingStarted&quot;&gt;Titanic Data Set&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&quot;&gt;Motor Trend Cars&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;But does anyone know similar datasets for networks analysis / graph theory? More concrete - I'm looking for &lt;strong&gt;Gold standard&lt;/strong&gt; datasets for comparing/evaluating/learning:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;centrality measures;&lt;/li&gt;&#xA;&lt;li&gt;network clustering algorithms.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I don't need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's quite difficult to provide exact features for &quot;gold standard dataset&quot;, but here are some thoughts. I think, real classic dataset should satisfy these criteria:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Multiple references in articles and textbooks;&lt;/li&gt;&#xA;&lt;li&gt;Inclusion in well-known network analysis software packages;&lt;/li&gt;&#xA;&lt;li&gt;Sufficient time of existence;&lt;/li&gt;&#xA;&lt;li&gt;Usage in a number of courses on graph analysis.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Concerning my field of interest, I also need labeled classes for vertices and/or precomputed (or predefined) &quot;authority scores&quot; (i.e. centrality estimates). After asking this question I continued searching, and here are some suitable examples:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://networkdata.ics.uci.edu/data.php?id=105&quot;&gt;Zachary's Karate Club&lt;/a&gt;: introduced in 1977, cited more than 1.5k times (according to Google Scholar), vertexes have attribute Faction (which can be used for clustering).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.orgnet.com/Erdos.html&quot;&gt;Erdos Collaboration Network&lt;/a&gt;: unfortunately, I haven't find this network in form of data-file, but it's rather famous, and if someone will enrich network with  mathematicians' specialisations data, it also could be used for testing clustering algorithms.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a political campaign where dozens of volunteers will be conducting door-knocking promotions over the next few weeks. Given a list with names, addresses and long/lat coordinates, what algorithms can be used to create an optimized walk list.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm building a recommender system and using SVD as one of the preprocessing techniques.&#xA;However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I take the SVD (A = USV^T), is there a standard way to normalize the matrix 'A' between 0 and 1? Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I want all of my similarity measurements to give results between 0 and 1 and my normalized euclidean distance in particular fails if the input matrix does not have values between 0 and 1.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;datasciencetoolKIT.org -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;datasciencetoolBOX.org -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and &lt;a href=&quot;http://en.wikipedia.org/wiki/Multilayer_perceptron&quot; rel=&quot;noreferrer&quot;&gt;Multilayer perceptron&lt;/a&gt; (MLP) that I put together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of &lt;code&gt;lambda * current_weight&lt;/code&gt; is added as well before the combined delta is subtracted at the end of the batch, where &lt;code&gt;lambda&lt;/code&gt; is the regularisation parameter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. &lt;em&gt;Does a smaller regularisation term per item work just as well?&lt;/em&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance &lt;code&gt;lambda * current_weight / N&lt;/code&gt; where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I understand Hadoop MapReduce and its features but I am confused about R MapReduce.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One difference I have read is that R utilizes maximum RAM. So do perform parallel processing integrated R with Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;My doubt is:&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;R can do all stats, math and data science related stuff, but why R MapReduce?&lt;/li&gt;&#xA;&lt;li&gt;Is there any new task I can achieve by using R MapReduce instead of Hadoop MapReduce? If yes, please specify.&lt;/li&gt;&#xA;&lt;li&gt;We can achieve the task by using R with Hadoop (directly) but what is the importance of MapReduce in R and how it is different from normal MapReduce?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[A B B C A]  &#xA;[A A B A A]  &#xA;[A B B C C]  &#xA;[A A A A A]  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There would be two separate clusters for A, two separate clusters for C, and one cluster for B.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output I'm looking for would ideally assign a unique ID to each cluster, something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[1 2 2 3 4]  &#xA;[1 1 2 4 4]  &#xA;[1 2 2 5 5]  &#xA;[1 1 1 1 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I'm convinced it must be out there.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering if there's a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that's being classified?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, I'm using precision/recall as a measure of performance, so might have something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                    Precision Recall&#xA;Person classifier   65%       40%&#xA;Company classifier  98%       90%&#xA;Cheese classifier   10%       50%&#xA;Egg classifier      100%      100%&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the dataset I'm running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say we had hundreds of such classifiers, I guess I'm looking for a good way to answer questions like &quot;Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they're underperforming?&quot;. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How can I choose an algorithm for the overall classifier?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am new to machine learning. I have a task at hand of predicting click probability given user information like city, state, OS version, OS family, device, browser family, browser version, etc. I have been advised to try logit since logit seems to be what MS and Google are using. I have some questions regarding logistic regression:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Click and non click is a very very unbalanced class and the simple GLM predictions do not look good. How can I make the data work better with the GLM?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. How can I deal with this distribution of categorical variables?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the variables that we get is device ID. This is a very unique feature that can be translated to a user's identity. How can I make use of it in logit, or should it be used in a completely different model based on user identity?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently working on a project that would benefit from personalized predictions.  Given an input document, a set of output documents, and a history of user behavior, I'd like to predict which of the output documents are clicked.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, I'm wondering what the typical approach to this kind of personalization problem is.  Are models trained per user, or does a single global model take in summary statistics of past user behavior to help inform that decision?  Per user models won't be accurate until the user has been active for a while, while most global models have to take in a fixed length feature vector (meaning we more or less have to compress a stream of past events into a smaller number of summary statistics).  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently searching for labeled datasets to use to train a model to extract named entities from informal text (think something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I'm looking for out of domain data that's a bit more &quot;informal&quot; than the news articles and journal entries that many of today's state of the art named entity recognition systems are trained on.  Any recommendations?  So far I've only been able to locate 50k tokens from twitter published here: &lt;a href=&quot;https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt&quot;&gt;https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;For a recommendation system I'm using cosine similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general &quot;average&quot; category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for example if these were my observations (rows are observations, cols are features):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[0, 0, 0, 1, 1, 1, 0, 1, 0],&#xA; [1, 0, 1, 0, 0, 0, 1, 0, 0],&#xA; [1, 1, 1, 1, 0, 1, 0, 1, 1],&#xA; [0, 0, 1, 0, 0, 1, 0, 1, 0]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosine similarity. Though, at the moment, I'm having trouble reasoning my way through angles between lines in high dimensional space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to build a nonparametric density function for a fairly large dataset that can be evaluated efficently, and can be updated efficiently when new points are added. There will only ever be a maximum of 4 independent variables, but we can start off with 2. Lets use a gaussian kernel. Let the result be a probability density function, i.e. its volume will be 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In each evaluation, we can omit all points for which the evaluation point is outside a certain ellipsoid corresponding to the minimum gaussian value we care about. We can change this threshold for accuracy or performance, and the maximum number of points inside the threshold will depend on the chosen covariance matrix of the kernel. Then, we can evaluate the distribution approximately using the subset of points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use a fixed kernel, then we can use the eigenvalues and eigenvectors we get from the covariance matrix to transform each point so that the threshold ellipsoid is a fixed circle. We can then shove all the transformed points into a spatial index, and efficiently find all points within the required radius of the evaluation point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, we would the kernel to be variable for two reasons. (1) to fit the data better, and (2) because adding or modifying points would require the fixed kernel to be updated, which would mean that the entire data set would need to be reindexed. With a variable kernel, we could make new/updated points only affect the closest points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, is there a spatial index that can efficiently find ellipses surrounding a given point from a set of around 10 million ellipses of different shapes and sizes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More generally though, does my approach look sound? I am open to answers like &quot;give up and precalculate a grid of results&quot;. Answers much appreciated!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to find which classification methods, that do not use a training phase, are available. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The scenario is gene expression based classification, in which you have a matrix of gene expression of m genes (features) and n samples (observations).&#xA;A signature for each class is also provided (that is a list of the features to consider to define to which class belongs a sample).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An application (non-training) is the &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015543&quot; rel=&quot;nofollow&quot;&gt;Nearest Template Prediction&lt;/a&gt; method. In this case it is computed the cosine distance between each sample and each signature (on the common set of features). Then each sample is assigned to the nearest class (the sample-class comparison resulting in a smaller distance). No already classified samples are needed in this case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A different application (training) is the &lt;a href=&quot;http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot; rel=&quot;nofollow&quot;&gt;kNN&lt;/a&gt; method, in which we have a set of already labeled samples. Then, each new sample is labeled depending on how are labeled the k nearest samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other non-training methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I would like to use ANN to automate trading currencies, preferably USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some papers and done some experiments but without much luck. I would like to get advice from EXPERTS to make this work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is what I did so far:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.&lt;/li&gt;&#xA;&lt;li&gt;Extracted all ticks for the time frame 12PM to 14PM for all days.&lt;/li&gt;&#xA;&lt;li&gt;From this data, created a data set where each entry consists of n bid values in sequence. &lt;/li&gt;&#xA;&lt;li&gt;Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.&lt;/li&gt;&#xA;&lt;li&gt;The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 output neuron. Input layer had linear TF, hidden had log TF and output had linear TF.&lt;/li&gt;&#xA;&lt;li&gt;Trained the network with back propagation with n-125 first and then 10.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For both n, the MSE did not drop below 0.5 and stayed at that value during full training. Assuming that this could be due to the time series being totally random, I used the R package to find partial autocorrelation on the data set (pacf). This gave non zero values for 2 and 3 lags only. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 1: What does this mean exactly? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I used hurst exponent to evaluate the randomness. In R, hurst(values) showed values above 0.9.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 2: It is supposed to be nearly random. Should it have a value closer to 0.5?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the (n-1)th bid value. It looks like ANN just takes the last bid as the next bid! I tried different network structures (all multilayer perceptions), different training parameters, etc, but results are same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 3: How can I improve the accuracy? Are there any other training methods than backpropagation?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Suppose I want to use CART as classification tree (I want a categorical response). I have the training set, and I split it using observation labels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, to build the decision tree (classification tree) how are selected the features to decide which label apply to testing observations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Supposing we are working on gene expression matrix, in which each element is a real number, is that done using features that are more distant between classes?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I would like to use non-atomic data, as a feature for a prediction. &#xA;Suppose I have a Table with these features:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;- Column 1: Categorical - House&#xA;- Column 2: Numerical - 23.22&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xA;- Column 5: A List [ 122, Boolean ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to predict/classify, for instance, Column 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am making something to automatically respond to questions, any type of question, like &quot;Where was Foo Born?&quot; ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I first make a query to a search engine, then I get some text data as a result, then I do all the parsing stuff (tagging, stemming, parsing, splitting ... )&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first approach was to make a table, each row with a line of text and a lot of features, like &quot;First Word&quot;, &quot;Tag of First Word&quot;, &quot;Chunks&quot;, etc...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But with this approach I am missing the relationships between the sentences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know if there is an algorithm that looks inside the tree structures (or vectors) and makes the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I m new to RHadoop and also to RMR...&#xA;I had an requirement to write a Mapreduce Job in R Mapreduce. I have Tried writing but While executing this it gives an Error.&#xA;Tring to read the file from hdfs&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Error:&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  : &#xA;   hadoop streaming failed with error code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Code :&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Sys.setenv(HADOOP_HOME=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop&quot;)&#xA;Sys.setenv(HADOOP_CMD=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/bin/hadoop&quot;)&#xA;&#xA;Sys.setenv(HADOOP_STREAMING=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar&quot;)&#xA;library(rmr2)&#xA;library(rhdfs)&#xA;hdfs.init()&#xA;day_file = hdfs.file(&quot;/hdfs/bikes_LR/day.csv&quot;,&quot;r&quot;)&#xA;day_read = hdfs.read(day_file)&#xA;c = rawToChar(day_read)&#xA;&#xA;XtX =&#xA;  values(from.dfs(&#xA;    mapreduce(&#xA;      input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xA;      map=&#xA;        function(.,Xi){&#xA;         yi =c[Xi[,1],]&#xA;         Xi = Xi[,-1]&#xA;         keyval(1,list(t(Xi)%*%Xi))&#xA;       },&#xA;  reduce = function(k,v )&#xA;  {&#xA;    vals =as.numeric(v)&#xA;    keyval(k,sum(vals))&#xA;  } ,&#xA;  combine = TRUE)))[[1]]&#xA;&#xA;XtY =&#xA; values(from.dfs(&#xA;    mapreduce(&#xA;     input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xA;     map=&#xA;       function(.,Xi){&#xA;         yi =c[Xi[,1],]&#xA;         Xi = Xi[,-1]&#xA;        keyval(1,list(t(Xi)%*%yi))&#xA;       },&#xA;     reduce = TRUE ,&#xA;     combine = TRUE)))[[1]]&#xA;solve(XtX,XtY)&#xA;&#xA;&#xA;&#xA;Input:&#xA;------------&#xA;&#xA;instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt&#xA;1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,985&#xA;2,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,801&#xA;3,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,1349&#xA;4,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,1562&#xA;5,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,1600&#xA;6,2011-01-06,1,0,1,0,4,1,1,0.204348,0.233209,0.518261,0.0895652,88,1518,1606&#xA;7,2011-01-07,1,0,1,0,5,1,2,0.196522,0.208839,0.498696,0.168726,148,1362,1510&#xA;8,2011-01-08,1,0,1,0,6,0,2,0.165,0.162254,0.535833,0.266804,68,891,959&#xA;9,2011-01-09,1,0,1,0,0,0,1,0.138333,0.116175,0.434167,0.36195,54,768,822&#xA;10,2011-01-10,1,0,1,0,1,1,1,0.150833,0.150888,0.482917,0.223267,41,1280,1321&#xA;&#xA;&#xA;&#xA; Please Suggest me any mistakes.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I download and installed CDH 5 package succesfully on a single linux node in pseudo-distributed Mode on my CentOS 6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Starting Hadoop and verifying it is Working Properly as in &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I succesfully finished the following steps&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 1: Format the NameNode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 2: Start HDFS&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 3: Create the /tmp Directory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 4: Create the MapReduce system directories:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 5: Verify the HDFS File Structure&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 6: Start MapReduce&lt;/p&gt;&#xA;&#xA;&lt;p&gt;while following command in step 7 I get the following error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step 7: Create User Directories&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;mkdir: '/user/hadoopuser': No such file or directory&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(where hadoopuser is my linux login username)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to success the step 7:?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please provide the sloution to procced the remaining installation.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Note that I am doing everything in R. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem goes as follow: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My original idea:  &lt;em&gt;make this a supervised learning problem&lt;/em&gt;. &#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those &lt;em&gt;unlabelled data&lt;/em&gt;, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&#xA;Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this approach wrong ? Please correct me if you think my approach is wrong. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question 3: The tricky part is: how to &lt;strong&gt;identify and extract the keywords&lt;/strong&gt; ? Using the &lt;code&gt;tm&lt;/code&gt; package in R ? what algorithm is the &lt;code&gt;tm&lt;/code&gt;   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas would be great.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;R base function &lt;code&gt;glm()&lt;/code&gt; uses Fishers Scoring for MLE, while the &lt;code&gt;glmnet&lt;/code&gt; appears to use the coordinate descent method to solve the same equation. Coordinate descent is more time-efficient than Fisher Scoring, as Fisher Scoring calculates the second order derivative matrix, in addition to some other matrix operations. which makes expensive to perform, while coordinate descent can do the same task in O(np) time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why would R base function use Fisher Scoring? Does this method have an advantage over other optimization methods? How does coordinate descent and Fisher Scoring compare? I am relatively new to do this field so any help or resource will be helpful.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I was building a model that predicts user churn for a website, where I have data on all users, both past and present.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can build a model that only uses those users that have left, but then I'm leaving 2/3 of the total user population unused.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good way to incorporate data from these users into a model from a conceptual standpoint?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a &lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_linear_regression&quot; rel=&quot;nofollow noreferrer&quot;&gt;Simple Linear Regression&lt;/a&gt; algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm new to data science, would appreciate any pointers to move further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit: nfmcclure's suggestions applied&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Before applying the suggestions&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZsyyQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the snapshot of what I've got after splitting the dataset where the reset occurs, and the slope of two set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/OEQCw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;finding the mean of the two slopes and drawing the line from the mean.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/i2qv5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this OK?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to use the sklearn_pandas module to extend the work I do in pandas and dip a toe into machine learning but I'm struggling with an error I don't really understand how to fix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was working through the following dataset on &lt;a href=&quot;https://www.kaggle.com/c/data-science-london-scikit-learn/data&quot; rel=&quot;nofollow&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's essentially an unheadered table (1000 rows, 40 features) with floating point values.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pdfrom sklearn import neighbors&#xA;from sklearn_pandas import DataFrameMapper, cross_val_score&#xA;path_train =&quot;../kaggle/scikitlearn/train.csv&quot;&#xA;path_labels =&quot;../kaggle/scikitlearn/trainLabels.csv&quot;&#xA;path_test = &quot;../kaggle/scikitlearn/test.csv&quot;&#xA;&#xA;train = pd.read_csv(path_train, header=None)&#xA;labels = pd.read_csv(path_labels, header=None)&#xA;test = pd.read_csv(path_test, header=None)&#xA;mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])&#xA;mapper_train&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DataFrameMapper(features=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',&#xA;       n_neighbors=3, p=2, weights='uniform'))])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So far so good. But then I try the fit&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mapper_train.fit_transform(train, labels)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;TypeError                                 Traceback (most recent call last)&#xA;&amp;lt;ipython-input-6-e3897d6db1b5&amp;gt; in &amp;lt;module&amp;gt;()&#xA;----&amp;gt; 1 mapper_train.fit_transform(train, labels)&#xA;&#xA;//anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)&#xA;    409         else:&#xA;    410             # fit method of arity 2 (supervised transformation)&#xA;--&amp;gt; 411             return self.fit(X, y, **fit_params).transform(X)&#xA;    412 &#xA;    413 &#xA;&#xA;//anaconda/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)&#xA;    116         for columns, transformer in self.features:&#xA;    117             if transformer is not None:&#xA;--&amp;gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xA;    119         return self&#xA;    120 &#xA;&#xA;TypeError: fit() takes exactly 3 arguments (2 given)`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong? While the data in this case is all the same, I'm planning to work up a workflow for mixtures categorical, nominal and floating point features and sklearn_pandas seemed to be a logical fit.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;When I say &quot;document&quot;, I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I made a similar question asking about distance between &quot;documents&quot; (Wikipedia articles, news stories, etc.).  I made this a separate question because search queries are considerably smaller than documents and are considerably noisier.  I hence don't know (and doubt) if the same distance metrics would be used here.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either vanilla lexical distance metrics or state-of-the-art semantic distance metrics are preferred, with stronger preference for the latter.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have 2 datasets, one with positive instances of what I would like to detect, and one with unlabeled instances. What methods can I use ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example, suppose we want to understand detect spam email based on a few structured email characteristics. We have one dataset of 10000 spam emails, and one dataset of 100000 emails for which we don't know whether they are spam or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we tackle this problem (without labeling manually any of the unlabeled data) ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can we do if we have additional information about the proportion of spam in the unlabeled data (i.e. what if we estimate that between 20-40% of the 100000 unlabeled emails are spam) ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am very new to machine learning and in my first project have stumbled across a lot of issues which I really want to get through.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using logistic regression with R's &lt;code&gt;glmnet&lt;/code&gt; package and alpha = 0 for ridge regression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn't much of a difference (0.61).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dependent variable/output is probability of click, based on if there is a click or not in historical data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version and OS family.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of these, for prediction I'm using state, device, user age, user gender, IP carrier, browser version, browser family, OS version and OS family; I am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. I am not using city because they are too many or mobile manufacturer because they are too few.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is that okay or should I be using the rejected variables?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To start, I create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After training the model, I save the coefficients and intercept. These are used for new incoming requests using the formula for logistic regression:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/AZBRq.png&quot; alt=&quot;1 / (1+e^-1*sum(a+k(ith)*x(ith)))&quot;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;a&lt;/code&gt; is intercept, &lt;code&gt;k&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th coefficient and &lt;code&gt;x&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th variable value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is my approach correct so far?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple GLM in R (that is where there is no regularized regression, right?) gave me 0.56 AUC. With regularization I get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn't happen is almost always greater than the max probability that a click happened.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So basically what should I do?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read how stochastic gradient descent is an effective technique in logit so how do I implement stochastic gradient descent in R? If it's not straightforward, is there a way to implement this system in Python? Is SGD implemented after generating a regularized logistic regression model or is it a different process altogether?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also there is an algorithm called follow the regularized leader (FTRL) that is used in click-through rate prediction. Is there a sample code and use of FTRL that I could go through?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using Neural Networks to solve different Machine learning problems. I'm using Python and &lt;a href=&quot;http://pybrain.org/&quot; rel=&quot;noreferrer&quot;&gt;pybrain&lt;/a&gt; but this library is almost discontinued. Are there other good alternatives in Python?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to run some analysis with some big datasets (eg 400k rows vs. 400 columns) with R (e.g. using neural networks and recommendation systems).&#xA;But, it's taking too long to process the data (with huge matrices, e.g. 400k rows vs. 400k columns).&#xA;What are some free/cheap ways to improve R performance?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm accepting packages or web services suggestions (other options are welcome).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a set of datapoints from the unit interval (i.e. 1-dimensional dataset with numerical values). I receive some additional datapoints online, and moreover the value of some datapoints might change dynamically. I'm looking for an ideal clustering algorithm which can handle these issues efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm&quot;&gt;sequential k-means clustering&lt;/a&gt; copes with the addition of new instances, and I suppose with minor modification it can work with dynamic instance values (i.e. first taking the modified instance from the respective cluster, then updating the mean of the cluster and finally giving the modified instance as an input to the algorithm just as the addition of an unseen instance).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern with using the k-means algorithm is the requirement of supplying the number of clusters as an input. I know that they beat other clustering algorithms (GAs, MSTs, Hierarchical Methods etc.) in time&amp;amp;space complexity. Honestly I'm not sure, but maybe I can get away with using one of the aforementioned algorithms. Even that my datasets are relatively large, the existence of a single dimension makes me wonder.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More specifically a typical test case of mine would contain about 10K-200K 1-dimensional datapoints. I would like to complete the clustering preferably under a second. The dynamic changes in the value points are assumed to be smooth, i.e. relatively small. Thus being able to use existing solutions (i.e. being able to continue clustering on the existing one when a value is changed or new one is added) is highly preferred. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So all in all:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Can you think of an algorithm which will provide a sweet spot between computational efficiency and the accuracy of clusters wrt. the problem defined above?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Are there some nice heuristics for the k-means algorithm to automatically compute the value of K beforehand?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have generated a dataset of pairwise distances as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id_1 id_2 dist_12&#xA;id_2 id_3 dist_23&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven't been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In my university, we have an HPC computing cluster. I use the cluster to train classifiers and so on. So, usually, to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash script that contains (among others) a command like &lt;code&gt;qsub script.py&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository, so I get the same python script there. Then I write that Bash script or edit it, so I can run the bash script.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you see this is really frustrating since, for every little update for the python script, I need to do many steps to have it executed at the computing cluster. Of course the task gets even more complicated when I have to put the data on the server and use the datasets' path on the server.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Extractive Multi-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Extractive Single-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Generative Single-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;li&gt;Generative Multi-Document Summarization (Generic or query-based)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;This question is in response to a comment I saw on another question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And Wikipedia has this: &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine#Issues&quot;&gt;http://en.wikipedia.org/wiki/Support_vector_machine#Issues&lt;/a&gt; . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have installed cloudera CDH5 Quick start VM on VM player. When I login through HUE in the first page I am the following error&lt;/p&gt;&#xA;&#xA;&lt;p&gt;“Potential misconfiguration detected. Fix and restart Hue.”&lt;img src=&quot;https://i.stack.imgur.com/vnq5P.png&quot; alt=&quot;Potential misconfiguration detected. Fix and restart Hue&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to solve this issue.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I know that there is no a clear answer for this question, but let's suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The &quot;best&quot; way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like correlation measures (&lt;a href=&quot;http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf&quot;&gt;http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf&lt;/a&gt;) etc?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using an experimental design to test the robustness of different classification methods, and now I'm searching for the correct definition of such design.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. Then, I run each classification method on every subset. Finally, I estimate the accuracy of each method as how many classifications on subsets are in agreement with the classification on the full dataset. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Classification-full     1    2    3    2    1    1    2&#xA;&#xA;Classification-subset1  1    2         2    3    1   &#xA;Classification-subset2       2    3         1    1    2&#xA;...&#xA;&#xA;Accuracy                1    1    1    1  0.5    1    1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a correct name to this methodology? I thought it can fall under &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_(statistics)&quot; rel=&quot;nofollow&quot;&gt;bootstrapping&lt;/a&gt; but I'm not sure about this.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to understand a neuroscience article:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Friston, Karl J., et al. &quot;Action and behavior: a free-energy formulation.&quot; &lt;em&gt;Biological cybernetics&lt;/em&gt; 102.3 (2010): 227-260. (&lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs00422-010-0364-z&quot; rel=&quot;nofollow noreferrer&quot;&gt;DOI 10.1007/s00422-010-0364-z&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Energy minus entropy&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Divergence plus surprise&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;• Complexity minus accuracy&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Mathematically, these correspond to:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/0eQgg.png&quot; alt=&quot;\\\\begin {aligned} F &amp;amp;= - \\\\langle \\\\ln p(\\\\tilde s, \\\\Psi \\\\vert m) \\\\rangle _q + \\\\langle \\\\ln q(\\\\Psi \\\\vert \\\\mu) \\\\rangle _q \\\\ &amp;amp;= D(q(\\\\Psi \\\\vert \\\\mu) \\\\parallel p(\\\\Psi \\\\vert \\\\tilde s, m)) - \\\\ln p(\\\\tilde s \\\\vert m) &amp;amp; \\\\qquad \\\\qquad &amp;amp; (5) \\\\ &amp;amp;= D(q(\\\\Psi \\\\vert \\\\mu) \\\\parallel p(\\\\Psi \\\\vert m)) - \\\\langle \\\\ln p(\\\\tilde s \\\\vert \\\\Psi, m) \\\\rangle _q \\\\end{aligned}&quot;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The things I am struggling with at this point are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the meaning of the || in the 2nd and 3rd versions of the equations;&lt;/li&gt;&#xA;&lt;li&gt;and the negative logs.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;As far as I know the development of algorithms to solve the Frequent Pattern Mining (FPM) problem, the road of improvements have some main checkpoints. Firstly, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Apriori_algorithm&quot;&gt;Apriori&lt;/a&gt; algorithm was proposed in 1993, by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=170072&quot;&gt;Agrawal et al.&lt;/a&gt;, along with the formalization of the problem. The algorithm was able to &lt;em&gt;strip-off&lt;/em&gt; some sets from the &lt;code&gt;2^n - 1&lt;/code&gt; sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Later, on year 1997, &lt;a href=&quot;http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html&quot;&gt;Zaki et al.&lt;/a&gt; proposed the algorithm &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm&quot;&gt;Eclat&lt;/a&gt;, which &lt;em&gt;inserted&lt;/em&gt; the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In 2000, &lt;a href=&quot;http://dl.acm.org/citation.cfm?doid=335191.335372&quot;&gt;Han et al.&lt;/a&gt; proposed an algorithm named &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm&quot;&gt;FPGrowth&lt;/a&gt;, along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to &lt;em&gt;strip-off&lt;/em&gt; non-frequent itemsets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strike&gt;As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered &quot;state-of-the-art&quot;? What is the &lt;em&gt;intuition&lt;/em&gt;/&lt;em&gt;main-contribution&lt;/em&gt; of such algorithms?&lt;/strike&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the FPGrowth algorithm still considered &quot;state of the art&quot; in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;When I started with artificial neural networks (NN) I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even beat my score on random forest!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For implementing NN I use Theano Stacked Auto Encoder with &lt;a href=&quot;https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py&quot;&gt;the code from tutorial&lt;/a&gt; that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at &lt;a href=&quot;http://deeplearning.net/tutorial/deeplearning.pdf&quot;&gt;tutorial&lt;/a&gt;, chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100-&gt;100-&gt;100, 60-&gt;60-&gt;60, 60-&gt;30-&gt;15, etc.), different learning and pre-train rates, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can it be that my deep NN with pre-training behaves so badly? What should I try? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a dataset which contains ~100,000 samples of 50 classes. I have been using SVM with an RBF kernel to train and predict new data. The problem though is the dataset is skewed towards different classes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, Class 1 - 30 (~3% each), Class 31 - 45 (~0.6% each), Class 46 - 50 (~0.2% each)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that the model tends to very rarely predict the classes which occur less frequent in the training set, even though the test set has the same class distribution as the training set. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am aware that there are technique such as 'undersampling' where the majority class is scaled down to the minor class. However, is this applicable here where there are so many different classes? Are there other methods to help handle this case?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to use ARMA/ARIMA with the &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/tsa.html#descriptive-statistics-and-tests&quot; rel=&quot;nofollow noreferrer&quot;&gt;statsmodel Python package&lt;/a&gt;, in order to predict the gas consumption. I tried with &lt;a href=&quot;https://github.com/denadai2/Gas-consumption-outliers/blob/master/exportWeb.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;a dataset&lt;/a&gt; of this format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ZUvBlUP.png&quot; alt=&quot;with this format&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using only the gas column.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pandas.tseries.offsets import *&#xA;&#xA;arma_mod20 = sm.tsa.ARMA(januaryFeb[['gas [m3]']], (5,3)).fit()&#xA;predict_sunspots = arma_mod20.predict('2012-01-13', '2012-01-14', dynamic=True)&#xA;ax = januaryFeb.ix['2012-01-13 00:00:00':'2012-01-15 22:00:00']['gas [m3]'].plot(figsize=(12,8))&#xA;ax = predict_sunspots.plot(ax=ax, style='r--', label='Dynamic Prediction');&#xA;ax.legend();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/oCPonu7.png&quot; alt=&quot;result&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is the prediction so bad?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I asked a data science question regarding how to decide on the best variation of a split test on the Statistics section of StackExchange. I hope I will have better luck here. The question is basically, &quot;Why is mean revenue per user the best metric to make your decision on in a split test?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original question is here: &lt;a href=&quot;https://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean&quot;&gt;https://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since it was not well received/understood I simplified the problem to a discrete set of purchases and phrased it as a classical probability problem. That question is here: &lt;a href=&quot;https://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn&quot;&gt;https://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The mean may be the best metric for such a decision but I am not convinced. We often have a lot of prior information so a Bayesian method would likely improve our estimates. I realize that this is a difficult question but Data Scientists are doing such split tests everyday. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am learning about matrix factorization for recommender systems and I am seeing the term &lt;code&gt;latent features&lt;/code&gt; occurring too frequently but I am unable to understand what it means. I know what a feature is but I don't understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UCI letter recognition:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 5.3% &lt;/li&gt;&#xA;&lt;li&gt;Boost -  13% &lt;/li&gt;&#xA;&lt;li&gt;MLP -    7.9% &lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   6.5%  &lt;/li&gt;&#xA;&lt;li&gt;Bayes -  11.5%  &lt;/li&gt;&#xA;&lt;li&gt;SVM -    3.3%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Parameters used:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,&#xA;min_sample_count=1&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Boost -  boost_type=REAL, weak_count=200, weight_trim_rate=0.95,&#xA;max_depth=7&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;MLP -    method=BACKPROP, param=0.001, max_iter=300 (default values - too&#xA;slow to experiment) &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;kNN(k=3) -   k=3&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Bayes -  none&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;SVM -    RBF kernel, C=10, gamma=0.01&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Digits:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 5.1%&lt;/li&gt;&#xA;&lt;li&gt;Boost -  23.4%&lt;/li&gt;&#xA;&lt;li&gt;MLP -    4.3%&lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   7.3%&lt;/li&gt;&#xA;&lt;li&gt;Bayes -  17.7%&lt;/li&gt;&#xA;&lt;li&gt;SVM -    4.2%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;MNIST:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTrees - 1.4%&lt;/li&gt;&#xA;&lt;li&gt;Boost -  out of memory&lt;/li&gt;&#xA;&lt;li&gt;MLP -    1.0%&lt;/li&gt;&#xA;&lt;li&gt;kNN(k=3) -   1.2%&lt;/li&gt;&#xA;&lt;li&gt;Bayes -  34.33%&lt;/li&gt;&#xA;&lt;li&gt;SVM -    0.6%&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a data science project using Python.&#xA;The project has several stages.&#xA;Each stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.&#xA;I store the code in git, so that part is covered.&#xA;I would like to hear about:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tools for data version control.&lt;/li&gt;&#xA;&lt;li&gt;Tools enabling to reproduce stages and experiments.&lt;/li&gt;&#xA;&lt;li&gt;Protocol and suggested directory structure for such a project.&lt;/li&gt;&#xA;&lt;li&gt;Automated build/run tools.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am given a time series data vector (ordered by months and years),which contains only &lt;code&gt;0&lt;/code&gt;s and &lt;code&gt;1&lt;/code&gt;s. &lt;code&gt;1&lt;/code&gt; s represent a person changes his job at a particular a month. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would love to gather more information on how to model something like this. Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What is the right approach and clustering algorithm for geolocation clustering?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using the following code to cluster geolocation coordinates:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;from scipy.cluster.vq import kmeans2, whiten&#xA;&#xA;coordinates= np.array([&#xA;           [lat, long],&#xA;           [lat, long],&#xA;            ...&#xA;           [lat, long]&#xA;           ])&#xA;x, y = kmeans2(whiten(coordinates), 3, iter = 20)  &#xA;plt.scatter(coordinates[:,0], coordinates[:,1], c=y);&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it right to use K-means for geolocation clustering, as it uses Euclidean distance, and not &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot; rel=&quot;noreferrer&quot;&gt;Haversine formula&lt;/a&gt; as a distance function?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;t-SNE, as in [1], works by progressively reducing the Kullback-Leibler (KL) divergence, until a certain condition is met.&#xA;The creators of t-SNE suggests to use KL divergence as a performance criterion for the visualizations:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence [2]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I tried two implementations of t-SNE:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;python&lt;/strong&gt;: sklearn.manifold.TSNE().&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;: tsne, from library(tsne).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Both these implementations, when verbosity is set, print the error (Kullback-Leibler divergence) for each iteration. However, they don't allow the user to get this information, which looks a bit strange to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;from sklearn.manifold import TSNE&#xA;X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])&#xA;model = TSNE(n_components=2, verbose=2, n_iter=200)&#xA;t = model.fit_transform(X)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;produces:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[t-SNE] Computing pairwise distances...&#xA;[t-SNE] Computed conditional probabilities for sample 4 / 4&#xA;[t-SNE] Mean sigma: 1125899906842624.000000&#xA;[t-SNE] Iteration 10: error = 6.7213750, gradient norm = 0.0012028&#xA;[t-SNE] Iteration 20: error = 6.7192064, gradient norm = 0.0012062&#xA;[t-SNE] Iteration 30: error = 6.7178683, gradient norm = 0.0012114&#xA;...&#xA;[t-SNE] Error after 200 iterations: 0.270186&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, as far as I understand, &lt;strong&gt;0.270186&lt;/strong&gt; should be the KL divergence. However i cannot get this information, neither from &lt;strong&gt;model&lt;/strong&gt; nor from &lt;strong&gt;t&lt;/strong&gt; (which is a simple numpy.ndarray).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To solve this problem I could: i) Calculate KL divergence by my self, ii) Do something nasty in python for capturing and parsing TSNE() function's output [3]. However: i) would be quite stupid to re-calculate KL divergence, when TSNE() has already computed it, ii) would be a bit unusual in terms of code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any other suggestion? Is there a standard way to get this information using this library?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I mentioned I tried &lt;em&gt;R&lt;/em&gt;'s tsne library, but I'd prefer the answers to focus on the &lt;em&gt;python&lt;/em&gt; sklearn implementation.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;References&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] &lt;a href=&quot;http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[2] &lt;a href=&quot;http://homepage.tudelft.nl/19j49/t-SNE.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://homepage.tudelft.nl/19j49/t-SNE.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[3] &lt;a href=&quot;https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call&quot;&gt;https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;New to the Data Science forum, and first poster here!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may be kind of a specific question (hopefully not too much so), but one I'd imagine others might be interested in.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for a way to basically query GitHub with something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Give me a collection of all of the public repositories that have more than 10 stars, at&#xA;least two forks, and more than three committers.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result could take any viable form: a JSON data dump, a URL to the web page, etc. It more than likely will consist of information from 10,000 repos or something large.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this sort of thing possible using the API or some other pre-built way, or am I going to have to build out my own custom solution where I try to scrape every page? If so, how feasible is this and how might I approach it?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I read in this post &lt;a href=&quot;https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data&quot;&gt;Is the R language suitable for Big Data&lt;/a&gt; that big data constitutes &lt;code&gt;5TB&lt;/code&gt;, and while it does a good job of providing information about the feasibility of working with this type of data in &lt;code&gt;R&lt;/code&gt; it provides very little information about &lt;code&gt;Python&lt;/code&gt;. I was wondering if &lt;code&gt;Python&lt;/code&gt; can work with this much data as well. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I'm almost sure this pattern was created by a disk testing program, but I'd like to reverse-engineer it anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already know that the pattern is aligned, with a periodicity of 256 characters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel's color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a snapshot of the pattern (you can see more than one), seen through &lt;code&gt;xxd&lt;/code&gt; (32x16):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/zOFSK.gif&quot; alt=&quot;Pattern to analyze&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either way, I am trying to find a way of visualizing this information. This probably isn't hard for anyone into signal analysis, but I can't seem to find a way using open-source software.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to avoid Matlab or Mathematica and I'd prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Update, 2014-07-25: given Emre's answer below, this is what the pattern looks like, given the first 30MB of the pattern, aligned at 512 instead of 256 (this alignment looks better):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4tDIA.png&quot; alt=&quot;Graphical pattern&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any further ideas are welcome!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a timeseries with hourly gas consumption. I want to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model&quot; rel=&quot;nofollow noreferrer&quot;&gt;ARMA&lt;/a&gt;/&lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot; rel=&quot;nofollow noreferrer&quot;&gt;ARIMA&lt;/a&gt; to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with &lt;a href=&quot;https://www.otexts.org/fpp/6/5&quot; rel=&quot;nofollow noreferrer&quot;&gt;Seasonal and Trend decomposition using Loess&lt;/a&gt; (STL)?)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/hYyH8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to find stock data to practice with, is there a good resource for this? I found this: &lt;a href=&quot;ftp://emi.nasdaq.com/ITCH/&quot;&gt;ftp://emi.nasdaq.com/ITCH/&lt;/a&gt; but it only has the current year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already have a way of parsing the protocol, but would like to have some more data to compare with. It doesn't have to be in the same format, as long as it has price, trades, and date statistics.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I cleared up some &quot;shared library&quot; problems by adding this to yarn-env.sh and hadoop-env.sh:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;export HADOOP_HOME=&quot;/usr/local/hadoop&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I also added this to hadoop-env.sh:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;based on the advice of this post at horton works &lt;a href=&quot;http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/&quot; rel=&quot;nofollow&quot;&gt;http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;starting yarn daemons&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;starting resourcemanager, &#xA;  logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;localhost: starting nodemanager, &#xA;  logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The only problem is, JPS says that the Resource Manager isn't running. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's going on here?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;senior, primary, lead, head, vp, director, stuff, principal, chief, &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;or the words &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;analyst, expert, modeler, researcher, scientist, developer, engineer, architect.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How can I get all such possible words with their distance ?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How can &lt;a href=&quot;http://en.wikipedia.org/wiki/NoSQL&quot;&gt;NoSQL&lt;/a&gt; databases like &lt;a href=&quot;http://en.wikipedia.org/wiki/MongoDB&quot;&gt;MongoDB&lt;/a&gt; be used for data analysis? What are the features in them that can make data analysis faster and powerful?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Basically, both are software systems that are based on data and algorithms.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I've found that some claim its advantage is mostly marginal in that it is best for catching 'lucky' models with high standard accuracy measurements and low AUC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So should I avoid relying on AUC for validating models or would a combination be best? Thanks for all your help.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;i want to become a &lt;strong&gt;data scientist&lt;/strong&gt;. I studied applied &lt;strong&gt;statistics&lt;/strong&gt; (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in &lt;strong&gt;Computer Science&lt;/strong&gt; focus in Intelligent Systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my study plan:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Machine learning&lt;/li&gt;&#xA;&lt;li&gt;Advanced machine learning&lt;/li&gt;&#xA;&lt;li&gt;Data mining&lt;/li&gt;&#xA;&lt;li&gt;Fuzzy logic&lt;/li&gt;&#xA;&lt;li&gt;Recommendation Systems&lt;/li&gt;&#xA;&lt;li&gt;Distributed Data Systems&lt;/li&gt;&#xA;&lt;li&gt;Cloud Computing&lt;/li&gt;&#xA;&lt;li&gt;Knowledge discovery&lt;/li&gt;&#xA;&lt;li&gt;Business Intelligence&lt;/li&gt;&#xA;&lt;li&gt;Information retrieval&lt;/li&gt;&#xA;&lt;li&gt;Text mining&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for the answers.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should choose a similar data setup for training my models as well. But I came across a research paper or two (in my area of work) which have used a &quot;class balancing&quot; data approach to training the models, implying an equal number of instances of benign and malicious traffic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, if I am building machine learning models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone shed more light on the &lt;em&gt;pros&lt;/em&gt; and &lt;em&gt;cons&lt;/em&gt; of both the choices and how to decide which one to go choose?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking about writing a program to generate a svg. Any other suggestions?  &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm currently facing a project that I could solve with a relational database in a relatively painful way. Having heard so much about NOSQL, I'm wondering if there is not a more appropriate way of tackling it:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose we are tracking a group of animals in a forest (n ~ 500) and would like to keep a record of a set of observations (this is a fictional scenario).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We would like to store the following information in a database:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a unique identifier for each animal&lt;/li&gt;&#xA;&lt;li&gt;a description of the animal with structured fields: Species, Genus, Family, ...&lt;/li&gt;&#xA;&lt;li&gt;a free text field with additional information&lt;/li&gt;&#xA;&lt;li&gt;each time-point at which it was detected close to a reference point&lt;/li&gt;&#xA;&lt;li&gt;a picture of the animal&lt;/li&gt;&#xA;&lt;li&gt;an indication whether two given animals are siblings&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;And:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;there might be additional features appearing later as more data comes in&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;We would like to be able to execute the following types of queries:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;return all the animals spotted between in a given time interval&lt;/li&gt;&#xA;&lt;li&gt;return all the animals of a given Species or Family&lt;/li&gt;&#xA;&lt;li&gt;perform a text search on the free text field&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which particular database system would you recommend ? Is there any tutorial / examples that I could use as a starting point ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Apologies if this is very broad question, what I would like to know is how effective is A/B testing (or other methods) of effectively measuring the effects of a design decision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance we can analyse user interactions or click results, purchase/ browse decisions and then modify/tailor the results presented to the user.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We could then test the effectiveness of this design change by subjecting 10% of users to the alternative model randomly but then how objective is this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we avoid influencing the user by the model change, for instance we could decided that search queries for 'David Beckham' are probably about football so search results become biased towards this but we could equally say that his lifestyle is just as relevant but this never makes it into the top 10 results that are returned.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious how this is dealt with and how to measure this effectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts are that you could be in danger of pushing a model that you think is correct and the user obliges and this becomes a self-fulfilling prophecy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've read an article on this: &lt;a href=&quot;http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/&quot;&gt;http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/&lt;/a&gt; and also the book: &lt;a href=&quot;http://shop.oreilly.com/product/0636920028529.do&quot;&gt;http://shop.oreilly.com/product/0636920028529.do&lt;/a&gt; which discussed this so it piqued my interest.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;There was a recent furore with &lt;a href=&quot;http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840&quot;&gt;facebook experimenting on their users to see if they could alter user's emotions&lt;/a&gt; and now &lt;a href=&quot;http://www.bbc.co.uk/news/technology-28542642&quot;&gt;okcupid&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whilst I am not a professional data scientist I read about &lt;a href=&quot;http://columbiadatascience.com/2013/11/25/data-science-ethics/&quot;&gt;data science ethics&lt;/a&gt; from &lt;a href=&quot;http://shop.oreilly.com/product/0636920028529.do&quot;&gt;Cathy O'Neill's book 'Doing Data Science'&lt;/a&gt; and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science &lt;em&gt;accidentally&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The article discusses a code of practice and mentions the &lt;a href=&quot;http://www.datascienceassn.org/code-of-conduct.html&quot;&gt;Data Science Association's Code of conduct&lt;/a&gt;, is this something that is in use? Rule 7 is of particular interest (quoted from their website):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;(a) A person who consults with a data scientist about the possibility&#xA;  of forming a client-data scientist relationship with respect to a&#xA;  matter is a prospective client.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(b) Even when no client-data scientist relationship ensues, a data&#xA;  scientist who has learned information from a prospective client shall&#xA;  not use or reveal that information.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(c) A data scientist subject to paragraph (b) shall not provide&#xA;  professional data science services for a client with interests&#xA;  materially adverse to those of a prospective client in the same or a&#xA;  substantially related industry if the data scientist received&#xA;  information from the prospective client that could be significantly&#xA;  harmful to that person in the matter&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a &lt;a href=&quot;http://www.cnet.com/news/senator-asks-ftc-to-investigate-facebooks-mood-study/&quot;&gt;senator&lt;/a&gt; to react to such things before something happens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Most vehicle license/number plate extractors I've found involve reading a plate from an image (OCR) but I'm interested in something that could tag instances of license plates in a body of text. Are there any such annotators out there?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;While running the below pig script I am getting error in line4: &#xA;If it is GROUP then I am getting error.&#xA;If I change from 'GROUP' TO 'group' in line4, then the script is running.&#xA;What is the difference between group and GROUP.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDSGROUPED = GROUP WORDS BY ZIPS;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DUMP WORDSSORT;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Have you heard of the &quot;Data Science Association&quot;? &#xA;&lt;br&gt;URL: &lt;a href=&quot;http://www.datascienceassn.org/&quot; rel=&quot;nofollow&quot;&gt;http://www.datascienceassn.org/&lt;/a&gt;&#xA;&lt;br&gt;Do you expect it to become a professional body like the Actuaries Institute?&#xA;&lt;br&gt;If yes, then why?&#xA;&lt;br&gt;If no, then why not and do you see anyone else becoming the professional body?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, is this question &quot;on-topic&quot; ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I'm currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, how should I adjust the data so that this is able to be plotted?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The times for an action can be between 2 seconds and a minute. And, at any given time I would estimate there could be about 100 actions taking place. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm open to any advice on how to think about this issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to build a recommendation engine using collaborative filtering. I have the usual [user, movie, rating] information. I would like to incorporate an additional feature like 'language' or 'duration of movie'. I am not sure what techniques I could use for such a problem. Please suggest references or packages in python/R. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;While running the below pig script I am getting error in line4: If it is &lt;code&gt;GROUP&lt;/code&gt; then I am getting error. If I change from &lt;code&gt;GROUP&lt;/code&gt; TO &lt;code&gt;group&lt;/code&gt; in line4, then the script is running. What is the difference between group and GROUP.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&#xA;&#xA;WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&#xA;&#xA;WORDSGROUPED = GROUP WORDS BY ZIPS;&#xA;&#xA;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&#xA;&#xA;WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&#xA;&#xA;DUMP WORDSSORT;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The calculation completes in IPython but inspection shows the results all come back as NaN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest the best approach to tackle the problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Here's my code thus far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;path = &quot;../../reuters.db&quot;&#xA;%pylab inline&#xA;import pandas as pd&#xA;import numpy as np&#xA;import pandas.io.sql as psql&#xA;import sqlite3 as lite&#xA;con = lite.connect(path)&#xA;with con:&#xA;    sql = &quot;SELECT * FROM Frequency&quot;&#xA;    df = psql.frame_query(sql, con)&#xA;    print df.shape&#xA;df = df.rename(columns={&quot;term&quot;:&quot;term_id&quot;, &quot;count&quot;:&quot;count_id&quot;})&#xA;pivoted = df.pivot('docid', 'term_id', 'count_id')&#xA;pivoted.to_sparse()&#xA;similarity_matrix = pivoted.dot(pivoted.T)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A mix of Data Science and Machine learning would be great.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;so I'm using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PySpark worker failed with exception:&#xA;Traceback (most recent call last):&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py&quot;, line 77, in main&#xA;    serializer.dump_stream(func(split_index, iterator), outfile)&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 191, in dump_stream&#xA;    self.serializer.dump_stream(self._batched(iterator), stream)&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 123, in dump_stream&#xA;    for obj in iterator:&#xA;  File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 180, in _batched&#xA;    for item in iterator:&#xA;TypeError: __init__() takes exactly 3 arguments (2 given)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the code for serializers is available &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and my code is &lt;a href=&quot;https://github.com/seashark97/Scalable-Sentiment-Analysis/blob/master/spark_test.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;h2&gt;Image Similarity based on Color Palette Distribution&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I am trying to compute similarity between two images based on their color palette distribution, let's say I have two sets of key value pairs as follows,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Img1: &lt;code&gt;{'Brown': 14, 'White': 13, 'Black': 40, 'Gray': 31}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Img2: &lt;code&gt;{'Pink': 82, 'Brown': 8, 'White': 7}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where the numbers denote the % of that color present in the image. What would be the best way to compute similarity on a scale of 0-100 between the two images?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to make the same NER for different language. The problem here is that I can't use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to use methods for unsupervised feature learning with CRF algorithm? Did anyone try this and got any good result? Is there any article or tutorial about this matter?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I still don't completely understand this way of feature creation so I don't want to spend to much time for something that won't work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the different classes of data science problems that can be solved using mapreduce programming model?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I need to build parse tree for some source code (on Python or any program language that describe by CFG).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have source code on some programming language and BNF this language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody give some advice how can I build parse tree in this case?&#xA;Preferably, with tools for Python.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goals are to:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;&lt;/li&gt;&#xA;&lt;li&gt;Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;&lt;/li&gt;&#xA;&lt;li&gt;Do the same as 1 and 2, but with procedures and/or diagnoses.&lt;/li&gt;&#xA;&lt;li&gt;Preferably, the results would be interpretable by a doctor&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I have looked at things like the &lt;a href=&quot;https://www.heritagehealthprize.com/c/hhp/details/milestone-winners&quot; rel=&quot;nofollow noreferrer&quot;&gt;Heritage Health Prize Milestone papers&lt;/a&gt; and have learned a lot from them, but they are focused on predicting hospitalizations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT #2 to add plaintext table:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;CKD is the target condition, &quot;chronic kidney disease&quot;, &quot;.any&quot; denotes that they have acquired that condition at any time, &quot;.isbefore.ckd&quot; means they had that condition before their first diagnosis of CKD.  The other abbreviations correspond with other conditions identified by ICD-9CM code groupings.  This grouping occurs in SQL during the import process. Each variable, with the exception of patient_age, is binary.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Does anyone know what (from your experience) is the best open source natural language generators (NLG) out there? What are the relative merits of each?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;So, I'm just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I've seen how an artificial neural network can parse image data and categorize the images (&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html&quot;&gt;demo with convnetjs&lt;/a&gt;), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm trying to wrap my head around if this is possible to be done with string inputs? The use-case I've got is a &quot;recommendation engine&quot; for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine &quot;downsampling&quot; the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I'd need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I've seen don't allow adding new neurons after the system has been trained?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what's a better tool for pattern-matching in string data)?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am attempting to use the tm package to convert a vector of text strings to a corpus element.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code looks something like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Corpus(d1$Yes)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where d1$Yes is a factor with 124 levels, each containing a text string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, d1$Yes[246] = &quot;So we can get the boat out!&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm receiving the following error:&#xA;&quot;Error: inherits(x, &quot;Source&quot;) is not TRUE&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure how to remedy this.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Well this looks like the most suited place for this question.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every website collect data of the user, some just for usability and personalization, but the majority like social networks track every move on the web, some free apps on your phone scan text messages, call history and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All this data siphoning is just for selling your profile for advertisers?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am working on a data set that has multiple traffic speed measurements per day. My data is from the city of chicago, and it is taken every minute for about six months. I wanted to consolidate this data into days only, so this is what I did:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;traffic &amp;lt;- read.csv(&quot;path.csv&quot;,header=TRUE)&#xA;traffic2 &amp;lt;- aggregate(SPEED~DATE, data=traffic, FUN=MEAN)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;this was perfect because it took all of my data and averaged it by date. For example, my original data looked something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DATE        SPEED  &#xA;12/31/2012   22&#xA;12/31/2012   25&#xA;12/31/2012   23&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the final looked like this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DATE        SPEED &#xA;10/1/2012    22&#xA;10/2/2012    23&#xA;10/3/2012    22&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The only problem, is my data is supposed to start at 9/1/2012. I plotted this data, and it turns out the data goes from 10/1/2012-12/31/2012 and then 9/1/2012-9/30/2012.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What in the world is going on here?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am currently on a project that will build a model (train and test) on Client-side Web data, but evaluate this model on Sever-side Web data.  Unfortunately building the model on Server-side data is not an option, nor is it an option to evaluate this model on Client-side data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This model will be based on metrics collected on specific visitors.  This is a real time system that will be calculating a likelihood based on metrics collected while visitors browse the website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for approaches to ensure the highest possible accuracy on the model evaluation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have the following ideas,&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Clean the Server-side data by removing webpages that are never seen Client-side.&lt;/li&gt;&#xA;&lt;li&gt;Collect additional data Server-side data to make the Server-side data more closely resemble Client-side data.&lt;/li&gt;&#xA;&lt;li&gt;Collect data on the Client and send this data to the Server.  This is possible and may be the best solution, but is currently undesirable. &lt;/li&gt;&#xA;&lt;li&gt;Build one or more models that estimate Client-side Visitor metrics from Server-side Visitor metrics and use these estimates in the Likelihood model.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any other thoughts on evaluating over one Population while training (and testing) on another Population?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am building a regression model and I need to calculate the below to check for correlations&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Correlation between 2 Multi level categorical variables&lt;/li&gt;&#xA;&lt;li&gt;Correlation between a Multi level categorical variable and&#xA;continuous variable &lt;/li&gt;&#xA;&lt;li&gt;VIF(variance inflation factor) for a Multi&#xA;level categorical variables&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please answer the below questions&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Which correlation coefficient works best for the above cases ? &lt;/li&gt;&#xA;&lt;li&gt;VIF calculation only works for continuous data so what is the&#xA;alternative? &lt;/li&gt;&#xA;&lt;li&gt;What are the assumptions I need to check before I use the correlation coefficient you suggest? &lt;/li&gt;&#xA;&lt;li&gt;How to implement them in SAS &amp;amp; R?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " ('&lt;p&gt;I assume that each person on Facebook is represented as a node (of a Graph) in Facebook, and relationship/friendship between each person(node) is represented as an edge between the involved nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that there are millions of people on Facebook, how is the Graph stored?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am trying to implement the Brown Clustering Algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Paper details: &quot;Class-Based n-gram Models of Natural Language&quot; by Brown et al&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm is supposed to in &lt;code&gt;O(|V|k^2)&lt;/code&gt; where &lt;code&gt;|V|&lt;/code&gt; is the size of the vocabulary and k is the number of clusters. I am unable to implement it this efficiently. In fact, the best I can manage is &lt;code&gt;O(|V|k^3)&lt;/code&gt; which is too slow. My current implementation for the main part of the algorithm is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for w = number of clusters + 1 to |V|&#xA;{&#xA;   word = next most frequent word in the corpus&#xA;&#xA;   assign word to a new cluster &#xA;&#xA;   initialize MaxQuality to 0&#xA;&#xA;   initialize ArgMax vector to (0,0)&#xA;&#xA;   for i = 0 to number of clusters - 1 &#xA;   {&#xA;      for j = i to number of clusters&#xA;      {&#xA;         Quality = Mutual Information if we merge cluster i and cluster j&#xA;&#xA;         if Quality &amp;gt; MaxQuality&#xA;         {&#xA;            MaxQuality = Quality &#xA;&#xA;            ArgMax = (i,j) &#xA;         }&#xA;      }&#xA;   }&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I compute quality as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. Before entering the second loop compute the pre-merge quality i.e. quality before doing any merges.&#xA;2. Every time a cluster-pair merge step is considered:&#xA;    i. assign quality := pre-merge quality&#xA;   ii. quality = quality - any terms in the mutual information equation that contain cluster i or cluster j (pre-merge)&#xA;  iii. quality = quality + any terms in the mutual information equation that contain (cluster i U cluster j)  (post-merge)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my implementation, the first loop has approx |V| iterations, the second and third loop approx k iterations each. To compute quality at each step requires approx a further k iterations. In total it runs in &lt;code&gt;(|V|k^3)&lt;/code&gt; time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you get it to run in &lt;code&gt;(|V|k^2)&lt;/code&gt;?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:  In addition to generalizing about a population, given a sample, I'm interested in generalizing about an individual's overall search behavior, given a time slice.  Theory and paper references are a plus!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution. &#xA;In fact the distribution resembles more closely the Landau Distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But my question is: &#xA;&lt;strong&gt;At what point can one say that the t-test is not a good method of significance testing?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don't require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: &lt;img src=&quot;https://i.stack.imgur.com/Edh2e.png&quot; alt=&quot;sample dashboard&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but applied to web analytics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions on what packages to use creating these dashboard reports? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first intuition is to use R markdown and knitr, but perhaps you've found a better solution. I can't seem to find rich examples of dashboards generated from R. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   y    x1  x2 x3 x4 x5 x6 ... x40&#xA;87169   14  0  1  0  0  2  ... 0 &#xA;46449   0   0  4  0  1  4  ... 12&#xA;846449  0   0  0  0  0  3  ... 0&#xA;....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am currently using a Genetic Algorithm to solve this and the results are coming out &#xA;with roughly a factor of two difference between observed and expected. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Data set looks like:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;25000 observations&lt;/li&gt;&#xA;&lt;li&gt;up to 15 predictors of different types: numeric, multi-class categorical, binary&lt;/li&gt;&#xA;&lt;li&gt;target variable is binary&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Which cross validation method is typical for this type of problems?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default I'm using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;There is a text summarization project called SUMMARIST. Apparently it is able to perform abstractive text summarization. I want to give it a try but unfortunately the demo links on the website do not work. Does anybody have any information regarding this? How can I test this tool?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.isi.edu/natural-language/projects/SUMMARIST.html&quot; rel=&quot;nofollow&quot;&gt;http://www.isi.edu/natural-language/projects/SUMMARIST.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regards,&#xA;PasMod&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am fitting a model in R.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;use &lt;code&gt;createFolds&lt;/code&gt; method to create several &lt;code&gt;k&lt;/code&gt; folds from the data set&lt;/li&gt;&#xA;&lt;li&gt;loop through the folds, repeating the following on each iteration:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;train&lt;/code&gt; the model on k-1 folds&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;predict&lt;/code&gt; the outcomes for the i-th fold&lt;/li&gt;&#xA;&lt;li&gt;calculate prediction accuracy&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;average the accuracy&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Does R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have  set of documents and I want classify them to true and false &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is I have to take the whole words in the documents then I classify them depend on the similarity words in these documents or I can take only some words that I interested in then I compare it with the documents. Which one is more efficient in classify document and can work with SVM.       &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm working on the dataset with lots of NA values with sklearn and pandas.DataFrame. I implemented different imputation strategies for different columns of the dataFrame based column names. For example NAs predictor 'var1' I impute with 0's and for 'var2' with mean.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try to cross validate my model using train_test_split it returns me a nparray which does not have column names. How can I impute missing values in this nparray?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. I do not impute missing values in the original data set before splitting on purpose so I keep test and validation sets separately.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have train and test sets of chronological data consisting of 305000 instances and 70000,appropriately. There are 15 features in each instance and only 2 possible class values ( NEW,OLD). The problem is that there are only 725 OLD instances in the train set and 95 in the test. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only algorithm which succeeds for me to handle imbalance is NaiveBayes in Weka (0.02 precision for OLD class), others (trees) classify each instance as NEW.&#xA;What is the best approach to handle the imbalance and the appropriate algorithm in such a case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am attempting to compile code using Knitr in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code below is returning the following error, and causes errors in the rest of the document.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;miss&amp;lt;-sample$sensor_glucose[!is.na(sample$sensor_glucose)]&#xA;# Error: &quot;## Warning: is.na() applied to non-(list or vector) of type 'NULL'&quot;&#xA;&#xA;str(miss)&#xA;# int [1:103] 213 113 46 268 186 196 187 153 43 175 ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Does anyone know how to remedy this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I was wondering does scikit-learn have forward selection/stepwise regression algorithm?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;We are storing the information about our users showing interest in our items. Based on this information, we would like to create a simple recommendation engine that will take the items I1, I2, I3 etc of the current user, search for all other users that had shown interest in those items, and then output the items I4, I5, I6 etc of the other users, sorted by their decreasing popularity. So, basically, the standard &quot;other buyer were also interested in...&quot; functionality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm asking myself what kind of a database is suitable for a realtime recommendation engine like this. My current idea is to build a trie of item IDs, then sort the item IDs of the current user (as the order of items is irrelevant) and to go down the trie; the children of the last trie node will build the needed output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that we have 2 million items so that according to our estimation the trie will have at least 1E12 nodes, so that we probably need a distributed sharded database to store it. Before we reinvent the wheel, are there any ready-to-use databases or generally, non-cloud solutions for recommendation engines out there?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;We have a classification algorithm to categorize Java exceptions in Production.&#xA;This algorithm is based on hierarchical human defined rules so when a bunch of text forming an exception comes up, it determines what kind of exception is (development, availability, configuration, etc.) and the responsible component (the most inner component responsible of the exception). In Java an exception can have several causing exceptions, and the whole must be analyzed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, given the following example exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;com.myapp.CustomException: Error printing ...&#xA;... (stack)&#xA;Caused by: com.foo.webservice.RemoteException: Unable to communicate ...&#xA;... (stack)&#xA;Caused by: com.acme.PrintException: PrintServer002: Timeout ....&#xA;... (stack)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;First of all, our algorithm splits the whole stack in three isolated exceptions. Afterwards it starts analyzing these exceptions starting from the most inner one. In this case, it determines that this exception (the second caused by) is of type &lt;code&gt;Availability&lt;/code&gt; and that the responsible component is a &quot;print server&quot;. This is because there is a rule that matches containing the word &lt;code&gt;Timeout&lt;/code&gt; associated to the &lt;code&gt;Availability&lt;/code&gt; type. There is also a rule that matches &lt;code&gt;com.acme.PrintException&lt;/code&gt; and determines that the responsible component is a print server. As all the information needed is determined using only the most inner exception, the upper exceptions are ignored, but this is not always the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see this kind of approximation is very complex (and chaotic) as a human have to create new rules as new exceptions appear. Besides, the new rules have to be compatible with the current ones because a new rule for classifying a new exception must not change the classification of any of the already classified exceptions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are thinking about using Machine Learning to automate this process. Obviously, I am not asking for a solution here as I know the complexity but I'd really appreciate some advice to achieve our goal.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Cross posting this from Cross Validated:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've seen this question asked before, but I have yet to come across a definitive source answering the specific questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What's the most appropriate statistical test to apply to a small A/B test?&lt;/li&gt;&#xA;&lt;li&gt;What's the R code and interpretation to analyze a small A/B test?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm running a small test to figure out which ads perform better. I have the following results:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 1:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,26&#xA;row-3,767 7&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 2:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,16&#xA;row-3,767 13&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Position 3:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;variation,impressions,clicks&#xA;row-1,753,2&#xA;row-3,767 7&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it's safe to say these numbers are small and likely to be not normally distributed. Also, it's click data so there's a binary outcome of clicked or not and the trials are independent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Appropriate test&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In analyzing each position for significance, I think comparison with a binomial or Poisson distribution makes the most sense. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;http://www.openintro.org/stat/textbook.php?stat_book=os&quot; rel=&quot;nofollow&quot;&gt;the OpenIntro Stats&lt;/a&gt; (and other sources) book, a variable follows a Poisson distribution &quot;... if the event being considered is rare, the population is large, and the events occur independently of each other.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same source classifies a binomial variable approximately the same way adding that the probability of success is the same and the number of trials is fixed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I appreciate this is not an either/or decision and analysis can be done using both distributions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given A/B (split) testing is a science that has been practiced for several years, I imagine that there is a canonical test. However, looking around the internet, I mostly come across analysis that uses the standard normal distribution. That just seems wrong :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a canonical test to use for A/B tests with small #'s of clicks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Interpretation and R code&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've used the following R code to test significance for each position:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Position 1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;binom.test(7, 767, p=(26/753))&#xA;&#xA;Exact binomial test&#xA;&#xA;data:  7 and 767&#xA;number of successes = 7, number of trials = 767, p-value = 1.077e-05&#xA;alternative hypothesis: true probability of success is not equal to 0.03452855&#xA;95 percent confidence interval:&#xA; 0.003676962 0.018713125&#xA;sample estimates:&#xA;probability of success &#xA;           0.009126467&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I interpret this result to mean: The probability of success in the test group is indeed different than the control group with a 95% confidence interval that the success probability is between .368% and 1.87%&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ppois(((26-1)/753), lambda=(7/767), lower.tail = F)&#xA;[1] 0.009084947&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I interpret this result to mean: Given a Poisson distribution with a click rate of 7 per 767 trials, there is a 0.9% chance of having a click rate of 26 or more per 753 trials in the same distribution. Contextualized in the ad example,&#xA;there is a .1% chance that the control ad actually performs the same as the test ad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the above interpretation correct? Does the test and interpretation change with the different positions (i.e. are the results of the Poisson test more appropriate for Position 3 given the small numbers)?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the tools I've found are (in no particular order):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scalding&lt;/li&gt;&#xA;&lt;li&gt;Breeze&lt;/li&gt;&#xA;&lt;li&gt;Spark&lt;/li&gt;&#xA;&lt;li&gt;Saddle&lt;/li&gt;&#xA;&lt;li&gt;H2O&lt;/li&gt;&#xA;&lt;li&gt;Spire&lt;/li&gt;&#xA;&lt;li&gt;Mahout&lt;/li&gt;&#xA;&lt;li&gt;Hadoop&lt;/li&gt;&#xA;&lt;li&gt;MongoDB&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If I need to be more specific to make the question answerable: I'm not particularly interested in clusters and Big Data at this moment, but I'm interested in sizable data (up to 100 GB) for information integration and predictive analytics.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am facing this bizarre issue while using &lt;code&gt;Apache Pig&lt;/code&gt; &lt;strong&gt;rank&lt;/strong&gt; utility. I am executing the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;email_id_ranked = rank email_id;&#xA;store email_id_ranked into '/tmp/';&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, basically I am trying to get the following result&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1,email1&#xA;2,email2&#xA;3,email3&#xA;... &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Issue is sometime pig dumps the above result but sometimes it dumps only the emails without the rank. Also when I dump the data on screen using &lt;code&gt;dump&lt;/code&gt; function pig returns both the columns. I don't know where the issue is. Kindly advice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if you need any more information. Thanks in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Pig version: Apache Pig version 0.11.0-cdh4.6.0&lt;/strong&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I need to do coreference resolution for German texts and I plan to use OpenNLP to perform this task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know OpenNLP coreference resolution does not support the German language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which components/data do I need to adapt the code such that it is possible to perform coreference resolution for German texts?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;does anybody know a libarary for performing coreference resolution on German texts?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know OpenNLP and Standord NLP are not able to perform coreference resolution for German Texts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only tool that I know is &lt;a href=&quot;http://www.cl.uzh.ch/static/news.php?om=view&amp;amp;nid=163&quot; rel=&quot;nofollow&quot;&gt;CorZu&lt;/a&gt; which is a python library.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I know that ARIMA can't detect multiple seasonality, but it is possible to &lt;a href=&quot;http://robjhyndman.com/hyndsight/dailydata/&quot; rel=&quot;nofollow&quot;&gt;use fourier functions to add a second seasonality&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to forecast gas consumption composed by a daily, weekly (week days-weekend), yearly seasonality. Does it make sense to apply three times the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html&quot; rel=&quot;nofollow&quot;&gt;STL decomposition by LOESS&lt;/a&gt;? The reason is that I applied the fourier method and I have bad results but I don't know if it is only because I applied it wrong.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm interested in the theoretical explanation, but here you find also the code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ARIMA + 2 STL:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;b &amp;lt;- ts(drop(coredata(dat.ts)), deltat=1/12/30/24, start=1)&#xA;fit &amp;lt;- stl(b, s.window=&quot;periodic&quot;)&#xA;b &amp;lt;- seasadj(fit)&#xA;dat.ts &amp;lt;- xts(b, index(dat.ts))&#xA;&#xA;# The weekdays are extracted&#xA;dat.weekdays &amp;lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xA;dat.weekdaysTS &amp;lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xA;fit &amp;lt;- stl(dat.weekdaysTS, s.window=&quot;periodic&quot;)&#xA;dat.weekdaysTS &amp;lt;- seasadj(fit)&#xA;&#xA;arima &amp;lt;- Arima(dat.weekdaysTS, order=c(3,0,5))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With fourier:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dat.weekdays &amp;lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xA;dat.weekdaysTS &amp;lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xA;z &amp;lt;- fourier(ts(dat.weekdaysTS, frequency=365.25), K=5)&#xA;arima &amp;lt;- Arima(dat.weekdaysTS, order=c(3,0,5),xreg=z)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lBxL5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are 4 different weight matrices that I got after training a &lt;strong&gt;restricted Boltzman machine (RBM)&lt;/strong&gt; with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though &lt;em&gt;none&lt;/em&gt; of weights are &lt;em&gt;exactly&lt;/em&gt; the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: what is the &lt;strong&gt;most likely reason&lt;/strong&gt; for weights to get &lt;strong&gt;very similar values&lt;/strong&gt;? Do they all just get to some local minimum? Or is it a sign of overfitting? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I currently use a kind of Gaussian-Bernoulli RBM, code may be found &lt;a href=&quot;https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPD.&lt;/strong&gt; My dataset is based on &lt;a href=&quot;http://www.pitt.edu/~emotion/ck-spread.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;CK+&lt;/a&gt;, which contains &gt; 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/FQMSp.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;img src=&quot;https://i.stack.imgur.com/Hjcaw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When training RBM, I take only non-zero pixels, so outer black region is ignored. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I know the difference between clustering and classification in machine learning, but I don't understand the difference between text classification and topic modeling for documents. Can I use topic modeling over documents to identify a topic? Can I use classification methods to classify the text inside these documents?  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;For experimenting we'd like to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Emoji&quot;&gt;Emoji&lt;/a&gt; embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Note that SentiWordNet does allow for &lt;em&gt;ambiguous&lt;/em&gt; meanings, too. Consider e.g. &lt;a href=&quot;http://sentiwordnet.isti.cnr.it/search.php?q=funny&quot;&gt;funny&lt;/a&gt;, which is not just positive: &quot;this tastes funny&quot; is probably not positive... same will hold for &lt;code&gt;;-)&lt;/code&gt; for example. But I don't think this is harder for Emoji than it is for regular words...)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking for a product that allows us to take in a collection of datastreams, and then after some event, will find any data that changes or correlates with that event. (For example, having a headache, and identifying that I drank too much beer last night and didn't drink enough water)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am using the &lt;code&gt;stepAIC&lt;/code&gt; function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;          Df     Sum of Sq    RSS       AIC&#xA;&amp;lt;none&amp;gt;                        350.71   -5406.0&#xA;- aaa      1     0.283        350.99   -5405.9&#xA;- bbb      1     0.339        351.05   -5405.4&#xA;- ccc      1     0.982        351.69   -5400.5&#xA;- ddd      1     0.989        351.70   -5400.5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Question Are the values listed under &lt;code&gt;Df&lt;/code&gt;, &lt;code&gt;Sum of Sq&lt;/code&gt;, &lt;code&gt;RSS&lt;/code&gt;, and &lt;code&gt;AIC&lt;/code&gt; the values for a model where only one variable would be considered as the independent variable (i.e. y ~&#xA;aaa, y ~ bbb, etc.)? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;While finding frequent subgraphs in single large graph, subgraph isomorphism (test) is not considered because its not anti-monotone. How and why subgraph isomorphism is not anti-monotone ?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I like to find the weight vector for input-space features in a &lt;a href=&quot;http://en.wikipedia.org/wiki/Structured_support_vector_machine&quot; rel=&quot;nofollow&quot;&gt;structured SVM&lt;/a&gt;. The idea is to identify the most important set of input-space features (based on the magnitude of their corresponding weights). I know that in a binary SVM the weight vector can be written as a &lt;a href=&quot;http://pyml.sourceforge.net/doc/howto.pdf&quot; rel=&quot;nofollow&quot;&gt;linear combination of examples&lt;/a&gt;, and the magnitude of those weights represents how much they were effective for the prediction problem at hand. But how do you compute the same for an SSVM?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I'm exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I'm considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)&lt;/li&gt;&#xA;&lt;li&gt;Using locality-sensitive hashing (LSH)&lt;/li&gt;&#xA;&lt;li&gt;Reducing first the dimensionality of the problem with a PCA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'd appreciate any thoughts or advices about possible other ways in which I could tackle this problem.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Let me show you an example of a hypothetical online clustering application:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZVOQN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/sQj4h.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This finally leads to my questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Does this &quot;problem&quot; have a name that it can be referred to?&lt;/li&gt;&#xA;&lt;li&gt;Are there &quot;standard&quot; solutions to this and ...&lt;/li&gt;&#xA;&lt;li&gt;... is there maybe even an R package for that?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Reasonable Inheritance of Cluster Identities in Repetitive Clustering&lt;/a&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying now the simplest features with just list of words and distance between sentences defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/dHB9X.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(A and B are corresponding sets of words in sentence A and B)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make sense at all? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to apply &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mean-Shift&lt;/a&gt; algorithm from scikit library to this distance, as it does not require number of clusters in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have no knowledge about the climate or soil. And I just want to find out more about these kind of dataset. I heard that Climate Corporation asked its candidates to perform statistical analysis on various climate dataset. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why I am asking this question. Please do not get me wrong. I am not trying to get the dataset to prepare myself for an interview, as I know they give out different dataset to people from different background. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that Climate Corporation only hires PHD, which I am not. I only want to play around with their dataset such that I can learn and implement &lt;strong&gt;time series analysis&lt;/strong&gt;. That's it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, if anyone does not mind sharing their dataset. Please post the link them below. Thank you very much. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a question about classifying documents using supervised learning and unsupervised learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: - I have a bunch of documents talking about football.&lt;br&gt;&#xA;As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.&lt;br&gt;&#xA;My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to create more accurate categorizations because if I want to test a new document I want know if this document can be related to these categorizations or not.  &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren't garbage. Scaling the features didn't help as well. My current thinking is that these kind of features don't mix well with the (sparse) bag of words features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn't seem to be the issue.&#xA;I have also tried changing the 'C' parameter value from 1 to 1e3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am facing similar issues with all svm implementations using scikit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I not waiting enough for it to complete ?&#xA;How much time should this execution take ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my experience it shouldn't require over a few minutes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my system configuration:&#xA;Ubuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've been analyzing a data set of ~400k records and 9 variables The dependent variable is binary. I've fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this so? I'm guessing that it's because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Where can I find free spatio-temporal dataset for download so that I can play with it in R ? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Caveat: I am a complete beginner when it comes to machine learning, but eager to learn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a large dataset and I'm trying to find pattern in it. There may / may not be correlation across the data, either with known variables, or variables that are contained in the data but which I haven't yet realised are actually variables / relevant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm guessing this would be a familiar problem in the world of data analysis, so I have a few questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The 'silver bullet' would be to throw this all this data into a stats / data analysis program and for it to crunch the data looking for known / unknown patterns trying to find relations. Is SPSS suitable, or are there other applications which may be better suited.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Should I learn a language like R, and figure out how to manually process the data. Wouldn't this comprimise finding relations as I would have to manually specify what and how to analyse the data?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How would a professional data miner approach this problem and what steps would s/he take?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I recently read &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf&quot; rel=&quot;nofollow&quot;&gt;Similarity Measures for Short Segments of Text&lt;/a&gt; (Metzler et al.).  It describes basic methods for measuring query similarity, and in the paper, the data consists of queries and their top results. Results are lists of page urls, page titles, and short page snippets.  In the paper, the authors collect 200 results per query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.  There's a substantial difference between 10 and 200.  Hence, how much data is commonly used in practice to measure query similarity (e.g., how many results per query)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;References are a plus!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to scrape some data from a website. &#xA;I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I've just started reading about AB testing, as it pertains to optimizing website design.  I find it interesting that most of the methods assume that changes to the layout and appearance are independent of each other.  I understand that the most common method of optimization is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multi-armed_bandit&quot; rel=&quot;nofollow&quot;&gt;'multi-armed bandit'&lt;/a&gt; procedure.  While I grasp the concept of it, it seems to ignore the fact that changes (changes to the website in this case) are not independent to each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if company is testing the placement and color of the logo on the website, they find the optimal color first then the optimal placement.  Not that I'm some expert on human psychology, but shouldn't these be related? Can the multi-armed bandit method be efficiently used in this case or more complicated cases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first instinct is to say no.  On that note, why haven't people used heuristic algorithms to optimize over complicated AB testing sample spaces?  For an example, I thought someone might have used a genetic algorithm to optimize a website layout, but I can find no examples of something like this out there. This leads me to believe that I'm missing something important in my understanding of AB testing as it applies to website optimization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why isn't heuristic optimization used on more complicated websites?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have installed &lt;a href=&quot;https://github.com/Factual/drake&quot; rel=&quot;nofollow&quot;&gt;Drake&lt;/a&gt; on Windows 7 64-bit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using JDK 1.7.0_51.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried both using the pre-compiled jar file and&#xA;compiling from the Clojure source using &lt;a href=&quot;https://github.com/technomancy/leiningen&quot; rel=&quot;nofollow&quot;&gt;leiningen&lt;/a&gt;.&#xA;The resulting Drake version is 0.1.6, the current development version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When running Drake, I get the current version number.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next, I tried to go through &lt;a href=&quot;https://github.com/Factual/drake/wiki/Tutorial&quot; rel=&quot;nofollow&quot;&gt;the tutorial&lt;/a&gt;. The command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java -jar drake.jar  -w .\\\\workflow.d&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;results in the following Exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.Exception: no input data found in locations: D:\\\\tools\\\\drake\\\\in.c&#xA;sv&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Even though the file exists and has text inside it. &#xA;The same scenario works in a similar installation on Ubuntu 12.04.&#xA;Am I doing something wrong, or is this a Windows-specific bug?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm studying reinforcement learning in order to implement a kind of time series pattern analyzer such as market.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most examples I have seen are based on the maze environment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another question is about buy-sell modeling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume that the agent randomly buy at time $t$ and sell at time $t + \\\\alpha$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's simple to calculate reward.&#xA;The problem is how can I model $Q$ matrix and how can I model signals between buy and sell actions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you share some source code or guidance for similar situation?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am hoping to model the characteristics of the users of a specific page on Facebook, which has roughly 2 million likes. I have been looking at the Facebook SDK/API, but I can't really see if what I would like to do is possible. It seems that the users share quite different amounts of data so I probably discard a lot of users and only use the ones with a quite open public profile. I would like to have the following data:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) See the individuals that have 'liked' the page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) See the list of friends for each person that have 'liked' the page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) See gender for each person (optional)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) See other pages that each person has liked (optional)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone tell me if it is possible to get this data? As mentioned earlier it is okay if I discard data for users that don't like to share this data.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. They can be considered to be noisy variations of a canonical string. I am looking for an algorithm or a library that will convert each list into this canonical string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is one such list.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Star Wars: Episode IV A New Hope | StarWars.com&lt;/li&gt;&#xA;&lt;li&gt;Star Wars Episode IV - A New Hope (1977)&lt;/li&gt;&#xA;&lt;li&gt;Star Wars: Episode IV - A New Hope - Rotten Tomatoes&lt;/li&gt;&#xA;&lt;li&gt;Watch Star Wars: Episode IV - A New Hope Online Free&lt;/li&gt;&#xA;&lt;li&gt;Star Wars (1977) - Greatest Films&lt;/li&gt;&#xA;&lt;li&gt;[REC] 4 poster promises death by outboard motor - SciFiNow&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For this list, any string matching the regular expression &lt;code&gt;^Star Wars:? Episode IV (- )?A New Hope$&lt;/code&gt; would be acceptable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked at Andrew Ng's course on Machine Learning on Coursera, but I was not able to find a similar problem.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've fit a GLM (Poisson) to a data set where one of the variables is categorical for the year a customer bought a product from my company, ranging from 1999 to 2012. There's a linear trend of the coefficients for the values of the variable as the year of sale increases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any problem with trying to improve predictions for 2013 and maybe 2014 by extrapolating to get the coefficients for those years?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have been developing a chess program which makes use of alpha-beta pruning algorithm and an evaluation function that evaluates positions using the following features namely material, kingsafety, mobility, pawn-structure and trapped pieces etc..... My evaluation function is derived from the &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$f(p) = w_1 \\\\cdot \\\\text{material} + w_2 \\\\cdot \\\\text{kingsafety} + w_3 \\\\cdot \\\\text{mobility} + w_4 \\\\cdot \\\\text{pawn-structure} + w_5 \\\\cdot \\\\text{trapped pieces}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $w$ is the weight assigned to each feature. At this point i want to tune the weights of my evaluation function using temporal difference, where the agent plays against itself and in the process gather training data from its environment (which is a form of reinforcement learning). I have read some books and articles in order to have an insight on how to implement this in Java but they seem to be theoretical rather than practical. I need a detailed explanation and pseudo codes on how to automatically tune the weights of my evaluation function based on previous games.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Which one will be the dominating programming language for next 5 years for analytics , machine learning . R verses python verses SAS. Advantage and disadvantage.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am exploring different types of parse tree structures. The two widely known parse tree structures are &#xA;a) Constituency based parse tree and &#xA;b) Dependency based parse tree structures. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am exploring how to model a data set using normal distributions with both mean and variance defined as linear functions of independent variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Something like N ~ (f(x), g(x)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generate a random sample like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def draw(x):&#xA;    return norm(5 * x + 2, 3 *x + 4).rvs(1)[0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I want to retrieve 5, 2 and 4 as the parameters for my distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generate my sample:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;smp = np.zeros((100,2))&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for i in range(0, len(smp)):&#xA;    smp[i][0] = i&#xA;    smp[i][1] = draw(i)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The likelihood function is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def lh(p):&#xA;    p_loc_b0 = p[0]&#xA;    p_loc_b1 = p[1]&#xA;    p_scl_b0 = p[2]&#xA;    p_scl_b1 = p[3]&#xA;&#xA;    l = 1&#xA;    for i in range(0, len(smp)):&#xA;        x = smp[i][0]&#xA;        y = smp[i][1]&#xA;        l = l * norm(p_loc_b0 + p_loc_b1 * x, p_scl_b0 + p_scl_b1 * x).pdf(y)&#xA;&#xA;    return -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the parameters for the linear functions used in the model are given in the p 4-variable vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using scipy.optimize, I can solve for the MLE parameters using an extremely low xtol, and already giving the solution as the starting point:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fmin(lh, x0=[2,5,3,4], xtol=1e-35)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which does not work to well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Warning: Maximum number of function evaluations has been exceeded.&#xA;array([ 3.27491346,  4.69237042,  5.70317719,  3.30395462])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Raising the xtol to higher values does no good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So i try using a starting solution far from the real solution:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; fmin(lh, x0=[1,1,1,1], xtol=1e-8)&#xA;Optimization terminated successfully.&#xA;         Current function value: -0.000000&#xA;         Iterations: 24&#xA;         Function evaluations: 143&#xA;array([ 1.,  1.,  1.,  1.])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which makes me think:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PDF are largely clustered around the mean, and have very low gradients only a few standard deviations away from the mean, which must be not too good for numerical methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how does one go about doing these kind of numerical estimation in functions where gradient is very near to zero away from the solution?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have created external table in Hive in the hdfs path 'hdfs://localhost.localdomain:8020/user/hive/training'. If I apply describe command I can find the table path as shown below. But when I browse through the namenode web page, the table name does not showing up in the path.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hive&amp;gt; describe extended testtable4;&#xA;OK&#xA;firstname   string  &#xA;lastname    string  &#xA;address string  &#xA;city    string  &#xA;state   string  &#xA;country string  &#xA;&#xA;    ***Detailed Table Information  Table(tableName:testtable4, dbName:default, owner:cloudera, createTime:1408765301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:firstname, type:string, comment:null), FieldSchema(name:lastname, type:string, comment:null), FieldSchema(name:address, type:string, comment:null), FieldSchema(name:city, type:string, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], location:hdfs://localhost.localdomain:8020/user/hive/training, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,, line.delim=    &#xA;    }), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1408765301}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)       &#xA;    Time taken: 0.7 seconds***&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am seeking a basic list of key data analysis methods used for studying social media platforms online. Are there such key methods, or does this process generally vary according to topic? And is there a standard order in which these methods are applied?(The particular context I'm interested in is how the news is impacting on social media)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;As what I described in the title, we are especially interested in those for dealing with big data----ts efficiency and stability, and used in industry not in experiment or university. Thanks!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Have anyone used Shark as repository from resulting datasets from Apache Spark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm starting some tests with Spark and read about this database tecnology. Have anyone been using it?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;strong&gt;General description of the problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a graph where some vertices are labeled with a type with 3 or 4 possible values. For the other vertices, the type is unknown.&#xA;My goal is to use the graph to predict the type for vertices that are unlabeled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible framework&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suspect this fits into the general framework of label propagation problems, based on my reading of the literature (e.g., see &lt;a href=&quot;http://lvk.cs.msu.su/~bruzz/articles/classification/zhu02learning.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; and &lt;a href=&quot;http://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/slides/pdf/Frequent_Subgraph_Mining.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another method that is mentioned often is &lt;code&gt;Frequent Subgraph Mining&lt;/code&gt;, which includes algorithms like &lt;code&gt;SUBDUE&lt;/code&gt;,&lt;code&gt;SLEUTH&lt;/code&gt;, and &lt;code&gt;gSpan&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Found in R&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only label propagation implementation I managed to find in &lt;code&gt;R&lt;/code&gt; is &lt;code&gt;label.propagation.community()&lt;/code&gt; from the &lt;code&gt;igraph&lt;/code&gt; library. &#xA;However, as the name suggests, it is mostly used to find communities, not for classifying unlabeled vertices.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There also seems to be several references to a &lt;code&gt;subgraphMining&lt;/code&gt; library (here for example), but it looks like it is missing from CRAN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know of a library or framework for the task described?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I would like to &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/base/html/summary.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;summarize&lt;/a&gt; (as in R) the contents of a CSV (possibly after &lt;a href=&quot;http://www.endmemo.com/program/R/readcsv.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;loading&lt;/a&gt; it, or storing it somewhere, that's not a problem). The summary should contain the quartiles, mean, median, min and max of the data in a CSV file for each numeric (integer or real numbers) dimension. The standard deviation would be cool as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also like to generate some plots to visualize the data, for example 3 plots for the 3 pairs of variables that are more correlated (&lt;a href=&quot;http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient&quot; rel=&quot;nofollow noreferrer&quot;&gt;correlation coefficient&lt;/a&gt;) and 3 plots for the 3 pairs of variables that are least correlated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;R requires only a few lines to implement this. Are there any libraries (or tools) that would allow a similarly simple (and efficient if possible) implementation in Java or Scala?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PD: This is a specific use case for a &lt;a href=&quot;https://datascience.stackexchange.com/questions/948/any-clear-winner-for-data-science-in-scala&quot;&gt;previous (too broad) question&lt;/a&gt;.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was wondering if there is any research or study made to calculate the volume of space is used by all scientific articles. It could be in pdf, txt, compressed, or any other format. Is there even a way to measure it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can some one point me towards realizing this study?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regards and thanks.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a question regarding the use of neural network. I am currently working with R (&lt;a href=&quot;http://cran.r-project.org/web/packages/neuralnet/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;neuralnet package&lt;/a&gt;) and I am facing the following issue.&#xA;My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?&#xA;Maybe something is wrong in my analysis&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I use the daily log return &#xA;&lt;img src=&quot;https://i.stack.imgur.com/A3r16.gif&quot; alt=&quot;r(t) = ln(s(t)/s(t-1))&quot;&gt;&lt;/li&gt;&#xA;&lt;li&gt;I normalise my data with the sigmoid function (sigma and mu computed on my whole set)&lt;/li&gt;&#xA;&lt;li&gt;I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm looking for the best solution to manage and host datasets for journalistic pursuits. I am assessing &lt;a href=&quot;https://www.documentcloud.org&quot; rel=&quot;nofollow&quot;&gt;https://www.documentcloud.org&lt;/a&gt; and &lt;a href=&quot;http://datahub.io/&quot; rel=&quot;nofollow&quot;&gt;http://datahub.io/&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain the differences between them, or recommend a superior solution?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to evaluate and compare several different machine learning models built with different parameters (i.e. downsampling, outlier removal) and different classifiers (i.e. Bayes Net, SVM, Decision Tree).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am performing a type of cross validation where I randomly select 67% of the data for use in the training set and 33% of the data for use in the testing set. I perform this for several iterations, say, 20.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, from each iteration I am able to generate a confusion matrix and compute a kappa. My question is, what are some ways to aggregate these across the iterations? I am also interested in aggregating accuracy and expected accuracy, among other things.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the kappa, accuracy, and expected accuracy, I have just been taking the average up to this point. One of the problems is that when I recompute kappa with the aggregated average and expected average, it is not the same with the aggregated kappa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the confusion matrix, I have been first normalizing the confusion matrix from each iteration and then averaging them, in an attempt to avoid an issue of confusion matrices with different numbers of total cases (which is possible with my cross validation scheme).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I recompute the kappa from this aggregated confusion matrix, it is also different from the previous two.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which one is most correct? Is there another way of computing an average kappa that is more correct?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks, and if more concrete examples are needed in order to illustrate my question please let me know.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am interested in knowing the differences in &lt;strong&gt;functionality&lt;/strong&gt; between SAP HANA and Exasol. Since this is a bit of an open ended question let me be clear. I am not interested in people debating which is &quot;better&quot; or faster. I am only interested in what each was designed to do so please keep your opinions out of it. I suspect it is a bit like comparing HANA to Oracle Exalytics where there is some overlap but the functionality goals are different. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;How is the concept of data different for different disciplines? Obviously, for physicists and sociologists, &quot;data&quot; is something different.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking for packages (either in python, R, or a standalone package) to perform online learning to predict stock data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have found and read about Vowpal Wabbit (&lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit/wiki&quot;&gt;https://github.com/JohnLangford/vowpal_wabbit/wiki&lt;/a&gt;),&#xA;which seems to be quite promising but I am wondering if there are any other packages out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;In SVMs the polynomial kernel is defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(scale * crossprod(x, y) + offset)^degree&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do the scale and offset parameters affect the model and what range should they be in? (intuitively please)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are the scale and offset for numeric stability only (that's what it looks like to me), or do they influence prediction accuracy as well?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can good values for scale and offset be calculated/estimated when the data is known or is a grid search required? The caret package always sets the offset to 1, but it does a grid search for scale. (Why) is an offset of 1 a good value?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS.: Wikipedia didn't really help my understanding:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For degree-d polynomials, the polynomial kernel is defined as&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/3stCR.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;where x and y are vectors in the input space, i.e. vectors of features&#xA;  computed from training or test samples, &lt;img src=&quot;https://i.stack.imgur.com/34dwi.png&quot; alt=&quot;&quot;&gt; &lt;strong&gt;is a constant trading&lt;/strong&gt;&#xA;  &lt;strong&gt;off the influence of higher-order versus lower-order terms&lt;/strong&gt; in the&#xA;  polynomial. When &lt;img src=&quot;https://i.stack.imgur.com/79cDG.png&quot; alt=&quot;&quot;&gt;, the kernel is called homogeneous.(&lt;strong&gt;A further&lt;/strong&gt;&#xA;  &lt;strong&gt;generalized polykernel divides &lt;img src=&quot;https://i.stack.imgur.com/Ps0qt.png&quot; alt=&quot;&quot;&gt; by a user-specified scalar&lt;/strong&gt;&#xA;  &lt;strong&gt;parameter &lt;img src=&quot;https://i.stack.imgur.com/sjfS1.png&quot; alt=&quot;&quot;&gt;.&lt;/strong&gt;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Neither did ?polydot's explanation in R's help system:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt;: The scaling parameter of the polynomial and tangent kernel is a&#xA;  convenient way of normalizing patterns (&amp;lt;-!?) without the need to modify the&#xA;  data itself&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;strong&gt;offset&lt;/strong&gt;: The offset used in a polynomial or hyperbolic tangent kernel (&amp;lt;- lol thanks)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Background:&#xA;I run a product that compares sets of data (data matching and data reconciliation).&#xA;To get the result we need to compare each row in a data set with every N rows on the opposing data set&#xA;Now however we get sets of up to 300 000 rows of data in each set to compare and are getting 90 Billion computations to handle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is this:&#xA;Even though we dont have the data volumes to use Hadoop, we have the computational need for something distributed. Is Hadoop a good choice for us?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm going to classify unstructured text documents, namely web sites of unknown structure. The number of classes to which I am classifying is limited (at this point, I believe there is no more than three). Does anyone have a suggested for how I might get started?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the &quot;bag of words&quot; approach feasible here? Later, I could add another classification stage based on document structure (perhaps decision trees).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am somewhat familiar with Mahout and Hadoop, so I prefer Java-based solutions. If needed, I can switch to Scala and/or Spark engine (the ML library).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm searching for data sets for evaluating text retrieval quality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TF-IDF is a popular similarity measure, but is it the best choice? And which &lt;em&gt;variant&lt;/em&gt; is the best choice? &lt;a href=&quot;https://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html&quot; rel=&quot;nofollow&quot;&gt;Lucenes Scoring&lt;/a&gt; for example uses IDF^2, and IDF defined as 1+log(numdocs/(docFreq+1)). TF in lucene is defined as sqrt(frequency)...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many more variants exist, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Okapi_BM25&quot; rel=&quot;nofollow&quot;&gt;Okapi BM25&lt;/a&gt;, which is used by the &lt;a href=&quot;http://xapian.org/docs/bm25.html&quot; rel=&quot;nofollow&quot;&gt;Xapian search engine&lt;/a&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to study the different variants, and I'm looking for &lt;strong&gt;evaluation data sets&lt;/strong&gt;. Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was curious about the ANOVA RBF kernel provided by kernlab package available in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tested it with a numeric dataSet of 34 input variables and one output variable. For each variable I have 700 different values. Comparing with other kernels, I got very bad results with this kernel.&#xA;For example using the simple RBF kernel I could predict with 0,88 R2 however with the anova RBF I could only get 0,33 R2.&#xA;I thought that ANOVA RBF would be a very good kernel. Any thoughts? Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;set.seed(100) #use the same seed to train different models&#xA;svrFitanovaacv &amp;lt;- train(R ~ .,&#xA;                       data = trainSet,&#xA;                       method = SVManova,&#xA;                       preProc = c(&quot;center&quot;, &quot;scale&quot;),&#xA;                       trControl = ctrl, tuneLength = 10) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = &quot;ROC&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;define custom model in caret package:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(caret)&#xA;#RBF ANOVA KERNEL&#xA;SVManova &amp;lt;- list(type = &quot;Regression&quot;, library = &quot;kernlab&quot;, loop = NULL)&#xA;prmanova &amp;lt;- data.frame(parameter = c(&quot;C&quot;, &quot;sigma&quot;, &quot;degree&quot;, &quot;epsilon&quot;),&#xA;                     class = rep(&quot;numeric&quot;, 4),&#xA;                     label = c(&quot;Cost&quot;, &quot;Sigma&quot;, &quot;Degree&quot;, &quot;Epsilon&quot;))&#xA;SVManova$parameters &amp;lt;- prmanova&#xA;svmGridanova &amp;lt;- function(x, y, len = NULL) {&#xA;library(kernlab)&#xA;sigmas &amp;lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)&#xA;expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,&#xA;            C = 2^(-40:len), degree = 1:2) # len = tuneLength in train&#xA;}&#xA;SVManova$grid &amp;lt;- svmGridanova&#xA;svmFitanova &amp;lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {&#xA;  ksvm(x = as.matrix(x), y = y,&#xA;       kernel = &quot;anovadot&quot;,&#xA;       kpar = list(sigma = param$sigma, degree = param$degree),&#xA;       C = param$C, epsilon = param$epsilon,&#xA;       prob.model = classProbs,&#xA;       ...) #default type = &quot;eps-svr&quot;&#xA;}&#xA;SVManova$fit &amp;lt;- svmFitanova&#xA;svmPredanova &amp;lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xA;  predict(modelFit, newdata)&#xA;SVManova$predict &amp;lt;- svmPredanova&#xA;svmProb &amp;lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xA;  predict(modelFit, newdata, type=&quot;probabilities&quot;)&#xA;SVManova$prob &amp;lt;- svmProb&#xA;svmSortanova &amp;lt;- function(x) x[order(x$C), ]&#xA;SVManova$sort &amp;lt;- svmSortanova&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;load data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataA2&amp;lt;-read.csv(&quot;C:/results/A2.txt&quot;,header = TRUE, &#xA;                             blank.lines.skip = TRUE,sep = &quot;,&quot;)&#xA;set.seed(1)&#xA;inTrainSet &amp;lt;- createDataPartition(dataA2$R, p = 0.75, list = FALSE) #[[1]]&#xA;trainSet &amp;lt;- dataA2[inTrainSet,]&#xA;testSet &amp;lt;- dataA2[-inTrainSet,]&#xA;#-----------------------------------------------------------------------------&#xA;#K-folds resampling method for fitting svr&#xA;ctrl &amp;lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10,&#xA;                     allowParallel = TRUE) #10 separate 10-fold cross-validations&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;link to data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;wuala.com/jpcgandre/Documents/Data%20SVR/?key=BOD9NTINzRHG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;We have ~500 biomedical documents each of some 1-2 MB. We want to use a non query-based method to rank the documents in order of their unique content score. I'm calling it &quot;unique content&quot; because our researchers want to know from which document to start reading. All the documents are of the same topic, in the biomedical world we know that there is always a lot of content overlap. So all we want to do is to arrange the documents in the order of their unique content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most Information Retrieval literature suggest query-based ranking which does not fit our need.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;It may be unlikely that anyone knows this but I have a specific question about Freebase.  Here is the Freebase page from the &lt;a href=&quot;http://www.freebase.com/m/014_d3&quot; rel=&quot;nofollow&quot;&gt;Ford Taurus automotive model&lt;/a&gt; .  It has a property called &quot;Related Models&quot;.  Does anyone know how this list of related models was compiled.  What is the similarity measure that they use?  I don't think it is only about other wikipedia pages that link to or from this page.  Alternatively, it may be that this is user generated.  Does anyone know for sure?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What is the best technology to be used to create my custom bag of words with N-grams to apply to. I want to know a functionality that can be achieved over GUI. I cannot use spot fire as it is not available in the organization. Though i can get SAP Hana or R-hadoop. But R-hadoop is bit challenging, any suggessions.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's &lt;a href=&quot;http://scikit-learn.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;scikit-learn&lt;/a&gt; for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in &lt;a href=&quot;https://stackoverflow.com/q/20106940/1435804&quot;&gt;this&lt;/a&gt; Stackoverflow post.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my machine learning task, I create a set of predictors.&#xA;Predictors come in &quot;bundles&quot; - multi-dimensional measurements (3 or 4 - dimensional in my case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The hole &quot;bundle&quot; makes sense only if it has been measured, and taken all together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is, different 'bundles' of predictors can be measured only for small part of the sample, and those parts don't necessary intersect for different 'bundles'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As parts are small, imputing leads to considerable decrease in accuracy(catastrophical to be more accurate)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible solutions&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could create dummy variables that would mark whether the measurement has taken place for each variable. The problem is, when random forests draws random variables, it does so individually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So there are two basic ways to solve this problem:&#xA;1) Combine each &quot;bundle&quot; into one predictor. That is possible, but it seems information will be lost. &#xA;2) Make random forest draw variables not individually, but by obligatory &quot;bundles&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Problem for random forest&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As random forest draws variables randomly, it takes features that are useless (or much less useful) without other from their &quot;bundle&quot;. I have a feeling that leads to a loss of accuracy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example I have variables &lt;code&gt;a&lt;/code&gt;,&lt;code&gt;a_measure&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;,&lt;code&gt;b_measure&lt;/code&gt;.&#xA;The problem is, variables &lt;code&gt;a_measure&lt;/code&gt; make sense only if variable &lt;code&gt;a&lt;/code&gt; is present, same for &lt;code&gt;b&lt;/code&gt;. So I either have to combine &lt;code&gt;a&lt;/code&gt;and &lt;code&gt;a_measure&lt;/code&gt; into one variable, or make random forest draw both, in case at least one of them is drawn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the best practice solutions for problems when different sets of predictors are measured for small parts of overall population, and these sets of predictors come in obligatory &quot;bundles&quot;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;The problem refers to decision trees building. According to Wikipedia '&lt;a href=&quot;http://en.wikipedia.org/wiki/Gini_coefficient&quot;&gt;Gini coefficient&lt;/a&gt;' should not be confused with '&lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity&quot;&gt;Gini impurity&lt;/a&gt;'. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) 'Gini impurity' - it is a standard decision-tree splitting metric (see in the link above);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) 'Gini coefficient' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking at pybrain for taking server monitor alarms and determining the root cause of a problem. I'm happy with training it using supervised learning and curating the training data sets. The data is structured something like this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;A&lt;/strong&gt; #1&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type 1&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;A&lt;/strong&gt; #2&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type 1&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Server Type &lt;strong&gt;B&lt;/strong&gt; #1&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alarm type &lt;strong&gt;99&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Alarm type 2&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So there are &lt;em&gt;n&lt;/em&gt; servers, with &lt;em&gt;x&lt;/em&gt; alarms that can be &lt;code&gt;UP&lt;/code&gt; or &lt;code&gt;DOWN&lt;/code&gt;. Both &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; are variable. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If Server A1 has &lt;em&gt;alarm 1 &amp;amp; 2&lt;/em&gt; as &lt;code&gt;DOWN&lt;/code&gt;, then we can say that &lt;em&gt;service a&lt;/em&gt; is down on that server and is the cause of the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If &lt;em&gt;alarm 1&lt;/em&gt; is down on all servers, then we can say that &lt;em&gt;service a&lt;/em&gt; is the cause.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There can potentially be multiple options for the cause, so straight classification doesn't seem appropriate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also like to tie later sources of data to the net. Such as just scripts that ping some external service.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the appropriate alarms may not be triggered at once, due to serial service checks, so it can start with one server down and then another server down 5 minutes later.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to do some basic stuff at first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pybrain.tools.shortcuts import buildNetwork&#xA;from pybrain.datasets import SupervisedDataSet&#xA;from pybrain.supervised.trainers import BackpropTrainer&#xA;&#xA;&#xA;INPUTS = 2&#xA;OUTPUTS = 1&#xA;&#xA;# Build network&#xA;&#xA;# 2 inputs, 3 hidden, 1 output neurons&#xA;net = buildNetwork(INPUTS, 3, OUTPUTS)&#xA;&#xA;&#xA;# Build dataset&#xA;&#xA;# Dataset with 2 inputs and 1 output&#xA;ds = SupervisedDataSet(INPUTS, OUTPUTS)&#xA;&#xA;&#xA;# Add one sample, iterable of inputs and iterable of outputs&#xA;ds.addSample((0, 0), (0,))&#xA;&#xA;&#xA;&#xA;# Train the network with the dataset&#xA;trainer = BackpropTrainer(net, ds)&#xA;&#xA;# Train 1000 epochs&#xA;for x in xrange(10):&#xA;    trainer.train()&#xA;&#xA;# Train infinite epochs until the error rate is low&#xA;trainer.trainUntilConvergence()&#xA;&#xA;&#xA;# Run an input over the network&#xA;result = net.activate([2, 1])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I[m having a hard time mapping variable numbers of alarms to static numbers of inputs. For example, if we add an alarm to a server, or add a server, the whole net needs to be rebuilt. If that is something that needs to be done, I can do it, but want to know if there's a better way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another option I'm trying to think of, is have a different net for each type of server, but I don't see how I can draw an environment-wide conclusion, since it will just make evaluations on a single host, instead of all hosts at once. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which type of algorithm should I use and how do I map the dataset to draw environment-wide conclusions as a whole with variable inputs?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&quot;Oversaw car manufacturing&quot; gets tagged as NNP-NN-NN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Rather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what's the best way to accomplish accuracy improvement?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Are there better models out there for POS/NER that can be incorporated into CoreNLP?&lt;/li&gt;&#xA;&lt;li&gt;Should I switch to other NLP tools?&lt;/li&gt;&#xA;&lt;li&gt;Or create training models with exception rules?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have been tasked with creating a pipeline chart with the live data and the budgeted numbers.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know what probability of each phase of reaching the next.  The problem is I have no Idea what to do about the pipeline budgeting with regards to time.  For instance what period of time should I have closed sales in the chart.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have honestly been working on trying to figure it out.  Each successive revision gets me farther from the answer.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am currently trying to implement logistic regression with iteratively reweightes LS, according to &quot;Pattern Recognition and Machine Learning&quot; by C. Bishop. In a first approach I tried to implement it in C#, where I used Gauss' algorithm to solve eq. 4.99. For a single feature it gave very promising (nearly exact) results, but whenever I tried to run it with more than one feature my system matrix became singular, and the weights did not converge. I first thought that it was my implementation, but when I implemented it in SciLab the results sustained. The SciLab (more concise due to matrix operators) code I used is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;phi = [1; 0; 1; 1];&#xA;t = [1; 0; 0; 0];&#xA;w= [1];&#xA;&#xA;w' * phi(1,:)'&#xA;&#xA;for in=1:100&#xA;    y = [];&#xA;    R = zeros(size(phi,1));&#xA;    R_inv = zeros(size(phi,1));&#xA;&#xA;    for i=1:size(phi,1)&#xA;        y(i) = 1/(1+ exp(-(w' * phi(i,:)')));&#xA;        R(i,i) = y(i)*(1 - y(i));&#xA;        R_inv(i,i) = 1/R(i,i);&#xA;    end&#xA;&#xA;    z = phi * w - R_inv*(y - t)&#xA;    w = inv(phi'*R*phi)*phi'*R*z&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the values for phi (input/features) and t (output/classes), it yields a weight of  -0.6931472, which is pretty much 1/3, which seems fine to me, for there is 1/3 probability of beeing assigned to class 1, if feature 1 is present (please forgive me, if my terms do not comply with ML-language completely, for I am an software developer). If I now added an intercept feature, which would accord to&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;phi = [1, 1; 1, 0; 1, 1; 1, 1];&#xA;w = [1; 1];&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;my R-matrix becomes singular and the last weights value is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;w  =&#xA;  - 5.8151677  &#xA;  1.290D+30  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which - to my reading - would mean, that the probability of belonging to class 1 would be close to 1 if feature 1 is present about 3% for the rest. There has got to be any error I made, but I do not get which one. For both implementations yield the same results I suspect that there is some point I've been missing or gotten wrong, but I do not understand which one.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am having an HTML string and want to find out if a word I supply is relevant in that string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Relevancy could be measured based on frequency in the text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example to illustrate my problem:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;this is an awesome bike store&#xA;bikes can be purchased online.&#xA;the bikes we own rock.&#xA;check out our bike store now&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to test a few other words:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bike repairs&#xA;dog poo&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;bike repairs&lt;/code&gt; should be marked as relevant whereas &lt;code&gt;dog poo&lt;/code&gt; should not be marked as relevant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How could this be done?&lt;/li&gt;&#xA;&lt;li&gt;How to I filter out ambiguous words like &lt;code&gt;in&lt;/code&gt; or &lt;code&gt;or&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks for your ideas!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess it's something Google does to figure out what keywords are relevant to a website. I am basically trying to reproduce their on-page rankings.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some good answers about sub-sampling and weighted random forest are given here: &lt;a href=&quot;https://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase&quot;&gt;What are the implications for training a Tree Ensemble with highly biased datasets?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which classification methods besides RF can handle the problem in the best way?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;As mentioned &lt;a href=&quot;https://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets&quot;&gt;before&lt;/a&gt;, I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.&#xA;I have trained a Generalized Boosted Regression model using &lt;code&gt;gbm()&lt;/code&gt; from the &lt;code&gt;gbm&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; and get the following output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xA;  1                  50       0.906     0.523  0.00978      0.0512  &#xA;  1                  100      0.91      0.561  0.0108       0.0517  &#xA;  1                  150      0.91      0.572  0.0104       0.0492  &#xA;  2                  50       0.908     0.569  0.0106       0.0484  &#xA;  2                  100      0.91      0.582  0.00965      0.0443  &#xA;  2                  150      0.91      0.584  0.00976      0.0437  &#xA;  3                  50       0.909     0.578  0.00996      0.0469  &#xA;  3                  100      0.91      0.583  0.00975      0.0447  &#xA;  3                  150      0.911     0.586  0.00962      0.0443  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That's clear.&#xA;And what is not transparent: how Kappa is calculated.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? &lt;/li&gt;&#xA;&lt;li&gt;What do &lt;code&gt;Accuracy SD&lt;/code&gt; and &lt;code&gt;Kappa SD&lt;/code&gt; mean?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to cluster a set of long-tailed / pareto-like data into several bins (actually the bin number is not determined yet). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which algorithm or model would anyone recommend?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a general methodological question. I have two columns of data, with one a column a numeric variable for age and another column a short character variable for text responses to a question. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to group the age variable (that is, create cut points for the age variable), based on the text responses. I'm unfamiliar with any general approaches for doing this sort of analysis. What general approaches would you recommend? Ideally I'd like to categorize the age variable based on linguistic similarity of the text responses.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/RFM_(customer_value)&quot; rel=&quot;nofollow&quot;&gt;RFM&lt;/a&gt; - is a ranking model when all customers are ranked according to their purchasing &lt;strong&gt;F&lt;/strong&gt; requency, &lt;strong&gt;R&lt;/strong&gt; recency and &lt;strong&gt;M&lt;/strong&gt; monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;predicting which customer will most likely spend more&lt;/li&gt;&#xA;&lt;li&gt;who is going to upgrade/renew subscribtion/refund etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update2&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I understand, this is simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.&lt;/li&gt;&#xA;&lt;li&gt;Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " ('&lt;p&gt;Suppose I am interested in classifying a set of instances composed by different content types, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a piece of &lt;strong&gt;text&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;an &lt;strong&gt;image&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;as &lt;code&gt;relevant&lt;/code&gt; or &lt;code&gt;non-relevant&lt;/code&gt; for a specific class &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my classification process I perform the following steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Given a sample, I subdivide it in text and image&lt;/li&gt;&#xA;&lt;li&gt;A first SVM binary classifier (&lt;code&gt;SVM-text&lt;/code&gt;), trained only on text, classifies the text as &lt;code&gt;relevant&lt;/code&gt;/&lt;code&gt;non-relevant&lt;/code&gt; for the class &lt;code&gt;C&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;A second SVM binary classifier (&lt;code&gt;SVM-image&lt;/code&gt;), trained only on images, classifies the image as &lt;code&gt;relevant&lt;/code&gt;/&lt;code&gt;non-relevant&lt;/code&gt; for the class &lt;code&gt;C&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Both &lt;code&gt;SVM-text&lt;/code&gt; and &lt;code&gt;SVM-image&lt;/code&gt; produce an estimate of the probability of the analyzed content (text or image) of being relevant for the class &lt;code&gt;C&lt;/code&gt;. Given this, I am able to state whether the text is relevant for &lt;code&gt;C&lt;/code&gt; and the image is relevant for &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, these estimates are valid for segments of the original sample (either the text or the image), while it is not clear how to obtain a general opinion on the whole original sample (text+image). How can I combine conveniently the opinions of the two classifiers, so as to obtain a classification for the whole original sample?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am working on a text classification problem using Random Forest as classifiers, and a bag-of-words approach. &#xA;I am using the basic implementation of Random Forests (the one present in scikit), that creates a binary condition on a single variable at each split. Given this, is there a difference between using simple tf (term frequency) features. where each word has an associated weight that represents the number of occurrences in the document, or tf-idf (term frequency * inverse document frequency), where the term frequency is also multiplied by a value that represents the ratio between the total number of documents and the number of documents containing the word)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my opinion, there should not be any difference between these two approaches, because the only difference is a scaling factor on each feature, but since the split is done at the level of single features this should not make a difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I right in my reasoning? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am interested in the field of &lt;strong&gt;named entity disambiguation&lt;/strong&gt; and want to learn more about it. I have heard that there are contests organised by various associations on these kind of research topics. These contests are very helpful as they give a practical experience in these fields. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found one such contest organised by Microsoft research &lt;a href=&quot;http://web-ngram.research.microsoft.com/erd2014/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; though the dates have already passed. Can anyone point me to any other such contests ? Also, is there a site which catalogues these contests so that one can just go there and know about all upcoming contests ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.  &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit... (Attempt to better explain what I meant by an algebraic system...)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about &quot;abstract algebras&quot; as discussed in Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Algebra#Abstract_algebra&quot;&gt;http://en.wikipedia.org/wiki/Algebra#Abstract_algebra&lt;/a&gt;&#xA;&lt;a href=&quot;http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts&quot;&gt;http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Boolean Algebras are (very simple) algebras that range over Boolean values.  A simple example of such an algebra would consist the values True and False and the operations AND, OR and NOT. One might argue this algebra is 'complete' as, from these two constants (free-variables) and three basic operations, arbitrary boolean functions can be constructed/described.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested to discover algebras where the values are (time-domain) time-series.  I'd like it to be possible to construct &quot;arbitrary&quot; functions, that map time-series to time-series, from a few operations which, individually, map time-series to time-series.  I am open to liberal interpretations of &quot;arbitrary&quot;. I would be especially interested in examples of these algebras where the operations consist 'higher-order functions' - where such operations have been developed for a specific domain.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a research scholar in data mining. I'm interested in C# implementation of K-Means clustering algorithm for mixed numeric and categorical data.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a rather large commute every day - it ranges between about an hour and about an hour and half of driving.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been tracking my driving times, and want to continue to do so. I am capturing the date, my time of departure, my time of arrival, the route I took (there are two or three possible ones), weather conditions (wet/dry and clear/hazy/foggy), and whether I stopped (and if so, for what reason - fuel/toilet break/food break, and for how long) for every journey to and from work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to create a system to analyse this data and suggest an optimal departure time (for the next journey) based on day of the week, weather conditions, and whether i need to stop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anecdotally, I can see that Tuesday mornings are worse than other mornings, the earlier I leave the more likely I am to take a toilet break or a food break, and obviously that the journey takes longer on rainy or foggy days than on clear and dry days - but I would like the system to empirically tell me that!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume this is a machine-learning and statistical analysis problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have absolutely no knowledge of machine-learning, or statistical methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What statistical methods should I use to do this kind of analysis to the point where the data will lead to suggestions like &quot;tomorrow is Tuesday and it is going to rain, so you must leave home between 7.50 and 8.00, and take route XYZ, to get the optimal driving time. Oh and chances are you will need a toilet break - and I have factored that in&quot;? (assume that I manually enter tomorrow’s weather forecast - I’ll look into integrating with a weather service later)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this is life-hacking for me, trying to optimise the hell out of a tedious process, and it is very personal - specific to me and my habits, specific to this route, and specific to the morning/evening commute times. Google Maps with Traffic, TomTom with IQ, and Waze do very well in the more open-ended situations of ad-hoc driving-time prediction. Even Apple is happy to tell me on my iPhone notification screen how long it will take me to get home if I leave right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note, it appears to me that traffic is not a consideration - that is to say, I do not think I need to know the actual traffic conditions - traffic is a function of day of the week and weather. For example, there are more people on the roads on Monday and Tuesday mornings, and people drive more slowly, and more people are in cars (opting to drive instead of cycle or take public transport) when it rains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To what extent can I let the data do all the talking? I have a somewhat ambiguous hidden agenda which may not be apparent from the data;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I should be at work at 9.30 (i.,e. 9.15 +/- 15 minutes) every day, but the occasional 10am arrival is OK&lt;/li&gt;&#xA;&lt;li&gt;I want to leave home as late as possible, and yet arrive at work as early as possible&lt;/li&gt;&#xA;&lt;li&gt;I want to leave work as early as possible, and yet have done at least 8 hours’ work&lt;/li&gt;&#xA;&lt;li&gt;it is OK for me to, say, leave half an hour early on one day but stay late on another to compensate&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I think I can come up with a procedural formula that can encompass all of these rules, but my gut feeling is that statistical analysis can make it a lot smarter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apart from the methods of analysis, the technology stack is not an issue. Java is my language of choice - I am quite familiar with programming in it, and in creating web applications.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming that it is possible, are there Java libraries that can provide the requisite methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What limitations are there? I want to keep capturing more and more data every day, making the data set bigger, hopefully, making the prediction more accurate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What other ways are there to do it? Can I push this data into, say, Wolfram Programming Cloud, or maybe something Google provides to get the desired results?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have time series data from mobile sensors for different motions such as walking, pushups, dumbellifts, rowing and so on. All these motions have different length of time series. For classifying them using &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_time_warping&quot;&gt;Dynamic Time Warping (DTW)&lt;/a&gt;, how do I choose an appropriate window size that will give good results?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking to choose my career in the area of decision science or predictive modeling and I am aware that this is kind of opinion based but I would like to have some suggestion from experts that I can use it to build my career in correct path. What are the tools should I know like R, SAS or any other. What are the thinks I should know to work in a data science or machine learning or predictive modeling. For me I am having problem in identifying steps that I should follow. Please suggest me some steps to follow.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Recently I was introduced to the field of Data Science (its been 6 months approx), and Ii started the journey with Machine Learning Course by Andrew Ng and post that started working on the Data Science Specialization by JHU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On practical application front, I have been working on building a predictive model that would predict attrition. So far I have used glm, bayesglm, rf in an effort to learn and apply these methods, but I find a lot of gap in my understanding of these algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My basic dilemma is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Whether I should focus more on learning the intricacies of a few algorithms or should I use the approach of knowing a lot of them as and when and as much as required?&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please guide me in the right direction, maybe by suggesting books or articles or anything that you think would help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would be grateful if you would reply with an idea of guiding someone who has just started his career in the field of Data Science, and wants to be a person who solves practical issues for the business world.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would read (as many as possible) resources (books,articles) suggested in this post and would provide a personal feed back on the pros and cons of the same so as to make this a helpful post for people who come across a similar question in future,and i think it would be great if people suggesting these books can do the same.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;One of the discussed nice aspects of the procedure that Vowpal Wabbit uses for updates to sgd &#xA;&lt;a href=&quot;http://lowrank.net/nikos/pubs/liw.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt; is so-called weight invariance, described in the linked as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Among these updates we mainly focus on a novel&#xA;set of updates that satisfies an additional invariance&#xA;property: for all importance weights of h, the update&#xA;is equivalent to two updates with importance weight&#xA;h/2. We call these updates importance invariant.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this mean and why is it useful?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am working on a text classification problem on tweets. At the moment I was only considering the content of the tweets as a source of information, and I was using a simple bag of words approach using term frequencies as features, using Random Forests (this is something I cannot change). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now my idea is to try to incorporate information present in the URLs used in tweets. Now, not all the tweets have URLs, and if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLs. For this reason, I suppose that having a single set of features containing both the tweet term frequencies and the URL term frequencies could be bad. Besides I'll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any suggestions regarding this issue? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm a Java developer and I want to pursue career in Data Science and machine learning. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please advise me where and how to begin? What subjects and mathematical/statistical skills are required and so on?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a problem and I'm having trouble representing it - first I thought I should use graph theory (nodes and edges) and now I'm not sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data is some tanks names and it's volumes, those tanks are connected by pipelines which I have the names and length.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;------(pipeline 1)------.-----(pipeline 2)------.----(pipeline 3)---&#xA;  |                     |    |                           |         |&#xA;[R tank 1]        [S tank 1] [S tank 2]            (pipeline 4) [S tank 3]&#xA;                                                         |&#xA;                                                     [S tank 4]&#xA;&#xA;R tank is sink (receiver) and S tank is source (sender)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Problem is the pipe names change doesn't occur where there is a tank - they change name because historical reasons, size or connections...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if I want to graphically show that S tank 2 is connected to pipeline 2 at point X and pipeline 2 connects to pipeline and the content goes to R tank 1, how should I do this? (I think the point X may not be relevant but if I had some way to get the distance travelled would be great).  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I couldn't quite think of how best to title this, so recommendations are welcome. Same goes for the tags (I don't have the reputation to use the tags that I thought were appropriate). The question is this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Suppose you have N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -&gt; y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD then PCA to reduce the data dimension for efficiency. But MATLAB and Octave cannot load such a large dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tools I can use to do SVD with such a large amount of data? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm going to start a Computer Science phd this year and for that I need a research topic. I am interested in Predictive Analytics in the context of Big Data. I am interested by the area of Education (MOOCs, Online courses...). In that field, what are the unexplored areas that can help me choose a strong topic? Thanks.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know if there is such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenization, POS tagging, parsing, training, testing .... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I am new to NLP and I need a tool which let me experiment, get familiar and progress&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Are there any general open-source programs or libraries (e.g., a Python library) for analyzing user search behavior?  By &quot;search behavior&quot;, I mean a user's interaction with a search engine, such as querying, clicking relevant results, and spending time on those results.  I'd like something with the following properties - it doesn't have to be all of them, but the more the merrier:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Models individual user behavior (aggregate and time-based)&lt;/li&gt;&#xA;&lt;li&gt;Models group user behavior&lt;/li&gt;&#xA;&lt;li&gt;Simulates individual user behavior, given a model&lt;/li&gt;&#xA;&lt;li&gt;Is easily extensible (to accept data input formats, user models, document models, etc., that end-users define)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Links are a plus!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Not sure if this is Math, Stats or Data Science, but I figured I would post it here to get the site used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a programmer, when you have a system/component implemented, you might want to allow some performance monitoring. For example to query how often a function call was used, how long it took and so on. So typically you care about count, means/percentile, max/min and similiar statistics. This could be measurements since startup, but also a rolling average or window.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if there is a good data structure which can be updated efficiently concurrently which can be used as the source for most of those queries. For example having a ringbuffer of rollup-metrics (count, sum, min, max) over increasing periods of time and a background aggregate process triggered regularly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The focus here (for me) is on in-memory data structures with limited memory consumption. (For other things I would use a RRD type of library).&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have post already the question few months ago about my project that I'm starting to work on. This post can be see here: &#xA;&lt;a href=&quot;https://datascience.stackexchange.com/questions/211/human-activity-recognition-using-smartphone-data-set-problem&quot;&gt;Human activity recognition using smartphone data set problem&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I know this is based around multivariate time series analysis and tasks are to classify and cluster the data. I have gathered some materials (e-books, tutorials etc.) on this but still can't see a more detailed picture of how even I should start. Here's the tutorial that looks like it might be helpful but the thing is my data looks differently and I'm not really sure if this can be applied to my work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So basically, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How I can start on some very basic analysis? How to read data so it any meaning for me.&#xA;Any tips and advises will be much appreciated!&#xA;Note: I'm just the beginner in data science.  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to implement item based collaborative filtering. Do any distance calculations allow for weighting of certain ranges of values within each vector? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I would like to be able to say values 10..22 within each vector are more significant than values within the range 0..10. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been experimenting with Pearson, Tanimoto and Euclidean algorithms, but they all seem to assume equal weighting for each value within the vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I approaching this problem in the right way, and if not, how do others deal with this problem? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want the text-based semantic clustering EMD do.&#xA;Is there a better way of using LDA to detect topics in text, there are so provide better results?&#xA;I'm going to do my EMD on discovery topics.&#xA;Thanks&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm interested in discovering some kind of dis-associations between the periods of a time series based on its data, e.g., find some (unknown number of) periods where the data is not similar with the data from another period.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also I would like to compare the same data but over 2 years (something like DTW?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I get my data Excel as a two-column list:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1=date (one per each day of the year), c2=Data To Analyze&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, what algorithms could I use and in what software?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update/Later edit:&lt;/strong&gt;&#xA;I'm looking for dates as cut-off points from which the DataToAnalyze could be part of another cluster of consecutive dates. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2014-1-1 --&amp;gt; 2014-3-10&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;are part of &lt;em&gt;Cluster_1&lt;/em&gt; based on DataToAnalyze. And:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2014-3-11 --&amp;gt; 2014-5-2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;are part of &lt;em&gt;Cluster_2&lt;/em&gt; based on DataToAnalyze, and so on. So, clusters of consecutive dates should be automatically determined based on some algorithms, which is what I'm looking for. Which ones (or which software) would be applicable to this problem?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to implement GD for standard task of NN training :) The best papers for practioneer I've founded so far are:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) &quot;Efficient BackProp&quot; by Yann LeCun et al.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) &quot;Stochastic Gradient Descent Tricks&quot; by Leon Bottou&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there some other must read papers on this topic?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am planning to use scikit linear support vector machine (SVM) classifier for text classification on a corpus consisting of 1 million labeled documents. What I am planning to do is, when a user enters some keyword, the classifier will first classify it in a category, and then a subsequent information retrieval query will happen in within the documents of that category catagory. I have a few questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How do I confirm that classification will not take much time? I don't want users to have to spend time waiting for a classification to finish in order to get better results.&lt;/li&gt;&#xA;&lt;li&gt;Is using Python's scikit library for websites/web applications suitable for this?&lt;/li&gt;&#xA;&lt;li&gt;Does anyone know how amazon or flipkart perform classification on user queries, or do they use a completely different logic?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a client that is managing several campaigns. However I'm not clear what percentage should be applied to each channel that bring traffic to my website, when assessing their participation in the objectives. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For those interested, here I leave the link to my profile on linkedin. &lt;a href=&quot;https://www.linkedin.com/pub/mario-mu%C3%B1oz-ahumada/52/287/28b&quot; rel=&quot;nofollow&quot;&gt;Specialist online markegin in Bogotá&lt;/a&gt;.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Below is the dataset where the response variable is play with two labels (yes, and no), &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;No. outlook temperature humidity    windy   play&#xA;1   sunny       hot     high        FALSE   no&#xA;2   sunny       hot     high        TRUE    no&#xA;3   overcast    hot     high        FALSE   yes&#xA;4   rainy       mild    high        FALSE   yes&#xA;5   rainy       cool    normal      FALSE   yes&#xA;6   rainy       cool    normal      TRUE    no&#xA;7   overcast    cool    normal      TRUE    yes&#xA;8   sunny       mild    high        FALSE   no&#xA;9   sunny       cool    normal      FALSE   yes&#xA;10  rainy       mild    normal      FALSE   yes&#xA;11  sunny       mild    normal      TRUE    yes&#xA;12  overcast    mild    high        TRUE    yes&#xA;13  overcast    hot     normal      FALSE   yes&#xA;14  rainy       mild    high        TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here are the decisions with their respective classifications: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1: (outlook,overcast) -&amp;gt; (play,yes) &#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]&#xA;&#xA;2: (humidity,normal), (windy,FALSE) -&amp;gt; (play,yes)&#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]&#xA;&#xA;3: (outlook,sunny), (humidity,high) -&amp;gt; (play,no) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]&#xA;&#xA;4: (outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 4]&#xA;&#xA;5: (outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 11]&#xA;&#xA;6: (outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Statement of problem: An ambulance is at the hospital dropping off a patient. The goal of the paramedic is to get released from the hospital as soon as possible. I am curious, what are the factors in how long an ambulance off loads a patient at the hospital? Can I predict how long an offload will take given certain variables. And how confident can I be in this model? The Dependent Variable is HospitalTime, it is a ratio type of data and is measured in seconds. The Independent Variables are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hospital, a nominal type of data recoded into integers, 1 would stand&#xA;for Lee Memorial.&lt;/li&gt;&#xA;&lt;li&gt;Ambulance, a nominal type of data recoded into integers, 9 would&#xA;stand for ambulance #9&lt;/li&gt;&#xA;&lt;li&gt;PatientPriority is an ordinal type of data recoded into integers. A 1&#xA;is a high priority, 2 is a medium priority and 3 is low acuity.&lt;/li&gt;&#xA;&lt;li&gt;MonthOfCall is an interval type of data recoded into integers. A 6&#xA;would be June and 12 is December. A 12 (December) is not twice as&#xA;much as a 6 (June) in this case.&lt;/li&gt;&#xA;&lt;li&gt;HourOfCall is an interval type of data recoded into integers. Once&#xA;again, an offload happening at 10:00 pm is not more than something&#xA;happening at 10:00 am.&lt;/li&gt;&#xA;&lt;li&gt;Officer1 and Officer2 are nominal data and are integers representing&#xA;an EMT and a paramedic.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My question is this: Given this type of data and my goal to predict the off loading time at the hospital, what kind of regression model should I look into?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked at my statistics books from university days and they are all using ratio data. My data is mixed with nominal, ordinal, interval and ratio.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have as much data as you could ask for. I have at least 100,000 observations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please push me in the right direction? What kind of model should I use with this type of data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Shown below are observations to give you a tiny peek at my data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;IncidentID,HospitalTime,Hospital,Ambulance,PatientPriority,MonthOfCall,HourOfCall,Officer1,Officer2&#xA;757620,1849,7,11,2,10,10,234,771,chr(10) 802611,2625,7,11,3,1,18,234,777,chr(10) &#xA;765597,1149,7,12,3,11,2,234,777,chr(10) 770926,1785,7,12,3,11,15,234,777,chr(10) &#xA;771689,3557,7,12,2,11,14,234,777,chr(10) 822758,1073,7,20,3,3,13,777,307,chr(10) &#xA;767249,2570,7,22,2,11,11,560,778,chr(10) 767326,1998,7,22,1,11,18,560,777,chr(10) &#xA;785903,1660,7,22,3,12,12,234,777,chr(10) 787644,2852,7,22,3,12,17,234,777,chr(10) &#xA;760294,1327,7,23,2,10,14,498,735,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) &#xA;994677,3653,7,32,2,2,15,181,159,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) &#xA;788471,2053,5,9,2,1,3,498,777,chr(10) 759983,1342,5,11,2,10,8,474,777,chr(10)&#xA;791243,1635,5,11,2,1,18,234,777,chr(10) 800796,1381,5,11,3,1,11,234,777,chr(10)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;P.S. This question is cross-posted in Stack-Overflow under the same title and author.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;The dataset that I am experimenting with is in the form of a table with columns userid and itemid. If there is a row for a given user and a given item, that means the user accessed the item (like in an online store). I am trying to cluster similar items based on this data. If a pair of items is accessed together often, then the items are similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because this is a case of a high dimensionality (# of users and items will be in 10,000's) I think I am justified in trying to use SVD as a pre-clustering step and then do some classical clustering. When I tried doing this I got poor clustering results when compared with simple hierarchical clustering. Items that weren't very similar were being bucketed together in one dimension, while there were available dimensions that weren't used. The results weren't completely random, but they were definitely worse than the output from the hierarchical clustering. I attempted the SVD step with Mahaut and Octave and the results were similar. For the hierarchical clustering I used the Jaccard measure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this point I am starting to doubt the notion of SVD as a way to reduce dimensionality. Do you think that SVD cannot be used effectively in this case (and why?) or do you think that I made some mistake along the way?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have several datasets with thousands of variables. This different datasets have different variables for the same thing. Is there a way to automatically/semi-automatically check compatible variables and make them consistent?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there is such thing, that would save me months of tedious work. The data is stored in SPSS format.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm coding a program that tests several classifiers over a database weather.arff, I found rules below, I want classify test objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand how the classification, it is described:&#xA;&quot;In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects. &quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to classify test objects?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;No. outlook temperature humidity    windy   play&#xA;1   sunny       hot     high        FALSE   no&#xA;2   sunny       hot     high        TRUE    no&#xA;3   overcast    hot     high        FALSE   yes&#xA;4   rainy       mild    high        FALSE   yes&#xA;5   rainy       cool    normal      FALSE   yes&#xA;6   rainy       cool    normal      TRUE    no&#xA;7   overcast    cool    normal      TRUE    yes&#xA;8   sunny       mild    high        FALSE   no&#xA;9   sunny       cool    normal      FALSE   yes&#xA;10  rainy       mild    normal      FALSE   yes&#xA;11  sunny       mild    normal      TRUE    yes&#xA;12  overcast    mild    high        TRUE    yes&#xA;13  overcast    hot     normal      FALSE   yes&#xA;14  rainy       mild    high        TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Rule found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1: (outlook,overcast) -&amp;gt; (play,yes) &#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]&#xA;&#xA;2: (humidity,normal), (windy,FALSE) -&amp;gt; (play,yes)&#xA;[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]&#xA;&#xA;3: (outlook,sunny), (humidity,high) -&amp;gt; (play,no) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]&#xA;&#xA;4: (outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) &#xA;[Support=0.21 , Confidence=1.00 , Correctly Classify= 4]&#xA;&#xA;5: (outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 11]&#xA;&#xA;6: (outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) &#xA;[Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Dung&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Many discussions of missing data in supervised (and unsupervised) learning deal with various methods of imputation, like mean values or EM. But in some cases the data will be missing as a necessary consequence of the data generation process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, let's say I'm trying to predict students' grades, and one of the inputs I want to analyze is the average grades of the student's siblings. If a particular student is an only child, then that value will be missing, not because we failed to collect the data, but because logically there is no data to collect. This is distinct from cases where the student has siblings, but we can't find their grades. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other examples abound: say we're in college admissions and we want to include students' AP exam results, but not all students took AP exams. Or we're looking at social network data, but not all subjects have facebook and/or twitter accounts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These data are missing, but they're certainly not missing at random. And many algorithms, such as all supervised learning packages in scikit-learn, simply demand that there be no missing values at all in the data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How have people dealt with this in the past, and what off-the-shelf solutions are there? For instance, I believe the gradient boosting algorithm in R uses trees with three possible branches: left, right, and missing. Any other alternatives out there?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've made a Naive Bayes classifier that uses the bag-of-words technique to classify spam posts on a message board. It works, but I think I could get much better results if my models considered the word orderings and phrases. (ex: 'girls' and 'live' may not trigger a high spam score, even though 'live girls' is most likely junk). How can I build a model that takes word ordering into account?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've considered storing n-grams (check-out-these, out-these-live, these-live-girls), but this seems to radically increase the size of the dictionary I keep score in and causes inconsistency as phrases with very similar wording but different order will slip through.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not tied to Bayesian classification, but I'd like something that someone without a strong background in statistics could grok and implement. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I would like to do some data mining and NLP experiments to do some research&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have decided to use NLTK or related tools and software&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which environment or operating system do you suggest for my purpose? I mean doing research on NLP&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Windows or Linux? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to Linux&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is your experience and your preferred OS?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As NLTK is in Python I thought Python is a good language for my purpose, do you suggest Python too?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Next week I'm going to begin prototyping a recommendation engine for work. I've implemented/completed the Netflix Challenge in Java before (for college) but have no real idea what to use for a production/enterprise level recommendation engine. Taking into consideration everything from a standalone programming language to things like Apache Mahout and Neo4j, does anyone have any advice on how to proceed?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Okay, here is the background:&#xA;I am doing text mining, and my basic flow is like this:&#xA;extract feature (n-gram), reduce feature count, score (tf-idf) and classify. for my own sake i am doing comparison between SVM and neural network classifiers. here is the weird part (or am i wrong and this is reasonable?), if i use 2gram the classifiers' result (accuracy/precision) is different and the SVM is the better one; but when i use 3-gram the results are exactly the same. what causes this? is there any explanation? is it the case of very separable classes?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking for a supervised learning algorithm that can take 2d data for input and output. As an example of something similar to my data, consider a black image with some sparse white dots. Blur that image using a full range of grayscale. Then create a machine that can take the blurred image as input and produce the original sharp image as output. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could make some sample 1D data by taking a region/radius around the original sharp point, but I don't know the exact radius. It would be significant data duplication and a lot of guessing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any good algorithm suggestions for this problem? Thanks for your time.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a hands on researcher and I like testing out viable solutions, so I tend to run a lot of experiments. For example, if I am calculating a similarity score between documents, I might want to try out many measures. In fact, for each measure I might need to make several runs to test the effect of some parameters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I've been tracking the runs inputs and their results by writing out the results into  files with as much info about the inputs. The problem is that retrieving a specific result becomes a challenge sometimes, even if I try to add the input info to th filename. I tried using a spreadsheet with links to results but this isn't making a huge difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tools/process do you use for the book keeping of your experiments?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a 35 year old IT professional who is purely technical. I am good at programming, learning new technologies, understanding them and implementing. I did not like mathematics at school, so I didn't score well in mathematics. I am very much interested in pursuing a career in Big Data analytics. I am more interested in Analytics rather than Big Data technologies (Hadoop etc.), though I do not dislike it.  However, when I look around in the internet, I see that, people who are good in analytics (Data Scientists) are mainly Mathematics graduates  who have done their PHds and sound like intelligent creatures, who are far far ahead of  me. I get scared sometimes to think whether my decision is correct, because learning advance statistics on your own is very tough and requires a of hard work and time investment. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know whether my decision is correct, or should I leave this piece of work to only intellectuals who have spend their life in studying in prestigious colleges and earned their degrees and PHDs.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm curious if anyone else has run into this. I have a data set with about 350k samples, each with 4k sparse features. The sparse fill rate is about 0.5%. The data is stored in a &lt;code&gt;scipy.sparse.csr.csr_matrix&lt;/code&gt; object, with &lt;code&gt;dtype='numpy.float64'&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using this as an input to sklearn's Logistic Regression classifier. The &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt; indicates that sparse CSR matrices are acceptable inputs to this classifier. However, when I train the classifier, I get extremely bad memory performance; the memory usage of my process explodes from ~150 MB to fill all the available memory and then everything grinds to a halt as memory swapping to disk takes over.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know why this classifier might expand the sparse matrix to a dense matrix? I'm using the default parameters for the classifier at the moment, within an updated anacoda distribution. Thanks!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scipy.__version__ = '0.14.0'&#xA;sklearn.__version__ = '0.15.2'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have product purchase count data which looks likes this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;user item1 item2&#xA;   a     2     4&#xA;   b     1     3&#xA;   c     5     6&#xA;   ...   ...   ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These data are imported into &lt;code&gt;python&lt;/code&gt; using &lt;code&gt;numpy.genfromtxt&lt;/code&gt;. Now I want to process it to get the correlation between &lt;code&gt;item1&lt;/code&gt; purchase amount and &lt;code&gt;item2&lt;/code&gt; purchase amount -- basically for each value &lt;code&gt;x&lt;/code&gt; of &lt;code&gt;item1&lt;/code&gt; I want to find all the users who bought &lt;code&gt;item1&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt; quantity then average the &lt;code&gt;item2&lt;/code&gt; over the same users. What is the best way to do this? I can do this by using &lt;code&gt;for&lt;/code&gt; loops but I thought there might be something more efficient than that. Thanks!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to figure out a good (and fast) solution to the following problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two models I'm working with, let's call them players and teams. A player can be on multiple teams and a team can have multiple players). I'm working on creating a UI element on a form that allows a user to select multiple teams (checkboxes). As the user is selecting (or deselecting) teams, I'd like to display the teams grouped by the players.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for examples:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;If the selected teams have no players that intersect, each team would have its own section. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If the user selects two teams and they have the same players, there would be one section containing the names of the two teams and all the players.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If TEAM_A has players [1, 2, 4, 5] and TEAM_B has players [1, 3, 5, 6]. There would be the following sections: SECTION_X = [TEAM_A, TEAM_B, 1, 5], SECTION_Y = [TEAM_A, 2, 3], SECTION _Z = [TEAM_B, 3, 5]&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I hope that's clear. Essentially, I want to find the teams that players have in common and group by that. I was thinking maybe there is a way to do this by navigating a bipartite graph? Not exactly sure how though and I might be overthinking it. I was hoping to do this by creating some type of data structure on the server and using it on the client. I would love to hear your suggestions and I appreciate any help you can give!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Lets say I have a database of users who rate different products on a scale of 1-5. Our recommendation engine recommends products to users based on the preferences of other users who are highly similar. My first approach to finding similar users was to use Cosine Similarity, and just treat user ratings as vector components. The main problem with this approach is that it just measures vector angles and doesn't take rating scale or magnitude into consideration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My question is this:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there any drawbacks to&lt;/strong&gt; &lt;strong&gt;just using the percentage difference between the vector components of two vectors as a measure of similarity&lt;/strong&gt;? What disadvantages, if any, would I encounter if I used that method, instead of Cosine Similarity or Euclidean Distance?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For Example, why not just do this:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n = 5 stars&#xA;a = (1,4,4)&#xA;b = (2,3,4)&#xA;&#xA;similarity(a,b) = 1 - ( (|1-2|/5) + (|4-3|/5) + (|4-4|/5) ) / 3 = .86667&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Instead of Cosine Similarity :&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a = (1,4,4)&#xA;b = (2,3,4)&#xA;&#xA;CosSimilarity(a,b) = &#xA;(1*2)+(4*3)+(4*4) / sqrt( (1^2)+(4^2)+(4^2) ) * sqrt( (2^2)+(3^2)+(4^2) ) = .9697&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;how to get the Polysemes of a word in wordnet or any other api. I am looking for any api. with java any idea is appreciated?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I know there is the normal &lt;em&gt;subtract the mean and divide by the standard deviation&lt;/em&gt; for standardizing your data, but I'm interested to know if there are more appropriate methods for this kind of discrete data. Consider the following case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 5 items that have been ranked by customers. First 2 items were ranked on a 1-10 scale. Others are 1-100 and 1-5. To transform everything to a 1 to 10 scale, is there another method better suited for this case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the data has a central tendency, then the standard would work fine, but what about when you have more of a halo effect, or some more exponential distribution?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I do movement building work for Effective Altruism (&lt;a href=&quot;http://en.m.wikipedia.org/wiki/Effective_altruism&quot; rel=&quot;nofollow&quot;&gt;http://en.m.wikipedia.org/wiki/Effective_altruism&lt;/a&gt;), and would like to level up our growth strategy. It occurred to me that a social network visualization tool which allowed us to strategically find and recruit new influencers/donors would be mega useful. I'd love to find something (preferably free), similar to InMaps, which would allow us to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Combine all of our social media connections into a single map&lt;/li&gt;&#xA;&lt;li&gt;Easily see who the superconnectors are&lt;/li&gt;&#xA;&lt;li&gt;Weight each person by their degree of social influence (perhaps some function of things like Klout score * amount of social media connections * number of Google mentions, etc) &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Does such a thing exist? If not, is anyone interested in pro bono work for an amazing cause? =)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I am a data science noob, so preferably the solution would be one with a nice GUI and minimal involvement of R or Python.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Suppose, for example, that the first search result on a page of Google search results is swapped with the second result. How much would this change the click-through probabilities of the two results? How much would its click-through probability drop if the fifth search result was swapped with the sixth? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can we say something, with some level of assurance, about how expected click-through probabilities change if we do these types of pairwise swaps within pages of search results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What we seek is a measure of the contribution to click-through rates made specifically by position bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Likely, how position ranking would affect the sales in Amazon or other online shopping website? If we cast the sales into two parts, the product quality and its ranking effect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;sales = alpha*quality + beta*position + epsilon&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we quantify the beta?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;When ML algorithms, e.g. Vowpal Wabbit or some of the factorization machines winning click through rate competitions (&lt;a href=&quot;https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution/55862#post55862&quot;&gt;Kaggle&lt;/a&gt;), mention that features are 'hashed', what does that actually mean for the model? Lets say there is a variable that represents the ID of an internet add, which takes on values such as '236BG231'. Then I understand that this feature is hashed to a random integer. But, my question is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is the integer now used in the model, as an integer (numeric) OR&lt;/li&gt;&#xA;&lt;li&gt;is the hashed value actually still treated like a categorical variable and one-hot-encoded? Thus the hashing trick is just to save space somehow with large data?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;let's assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or &quot;epoch&quot;, I use each training sample exactly once after randomly reordering the whole training set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody know any vectorized implementation of stochastic gradient descent? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: I've been asked why would I like to use online gradient descent if the size of my dataset is fixed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From [1], one can see that online gradient descent converges slower than batch gradient descent to the minimum of the empirical cost. However, it converges faster to the minimum of the expected cost, which measures generalization performance. I'd like to test the impact of these theoretical results in my particular problem, by means of cross validation. Without a vectorized implementation, my online gradient descent code is much slower than the batch gradient descent one. That remarkably increases the time it takes to the cross validation process to be completed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: I include here the pseudocode of my on-line gradient descent implementation, as requested by ffriend. I am solving a regression problem.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Method: on-line gradient descent (regression)&#xA;Input: X (nxp matrix; each line contains a training sample, represented as a length-p vector), Y (length-n vector; output of the training samples)&#xA;Output: A (length-p+1 vector of coefficients)&#xA;&#xA;Initialize coefficients (assign value 0 to all coefficients)&#xA;Calculate outputs F&#xA;prev_error = inf&#xA;error = sum((F-Y)^2)/n&#xA;it = 0&#xA;while abs(error - prev_error)&amp;gt;ERROR_THRESHOLD and it&amp;lt;=MAX_ITERATIONS:&#xA;    Randomly shuffle training samples&#xA;    for each training sample i:&#xA;        Compute error for training sample i&#xA;        Update coefficients based on the error above&#xA;    prev_error = error&#xA;    Calculate outputs F&#xA;    error = sum((F-Y)^2)/n&#xA;    it = it + 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;[1] &quot;Large Scale Online Learning&quot;, L. Bottou, Y. Le Cunn, NIPS 2003.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I hope you can help me, as I have some questions on this topic. I'm new in the field of deep learning, and while I did some tutorials, I can't relate or distinguish concepts from one another.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I need some help with a single layered perceptron with multiple classes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need to do is classify a dataset with three different classes, by now I just learnt how to do it with two classes, so I have no really a good clue how to do it with three.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The dataset have three different classes: Iris-setosa, Iris-versicolor and Iris-versicolor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The url with the dataset and the information is in : &lt;a href=&quot;http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data&quot; rel=&quot;nofollow&quot;&gt;http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I really appreciate any help anyone can give to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have some very complicated data about some movie sales online, first for each data entry, I have a key which is a combination of five keys, which are territory, day, etc, and then, for each key I have the sales for a period of time, and other information, like the movie's box office and genre. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each day, there is a delay for the data loading to the database, around ten hours, I try to fill the gap, do some data extrapolations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each movie we sell, there is some decay of selling since the new release of the movie, i.e. usually for each movie, it follows some sales decay pattern.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a recent day, I pulled some data, and I found that some decay pattern:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/H0lSJ.png&quot; alt=&quot;decay curve 1&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/L5DAf.png&quot; alt=&quot;decay curve 2&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/RJPjK.png&quot; alt=&quot;decay curve 3&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And for that day, the sales for each key can range from around $150000 to $0. The pic is as follow:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/o7tsq.png&quot; alt=&quot;one day sales curve&quot;&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the picture, the 15000 means there are around 15000 keys for each day. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;found this article,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&amp;amp;Pyo.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&amp;amp;Pyo.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to predict for each key, the sales amount, like for a movie, territory, day etc combination, the sales amount, how much dollars, means for that movie, that territory, that day, how much money we get from selling online. I tried ARIMA time series model, but there is some concerns for that model, seen from the pics, there is some seasonal thing, and decay thing for the movie, so the sales prediction can not be always flat, there may be a pump after a going down, it may happens on a weekend, since there is seasonal thing, and the decay trend, etc, how to capture these things. Thank you for your reply!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not sure whether can be applied, and how to be applied here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot in advance. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;First of all I know the question may be not suitable for the website but I'd really appreciate it if you just gave me some pointers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm a 16 years old programmer, I've had experience with many different programming languages, a while ago I started a course at Coursera, titled introduction to machine learning and since that moment i got very motivated to learn about AI, I started reading about neural networks and I made a working perceptron using Java and it was really fun but when i started to do something a little more challenging (building a digit recognition software), I found out that I have to learn a lot of math, I love math but the schools here don't teach us much, now I happen to know someone who is a math teacher do you think learning math (specifically calculus) is necessary for me to learn AI or should I wait until I learn those stuff at school?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also what other things would be helpful in the path of me learning AI and machine learning? do other techniques (like SVM) also require strong math?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry if my question is long, I'd really appreciate if you could share with me any experience you have had with learning AI.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I was wondering if someone could point me to suitable database formats for building up a user database:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;basically I am collecting logs of impressions data, and I want to compile a user database &lt;/p&gt;&#xA;&#xA;&lt;p&gt;which sites user visits, country/gender/..? and other categorisations with the aim of &#xA;a) doing searches: give me all users visiting games sites from france...&#xA;b) machine learning: eg clustering users by the sites they visit&lt;/p&gt;&#xA;&#xA;&lt;p&gt;so I am interested in storing info about 100's of millions of users&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with indexes? on user, sites, geo-location&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what are suitable database systems. Can someone suggest suitable reading material? &#xA;I was imagining Hbase might be suitable...&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am new to Natural Language Processing, I think NLP is a challenging field, the syntax and semantic ambiguities could cause a lot of problems. For example I think for these problems machine translation is a hard task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore there are probably many approaches and methods that have been applied to this field. But what are the latest and most promising approaches and methods in the field of NLP?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are these techniques highly dependent on the target language?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking for an online console for the language R. Like I write the code and the server should execute and provide me with the output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similar to the website Datacamp.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a variable whose value I would like to predict, and I would like to use only one variable as predictor. For instance, predict traffic density based on weather.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, I thought about using &lt;a href=&quot;http://en.wikipedia.org/wiki/Self-organizing_map&quot;&gt;Self-Organizing Maps&lt;/a&gt; (SOM), which performs unsupervised clustering + regression. However, since it has an important component of dimensionality reduction, I see it as more appropriated for a large number of variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this &lt;em&gt;simple&lt;/em&gt; case: I used &quot;Data Mining&quot; instead of &quot;machine learning&quot; in the title of my question, because I think maybe a linear regression could do the job...&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&quot; rel=&quot;nofollow noreferrer&quot;&gt;Automated Time Series Forecasting for Biosurveillance&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;in the above paper, page 4, two models, non-adaptive regression model, adaptive regression model, the non-adaptive regression model's parameter estimation method is &quot;least squares&quot;, what is the parameter estimation for the adaptive regression model? is there any package in R to do parameter estimation for this kind of adaptive regression model? If I add more predictors in the adaptive regression model, can R still solve it? and how?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How would I do parameter estimation and prediction for the adaptive regression model using R, as in the 4th page of the paper linked below?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&quot; rel=&quot;nofollow&quot;&gt;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anyone clarify this for me?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you know adaptive regression models very well, share some useful link, or describe the model/parameter estimation/prediction, that would be very helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you so much!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I want to identifies different queries in sentences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like - &lt;code&gt;Who is Bill Gates and where he was born?&lt;/code&gt; or &lt;code&gt;Who is Bill Gates, where he was born?&lt;/code&gt; contains two queries &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Who is Bill Gates?&lt;/li&gt;&#xA;&lt;li&gt;Where Bill Gates was born&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I worked on Coreference resolution, so I can identify that &lt;code&gt;he&lt;/code&gt; points to &lt;code&gt;Bill Gates&lt;/code&gt; so resolved sentence is &quot;Who is Bill Gates, where Bill Gates was born&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like wise&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MGandhi is good guys, Where he was born?&#xA;single query&#xA;who is MGandhi and where was he born?&#xA;2 queries&#xA;who is MGandhi, where he was born and died?&#xA;3 quries&#xA;India won world cup against Australia, when?&#xA;1 query (when India won WC against Auz)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can perform Coreference resolution (Identifying and converting &lt;code&gt;he&lt;/code&gt; to &lt;code&gt;Gandhi&lt;/code&gt;) but not getting how can I distinguish queries in it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to do this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to find &quot;Sentence disambiguation&quot; like &quot;word sense disambiguation&quot;, but nothing exist like that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help or suggestion would be much appreciable. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Recently in a data analytic job interview for an e-commerce site, they asked me, do i have some knowledge of buyer classification problem. Unfortunately i heard this term for the first time.&lt;br&gt;&#xA;After interview i tried to search a lot about it over google but didn't find something meaningful. Please any one let me know if you have heard this term before and paste some links explaining this concept. Thanks   &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am kind of a newbie on machine learning and I would like to ask some questions based on a problem I have .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say I have x y z as variable and I have values of these variables as time progresses like :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;t0  = x0 y0 z0  &lt;br&gt;&#xA;t1  = x1 y1 z1  &lt;br&gt;&#xA;tn  = xn yn zn  &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want a model that when it's given 3 values of x , y , z I want a prediction of them like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input : x_test y_test z_test &#xA;Output : x_prediction y_prediction z_prediction&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These values are float numbers. What is the best model for this kind of problem? &#xA;Thanks in advance for all the answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More details:&#xA;Ok so let me give some more details about the problems so as to be more specific.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have run certain benchmarks and taken values of performance counters from the cores of a system per interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The performance counters are the x , y , z in the above example.They are dependent to each other.Simple example is x = IPC , y  =  Cache misses , z  = Energy at Core.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I got this dataset of all these performance counters per interval .What I want to do is create a model that after learning from the training dataset , it will be given a certain state of the core ( the performance counters) and predict the performance counters that the core will have in the next interval.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to setup a big data infrastructure using Hadoop, Hive, Elastic Search (amongst others), and I would like to run some algorithms over certain datasets. I would like the algorithms themselves to be scalable, so this excludes using tools such as Weka, R, or even RHadoop. The &lt;a href=&quot;https://mahout.apache.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;Apache Mahout Library&lt;/a&gt; seems to be a good option, and it features &lt;a href=&quot;https://mahout.apache.org/users/basics/algorithms.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;algorithms for regression and clustering tasks&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I am struggling to find is a solution for anomaly or outlier detection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since Mahout features Hidden Markov Models and a variety of clustering techniques (including K-Means) I was wondering if it would be possible to build a model to detect outliers in time-series, using any of this. I would be grateful if somebody experienced on this could advice me&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;if it is possible, and in case it is&lt;/li&gt;&#xA;&lt;li&gt;how-to do it, plus&lt;/li&gt;&#xA;&lt;li&gt;an estimation of the effort involved and&lt;/li&gt;&#xA;&lt;li&gt;accuracy/problems of this approach.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm looking for a topic for my masters thesis. Machine learning is my primary domain and I want to work on probabilistic models and applied probability in Machine Learning. Please suggest some exciting new topics that would make for a good masters thesis subject.&#xA;Anything related to Markov chains Monte Carlo, Bayesian methods, Probabilistic graphical models, Markov models and so on in context of machine learning would be great!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've built a toy Random Forest model in &lt;code&gt;R&lt;/code&gt; (using the &lt;code&gt;German Credit&lt;/code&gt; dataset from the &lt;code&gt;caret&lt;/code&gt; package), exported it in &lt;code&gt;PMML 4.0&lt;/code&gt; and deployed onto Hadoop, using the &lt;code&gt;Cascading Pattern&lt;/code&gt; library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've run into an issue where &lt;code&gt;Cascading Pattern&lt;/code&gt; scores the same data differently (in a binary classification problem) than the same model in &lt;code&gt;R&lt;/code&gt;. Out of 200 observations, 2 are scored differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this? Could it be due to a difference in the implementation of Random Forests? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to analyze the effectiveness and efficiency of kernel methods for which I would require 3 different data-set in 2 dimensional space for each of the following cases:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;BAD_kmeans: The data set for which the kmeans clustering algorithm&#xA;will not perform well.&lt;/li&gt;&#xA;&lt;li&gt;BAD_pca: The data set for which the Principal Component Analysis&#xA;(PCA) dimension reduction method upon projection of the original&#xA;points into 1-dimensional space (i.e., the first eigenvector) will&#xA;not perform well.&lt;/li&gt;&#xA;&lt;li&gt;BAD_svm: The data set for which the linear Support Vector Machine&#xA;(SVM) supervised classification method using two classes of points&#xA;(positive and negative)  will not perform well.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Which packages can I use in R to generate the random 2d data-set for each of the above cases ? A sample script in R would help in understanding&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I would like to learn both Python and R for usage in data science projects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently unemployed, fresh out of university, scouting around for jobs and thought it would be good if I get some Kaggle projects under my profile.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have very little knowledge in either language. Have used Matlab and C/C++ in the past. But I haven't produced production quality code or developed an application or software in either language. It has been dirty coding for academic usage all along.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have used a little bit of Python, in a university project, but I dont know the fundamentals like what is a package , etc etc. ie havent read the intricacies of the language using a standard Python Textbook etc..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have done some amount of coding in C/C++ way back (3-4 years back then switched over to Matlab/Octave).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to get started in Python Numpy Scipy scikit-learn and pandas etc. but just reading up Wikipedia articles or Python textbooks is going to be infeasible for me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And same goes with R, except that I have zero knowledge of R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any suggestions?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is there a good java library for doing time series energy consumption forecasting based on weather data and other variables?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Most Treebank conversion which I found in the web are from constituency treebank to dependency treebank, I wonder why there is little jobs in the opposite direction?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am looking for a thesis to complete my master M2, I will work on a topic in the big data's field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some topics or project that would make for a good masters thesis subject.&lt;br&gt;&#xA;I add that I have bases in data warehouses, databases, data mining, good skills in programming, system administration and cryptography ... &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Where is the difference between one-class, binary-class and multinominal-class classification?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Couldn't I just use all the methods that I mentioned above to reach my goal?&#xA;e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C1 -&gt; 1 or 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C2 -&gt; 1 or 0&#xA;... and so on ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But where is the difference between this two methods? (except of that I have to use different algorithms)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;A Random Forest (RF) is created by an ensemble of Decision Trees's (DT). By using bagging, each DT is trained in a different data subset. Hence, &lt;strong&gt;is there any way of implementing an on-line random forest by adding more decision tress on new data?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, we have 10K samples and train 10 DT's. Then we get 1K samples, and instead of training again the full RF, we add a new DT. The prediction is done now by the Bayesian average of 10+1 DT's.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition, if we keep all the previous data, the new DT's can be trained mainly in the new data, where the probability of picking a sample is weighted depending how many times have been already picked.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Where is the difference between one-class, binary-class and multinominal-class classification?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Couldn't I just use all the methods that I mentioned above to reach my goal?&#xA;e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C1 -&gt; 1 or 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is unlabeled-data C2 -&gt; 1 or 0&#xA;... and so on ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But where is the difference between this two methods? (except of that I have to use different algorithms)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am drawing samples from two classes in the two-dimensional Cartesian space, each of which has the same covariance matrix $[2, 0; 0, 2]$.  One class has a mean of $[1.5, 1]$ and the other has a mean of $[1, 1.5]$.  If the priors are $4/7$ for the former and $3/7$ for the later, how would I derive the equation for the ideal decision boundary?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it turns out that misclassifying the second class is twice as expensive as the first class, and the objective is to minimize the expected cost, what equation would I use for the best decision boundary?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Are there any good sources that explain how decision trees can be implemented in a scalable way on a distributed computing system.  Where in a given source is this explained?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Can anyone explain how field-aware factorization machines (FFM) compare to standard Factorization Machines (FM)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Standard:&#xA;&lt;a href=&quot;http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&quot;&gt;http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Field Aware&quot;:&#xA;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf&quot;&gt;http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf&lt;/a&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;There're many data points, each of which is associated with two coordinates and a numeral value, or three coordinates. And I wish it is coloured.&#xA;I checked packages &quot;scatterplot3d&quot; and &quot;plot3D&quot; but I couldn't find one like the example I give. It is like it has a fitting surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data is basically like the following. In this way I think this kind of plot is gonna be perfectly suitble for this data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ki,kt,Top10AverageF1Score&#xA;    360,41,0.09371256716549396&#xA;    324,41,0.09539634212851525&#xA;    360,123,0.09473510831594467&#xA;    36,164,0.09773486852645874&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I also may have one more additional variable, which makes it like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    NeighborhoodSize,ki,kt,Top10AverageF1Score&#xA;    10,360,41,0.09371256716549396&#xA;    15,324,41,0.09539634212851525&#xA;    15,360,123,0.09473510831594467&#xA;    20,36,164,0.09773486852645874&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Do you also have any good idea for visualizing the second case? What kind of plot and which packages and functions, etc.&lt;/strong&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/5whM7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want to analyze &lt;a href=&quot;http://grouplens.org/datasets/movielens/&quot; rel=&quot;nofollow&quot;&gt;MovieLens data set&lt;/a&gt; and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according &lt;code&gt;'userId'&lt;/code&gt; and &lt;code&gt;'Time'&lt;/code&gt; columns. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [36]: df.head(10)&#xA;Out[36]: &#xA;        userId  movieId  Rating       Time                         movieName  \\\\&#xA;40034        1      150       5  978301777                  Apollo 13 (1995)   &#xA;77615        1     1028       5  978301777               Mary Poppins (1964)   &#xA;550485       1     2018       4  978301777                      Bambi (1942)   &#xA;400889       1     1962       4  978301753         Driving Miss Daisy (1989)   &#xA;787274       1     1035       5  978301753        Sound of Music, The (1965)   &#xA;128308       1      938       4  978301752                       Gigi (1958)   &#xA;497972       1     3105       5  978301713                 Awakenings (1990)   &#xA;28417        1     2028       5  978301619        Saving Private Ryan (1998)   &#xA;6551         1     1961       5  978301590                   Rain Man (1988)   &#xA;35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)   &#xA;&#xA;                            genre  &#xA;40034                       Drama  &#xA;77615   Children's|Comedy|Musical  &#xA;550485       Animation|Children's  &#xA;400889                      Drama  &#xA;787274                    Musical  &#xA;128308                    Musical  &#xA;497972                      Drama  &#xA;28417            Action|Drama|War  &#xA;6551                        Drama  &#xA;35492        Action|Crime|Romance  &#xA;&#xA;[10 rows x 6 columns]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can not understand that the same user with user Id 1 see or rated the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id:2018) exactly at the same time (up to the milleseconds). If somebody works already with this data set, please, clear this situation.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am not sure whether I formulated the question correctly. Basically, what I want to do is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's suppose I have a list of 1000 strings which look like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;cvzxcvzx&lt;strong&gt;string&lt;/strong&gt;cvzcxvz&lt;/p&gt;&#xA;&#xA;&lt;p&gt;otortorotr&lt;strong&gt;string&lt;/strong&gt;grptprt&lt;/p&gt;&#xA;&#xA;&lt;p&gt;vmvmvmeop&lt;strong&gt;string2&lt;/strong&gt;vmrprp&lt;/p&gt;&#xA;&#xA;&lt;p&gt;vccermpqp&lt;strong&gt;string2&lt;/strong&gt;rowerm&lt;/p&gt;&#xA;&#xA;&lt;p&gt;proororor&lt;strong&gt;string3&lt;/strong&gt;potrprt&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mprto2435&lt;strong&gt;string3&lt;/strong&gt;famerpaer&lt;/p&gt;&#xA;&#xA;&lt;p&gt;etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to extract these reoccuring strings that occur on the list. What solution should I use? Does anyone know about algorithm that could do this?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I need some serious help. I am supposed to implement a project (Non-Existing as of now) for my Machine Learning course. I have no basics in AI or Data mining or Machine learning. I have been searching for a while and unable to find something that i can finish implementing in 3-4 weeks time. It carries a huge chunk of my final marks and no matter how much i try i am unable to understand how it works!&#xA;Can the machine learning masters please help me out with this. I need a project suggestion to start with. And i want to know how to proceed after gathering the data set. I am totally blank and running out of time for my graduation :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Appreciate your suggestions! Thanks in advance.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a question about memory usage. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to do 4 things:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1) make a dataframe from one of several columns from a datasource, say a json string&#xA;2) make the third column of the original dataset the index to the dataframe&#xA;3) change the name of another column&#xA;4) change the series i've created to a dataframe&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question is about memory efficiency. It seems that for step 1), I am first loading a whole dataframe, then run a concat command to concatenate the columns I want. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For step 2, I again need to resave the new dataframe as another object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For step 3, it seems to stick so nothing there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please advise on a more efficient way to go about this, if that exists.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   df = pd.DataFrame(jsonobject)&#xA;   df = df.set_index(&quot;columnC&quot;)&#xA;   df.index.names= [&quot;foo&quot;]&#xA;   df1 = df[&quot;foo&quot;].map(lambda x:x[&quot;id&quot;])&#xA;   df2 = pd.DataFrame(df1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to change this to a model of daily exception reporting and weekly trend reporting. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Features might include: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;User report consolidation (so there's only one daily email)&lt;/li&gt;&#xA;&lt;li&gt;report ordering based upon level of variance from past performance (see the most important stuff first)&lt;/li&gt;&#xA;&lt;li&gt;HTML email support (with my audience, pretty counts)&lt;/li&gt;&#xA;&lt;li&gt;Web interface to allow preference changes, including LDAP support (make administration easier)&lt;/li&gt;&#xA;&lt;li&gt;Unsubscribe feature at the report level&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here's what I'd like to know:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What are the practical problems I might run into?&lt;/li&gt;&#xA;&lt;li&gt;What is the best way to display the new reports?&lt;/li&gt;&#xA;&lt;li&gt;How should  I define an &quot;exception&quot;? How can I know if my definition is a good one?&lt;/li&gt;&#xA;&lt;li&gt;I assume I'd be using a mix of Python, SQL, and powershell. Anything else I should consider, e.g. R? What are some good resources?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have Train and Test data, how to calculate classification accuracy with confusion matrix ? Thanks&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@attribute outlook {sunny, overcast, rainy}&#xA;@attribute temperature {hot, mild, cool}&#xA;@attribute humidity {high, normal}&#xA;@attribute windy {TRUE, FALSE}&#xA;@attribute play {yes, no}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Train:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1   sunny       hot     high    FALSE   no&#xA;2   sunny       hot     high    TRUE    no&#xA;3   overcast    hot     high    FALSE   yes&#xA;4   rainy       mild    high    FALSE   yes&#xA;5   rainy       cool    normal  FALSE   yes&#xA;6   rainy       cool    normal  TRUE    no&#xA;7   sunny       cool    normal  FALSE   yes&#xA;8   rainy       mild    normal  FALSE   yes&#xA;9   sunny       mild    normal  TRUE    yes&#xA;10  overcast    mild    high    TRUE    yes&#xA;11  overcast    hot     normal  FALSE   yes&#xA;12  rainy       mild    high    TRUE    no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Test:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;overcast    cool    normal  TRUE    yes&#xA;sunny       mild    high    FALSE   no&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Rules found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(humidity,normal), (windy,FALSE) -&amp;gt; (play,yes) [Support=0.33 , Confidence=1.00 , Correctly Classify= 4, 8, 9, 12]&#xA;(outlook,overcast) -&amp;gt; (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 2, 11]&#xA;(outlook,rainy), (windy,FALSE) -&amp;gt; (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 3]&#xA;(outlook,sunny), (temperature,hot) -&amp;gt; (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 0, 1]&#xA;(outlook,sunny), (humidity,normal) -&amp;gt; (play,yes) [Support=0.17 , Confidence=1.00 , Correctly Classify= 10]&#xA;(outlook,rainy), (windy,TRUE) -&amp;gt; (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 5, 13]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;For my Computational Intelligence class, I'm working on classifying short text. One of the papers that I've found makes a lot of use of &lt;em&gt;granular computing&lt;/em&gt;, but I'm struggling to find a decent explanation of what exactly it is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I can gather from the paper, it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I'm asking about rough sets as well, because I'm curious about them and how they relate to fuzzy sets. If at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: &lt;a href=&quot;http://ijcai.org/papers11/Papers/IJCAI11-298.pdf&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the paper I'm referencing.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have data collected from a computer simulation of football games which seem to have recurring patterns of the following form.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;if madrid plays arsernal and the match ends under 3 goal, then on their next match against each others, madrid will win. if madrid happens to loose and then plays against chelsea next, they will win 90% of the time.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;how do I find such inferences from simulation generated data like this. There are other forms of hidden patterns that I believe exists in the dataset.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I cant seem to figure out why I have a high percentage error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to get a perceptron between X1 and X2 which are Gaussian distributed data sets with distinct means and identical co-variances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;N=200;&#xA;C= [2 1; 1 2]; %Covariance&#xA;m1=[0 2];&#xA;m2=[1.5 0];%mean&#xA;X1 = mvnrnd(m1, C, N/2);&#xA;X2 = mvnrnd(m2, C, N/2);&#xA;&#xA;X = [X1; X2];&#xA;X = [X ones(N,1)]; %bias&#xA;y = [-1*ones(N/2,1); ones(N/2,1)]; %classification&#xA;&#xA;%Split data into training and test &#xA;ii = randperm(N);&#xA;Xtr = X(ii(1:N/2),:);&#xA;ytr = X(ii(1:N/2),:);&#xA;Xts = X(ii(N/2+1:N),:);&#xA;yts = y(ii(N/2+1:N),:);&#xA;Nts = N/2;&#xA;&#xA;w = randn(3,1);&#xA;eta = 0.001;&#xA;%learn from training set&#xA;for iter=1:500 &#xA;j = ceil(rand*N/2);&#xA;if( ytr(j)*Xtr(j,:)*w &amp;lt; 0)&#xA;w = w + eta*Xtr(j,:)'; &#xA;end&#xA;end&#xA;&#xA;%apply what you have learnt to test set&#xA;yhts = Xts * w;&#xA;disp([yts yhts])&#xA;PercentageError = 100*sum(yts .*yhts &amp;lt; 0)/Nts;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong and how can I address this challenge?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Sorry, if this topic is not connected directly to Data Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to understand how the &lt;a href=&quot;http://graphlab.com/learn/gallery/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Graphlab tool&lt;/a&gt; works. Firstly I want to execute the toy examples from the Gallery site. When I try to execute the example code, everything is OK except one command: I can not see the graphlab plot after &lt;code&gt;show()&lt;/code&gt;. The command &lt;code&gt;show()&lt;/code&gt; returns to me some kind of object in IPython and nothing in the IPython Notebook.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the example code has the plot, which depends directly on the matplotlib module, I can produce the real plots and save it on my machine. Consequently, I suppose the main error depends on the graphlab (or object from its class). &#xA;If somebody already used this tool and rendered the plot, can he/she tell me, how I can execute the plots command?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [8]: import graphlab&#xA;&#xA;In [9]: from IPython.display import display&#xA;&#xA;        from IPython.display import Image&#xA;&#xA;        graphlab.canvas.set_target('ipynb')&#xA;&#xA;In [10]:import urllib&#xA;&#xA;        url = 'https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv'&#xA;&#xA;        urllib.urlretrieve(url, filename='freebase_performances.csv')  # downloads an 8MB file to the working directory&#xA;&#xA;Out[10]: ('freebase_performances.csv', &amp;lt;httplib.HTTPMessage instance at 0x7f44e153cf38&amp;gt;)&#xA;&#xA;In [11]: data = graphlab.SFrame.read_csv('remote://freebase_performances.csv', column_type_hints={'year': int})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;...&#xA;...&#xA;...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [15]:data.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;No plot after this line&#xA;...&#xA;...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;In [19]:print data.show()&#xA;&#xA;&amp;lt;IPython.core.display.Javascript object at 0x7f44e14c0850&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The object of graphlab (?) after print command&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Our system allows an admin to manage a database of university courses. These courses have multiple fields, like the department, a title, and a description.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am adding the ability to add learning objectives to a course. To simplify the problem, let's say that learning objectives are just tags. Courses can have more than one learning objective associated with them. So a course like CHEM 101 might have &quot;chemistry&quot;, &quot;technology&quot;, &quot;science&quot;, and several others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming I can reduce a course to a set of features, (using keywords/stemming/nlp, I suppose?), what kind of problem is this and what algorithm would you suggest? It seems very similar to a classification problem, but I want to provide a sorted list of suggestions with the most relevant at the top.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm training a NN with 8 features and 8000 training examples with a single output (0, 1) using the scipy.optimise CG algorithm and the results are somewhat inconsistent. The goal is to get the NN to be as 'precise' as possible (recall doesn't really matter too much) so I've set the threshold for y value quite high (0.75). Most of the time it gets a precision of around 80%, however sometimes it fails (using exactly the same parameters, lambda etc..) to generate any outputs which are above the 0.75 threshold, meaning the precision equals 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've successfully trained NNs before without these unusual results (albeit the goal was a somewhat more conventional multi-class classifier with many more features).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering if the training NNs with fewer features increases the chances of it getting stuck at a local optima; or getting stuck at local optima has a more significant impact on NNs with fewer features?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any thoughts on what's going on!?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the common/best practices to handle time data for machine learning application?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if in data set there is a column with timestamp of event, such as &quot;2014-05-05&quot;, how you can extract useful features from this column if any?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;The most online tutorials like to use a simple example to introduce to machine learning by classify unknown text in spam or not spam. They say that this is a binary-class problem. But why is this a binary-class problem? I think it is a one-class problem! I do only need positive samples of my inbox to learn what is not spam. If I do take a bunch of not spam textes as positiv samples and a bunch of spam-mails as negativ samples, then of course it's possible to train a binary-classifier and make predictions from unlabeled data, but where is the difference to the onc-class-approach? There I would just define a training-set of all non spam examples and train some one-class classifier. What do you think?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm doing some data analysis in a Statistical Pattern Recognition course using PRML. We analyzed a lot of matrix properties, like eigenvalues, column independence, positive semi-definite matrix, etc. When we are doing, for example, linear regression, we need to calculate some of those properties, and fit them into the equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is, my question is about the intuition behind these matrix properties, and their implications in the ML/DM literature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone could answer, can you teach me what is the importance of eigenvalue, positive semi-definite matrix, and column independence for ML/DM. And possibly, other important matrix properties you think important in study the dataset, and why.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be really appreciated if someone can answer this question.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm examining the activity of customers over the years which have about one event per year. This results is many short time-series for which I found the distributions (hit/miss over 4 years sorted by probability in the data):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0000 : 0.31834&#xA;0001 : 0.17582&#xA;0010 : 0.13605&#xA;0100 : 0.13554&#xA;1000 : 0.12886&#xA;0011 : 0.01717&#xA;1100 : 0.01650&#xA;0110 : 0.01578&#xA;0101 : 0.01220&#xA;1010 : 0.01117&#xA;1001 : 0.00883&#xA;0111 : 0.00571&#xA;1110 : 0.00565&#xA;1111 : 0.00496&#xA;1101 : 0.00384&#xA;1011 : 0.00351&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Apparently a purely uncorrelated binomial model wouldn't do, but one can observe that if both, the number of 1's and 11's coincide, then the probabilities are approximately equal (apart from a small recency effect of 0001).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you see a way to approach such data to deduce a probabilistic model? Basically where I have only a few probability parameters which roughly explain this distribution?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm planing to write a classification program that is able to classify unknown text in around 10 different categories and if none of them fits it would be nice to know that. It is also possible that more then one category is right.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My predefined categories are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1 = &quot;politics&quot;&#xA;c2 = &quot;biology&quot;&#xA;c3 = &quot;food&quot;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about the right approach in how to represent my training-data or what kind of classification is the right one. The first challenge is about finding the right features. If I only have text (250 words each) what method would you recommend to find the right features? My first approach is to remove all stop-words and use the POS-Tagger (&lt;a href=&quot;http://nlp.stanford.edu/software/tagger.shtml&quot; rel=&quot;nofollow&quot;&gt;Stanford NLP POS-Tagger&lt;/a&gt;) to find nouns, adjective etc. I count them an use all frequently appeared words as features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;e.g. politics, I've around 2.000 text-entities. With the mentioned POS-Tagger I found:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;law:           841&#xA;capitalism:    412&#xA;president:     397&#xA;democracy:     1007&#xA;executive:     112&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Would it be right to use only that as features? The trainings-set would then look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Training set for politics:&#xA;feature law         numeric&#xA;feature capitalism  numeric&#xA;feature president   numeric&#xA;feature democracy   numeric&#xA;feature executive   numeric&#xA;class politics,all_others&#xA;&#xA;sample data:&#xA;politics,5,7,1,9,3&#xA;politics,14,4,6,7,9&#xA;politics,9,9,9,4,2,1&#xA;politics,5,8,0,7,6&#xA;...&#xA;all_others,0,2,4,1,0&#xA;all_others,0,0,1,1,1&#xA;all_others,7,4,0,0,0&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Would this be a right approach for binary-classification? Or how would I define my sets? Or is multi-class classification the right approach? Then it would look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Training set for politics:&#xA;feature law         numeric&#xA;feature capitalism  numeric&#xA;feature president   numeric&#xA;feature democracy   numeric&#xA;feature executive   numeric&#xA;feature genetics    numeric&#xA;feature muscle      numeric&#xA;feature blood       numeric&#xA;feature burger      numeric&#xA;feature salad       numeric&#xA;feature cooking     numeric &#xA;class politics,biology,food&#xA;&#xA;sample data:&#xA;politics,5,7,1,9,3,0,0,2,1,0,1&#xA;politics,14,4,6,7,9,0,0,0,0,0,1&#xA;politics,9,9,9,4,2,1,1,1,1,0,3&#xA;politics,5,8,0,7,6,2,2,0,1,0,1&#xA;...&#xA;biology,0,2,4,1,0,4,19,5,0,2,2&#xA;biology,0,0,1,1,1,12,9,9,2,1,1&#xA;biology,7,4,0,0,0,10,10,3,0,0,7&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What would you say?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a newbie to data science with a typical problem. I have a data set with metric1, metric2 and metric3. All these metrics are interdependent on each other. I want to detect anomalies in metric3. Currently, I am using Nupic from numenta.org for my analysis and it doesn't seem to be effective. Is there any ML library which can detect anomalies in multiple parameters?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I tried to use OMP algorithm available in scikit-learn. My net datasize which includes both target signal and dictionary ~ 1G. However when I ran the code, it exited with mem-error.&#xA;The machine has 16G RAM, so I don't think this should have happened. I tried with some logging where the error came and  found that the data got loaded completely into numpy arrays. And it was the algorithm itself that caused the error. Can someone help me with this&#xA;or sugggest more memory efficient algorithm for feature selection, or is subsampling the &#xA;data my only option. Are there some deterministic good subsampling techniques.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;Relevant code piece:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n=8;&#xA;y=mydata[:,0];&#xA;X=mydata[:,[1,2,3,4,5,6,7,8]];&#xA;#print y;&#xA;#print X;&#xA;print &quot;here&quot;;&#xA;omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5,copy_X = False, normalize=True);&#xA;omp.fit(X,y);&#xA;coef = omp.coef_;&#xA;print omp.coef_;&#xA;idx_r, = coef.nonzero();&#xA;for id in idx_r:&#xA;        print coef[id], vars[id],&quot;\\\\n&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;/usr/local/lib/python2.7/dist-packages/sklearn/base.py&quot;, line 324, in score&#xA;return r2_score(y, self.predict(X), sample_weight=sample_weight)&#xA;File &quot;/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.py&quot;, line 2332, in r2_score&#xA;numerator = (weight * (y_true - y_pred) ** 2).sum(dtype=np.float64)&#xA;MemoryError&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;In most data acquisition settings it is useful to tag your data with time and location. If I write the data to csv file, what are the best formats that I can use for this two variables if I want to create a heatmap on Google Maps? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a continuous variable, sampled over a period of a year at irregular intervals. Some days have more than one observation per hour, while other periods have nothing for days. This makes it particularly difficult to detect patterns in the time series, because some months (for instance October) are highly sampled, while others are not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/7MEXt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is what would be the best approach to model this time series?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I believe most time series analysis techniques (like ARMA) need a fixed frequency. I could aggregate the data, in order to have a constant sample or choose a sub-set of the data that is very detailed. With both options I would be missing some information from the original dataset, that could unveil distinct patterns.&lt;/li&gt;&#xA;&lt;li&gt;Instead of decomposing the series in cycles, I could feed the model&#xA;with the entire dataset and expect it to pick up the patterns. For&#xA;instance, I transformed the hour, weekday and month in categorical&#xA;variables and tried a multiple regression with good results (R2=0.71)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I have the idea that machine learning techniques such as ANN can also pick these patterns from uneven time series, but I was wondering if anybody has tried that, and could provide me some advice about the best way of representing time patterns in a Neural network.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'd like to apply some of the more complex supervised machine learning techniques in python - deep learning, generalized addative models, proper implementation of regularization, other cool stuff I dont even know about, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any recommendations how I could find expert ML folks that would like to collaborate on projects?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I had a conversation with someone recently and mentioned my interest in data analysis and who I intended to learn the necessary skills and tools. They suggested to me that while it is great to learn the tools and build the skills there is little point in doing so unless i have specialized  knowledge in a specific field. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;They basically summed it to that I'd just be like a builder with a pile of tools who could build a few wooden boxes and may be build better things (cabins, cupboards etc), but without knowledge in a specific field I'd never be a builder people would come to for a specific product.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone found this or have any input on what to make of this ? It would seem if it was true one would have to learn the data science aspects of things and then learn  a new field just to  become specialized.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;h1&gt;My Background&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I am a graduate student in Civil Engineering. For the analyses of road traffic data (vehicle trajectories as time series) I work with big data sets mostly about a million data points or more.&lt;br&gt;&#xA;I started using R language when MS Excel could not open the big data files. Using basic statistics knowledge and R code I developed few algorithms to identify certain patterns in the data which worked for many applications. But I still lack serious programming skills in R.&lt;br&gt;&#xA;Now, I am familiar with basic inferential statistics and R packages (plyr, dplyr, ggplot2, etc). Recently I came to know that Machine Learning algorithms also help in defining patterns in the data through supervised/ unsupervised learning and their application might improve the accuracy of prediction of certain 'behaviors' of drivers using the traffic data.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Question&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Having the basic knowledge of Statistics and R, I want to learn about the data science/ machine learning as a beginner. I know that some concepts in Stats. and ML overlap and that might bridge the gap in my learning of ML. Keeping my background in mind, what resources (books/ online courses) would you recommend me to start learning data science and apply it in my field?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I would like to ask your opinion on how to choose a similarity measure. I have a set of vectors of length N, each element of which can contain either 0 or 1. The vectors are actually ordered sequences, so the position of each element is important. Suppose I have three vectors of length 10, x_1 x2, x3: x1 has three 1 at positions 6,7,8 (indexes start from 1. Both x2 and x3 have an additional 1, but x2 has it in position 9 while x3 has it in position 1. I am looking for a metric according to which x1 is more similar to x2 than to x3, in that the additional 1 is closer to the &quot;bulk&quot; of ones. I guess this is a relatively common problem, but I am confused on the best way to approach it.&#xA;Many thanks in advance!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm just starting to work on a relatively large dataset after ML course in Coursera.&#xA;Trying to work on &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&quot; rel=&quot;nofollow&quot;&gt;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&lt;/a&gt;.&#xA;Got an accuracy of 5.2 in training and test set with linear regression using gradient descent in octave.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried adding all possible quadratic features (515345 instances and 4275 features), but the code just won't stop executing in my HP Pavilion g6 2320tx, with 4GB RAM in Ubuntu 14.04.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this beyond the data size capacity of Octave ?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a data set that is pivoted in to the following format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[key] [id] [0] [1] [5] [10] [15] [60] [120] [180],.. [365]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So key could be&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Products] [1000] [15,000] [4000]... etc&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where products is the category of item being reviewed and key is the identifier for the product; the only fields (0, 1,... 180,.. [365]) are individual daily samples identify how many of &quot;x&quot; product were logged as either sold, in-stock etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need to do is perform some kind of analysis on an entire slew of products and their inventory levels. i.e. each import of data I need to make sure the incoming data is accurate or predictably accurate and that some human did not typo a stock level. The problem is, using a simple average or rolling average can introduce significant variance and smoothing out the average renders my analysis less reliable. Ideally this analysis would trigger an alarm that someone would have to investigate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better and more accurate way of performing this analysis?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;The meaning of multi-class classification rules&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Example: I have two classification rules (Refund is a predictor and Cheat is a binary response):&#xA;(Refund, No) → (Cheat, No) Support = 0.4, Confidence = 0.57&#xA;(Refund, No) → (Cheat, Yes) Support = 0.3, Confidence = 0.43&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;=&gt; multi-class classification rules:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(Refund, No) → (Cheat, No) v (Cheat, Yes)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;By &quot;large&quot;, I mean in the range of 100m to 10b rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently using both Hadoop MapReduce and Amazon RedShift. MapReduce has been a little disappointing here. Redshift works very well if the data is distributed well for the given query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there other technologies that I should be looking at here? If so, what are the trade offs?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was going through an IEEE Research paper which has used Fuzzy ARTMAP for predicting the price of electricity given some highly correlated data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per my basic understanding about Fuzzy ARTMAP it is a classification algorithm, so how will it be able to predict continuous data?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The text from research paper is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In the architecture of the FA network, the preprocessing stages take&#xA;  the input vector and contribute to produce complement coding, which&#xA;  avoids category proliferation, i.e., the creation of a relatively&#xA;  large number of categories to represent the training data. A sequence&#xA;  of input vectors (price and demand) and their respective target&#xA;  vectors are introduced to the FA network in order to classify the&#xA;  input pattern correctly. The classiﬁed input patterns are then grouped&#xA;  into labels using membership functions.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I was using MATLAB to implement the same, so is there a library in MATLAB to approach towards the solution. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I need to collect several large datasets (thousands of samples, dozens of features) for regression with only categorical inputs. I already look for such datasets in the UCI repository, but I did not find any suitable one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody know of any such dataset, or of any additional dataset repository on the Internet?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;n this paper (&lt;a href=&quot;http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;)  they suppose a &quot;unified architecture for NLP&quot; with deep neural networks with multitask learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is to understand the layer architecture in figure 1, see below:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/RoU0P.png&quot; alt=&quot;unified deep learning architecture for NLP&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is someone able to give me a concrete, reproducible example how this architecture processing 3 sentences through their layers?&lt;/li&gt;&#xA;&lt;li&gt;What are the outputs after each layer? &lt;/li&gt;&#xA;&lt;li&gt;Why they choose which layer?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Thans in advance!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29&quot; rel=&quot;nofollow&quot;&gt;database&lt;/a&gt; of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to formulate a supervised classifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My present approach is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[date] [CDR/Position] [#calls] [#sms] [#internetConnections]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My purpose is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I'd like to find the reason they are similar/dissimilar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I apply?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a biosemi bdf of EEG data which contains 32 channel. &#xA;I've opened it using biosig, everything works great, a first list is channel and inside each list there are eeg data.&#xA;But if I open it using MNE it the first list is eeg data, and the second list (inside the list of eeg data) are two list of eeg data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;this is how I open the data using MNE&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;raw_file=read_raw_edf(&quot;E:\\\\eegDATA\\\\\\\\256\\\\s02_reduced.bdf&quot;,preload=True,verbose=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Am I missing something here?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to create a logistic regression model in jpmml, then write the PMML to a file. The problem I'm having, is that I can't find any way to create a custom tag, such as &quot;shortForm&quot; and &quot;longForm&quot; in the following example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;MapValues outputColumn=&quot;longForm&quot;&amp;gt;&#xA;  &amp;lt;FieldColumnPair field=&quot;gender&quot; column=&quot;shortForm&quot;/&amp;gt;&#xA;  &amp;lt;InlineTable&amp;gt;&#xA;    &amp;lt;row&amp;gt;&amp;lt;shortForm&amp;gt;m&amp;lt;/shortForm&amp;gt;&amp;lt;longForm&amp;gt;male&amp;lt;/longForm&amp;gt;&#xA;    &amp;lt;/row&amp;gt;&#xA;    &amp;lt;row&amp;gt;&amp;lt;shortForm&amp;gt;f&amp;lt;/shortForm&amp;gt;&amp;lt;longForm&amp;gt;female&amp;lt;/longForm&amp;gt;&#xA;    &amp;lt;/row&amp;gt;&#xA;  &amp;lt;/InlineTable&amp;gt;&#xA;&amp;lt;/MapValues&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's what I have so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MapValues mv = new MapValues(&quot;output&quot;)&#xA;  .withFieldColumnPairs(&#xA;        new FieldColumnPair( new FieldName(&quot;gender&quot;), &quot;shortForm&quot; )&#xA;  ).withInlineTable(&#xA;        new InlineTable().withRows(&#xA;                new Row().with???( new ??? )&#xA;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In short, I am asking for an API call I can use to instantiate the &quot;shortForm&quot; element in the example, and attach it to the &quot;row&quot; object. I've been all through the API, examples, and Google/SO, and can't find a thing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;We have a ruby-on-rails platform (w/ postgreSQL db) for people to upload various products to trade. Of course, many of these products listed are the same, while they are described differently by the consumer (either through spelling, case etc.) &quot;lots of duplicates&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the purposes of analytics and a better UX, we're aiming to create an evolving &quot;master product list&quot;, or &quot;whitelist&quot;, if you will, that will have users select from an existing list of products they are uploading, OR request to add a new one. We also plan to enrich each product entry with additional information from the web, that would be tied to the &quot;master product&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some methods we're proposing to solve this problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A) Take all the &quot;items&quot; listed in the website (~90,000), de-dupe as much as possible by running select &quot;distinct&quot; queries (while maintaining a key-map back to original data by generating an array of item keys from each distinct listing in a group-by.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;THEN&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A1) Running this data through mechanical turk, and asking each turk user to list data in a uniform format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OR&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A2) Running each product entry through the Amazon products API and asking the user to identify a match.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A3) A better method?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two questions regarding such a dataset: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What algorithms can we use to ensure that the model not only predicts the fraudulent transactions accurately, but also predicts the value of fraud associated with these.  &lt;/li&gt;&#xA;&lt;li&gt;Assuming that we can quantify the cost involved in each false positive (tagging a non-fraudulent transaction as fraudulent) and cost incurred due to a false negative (tagging a fraudulent transaction as non-fraudulent), how can we optimize the model to maximize savings (or minimize losses)?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;We're currently using Redshift as our data warehouse, which we're very happy with. However, we now have a requirement to do machine learning against the data in our warehouse. Given the volume of data involved, ideally I'd want to run the computation in the same location as the data rather than shifting the data around, but this doesn't seem possible with Redshift. I've looked at MADlib, but this is not an option as Redshift does not support UDFs (which MADlib requires). I'm currently looking at shifting the data over to EMR and processing it with the Apache Spark machine learning library (or maybe H20, or Mahout, or whatever). So my questions are: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;is there a better way?&lt;/li&gt;&#xA;&lt;li&gt;if not, how should I make the data accessible to Spark? The options I've identified so far include: use Sqoop to load it into HDFS, use DBInputFormat, do a Redshift export to S3 and have Spark grab it from there. What are the pros/cons for these different approaches (and any others) when using Spark?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that this is off-line batch learning, but we'd like to be able to do this as quickly as possible so that we can iterate experiments quickly.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a data set that keeps track of who referred someone to a program, and includes the geo coordinates of both parties for each record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What will be the best way to visualize this kind of data set? This visualization should also be able to use the geo coordinates to place this entities in the map to form clusters, or to superimpose them on a real map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested in an algorithm and/or a library that will help me do this. Library should be preferably written in Java, Python, Scala, or NodeJS. The record count can be as big as a thousand or hundreds of thousands.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to use Latent Dirichlet Allocation for a project and I am using Python with the gensim library. After finding the topics I would like to cluster the documents using an algorithm such as k-means(Ideally I would like to use a good one for overlapping clusters so any recommendation is welcomed). I managed to get the topics but they are in the form of:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;0.041*Minister + 0.041*Key + 0.041*moments + 0.041*controversial + 0.041*Prime&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to apply a clustering algorithm, and correct me if I'm wrong, I believe I should find a way to represent each word as a number using either tfidf or word2vec.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any ideas of how I could &quot;strip&quot; the textual information from e.g. a list, in order to do so and then place them back in order to make the appropriate multiplication?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance the way I see it if the word Minister has a tfidf weight of 0.042 and so on for any other word within the same topic I should be to compute something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;0.041*0.42 + ... + 0.041*tfidf(Prime) and get a result that will be later on used in order to cluster the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your time.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have measurements of 4 devices at two different points of time. A measurement basically consists of an array of ones and zeros corresponding to a bit value at the corresponding location:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;whos measurement1_dev1_time1&#xA;&#xA;Name                         Size               Bytes  Class      Attributes&#xA;&#xA;measurement1_dev1_time1      4096x8             32768  logical&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I assume that for a specific device the changes between time 1 and 2 of the measurements are unique. However, since I am dealing with 32768 bits at different locations, it is quite hard to visualize if there is some kind of dependency. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As every bit at location &lt;code&gt;x&lt;/code&gt;can be regarded as one dimension of an observation I thought to use PCA to reduce the number of dimensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, for every of the 5 devices:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I randomly sample &lt;code&gt;n&lt;/code&gt; measurements at point &lt;code&gt;t1&lt;/code&gt;and &lt;code&gt;t2&lt;/code&gt; seperatly&lt;/li&gt;&#xA;&lt;li&gt;I prepare an array as input for &lt;code&gt;pca()&lt;/code&gt; with &lt;code&gt;m&lt;/code&gt;*n columns (&lt;code&gt;m&lt;/code&gt;&amp;lt; 32768; its a subset of all the observed bits, as the original data might be too big for pca) and 4 rows (one row for each device).&lt;/li&gt;&#xA;&lt;li&gt;On this array &lt;code&gt;A&lt;/code&gt; I calculate the pca: ``[coeff score latent] = pca(zscore(A))```&lt;/li&gt;&#xA;&lt;li&gt;Then I try to visualize it using &lt;code&gt;biplot&lt;/code&gt;: &lt;code&gt;biplot(coeff(:,1:2), 'score', score(:,1:2))&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;However, this gives me really strange results. Maybe PCA is not the right approach for this problem? I also modified the input data to do the PCA not on the logical bit array itself. Instead, I created a vector, which holds the indices where there is a '1' in the original measurement array. Also this produces strange results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I am completely new to PCA I want to ask you if you either see a flaw in the process or if PCA is just not the right approach for my goal and I better look for other dimension reduction approaches or clustering algorithms.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I did small survey and get such data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-------------| Yes | No | Dont_Know |  &#xA;|-------------|     |    |           |  &#xA;| Employee    | 60  | 5  | 5         |  &#xA;| Workers     | 17  | 0  | 1         |  &#xA;| Businessmen | 71  | 5  | 10        |  &#xA;| Jobless     | 4   | 30 | 0         |  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;R code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dt &amp;lt;- data.frame(workers = c(&quot;Employee&quot;,&#xA;                             &quot;Workers&quot;, &#xA;                             &quot;Businessmen&quot;, &#xA;                             &quot;Jobless&quot;), &#xA;                 yes = c(60,17,71,4), &#xA;                 no = c(5,0,5,30), &#xA;                 dont_know = c(5,1,10,0)&#xA;                )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What kind of test I must do, if I want to show, that the Jobless people are often choosing &lt;strong&gt;No&lt;/strong&gt; answer?   &lt;/li&gt;&#xA;&lt;li&gt;Is the difference between Jobless and Businessmen answers significant? &lt;/li&gt;&#xA;&lt;li&gt;And what is about other groups?  &lt;/li&gt;&#xA;&lt;li&gt;What another information I can get from such data or what kind questions I can ask from such data?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have two classes (A,B) that I want to classify using a SVM. Say that I have a class C and a function f. Can I do this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;A' =  f(A,C) = |A-C|&#xA;B' =  f(B,C) = |B-C|&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and then perform the classification on A' and B' instead? In the context of my problem A and B are classes where elements are vectors. The f function measures the Mahalanobis distance of each vector with respect to the distribution imposed by C.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am dealing with a lot of categorical data right now and I would like to use an appropriate data mining method in any tool [preferably R] to find the effect of each parameter [categorical parameters] over my target variable. To give a brief notion about the data that am dealing with, my target variable denotes the product type [say, disposables and non-disposables] and I have parameters like root cause,symptom,customer name, product name etc. As my target can be considered as a binary value, I tried to find the combination of values leading to the desired categories using Apriori but, I have more than 2 categories in that attribute and I want to use all of them and find the effect of the mentioned parameters over each category. I really wanted to try SVM and use hyperplanes to separate the content and get n-dimensional view. But, I do not have enough knowledge to validate the technique, functions am using to do the analysis. Currently I have like 9000 records and each of them represents a complaint from the user. There are lot of columns available in the dataset which is what I am trying to use to determine the target variable [ myForumla &amp;lt;- Target~. ] I tried with just 4 categorical columns too. Not getting a proper result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can just the categorical variables be used to develop a SVM model and get visualization with n hyper planes? Is there any appropriate data mining technique available for dealing with just the categorical data?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I think this is something that experienced programmers do all the time. But, given my limited programming experience, please bear with me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an excel file which has particular cell entries that read &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;[[{&quot;from&quot;: &quot;4&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;8&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;9&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;3&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}], [{&quot;from&quot;: &quot;14&quot;, &quot;response&quot;: false, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;15&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;17&quot;, &quot;response&quot;: false, &quot;value&quot;: 20}, {&quot;from&quot;: &quot;13&quot;, &quot;response&quot;: true, &quot;value&quot;: 20}]]&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, for each such entry I want to take the information in each of the curly brackets and make a row of data out of it. Each such row would have 3 columns. For example, the row formed from the first entry within curly brackets should have the entries &quot;4&quot; &quot;true&quot; and &quot;20&quot; respectively. The part I posted should give me 6 such rows, and for n such repetitions I should end up with a matrix of 6n rows, and 4 columns ( an identifier, plus the 3 columns mentioned).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be most efficient way to do this? By &quot;doing this&quot; I mean learning the trick, and then implementing it. I have access to quite a few software packages(Excel, Stata, Matlab, R) in my laboratory, so that should not be an issue.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am new to D3 programming (any programming, for that matter). I have protein-protein interaction data in JSON format and csv format. I would like to use that data for network visualization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data attributes: Protein Name, Protein Group, Protein type, Protein Source Node, Protein Target Node&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone suggest good network visualizations for such data. How does it work with hive plots?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I was wondering if anyone was aware of any methods for visualizing an SVM model where there are more than three continuous explanatory variables. In my particular situation, my response variable is binomial, with 6 continuous explanatory variables (predictors), one categorical explanatory variable (predictor). I have already reduced the number of predictors and I am primarily using R for my analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I am unaware if such a task is possible/ worth pursuing.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your time.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;this is my first ever stack exchange question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to build a tool right now and one of the features of the tool is the ability to break down a product or service into it's associated attributes/properties/classes/keywords/entities. (Choose which word best suits, as I have no idea).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example if we had a Camera as the product. I would like to be able to generate a breakdown of everything that is associated to a camera. Such as;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Digital, Film, Optical, LCD, Glass, CCD, CMOS, RGB, Lens, Shutter, Negative, Polaroid, Darkroom, Flash, Resolution, Stabilisation, Batteries, Zoom, Angle, Telephoto, Macro, Filters, Memory, CF, SD&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The list could go on for quite some time, those were jsut a few off the top of my head.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How on earth could I go about retrieving such attributes automatically? Is there a database out there that has such info? Are there any special tricks anyone has up their sleeve to be able to accumulate datasets such as the example above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Very interested in your answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks :)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The questionnaire for the data is &lt;a href=&quot;http://www.cc.gatech.edu/gvu/user_surveys/survey-1997-10/questions/general.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The clean data is available &lt;a href=&quot;http://wikisend.com/download/586046/DataRaw.arff&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&#xA;NB: The Column CompuPlat has missing values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;part of dataset&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;CMFam CMHobb  CMNone  CMOther CMPol   CMProf  CMRel&#xA;0   0   1   0   0   0   0&#xA;0   0   0   0   0   0   0&#xA;1   1   0   0   0   1   0&#xA;0   0   0   1   0   0   0&#xA;0   0   0   0   1   1   0&#xA;1   0   0   0   0   1   1&#xA;Community Membership_Family&#xA;Community Membership_Hobbies&#xA;Community Membership_None&#xA;Community Membership_Other&#xA;Community Membership_Political&#xA;Community Membership_Professional&#xA;Community Membership_Religious&#xA;Community Membership_Support&#xA;&lt;/code&gt;&#xA;I want to club all of them in a variable CM&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Data sample contains a single feature: random integer number from 1 to 4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possble to change &lt;code&gt;1,2,3,4&lt;/code&gt; representation on the filter card to some custom names, say: &lt;code&gt;Type1,Type2,Type3,Type4&lt;/code&gt;? (not changing data set)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4ZPYM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or starting point on how to do this using Neuroph or maybe in other framework/approaches?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit for more info as suggested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have very little experience on neural networks though I have some introduction with it in college. However I am very much familar with java&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data is around 3 megabytes only and consists of rules and relationships for a single domain. This means that the data is complex but relatively limited though still free-form English language.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have 1-4 gram text data from wikipedia for 14 categories, which I am using for NE classification.&#xA;I feed named entity from sentence to lucene indexer which searches named entity from these 14 categories. &#xA;Issue I am facing is, for single entity I get multiple classes as a result with same score.&#xA;like while search &lt;code&gt;titanic&lt;/code&gt;, indexer gives this result&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Book&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Movie&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Score    - 11.23&#xA;Title    - titanic&#xA;Category - Product&lt;/p&gt;&#xA;&#xA;&lt;p&gt;now problem is which class to be considered?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already tried with classifiers (NB,ME in nltk,scikit learn), but as it consider each entity from dataset as feature, it works as indexer only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why lucene?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Tz8Uy.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am running SVM algorithm in R.It is taking long time to run the algorithm.I have system with 32GB RAM.How can I use that whole RAM memory to speed my process.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm looking for an (ideally free) API that would have time series avg/median housing prices by zip code or city/state. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quandl almost fits the bill, but it returns inconsistent results across different zip codes and the data is not as up to date as I'd like (it's mid November, and the last month is August).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also looked at Zillow, but storing their data is against TOS, and at 1,000 calls daily--it would take forever to pull in the necessary data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions (even if they aren't free) would be much appreciated!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a list user data: user name, age, sex, address, location etc., and &lt;/p&gt;&#xA;&#xA;&lt;p&gt;a set of product data: Product name, Cost, description etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I would like to build a recommendation engine that will be able to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1 Figure out similar products&lt;/p&gt;&#xA;&#xA;&lt;p&gt;eg :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;name  :   category   :  cost    :   ingredients&lt;/p&gt;&#xA;&#xA;&lt;p&gt;x     :     x1   :        15  :       xx1, xx2, xx3&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y     :    y1   :        14   :     yy1, yy2, yy3&lt;/p&gt;&#xA;&#xA;&lt;p&gt;z     :    x1  :          12   :     xx1, xy1 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;here x and z are similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2 Recommend relevant products from the product list to a user&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I implement this using mahout?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I do at the moment some data experiments with the &lt;a href=&quot;http://graphlab.com/products/create/docs/&quot; rel=&quot;nofollow&quot;&gt;Graphlab toolkit&lt;/a&gt;. I have at the first next SFrame, with the three columns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Users Items Rating&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The pair in the same row from every &lt;code&gt;Users&lt;/code&gt; and &lt;code&gt;Items&lt;/code&gt; values build the unique key and the &lt;code&gt;Rating&lt;/code&gt; is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)&lt;/li&gt;&#xA;&lt;li&gt;Take the logarithm by every rating value&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Afterward I create a recommender model and evaluate the basic metrics for it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dataset is comming from some music site, the users rated some tracks. I have approximately 100 000 users and 300 000 tracks. Total number of ratings is over 3 millions (actually the matrix is sparse). This is the most simple data set, which I analyze now. In the future I can (and will) use some additional information about the users and tracks (f.e. duration, year, genre, band etc). At the moment I just interest to collect some methods for rating normalisation without to use additional information (users &amp;amp; items features). My problem is, the data set doesn't have any &lt;code&gt;Rating&lt;/code&gt; at the first. I create someself the column &lt;code&gt;Rating&lt;/code&gt;, based on the number of events for unique &lt;code&gt;User-Item&lt;/code&gt; pair (I have this information). You can of course understand that some users can hear some tracks many times, and another users only one time. Consequently the dispersion is very high and I want to reduce it (normalise the ratings value).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working through the &lt;a href=&quot;https://www.coursera.org/course/nlp&quot; rel=&quot;nofollow&quot;&gt;Coursera NLP course by Jurafsky &amp;amp; Manning&lt;/a&gt;, and the &lt;a href=&quot;https://class.coursera.org/nlp/lecture/32&quot; rel=&quot;nofollow&quot;&gt;lecture on Good-Turing smoothing&lt;/a&gt; struck me odd.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example given was:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;You are fishing (a scenario from Josh Goodman), and caught:&lt;br&gt;&#xA;  10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish&lt;br&gt;&#xA;  ...&lt;br&gt;&#xA;  How likely is it that the next species is new (i.e. catfish or bass)&lt;br&gt;&#xA;  Let's use our estimate of things-we-saw-once to estimate the new things.&lt;br&gt;&#xA;  3/18 (because N_1=3)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I get the intuition of using the count of uniquely seen items to estimate the number of unseen item types (N = 3), but the next steps seem counterintuitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is the denominator left unchanged instead of incremented by the estimate of unseen item types? I.e., I would expect the probabilities to become:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Carp : 10 / 21&lt;br&gt;&#xA;  Perch : 3 / 21&lt;br&gt;&#xA;  Whitefish : 2 / 21&lt;br&gt;&#xA;  Trout : 1 / 21&lt;br&gt;&#xA;  Salmon : 1 / 21&lt;br&gt;&#xA;  Eel : 1 / 21&lt;br&gt;&#xA;  Something new : 3 / 21&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It seems like the Good-Turing count penalizes seen items too much (trout, salmon, &amp;amp; eel are each taken down to 1/27); coupled with the need to adjust the formula for gaps in the counts (e.g., Perch &amp;amp; Carp would be zeroed out otherwise), it just feels like a bad hack.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to build an item-item similarity matching recommendation engine with mahout. The data set is as in the following format ( attributes are in text not in numerals format )&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name : category : cost : ingredients&#xA;&#xA;x : xx1 : 15 : xxx1, xxx2, xxx3&#xA;&#xA;y : yy1 : 14 : yyy1, yyy2, yyy3&#xA;&#xA;z : xx1 : 12 : xxx1, xxy1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So in-order to use this data set for mahout to train, what is the right way to convert this in to numeric (as CSV Boolean data set) format accepted by mahout.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to build a data set on several log files of one of our products.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The different log files have their own layout and own content; I successfully grouped them together, only one step remaining...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Indeed, the log &quot;messages&quot; are the best information. I don't have the comprehensive list of all those messages, and it's a bad idea to hard code based on those because that list can change every day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I would like to do is to separate the indentification text from the value text (for example: &quot;Loaded file XXX&quot; becomes (identification: &quot;Loaded file&quot;, value: &quot;XXX&quot;)). Unfortunately, this example is simple, and in real world there are different layouts and sometimes multiple values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about using string kernels, but it is intended for clustering ... and cluseting is not applicable here (I don't know the number of different types of messages and eventhough, it would be too much).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you have any idea?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S: For those who programs, this can be easier to understand. Let's say that the code contains as logs printf(&quot;blabla %s&quot;, &quot;xxx&quot;) -&gt; I would like to have &quot;blabla&quot; and &quot;xxx&quot; seperatated&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I would like to run an R script using a single command (e.g. bat file or shortcut).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This R script asks the user to choose a file and then plots information about that file. All is done via dialog boxes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't want the user to go inside R - because they don't know it at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I was using r cmd and other similar stuffs, but as soon as the plots are displayed, R exits and closes the plots.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can I do?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently finishing up a B.S. in mathematics and would like to attend graduate school (a master's degree for starters, with the possibility of a subsequent Ph.D.) with an eye toward entering the field of data science. I'm also particularly interested in machine learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the graduate degree choices that would get me to where I want to go?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a consensus as to whether a graduate degree in applied mathematics, statistics, or computer science would put me in a better position to enter the field of data science?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you all for the help, this is a big choice for me and any input is very much appreciated. Typically I ask my questions on Mathematics Stack Exchange, but I thought asking here would give me a broader and better rounded perspective.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;From &lt;a href=&quot;http://nshorter.com/ResearchPapers/MachineLearning/A_Roadmap_to_SVM_SMO.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;A_Roadmap_to_SVM_SMO.pdf&lt;/a&gt;, pg 12.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://s13.postimg.org/9dx9t4w47/whatwhat.png&quot; alt=&quot;a busy cat&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assume I am using linear kernel, how will I be able to get both the first and second inner product?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My guess, inner product of datapoint with datapoint j labelled class A for the first inner product of the equation and inner product of datapoint j with datapoints labelled class B for second inner product?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;To all:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been wracking my brain at this for a while and thought maybe someone here would know of a package or algorithm to handle the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have nominal multivariant timeseries that look like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;          Time Var1 Var2 Var3 Var4 Var5 ... VarN&#xA;             0     A     A   B    C    A   ... H&#xA;             1     A     A   B    D    D   ... H&#xA;             2     B     A   C    D    D   ... H&#xA;             ..&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And so on from times 0 to 1,000,000. What I would like to do is search the time series for rules of the type:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given Var3 is in state B in the previous step and Var5 is in state D in the previous step, than Var1 will be in state B. What I want to do is have the rules that include the time interval explicitly. A simpler case of interest would simply be to reduce the time series to &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;               Time    Var1 Var2 Var3 Var4 Var5 ... VarN&#xA;                0        0    0    0     0   0   ... 0&#xA;                1        0    0    0     1   1   ... 0&#xA;                2        1    0    1     0   0   ... 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where the the variable is 1 if its state is different from the previous step and zero otherwise. Then I just want to have rules that say something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If Var4 and Var5 changed in the previous step than Var1 will change in the current step. Which would be easy for a lag of one, as I could just make the data into something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   Var1 Var2 Var3 Var4 Var5 ... VarN Var1_t-1 Var2_t-1 Var3_t-1 ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then do sequence mining, but if I want to have rules that aren't just a single lag but could be lags from 1 to 500 than my data set begins to be a little difficult to work with. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be greatly appreciated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit to respond to comment:&#xA;Each column could be in one of 7 different states. As far as a target, it is non-specific, any rules between the columns would be of interest. However, predicting columns 30-40 and 62-75 would be particularly interesting.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I browsed a sample for available data at &lt;a href=&quot;http://dbpedia.org/page/Sachin_Tendulkar&quot; rel=&quot;nofollow&quot;&gt;http://dbpedia.org/page/Sachin_Tendulkar&lt;/a&gt;. I wanted these properties as columns, so I downloaded the CSV files from &lt;a href=&quot;http://wiki.dbpedia.org/DBpediaAsTables&quot; rel=&quot;nofollow&quot;&gt;http://wiki.dbpedia.org/DBpediaAsTables&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when I browse the data for the same entity &quot;Sachin_Tendulkar&quot;, I find that many of the properties are not available. e.g. the property &quot;dbpprop:bestBowling&quot; is not present.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I get all the properties that I can browse through the direct resource page.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Common model validation statistics like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov–Smirnov test&lt;/a&gt; (KS), &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;AUROC&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Gini_coefficient&quot; rel=&quot;nofollow&quot;&gt;Gini coefficient&lt;/a&gt; are all functionally related. However, my question has to do with proving how these are all related. I am curious if anyone can help me prove these relationships. I haven't been able to find anything online, but I am just genuinely interested how the proofs work. For example, I know Gini=2AUROC-1, but my best proof involves pointing at a graph. I am interested in formal proofs. Any help would be greatly appreciated!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am having some difficulty in seeing connection between PCA on second order moment matrix in estimating parameters of Gaussian Mixture Models. Can anyone connect the above??&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Hi this is my first question in the Data Science stack. I want to create an algorithm for text classification. Suppose i have a large set of text and articles. Lets say around 5000 plain texts. I first use a simple function to determine the frequency of all the four and above character words. I then use this as the feature of each training sample. Now i want my algorithm to be able to cluster the training sets to according to their features, which here is the frequency of each word in the article. (Note that in this example, each article would have its own unique feature since each article has a different feature, for example an article has 10 &quot;water and 23 &quot;pure&quot; and another has 8 &quot;politics&quot; and 14 &quot;leverage&quot;). Can you suggest the best possible clustering algorithm for this example?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;&lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&quot; rel=&quot;nofollow&quot;&gt;https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the description given in the above link, &#xA;the Attribute information specifies &quot;average and covariance over all 'segments', each segment being described by a 12-dimensional timbre vector&quot;. So the covariance matrix should have 12*12 = 144 elements. But why is the number of timbre covariance features only 78 ?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;GBMs, like random forests, build each tree on a different sample of the dataset and hence, going by the spirit of ensemble models, produce higher accuracies. However, I have not seen GBM being used with dimension sampling at every split of the tree like is common practice with random forests. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there some tests that show that dimensional sampling with GBM would decrease its accuracy because of which this is avoided, either in literature form or in practical experience? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have historic error of time series.  I want to analyze error series to improve forecast series. Are there any methods to do this?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;What is the standard way for evaluating and comparing different algorithms while developing recommendation system? Whether we need to have a predetermined annotated ranked dataset and then compare with precision/recall/F measure of  different algorithms ? Is this the best way for evaluation ? Or is there any other way to compare results of various recommendation algorithms ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm from programming background. I'm now learning Analytics. I'm learning concepts from basic statistics to model building like linear regression, logistic regression, time-series analysis, etc.,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As my previous experience is completely on programming, I would like to do some analysis on the data which programmer has.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say, Lets have the details below(I'm using SVN repository)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;personname, code check-in date, file checked-in, number of times checkedin, branch, check-in date and time, build version, Number of defects, defect date, file that has defect, build version, defect fix date, defect fix hours, (please feel free to add/remove how many ever variables needed)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I Just need a trigger/ starting point on what can be done with these data. can I bring any insights with this data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;or can you provide any links that has information about similar type of work done.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to determine whether or not we are 90% confident that the mean of a &lt;em&gt;proposed&lt;/em&gt; &lt;em&gt;population&lt;/em&gt; is at least 2 times that of the mean of the &lt;em&gt;incumbant&lt;/em&gt; &lt;em&gt;population&lt;/em&gt; based on samples from each population which is all the data I have right now. Here are the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;incumbantvalues = (7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;proposedvalues =  (17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have no idea if either population is or will be normal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The ratio of the &lt;em&gt;sample&lt;/em&gt; means does exceed 2.0 but how does that translate to confidence that the proposed population mean will be at least twice that of the mean of the incumbant population with 90% confidence ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can re-sampling (bootstrapping with replacement) help answer this question ?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am doing a text classification task(5000 essays evenly distributed by 10 labels). I explored &lt;code&gt;LinearSVC&lt;/code&gt; and got an accuracy of 80%. Now I guess whether accuracy could be raised by using &lt;code&gt;ensemble&lt;/code&gt; classifier with &lt;code&gt;SVM&lt;/code&gt; as base estimator?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I do not know how to employ an &lt;code&gt;ensemble&lt;/code&gt; classifier incorporating all the features? Please note that I do not want to combine the different features directly in a single vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, &lt;strong&gt;My first question:&lt;/strong&gt; in order to improve the current accuracy, is it possible to use &lt;code&gt;ensemble&lt;/code&gt; classifier with &lt;code&gt;svm&lt;/code&gt; as base estimator?&#xA;&lt;strong&gt;My second question&lt;/strong&gt; How to employ an &lt;code&gt;ensemble&lt;/code&gt; classifier incorporating all features?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What are some possible techniques for smoothing proportions across very large categories, in order to take into account the sample size? The application of interest here is to use the proportions as input into a predictive model, but I am wary of using the raw proportions in cases where there is little evidence and I don't want to overfit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example, where the ID denotes a customer and impressions and clicks are the number of ads shown and clicks the customer has made, respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/3oHzQ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a visualization problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Creating a comparison report of PR event efficiency. Say, show or exhibition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are two dimensions of comparison:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;compare vs the same event performance in the past years &lt;/li&gt;&#xA;&lt;li&gt;compare vs another type of analogical/competitive events &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;There is also a number of comparison aspects:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Audience&lt;/li&gt;&#xA;&lt;li&gt;Media Coverage&lt;/li&gt;&#xA;&lt;li&gt;Social Buzz&lt;/li&gt;&#xA;&lt;li&gt;ROI&lt;/li&gt;&#xA;&lt;li&gt;.... etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Each aspect is a set of some final KPI-s (just numbers, which can be compared vs another &quot;dimensions&quot;), plus maybe some descriptive text and pictures (which couldn't be a metric but should be attached to the report).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So finaly it looks like a three-dimensional coube:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Years&lt;/li&gt;&#xA;&lt;li&gt;Another Events&lt;/li&gt;&#xA;&lt;li&gt;Aspects&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;If I put it in plain Word or PPT it will look like a document with dozen of slides/papers and linear structure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas how to compile an elegant user-friendly report?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;A short while ago, I came across this ML framework that has implemented several different algorithms ready for use. The site also provides a handy API that you can access with an API key.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have need of the framework to solve a website classification problem where I basically need to categorize several thousand websites based on their HTML content. As I don't want to be bound to their existing API, I wanted to use the framework to implement my own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, besides some introductory-level data mining courses and associated reading, I know very little as to what exactly I would need to use. Specifically, I'm at a loss as to what exactly I need to do to train the classifier and then model the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The framework already includes some classification algorithms like NaiveBayes, which I know is well suited to the task of text classification, but I'm not exactly sure how to apply it to the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me a rough guidelines as to what exactly I would need to do to accomplish this task?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to make a prediction for the result of the parliamentary elections. My output will be the % each party receives. There is more than 2 parties so logistic regression is not a viable option. I could make a separate regression for each party but in that case the results would be in some manner independent from each other. It would not ensure that the sum of the results would be 100%.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What regression (or other method) should I use? Is it possible to use this method in R or Python via a specific library?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have some text files containing moview reviews I need to find out whether the review is good or bad. I tried the following code but its not working:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import nltk&#xA;with open(&quot;c:/users/user/desktop/datascience/moviesr/movies-1-32.txt&quot;, 'r') as m11:&#xA;    mov_rev = m11.read()&#xA;mov_review1=nltk.word_tokenize(mov_rev)&#xA;bon=&quot;crap aweful horrible terrible bad bland trite sucks unpleasant boring dull moronic dreadful disgusting distasteful flawed ordinary slow senseless unoriginal weak wacky uninteresting unpretentious &quot;&#xA;bag_of_negative_words=nltk.word_tokenize(bon)&#xA;bop=&quot;Absorbing Big-Budget Brilliant Brutal Charismatic Charming Clever Comical Dazzling Dramatic Enjoyable Entertaining Excellent Exciting  Expensive Fascinating Fast-Moving First-Rate Funny Highly-Charged Hilarious Imaginative Insightful Inspirational Intriguing Juvenile Lasting Legendary Pleasant Powerful Ripping Riveting Romantic Sad  Satirical Sensitive  Sentimental Surprising Suspenseful Tender Thought Provoking Tragic Uplifting Uproarious&quot;&#xA;bop.lower()&#xA;bag_of_positive_words=nltk.word_tokenize(bop)&#xA;vec=[]&#xA;for i in bag_of_negative_words:&#xA;    if i in mov_review1:&#xA;        vec.append(1)&#xA;    else:&#xA;        for w in bag_of_positive_words:&#xA;            if w in moview_review1:&#xA;                vec.append(5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;so i am trying to check whether the review contains a positive word or a negative word. If it contains negative word then a value 1 will be assigned to the vector vec else a value of 5 will be assigned.&#xA;but the output i am getting is an empty vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;please help..&#xA;Also please suggest others way of solving this problem.&#xA;thanks in advance&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;In this &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF&quot; rel=&quot;nofollow&quot;&gt;wiki page&lt;/a&gt; there is a function &lt;code&gt;corr()&lt;/code&gt; that calculates the Pearson coefficient of correlation, but my question is that: is there any function in Hive that enables to calculate the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kendall%27s_W&quot; rel=&quot;nofollow&quot;&gt;Kendall coefficient&lt;/a&gt; of correlation of a pair of a numeric columns in the group?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm looking for API suggestions for enriching data on companies. Currently I use the Crunchbase API to look up a company's name or domain and I am trying to gather the domain/name (if I don't already have both), contact email (this one is a long shot), and the location of their headquarters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works incredibly well if Crunchbase has the company in their API, but I'd say this only happens about 25% of the time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd love to get some suggestions on some free APIs that I could use along with Crunchbase. I'd also love to see if anyone has had positive or negative experiences with paid APIs! &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have some question regarding to the choice of the better implementation. I would know the differences and advantages of &lt;a href=&quot;https://mahout.apache.org/&quot; rel=&quot;nofollow&quot;&gt;Mahout Apache&lt;/a&gt; (Java implementation) versus &lt;a href=&quot;http://graphlab.com/index.html&quot; rel=&quot;nofollow&quot;&gt;Graphlab&lt;/a&gt; (Python implementation) in the area of the data sciences. Specially in the area of recommenders and classifiers. Can anybody here get some (qualified) feedback about both possibilities?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Which of the following is best (or widely used) for calculating item-item similarity measure in mahout and why ?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Pearson Correlation&#xA;Spearman Correlation&#xA;Euclidean Distance&#xA;Tanimoto Coefficient&#xA;LogLikelihood Similarity&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any thumb-rule to chose from these set of algorithm also how to differentiate each of them ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to match new product description with the existing ones. Product description looks like this: ￼Panasonic DMC-FX07EB digital camera silver. These are steps to be performed:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tokenize description and recognize attributes: Panasonic =&gt; Brand, DMC-FX07EB =&gt; Model, etc.&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;Get few candidates with similar features&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;Get the best candidate.&lt;br/&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am having problem with the first step (1). In order to get 'Panasonic =&gt; Brand', DMC-FX07EB =&gt; Model, silver =&gt; color, I need to have index where each token of the product description correspond to certain attribute name (Brand, model, color, etc.) in the existing database. The problem is that in my database product descriptions are presented as one atomic attribute e.g. 'description' (no separated product attributes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically I don't have training data, so I am trying to build index of all product attributes so I can build training data. So far I have attributes from bestbuy.com and semantics3.com APIs, but both sources lack most of attributes or contain irrelevant ones. Any suggestions for better APIs to get product attributes? Better approach to do this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. For every product there is a matched product description in the Database, which is as well in a form of one atomic attribute. I have checked this &lt;a href=&quot;https://stackoverflow.com/questions/18496925/how-to-parse-product-titles-unstructured-into-structured-data&quot;&gt;question on SO&lt;/a&gt;, it helped me and it seems we have same approach but I am still trying to get training data. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am currently using SVM and scaling my training features to the range of [0,1].&#xA;I first fit/transform my training set and then apply the &lt;strong&gt;same&lt;/strong&gt; transformation to my testing set. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ### Configure transformation and apply to training set&#xA;    min_max_scaler = MinMaxScaler(feature_range=(0, 1))&#xA;    X_train = min_max_scaler.fit_transform(X_train)&#xA;&#xA;    ### Perform transformation on testing set&#xA;    X_test = min_max_scaler.transform(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Let assume that a given feature in the training set has a range of [0,100], and that same feature in the testing set has a range of [-10,120]. In the training set that feature will be scaled appropriately to [0,1], while in the testing set that feature will be scaled to a range outside of that first specified, something like [-0.1,1.2].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering what the consequences of the testing set features being out of range of those being used to train the model? Is this a problem?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm very passionate about how computers can be made able to think intelligently and independently (in our favour, of course!). I'm currently studying Bachelors science of information technology at UTS (University of Technology:Sydney). I have two months before I start my second year, and have not yet been able to decide on which major should I select that can lead myself towards dedicated study of Artificial Intelligence (which I love with my life).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the following majors available:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Internetworking and Applications &lt;/li&gt;&#xA;&lt;li&gt;Data Analytics&lt;/li&gt;&#xA;&lt;li&gt;(there are other two as well, but business oriented).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://uts.edu.au&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the link to my subjects. I believe that being able to play with data is a sign of intelligence (I may be wrong too!). Will one of these subjects form me a good foundation for my further study in A.I.? Or should I jump into Engineering? Or Pure Science?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is there a method/class available in Apache Mahout to perform n-fold cross validation?&#xA;If yes how it can be done?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I would like to pose a question about how to treat additional holders in the propensity-to-buy models of banking products.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Up to now I was only taking into considerations the clients as first holders. &#xA;For example, if a client ‘1’ appears as the first holder of a saving account ‘A’ with a balance at the end of the month of 100€ and as an additional holder of a saving account ‘B’ with a balance at the end of the month of 50€, the saving balance at the end of the month for the client is considered to be just 100€.&#xA;Moreover, if a client only appears as an additional holder (and he/she is not a first account holder of ANY product), he/she is dismissed by the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I have been told to include additional holders in the models (additional holders have the same rights of the first holders).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possibility is to recalculate all the variables summing up the position as first and additional holder (in the previous example, the balance at the end of the month of client ‘1’ would be 150€). Together with this, I would create some variable that represents the maximum degree of intervention of the client in the account (ex. 'first holder', 'second holder').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another possibility would be to “double” all the variables, considering the client as first and additional holder (in the example, we would create two variables:  the balance at the end of the month as FH =100€, :  the balance at the end of the month as AH =50€).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Did any of you encounter a similar problem?It would be very helpful to understand how you solved it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want to create a model to predict the propensity to buy a certain product. As my proportion of 1's is very low, I decided to apply oversampling (to get a 10% of 1's and a 90% of 0's). Now, I want to discretize some of the variables. To do so I run a tree for each variable against the target. My question is...shall I define the prior probabilities when I do this (run the trees), or it doesn't matter and I can use the over-sampled dataset just like that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to write a data-mining service in &lt;a href=&quot;http://golang.org&quot;&gt;Google Go&lt;/a&gt; which collects data through scraping and APIs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However as Go lacks good ML support I would like to do the ML stuff in Python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having a web background I would connect both services with something like RPC but as I believe that this is a common problem in data science I think that there is some better solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example most (web) protocols lack at:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;buffering between processes&lt;/li&gt;&#xA;&lt;li&gt;clustering over multiple instances&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So what (type of libraries) do data scientists use to connect different languages/processes?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bodo&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I've been toying with this idea for a while. I think there is probably some method in the text mining literature, but I haven't come across anything just right...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is/are some methods for tackling a problem where the number of variables it its self a variable.  This is not a missing data problem, but one where the nature of the problem fundamentally changes. Consider the following example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I want to predict who will win a race, a simple multinomial classification problem. I have lots of past data on races, plenty to train on.  Lets further suppose I have observed each contestant run multiple races. The problem however is that the number or racers is variable. Sometimes there are only 2 racers, sometimes there are as many as 100 racers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One solution might be to train a separate model for each number or racers, resulting in 99 models in this case, using any method I choose.  E.g. I could have 100 random forests. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another solution might be to include an additional variable called 'number_of_contestants' and have input field for 100 racers and simply leave them blank when no racer is present.  Intuitively, it seems that this method would have difficulties predicting the outcome of a 100 contestant race if the number of racers follows a Poisson distribution (which I didn't originally specify in the problem, but I am saying it here).    &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thoughts?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I would like to know how exactly mahout user based and item based recommendation differ from each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It defines that&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://mahout.apache.org/users/recommender/userbased-5-minutes.html&quot;&gt;User-based&lt;/a&gt;: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html&quot;&gt;Item-based&lt;/a&gt;: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed off line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But though there are two kind of recommendation available, what I understand is that both these will take some data model ( say 1,2 or 1,2,.5 as item1,item2,value or user1,user2,value where value is not mandatory) and will perform all calculation as the similarity measure and recommender build-in function we chose and we can run both user/item based recommendation on the same data ( is this a correct assumption ?? ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I would like to know how exactly and in which all aspects these two type of algorithm differ. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am interested in graph problems like 2-color, max-clique, stable sets, etc but the documentation for scipy.optimize.anneal seems to be for ordinary functions. How would one apply this library towards graph formulations?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I came across a package in R which has a function called &lt;code&gt;sann&lt;/code&gt; for simulated annealing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;sann&lt;/code&gt; uses parameters &lt;code&gt;fn&lt;/code&gt; and &lt;code&gt;gr&lt;/code&gt; to optimize and to select new points, respectively. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For something like the max-clique or max-stable set problems, &lt;code&gt;fn&lt;/code&gt; would be a summing function, but it's less clear how one would formulate &lt;code&gt;gr&lt;/code&gt; to fix these graph computations. In these cases, how would &lt;code&gt;gr&lt;/code&gt; &quot;select&quot;?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;My data looks like this: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/9LgwU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this error showing up?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Can anyone suggest any good books to learn hadoop and map reduce basics?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also something for Spark, and Spark Streaming?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Although I have seen a few good questions asked about data anonymization, I was wondering if there were answers to this more specific variant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am seeking a tool (or to design one) that will anonymize human names from a specific country: particularly first names in unstructured text. Many of the tools that I have seen have considered the wider dimensions of data anonymization; with an equal focus on dates of birth, addresses, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An imperative aspect is that it needs to have near absolute recall. The major pitfalls, as far as I can see, are diminutive variants (&quot;Tommy&quot; instead of &quot;Thomas&quot;, &quot;Ben&quot; instead of &quot;Benjamin&quot;, etc.) and typos. These two factors prevent a simple regex based on a database of names (based on censuses, etc.)&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Few things in life give me pleasure like scraping structured and unstructured data from the Internet and making use of it in my models. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, the Data Science Toolkit (or &lt;code&gt;RDSTK&lt;/code&gt; for R programmers) allows me to pull lots of good location-based data using IP's or addresses and the &lt;code&gt;tm.webmining.plugin&lt;/code&gt; for R's &lt;code&gt;tm&lt;/code&gt; package makes scraping financial and news data straightfoward. When going beyond such (semi-) structured data I tend to use &lt;code&gt;XPath&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm constantly getting throttled by limits on the number of queries you're allowed to make. I think Google limits me to about 50,000 requests per 24 hours, which is a problem for Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From a &lt;em&gt;technical&lt;/em&gt; perspective getting around these limits is easy -- just switch IP addresses and purge other identifiers from your environment. However, this presents both ethical and financial concerns (I think?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a solution that I'm overlooking?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;The &lt;code&gt;mnlogit&lt;/code&gt; package in R allows for the fast estimation of multinomial logit models.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The specification of forumlas is a bit different from most other regression models/packages in R, however. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the &lt;code&gt;Fish&lt;/code&gt; dataset as a reproducible example, &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; require(mnlogit)&#xA;Loading required package: mnlogit&#xA;&#xA;Package: mnlogit&#xA;Version: 1.1.1&#xA;Multinomial Logit Choice Models.&#xA;Scientific Computing Group, Sentrana Inc, 2013.&#xA;&#xA;&amp;gt; data(Fish, package ='mnlogit')&#xA;&amp;gt; head(Fish)&#xA;           mode   income     alt   price  catch chid&#xA;1.beach   FALSE 7083.332   beach 157.930 0.0678    1&#xA;1.boat    FALSE 7083.332    boat 157.930 0.2601    1&#xA;1.charter  TRUE 7083.332 charter 182.930 0.5391    1&#xA;1.pier    FALSE 7083.332    pier 157.930 0.0503    1&#xA;2.beach   FALSE 1250.000   beach  15.114 0.1049    2&#xA;2.boat    FALSE 1250.000    boat  10.534 0.1574    2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm trying to understand the difference between the model specification of &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fm &amp;lt;- formula(mode ~ 0 + price | income | catch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fm &amp;lt;- formula(mode ~ 0 + price | income + catch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;while the documentation covers the detail of such changes  in the general coeffcient area of the forumla (i.e. where &lt;code&gt;price&lt;/code&gt; is), I don't see an explanation of how operators like &lt;code&gt;+&lt;/code&gt; affect the alternative-specific area of the formula/code, relative to &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to create 3D bars on this map. Can anyone please advise if this is possible, and how?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://leafletjs.com/examples/choropleth.html&quot; rel=&quot;nofollow&quot;&gt;http://leafletjs.com/examples/choropleth.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My data: UFO sightings in the USA (location wise).&#xA;Count of these sightings per location will be the height of the 3D bar.&#xA;Base map is a choropleth with US population density values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't mind integrating Javascript or d3.js into the code to create the 3D bars.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Using SAS Studio (online, student version)...&#xA;Need to do a &quot;nested likelihood ratio test&quot; for a logistic regression. &#xA;Entirety of instructions are: &quot;Perform a nested likelihood ratio test comparing your full model (all predictors included)to a reduced model of interest.&quot;&#xA;The two models I have are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Proc Logistic Data=Project_C;&#xA;Model Dem (event='1') = VEP TIF Income NonCit Unemployed Swing;&#xA;Run;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Proc Logistic Data=Project_C;&#xA;Model Dem (Event='1') = VEP TIF Income / clodds=Wald clparm=Wald expb rsquare;&#xA;Run;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I honestly have no idea where to even start. &#xA;Any suggestions would be appreciated. &#xA;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In recent years, the term &quot;data&quot; seems to have become a term widely used without specific definition. Everyone seems to use the phrase. Even people as technology-impaired as my grandparents use the term and seem to understand words like &quot;data breach.&quot; But I don't understand what makes &quot;data science&quot; a new discipline. Data has been the foundation of science for centuries. Without data, there would be no Mendel, no Schrödinger, etc. You can't have science without interpreting  and analyzing data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But clearly it means something. Everyone is talking about it.  So what exactly do people mean by data when they use terms like &quot;big data&quot; and why has this become a discipline in itself? Also, if it is an emerging discipline, where can I find more serious/in-depth information so I can better educate myself? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;For simplicity let's assume the feature space is the XY plane.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm running a test on MapReduce algorithm in different environments, like Hadoop and MongoDB, and using different types of data. What are the different methods or techniques to find out the execution time of a query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I'm inserting a huge amount of data, consider it to be 2-3GB, what are the methods to find out the time for the process to be completed.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to set up a cluster (1 namenode, 1 datanode) on AWS.&#xA;I'm using free one year trial period of AWS, but the challenge is, instance is created with 1GB of RAM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I'm a student, I cannot afford much. Can anyone please suggest me some solution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, it would be great if you could provide any links for setting up multi cluster hadoop with spark on AWS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I cannot try in GCE as my trial period is exhausted. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Having:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a set of soft fuzzy classifiers (classification onto overlapping sets) $C_i(x) \\\\to [0,1]$;&lt;/li&gt;&#xA;&lt;li&gt;a corresponding set of weak estimators $R_i(z)$ of the form $R_i(z) = \\\\mathit{EX}(y\\\\mid z)$.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The estimators $R_i$ are just some kind of regression, Kalman or particle filters. The classifiers $C_i$ are fixed and static. How to make a strong estimator out of a weighted combination of the form:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$L(x, z) = \\\\sum_{i}C_i(x)R_i(z)Q_i$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words how to choose the weights $Q_i$? Is there some kind of online approach to this problem? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is brief description of a practical application. When an event $E$ is registered, multiple measurements are made. Based on these measurements, the classifiers $C_i$ make a soft assignment of the event to multiple overlapping categories. What we get is fit ratios for the soft clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now there is some chance that event $E$ may trigger a subsequent event $D$, depending on another variable $z$ -- independent from the event $E$. We know that all the soft cluster &quot;memberships&quot; may influence the probability of event $D$ being triggered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We want to estimate the probability that $E$ triggers $D$, given the $C_i$ fitness ratios and value of $z$.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Are there any algorithms which were developed using partial differential equations for tackling some of the machine learning problems? Most works I see online are in the field of computer vision and a few bizarre ones in topic modelling. But just curious if someone has used or seen it being used for some decision making process or classification problems?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm new to apache spark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to configure multi cluster spark without hadoop?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, can you please provide the steps. &#xA;I would like to create clusters on Google Compute Engine  (1-master, 1-worker)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Say I used spectral clustering to cluster a data-set $D$ of points $X_0 - X_n$ into a number $C$ of clusters. How can I efficiently assign a new single point $X_{n+1}$ to his convenient cluster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set $X_0 - X_{n+1}$), or is there an optimized way to extend to the point $X_{n+1}$?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Are there any articles or discussions about extracting part of text that holds the most of information about current document.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I have a large corpus of documents from the same domain. There are parts of text that hold the key information what single document talks about. I want to extract some of those parts and use them as kind of a summary of the text. Is there any useful documentation about how to achieve something like this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be really helpful if someone could point me into the right direction what I should search for or read to get some insight in work that might have already been done in this field of Natural language processing.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;&lt;br&gt;&#xA;Im traying to integrate Hadoop and R, I was install the pachages rJava and Rhipe in R, I do this steps to start Hadoop and R:&lt;br&gt;&#xA;-starting Hadoop services.,&lt;br&gt;&#xA;-loading rJava and Rhipe packages by library function.&lt;br&gt;&#xA;-Calling rhinit() to initialize Rhipe.&lt;br&gt;&#xA;the problem here is when I call rhinit() funtion, it show this error:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;blockquote&gt;&#xA;    &lt;p&gt;Initializing Rhipe v0.73&lt;br&gt;&#xA;    Error in .jnew(&quot;org/godhuli/rhipe/PersonalServer&quot;) : &#xA;     java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream'&lt;/p&gt;&#xA;  &lt;/blockquote&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;please some helps to fixe this problem.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm wondering if there is a web framework well suited for placing recommendations on content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In most cases, a data scientist goes through after the fact and builds (or uses) a completely different tool to create recommendations. This involves analyzing traffic logs, a history of shopping cart data, ratings, and so forth. It usually comes from multiples sources (the web server, the application's database, Google Analytics, etc) and then has to be cleaned up and processed, THEN delivered back to the application in way it understands.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a web framework on the market which handles collecting this data up front, as to minimize the retrospective data wrangling?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a huge file of customer complaints about the products my company owns and I would like to do a data analysis on those descriptions and tag a category to each of them. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: I need to figure out the number of complaints on &lt;strong&gt;&lt;em&gt;Software&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Hardware&lt;/em&gt;&lt;/strong&gt; side of my product from the customer complaints. Currently, I am using excel to do the data analysis which do seek a significant amount of manual work to get a tag name to the complaints.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I'm still new to this topic and confuse the terms quite often. So here is the changed question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine that we pick kNN for some dataset. I want to remain as general as possible, thus $k$ will not be specified here. Further we select, at some point, an observation where the number of neighbors that fulfill the requirement to be in the neighbourhood are actually more than the specified $k$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighborhood (number of neighbors). Which observations will be left out and why? Also is this a problem that occurs often, or is it something of an anomaly?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How can I connect to Titan database from Python ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I understand is that Titan (Graph database) provides an interface (Blueprint) to Cassandra (Column Store) and &#xA;bulb is a python interface to graph DB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how can I start programming in python to connect with titan DB?&#xA;Is there any good documentation/tutorial available ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I know that Spark is fully integrated with Scala. It's use case is specifically for large data sets. Which other tools have good Scala support? Is Scala best suited for larger data sets? Or is it also suited for smaller data sets? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to find an equivalent of Hinton Diagrams for multilayer networks to plot the weights during training. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The trained network is somewhat similar to a Deep SRN, i.e. it has a high number of multiple weight matrices which would make the simultaneous plot of several Hinton Diagrams visually confusing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of a good way to visualize the weight update process for recurrent networks with multiple layers? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't found much papers on the topic. I was thinking to display time-related information on the weights per layer instead if I can't come up with something. E.g. the weight-delta over time for each layer (omitting the use of every single connection). PCA is another possibility, though I'd like to not produce much additional computations, since the visualization is done online during training.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently going into the world of machine learning and Neural Networks, thanks to &lt;a href=&quot;https://github.com/cazala/synaptic&quot; rel=&quot;nofollow&quot;&gt;synaptic (js)&lt;/a&gt; that interests me a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I read a lot, wikipedia links and &lt;a href=&quot;https://github.com/cazala/synaptic/wiki/Neural-Networks-101&quot; rel=&quot;nofollow&quot;&gt;synaptic's NN 101&lt;/a&gt;, but there's a lot of basics questions that I don't understand (but I'd like to) in the use of machine learning (NN) and the point of these technologies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say, I wan't my network to (kind of) learn (something like) gravity, so to train it I set in input 10 objects with a mass, and a position x, y (and z) and I set output the new x, y (and z) of each objects.&#xA;I guess I should give it several configurations and everything but &lt;strong&gt;here is the question; can it, then, be able to compute the interactions between 10000, 100000 objects?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this stage in my learning, what I don't clearly get is what is the point of teaching/training neurons to compute XOR like it's shown in &lt;a href=&quot;https://github.com/cazala/synaptic#documentation&quot; rel=&quot;nofollow&quot;&gt;synaptic's documentation&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var trainingSet = [&#xA;    {&#xA;        input: [0,0],&#xA;        output: [0]&#xA;    },&#xA;    {&#xA;        input: [0,1],&#xA;        output: [1]&#xA;    },&#xA;    {&#xA;        input: [1,0],&#xA;        output: [1]&#xA;    },&#xA;    {&#xA;        input: [1,1],&#xA;        output: [0]&#xA;    },&#xA;];&#xA;&#xA;var trainer = new Trainer(myNetwork);&#xA;trainer.train(trainingSet);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Were we just give it all the possible inputs and outputs to a XOR.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Well, as I'm all new to the technologies I think my questions are full of non-sense and everything but thanks for reading and help you might bring :)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found out that &lt;a href=&quot;http://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot; rel=&quot;noreferrer&quot;&gt;tf-idf&lt;/a&gt; could be very useful for this. However, I was unsure as to &lt;em&gt;when&lt;/em&gt; exactly to use it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Retrieve site content, parse for plain text &lt;/li&gt;&#xA;&lt;li&gt;Normalize and stem content &lt;/li&gt;&#xA;&lt;li&gt;Tokenize into unigrams (maybe bigrams too)&lt;/li&gt;&#xA;&lt;li&gt;Retrieve a count of each unigram for the given document, filtering low length and low occurrence words&lt;/li&gt;&#xA;&lt;li&gt;Train a classifier such as NaiveBayes on the resulting set&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My question is the following: &lt;strong&gt;Where would tf-idf fit in here&lt;/strong&gt;? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any insight would be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Upon closer inspection, I think I may have run into a misunderstanding at to how TF-IDF operates. At the above step 4 that I describe, would I have to feed the &lt;em&gt;entirety&lt;/em&gt; of my data into TF-IDF at once? If, for example, my data is as follows: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[({tokenized_content_site1}, category_string_site1), &#xA; ({tokenized_content_site2}, category_string_site2), &#xA;...&#xA; ({tokenized_content_siten}, category_string_siten)}]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here, the outermost structure is a list, containing tuples, containing a dictionary (or hashmap) and a string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would I have to feed the &lt;em&gt;entirety&lt;/em&gt; of that data into the TF-IDF calculator at once to achieve the desired effect? Specifically, I have been looking at the &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html&quot; rel=&quot;noreferrer&quot;&gt;scikit-learn&lt;/a&gt; TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have used Neo4J to implement a content recommendation engine. I like Cypher, and find graph databases to be intuitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at scaling to a larger data set, I am not confident No4J + Cypher will be performant. Spark has the GraphX project, which I have not used in the past.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anybody switched from Neo4J to Spark GraphX? Do the use cases overlap, aside from scalability? Or, does GraphX address a completely different problem set than Neo4J?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to connect with cassandra in R using &lt;a href=&quot;http://cran.r-project.org/web/packages/RJDBC/index.html&quot; rel=&quot;nofollow&quot;&gt;RJDBC&lt;/a&gt;. When I execute &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;casscon &amp;lt;- dbConnect(cassdrv, &quot;jdbc:cassandra://ipaddrs:9160/demodb&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in .jcall(drv@jdrv, &quot;Ljava/sql/Connection;&quot;, &quot;connect&quot;, as.character(url)[1],&#xA;: java.lang.StringIndexOutOfBoundsException: String index out of range: -1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can't figure out the problem. I need a solution for this.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a corpus of text with a corresponding topics. For example &lt;code&gt;&quot;A rapper Tupac was shot in LA&quot;&lt;/code&gt; and it was labelled as &lt;code&gt;[&quot;celebrity&quot;, &quot;murder&quot;]&lt;/code&gt;. So basically each vector of features can have many labels (not the same amount. The first feature vector can have 3 labels, second 1, third 5).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I would have just one label corresponded to each text, I would try a &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes classifier&lt;/a&gt;, but I do not really know how should I proceed if I can have many labels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to transform Naive Bayes into multi label classification problem (if there is a better approach - please let me know)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; few things about the data I have.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;approximately 10.000 elements in the dataset&lt;/li&gt;&#xA;&lt;li&gt;text is approximately 2-3 sentences&lt;/li&gt;&#xA;&lt;li&gt;maximum 7 labels per text&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm wonder if it's possible to export a model trained in R, to OpenCV's Machine Learning (ML) library format? The latter appears to save/read models in &lt;a href=&quot;http://docs.opencv.org/modules/ml/doc/statistical_models.html#cvstatmodel-load&quot; rel=&quot;nofollow&quot;&gt;XML/YAML&lt;/a&gt;, whereas the former might be exportable via &lt;a href=&quot;http://cran.r-project.org/web/packages/pmml/index.html&quot; rel=&quot;nofollow&quot;&gt;PMML&lt;/a&gt;. Specifically, I'm working with Random Forests, which are classifiers available both in R and OpenCV's ML library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any advice on how I can get the two to share models would be greatly appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm working on a project which asks fellow students to share their original text data for further analysis using data mining techniques, and, I think it would be appropriate to anonymize student names with their submissions.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Setting aside the better solutions of a url where students submit their work and a backend script inserts the anonymized ID, &lt;strong&gt;What sort of solutions could I direct students to implement on their own to anonymized their own names?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm still a noob in this area. I don't know what are the norms. I was thinking the solution could be a hashing algorithm. That sounds like a better solution than making up a fake name as two people could pick the same fake name.possible people could pick the same fake name. &lt;strong&gt;What are some of the concerns I should be aware of?&lt;/strong&gt; &lt;/p&gt;&#xA;\",),\n",
       " ('&#xA;For an imbalanced set of data is it better to choose an L1 or L2 regularization?&lt;br&gt;&#xA;Is there a cost function more suitable to imbalanced datasets to improve the model score (log_loss in particular)?&lt;br&gt;&lt;br&gt;  &#xA;&#xA;',),\n",
       " (\"&lt;p&gt;In &lt;a href=&quot;http://web.cse.ohio-state.edu/~mbelkin/papers/PLM_UCTHESIS_03.pdf&quot; rel=&quot;nofollow&quot; title=&quot;Belkin&amp;#39;s thesis about Laplacian Eigenmaps&quot;&gt;his thesis&lt;/a&gt; (section 2.3.3) Belkin uses the heat equation to derive an approximation for $\\\\mathcal{L}f$:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\\\\mathcal{L}f(x_i)\\\\approx \\\\frac{1}{t}\\\\Big(f(x_i)-\\\\alpha \\\\sum_{x_j, ||x_i-x_j||&amp;lt;\\\\epsilon}e^{-\\\\frac{||x_i-x_j||^2}{4t}}f(x_j)\\\\Big)$$&#xA;where $$\\\\alpha=\\\\Big(\\\\sum_{x_j, ||x_i-x_j||&amp;lt;\\\\epsilon}e^{-\\\\frac{||x_i-x_j||^2}{4t}}\\\\Big)^{-1}$$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm not sure how these considerations lead to this choice of weights for the weight matrix (which will be used to construct the Laplacian):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$W_{ij} =&#xA;\\\\begin{cases} &#xA;e^{-\\\\frac{||x_i-x_j||^2}{4t}} &amp;amp; if\\\\ ||x_i-x_j||&amp;lt;\\\\epsilon \\\\\\\\ &#xA;0 &amp;amp; otherwise &#xA;\\\\end{cases}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A very vague idea of mine was that the factors $\\\\alpha$ and $\\\\frac{1}{t}$ don't change for a given $x_i$ so if one choses the weights like above the resulting discrete Laplacian would (let aside those two constants) converge to the continuous version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or tips what I'd have to read up to in order to get a better understanding?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Is the k-nearest neighbour algorithm a discriminative or a generative classifier? My first thought on this was that it was generative, because it actually used Bayes's theorem to compute the posterior. Searching further for this, it seems like it is a discriminative model, but I couldn't find the explanation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is KNN discriminative first of all? And if it is, is that because it doesn't model the the priors or the likelihood?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to get to grips with sci-kit learn for some simple machine learning projects but I'm coming unstuck with Pipelines and wonder what I've done wrong...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to work through a &lt;a href=&quot;http://www.kaggle.com/c/data-science-london-scikit-learn/data&quot; rel=&quot;nofollow&quot;&gt;tutorial on Kaggle&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;&#xA;train = pd.read_csv(local path to training data) train_labels =&#xA;pd.read_csv(local path to labels)&#xA;&#xA;&#xA;from sklearn.decomposition import PCA&#xA;from sklearn.svm import LinearSVC&#xA;from sklearn.grid_search import GridSearchCV&#xA;&#xA;pca = PCA()&#xA;clf = LinearSVC()&#xA;&#xA;n_components = arange(1, 39)&#xA;loss = ['l1','l2']&#xA;penalty = ['l1','l2']&#xA;C = arange(0, 1, .1)&#xA;whiten = [True, False]&#xA;&#xA;from sklearn.pipeline import Pipeline&#xA;&#xA;#set up pipeline&#xA;pipe = Pipeline(steps=[('pca', pca), ('clf', clf)])&#xA;&#xA;#set up GridsearchCV&#xA;estimator = GridSearchCV(pipe, dict(pca__n_components = n_components, pca__whiten = whiten, clf__loss = loss, clf__penalty = penalty, clf__C = C)) &#xA;&#xA;&amp;gt; estimator&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Returns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=None,&#xA;       estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,&#xA;     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',&#xA;     random_state=None, tol=0.0001, verbose=0))]),&#xA;       fit_params={}, iid=True, loss_func=None, n_jobs=1,&#xA;       param_grid={'clf__penalty': ['l1', 'l2'], 'clf__loss': ['l1', 'l2'], 'clf__C': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9]), 'pca__n_components': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,&#xA;       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,&#xA;       35, 36, 37, 38]), 'pca__whiten': [True, False]},&#xA;       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,&#xA;       verbose=0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But when I try to train data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;estimator.fit(train, train_labels)&#xA;The error is:&#xA;&#xA;    428         for test_fold_idx, per_label_splits in enumerate(zip(*per_label_cvs)):&#xA;    429             for label, (_, test_split) in zip(unique_labels, per_label_splits):&#xA;--&amp;gt; 430                 label_test_folds = test_folds[y == label]&#xA;    431                 # the test split can be too big because we used&#xA;    432                 # KFold(max(c, self.n_folds), self.n_folds) instead of&#xA;&#xA;IndexError: too many indices for array&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can anyone point me in the right direction?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When classifying instances, the attribute with the missing value is simply not included in the probability calculation (&lt;a href=&quot;http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In training, &quot;the instance [with the missing data] is not included in frequency count for attribute value-class combination.&quot; (&lt;a href=&quot;http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html&quot; rel=&quot;nofollow&quot;&gt;http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does that mean that particular training record simply isn't included in the training phase? Or does it mean something else?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking to graph and interactively explore live/continuously measured data. There are quite a few options out there, with plot.ly being the most user-friendly. Plot.ly has a fantastic and easy to use UI (easily scalable, pannable, easily zoomable/fit to screen), but cannot handle the large sets of data I'm collecting. Does anyone know of any alternatives?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have MATLAB, but don't have enough licenses to simultaneously run this and do development at the same time. I know that LabVIEW would be a great option, but it is currently cost-prohibitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database. But it seams Neo4j doesn't scale well. The alternative I found out are Titan  and oriebtDB. I have gone through &lt;a href=&quot;http://db-engines.com/en/system/Neo4j%3BOrientDB%3BTitan&quot; rel=&quot;noreferrer&quot;&gt;this&lt;/a&gt; comparison on these three Databases, But I would like to get more details on  these databases. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support of these databases. Also is there any other good database options ?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;With increasingly sophisticated methods that work on large scale datasets, financial applications are obvious. I am aware of machine learning being employed on financial services to detect fraud and flag fraudulent activities but I have a lesser understanding of how it helps to predict the price of the stock the next day and how many stocks of a particular company to buy. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do the hedge funds still employ portfolio optimization techniques that are right out of the mathematical finance literature or have they started to use machine learning to hedge their bets? More importantly, what are the features that are used by these hedge funds and what is a representative problem set up? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I was wondering if anyone knew which piece of software is being used in this video? It is an image recognition system that makes the training process very simple.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098&quot; rel=&quot;nofollow&quot;&gt;http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example is with car images, though the video should start at the right spot.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm training random forest models in R using randomForest() with 1000 trees and data.frames with about 20 predictors and 600K rows. On my laptop everything works fine but when I move to amazon ec2 to run the same thing i get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error: cannot allocate vector of size 5.4 Gb&#xA;Execution halted&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm using the c3.4xlarge instance type so it's pretty beefy... does anyone know a workaround for this to get it to run on this instance? I would love to know the memory nuances that causes this problem only on the ec2 instance and not on my laptop (OS X 10.9.5 Processor  2.7 GHz Intel Core i7; Memory  16 GB 1600 MHz DDR3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a non-function (not in closed form) that takes in a few parameters (about 20) and returns a real value. A few of these parameters are discrete while others are continuous. Some of these parameters can only be chosen from a finite space of values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I don't have the function in closed form, I cannot use any gradient based methods. However, the discrete nature and the boxed constraints on a few of those parameters restrict even the number of derivative free optimization techniques at my disposal. I am wondering what are the options in terms of optimization methods that I can use. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;My apologies in advance as I am new to this. I have searched the internet and tried various processes and nothing seems to work or address this situation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset of 30,000 transactions and 500,000 items. Average item size for a transaction is 50. The dataset is sparse, so the support number must be set quite low. Furthermore, the rules become more valuable the larger the number of items in the rule. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried running this in arules and the tests fail after exceeding 64 gb of RAM (the limit of the machine). I have tried reducing items and transactions to smaller subsets, but still hit this memory limit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately, I am looking for ways to cluster large groups of similar accounts by selection of items and generate confidence and lift of various next items selected from those clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question: are there alternative, more efficient ways to do this, or other approaches to consider?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;h3&gt;Background&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I'm working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the things I've been working on is clustering of these time series. My work is academic for the moment, and while I'm doing other analysis of the data as well, I have a specific goal to carry out some clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I've found several papers related to this.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Question&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case the above is unclear, consider these examples:&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example 1&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example 2&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Possible Directions&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I've wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I'd need to be able to detect such changes. Also, I just don't know if this is a suitable way or working with the data.&lt;/li&gt;&#xA;&lt;li&gt;I've also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I'd then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I &lt;em&gt;think&lt;/em&gt; this would work, but it's potentially quite onerous and I'd hate to go down this path if there's a better way that I'm missing.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Further Notes&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;These are things that have come up in comments, or things I've thought of due to comments, which might be relevant. I'm putting them here so people don't have to read through everything to get relevant information.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I'm working in Python, but have rpy for those places where R is more suitable. I'm not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I'm happy to figure out implementation details myself.&lt;/li&gt;&#xA;&lt;li&gt;I have a lot of working &quot;rough draft&quot; code - I've done some DTW runs, I've done a couple of different types of clustering, etc. I think I largely understand the direction I'm taking, and what I'm really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).&lt;/li&gt;&#xA;&lt;li&gt;I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: &lt;a href=&quot;http://www.cs.ucr.edu/~eamonn/selected_publications.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.cs.ucr.edu/~eamonn/selected_publications.htm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is that I have an over-represented category which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one category, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My first question:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My second question:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as &quot;Entertainment&quot;, it might receive &quot;News&quot; now). Is this indicative of an error in my implementation, or expected?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to figure out a strange phenomenon, when I use matrix factorization (the Netflix Prize solution) for a rating matrix: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$R = P^T * Q + B_u + B_i$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with ratings ranging from 1 to 10.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I evaluate the model by each label's absolute mean average error in test set, the first column is origin_score, the second(we don't transform the data, then train and its  prediction error), the third(we transform the data all by dividing 2, train, and when I use this model to make prediction, firstly reconstruct the matrix and then just multiply 2 and make it back to the same scale)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you see, in grade 3-4 (most samples are label from 3-4), it's more precise while in high score range(like 9 and 10, just 2% of the whole traiing set), it's worse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;+----------------------+--------------------+--------------------+&lt;br&gt;&#xA;| rounded_origin_score | abs_mean_avg_error | abs_mean_avg_error  | &#xA;+----------------------+--------------------+---------------------+&lt;br&gt;&#xA;| 1.0                  | 2.185225396100167  |  2.559125413626183  |&#xA;| 2.0                  | 1.4072212825108161 |  1.5290497332538155 |&#xA;| 3.0                  | 0.7606073396581479 |  0.6285151230269825 |&#xA;| 4.0                  | 0.7823491986435621 |  0.6419077576969795 |&#xA;| 5.0                  | 1.2734369551159568 |  1.256590210555053  |&#xA;| 6.0                  | 1.9546560495715863 |  2.0461809588933835 |&#xA;| 7.0                  | 2.707229888048017  |  2.8866856489147494 |&#xA;| 8.0                  | 3.5084244741417137 |  3.7212155956153796 |&#xA;| 9.0                  | 4.357185793060213  |  4.590550124054919  |&#xA;| 10.0                 | 5.180752400467891  |  5.468600926567884  |&#xA;+----------------------+--------------------+---------------------+&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've re-train the model several times, and got same result, so I think it's not effect by randomness.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    date        timeframe  fruit_amt   veg_amt &amp;lt;br/&amp;gt; &#xA;4352  05/23/2013  before     0.25        0.75 &amp;lt;br/&amp;gt; &#xA;5002  05/24/2014  after      0.06        0.25 &amp;lt;br/&amp;gt; &#xA;4352  04/16/2014  after      0           0 &amp;lt;br/&amp;gt; &#xA;4352  05/23/2013  after      0.06        0.25 &amp;lt;br/&amp;gt; &#xA;5002  05/24/2014  before     0.75        0.25 &amp;lt;br/&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Desired:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    B_fr05/23/2013   B_veg05/23/2013  A_fr05/23/2013  A_veg05/23/2013   B_fr05/24/2014   B_veg05/24/2014   (etc)  &amp;lt;br/&amp;gt;&#xA;4352  0.25             0.75             0.06            0.25              .                .  &amp;lt;br/&amp;gt;&#xA;5002  .                .                .               .                 0.75             0.25 &amp;lt;br/&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I recently read a lot about the &lt;strong&gt;n-armed bandit problem&lt;/strong&gt; and its solution with various algorithms, for example for webscale content optimization. Some discussions were referring to '&lt;strong&gt;contextual bandits&lt;/strong&gt;', I couldn't find a clear definition what the word 'contextual' should mean here. Does anyone know what is meant by that, in contrast to 'usual' bandits?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I was looking to learn about Bayesian theory in decision tree and how it avoids overfitting but couldn't find any tutorials for someone just starting. Do you know any resources to learn about it?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I would like to pick up on the topic of deep learning. Should I begin from the topic of AI before working my way into Deep learning?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am working on a project with two data sets. A time vs. speed data set (let's call it traffic), and a time vs. weather data set (called weather). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking to find a correlation between these two sets using Pig. However the traffic data set has the time field, D/M/Y hr:min:sec, and the weather data set has the time field, D/M/Y. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to this I would like to average the speed per day and put it into a single D/M/Y value inside the traffic file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then plan to use:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;data = JOIN speed BY day, JOIN weather BY day with 'merge'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I will then find the correlation using: (I am borrowing this code from elsewhere)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;set = LOAD 'data.txt' AS (speed:double, weather:double)&#xA;rel = GROUP set ALL&#xA;cor = FOREACH rel GENERATE COR(set.speed, set.weather)&#xA;dump cor;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my first experience with Pig (I've never even used SQL), so I would like to know a few things:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. How can I merge the rows of my traffic file (ie. average D/M/Y hr:min:sec into D/M/Y)?&#xA;2. Is there a better way to find a correlation between the fields of different datasets?&#xA;3. Are the JOIN BY and the COR() functions used appropriately in my above code?  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am new to data science/ machine learning world. I know that in Statistics we assume that a certain event/ process has some particular distribution and the samples of that random process are part of some sampling distribution. The findings from the data could then be generalized by using confidence intervals and significance levels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we generalize our findings once we &quot;learn&quot; the patterns in the data set? What is the alternative to confidence levels here?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to determine what is the best number of hidden neurons for my MATLAB neural network. I was thinking to adopt the following strategy:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Loop for some values of hidden neurons, e.g. 1 to 40;&lt;/li&gt;&#xA;&lt;li&gt;For each NN with a fixed number of hidden neurons, perform a certain number of   training (e.g. 40, limiting the number of epoch for time reasons: I was thinking to doing this because the network seems to be hard to train, the MSE after some epochs is very high)&lt;/li&gt;&#xA;&lt;li&gt;Store the MSE obtained with all the nets with different number of hidden neurons&lt;/li&gt;&#xA;&lt;li&gt;Perform the previous procedure more than 1 time, e.g. 4, to take into account the initial random weight, and take the average of the MSEs&lt;/li&gt;&#xA;&lt;li&gt;Select and perform the &quot;real&quot; training on a NN with a number of hidden neurons such that the MSE previously calculated is minimized&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The MSE that I'm referring is the validation MSE: my samples splitting in trainining, testing and validation to avoid overfitting is 70%, 15% and 15% respectively)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other informations related to my problem are:&lt;br&gt;&#xA;fitting problem&lt;br&gt;&#xA;9 input neurons&lt;br&gt;&#xA;2 output neurons&lt;br&gt;&#xA;1630 samples  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This strategy could be work? Is there any better criterion to adopt? Thank you&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: Test done, so the result suggest me to adopt 12 neurons? (low validation MSE and  number of neurons lower than 2*numberOfInputNeurons? but also 18 could be good...&#xA;&lt;img src=&quot;https://i.stack.imgur.com/qxOSV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In &lt;a href=&quot;http://www.wired.com/2014/01/how-to-hack-okcupid/all/&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; article, Chris McKinlay says he used AdaBoost to choose the proper &quot;importances&quot; of questions he answered on okcupid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you haven't read and don't want to read the article, or are unfamiliar with okcupid and the question system, here's the data and problem he had:&#xA;The goal is to &quot;match&quot; as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions. These questions may have between 2 and 4 answers each, and for the sake of simplicity, let's pretend that the formula for a match% $\\\\ M $ between you and another user is given by&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\ M = Q_a/Q_c $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\\\\ Q_c $ is the number of questions you and the other user have in common, and&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\ Q_a $ is the number of questions you both answered with the same value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The real formula is slightly more complex, but the approach would be the same regarding &quot;picking&quot; a correct answer (he actually used boosting to find the ideal &quot;importance&quot; to place on a given question, rather than the right answer).&#xA;In any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of $\\\\ M $ over all users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I've watched the MIT course on AI up to and including the lecture on boosting, but I don't understand how you would apply it to a problem like this. Honestly I don't even know where to begin with choosing rules for the weak learners. I don't have any &quot;rules&quot; about what values to choose for each question (if the user is under 5'5, choose A, etc) - I'm just trying to fit the data I have.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this not the way boosting is supposed to be used? Is there likely some other optimization left out of how he figured this out?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;If I execute the following code I have no problem:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require(foreign)&#xA;require(nnet)&#xA;require(ggplot2)&#xA;require(reshape2)&#xA;&#xA;ml &amp;lt;- read.dta(&quot;http://www.ats.ucla.edu/stat/data/hsbdemo.dta&quot;)&#xA;ml$prog2 &amp;lt;- relevel(ml$prog, ref = &quot;academic&quot;)&#xA;test &amp;lt;- multinom(prog2 ~ ses + write, data = ml)&#xA;predict(test, newdata = dses, &quot;probs&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but if I try:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require(caret)&#xA;ml &amp;lt;- read.dta(&quot;http://www.ats.ucla.edu/stat/data/hsbdemo.dta&quot;)&#xA;ml$prog2 &amp;lt;- relevel(ml$prog, ref = &quot;academic&quot;)&#xA;test &amp;lt;- train(prog2 ~ ses + write,method=&quot;multinom&quot; ,data = ml)&#xA;predict(test$finalModel, newdata = dses, &quot;probs&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it returns &lt;code&gt;Error in eval(expr, envir, enclos) : object 'sesmiddle' not found&lt;/code&gt;, why?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I have done so far is to break each serie into daily data, for exemple: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;from:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2013-03-03 - 2013-03-09 37&lt;/p&gt;&#xA;&#xA;&lt;p&gt;to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2013-03-03 37 &#xA;2013-03-04 37&#xA;2013-03-05 37&#xA;2013-03-06 37&#xA;2013-03-07 37&#xA;2013-03-08 37&#xA;2013-03-09 37&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other approach would be to &quot;merge&quot; daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don't know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So three questions: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel like breaking weekly/monthly data into daily data like i've done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last but not least: &lt;strong&gt;when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?&lt;/strong&gt; I think this is a compromise between the number of data and the complexity of the model but I can't see any strong argument to choose between those options. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to predict clients comportement from market rates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The value of the products depends on the actual rate but this is not enough. The comportement of the client also depends on their awareness wich depends on the evolution of rates. I've added this in model using past 6 month rates as features in polynomial regression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact media coverage of rate mostly depends on rate variations and I wanted to add that in my model.  The idea would be to add a derivative/variation of rate as a feature. But I anticipated something wrong, example with only two month , my variation will be of the form $x_n - x_{n-1}$ that is a simple linear combination of actual and past rates. So for a 1d polynomial regression i will have:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ x_{n+1} = a * x_{n} +  b * x_{n-1} + c * (x_{n} - x_{n-1})$$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;instead of: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ x_{n+1} = a_0 * x_{n} +  b_0 * x_{n-1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;wich is strictly equivalent with $ a + c = a_0 $ and $b-c= b_0$. Higher polynomial degree results in a more or less equivalent result. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking about a way to include derivative information but it seems not possible. So I am wondering if all the information is included in my curve. Is this a general idea ? all information is somewhat directly contained in data and modifications of features will result in higher order objective function ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to predict a time serie from another one. My approach is based on a moving windows. I predict the output value of the serie from the following features: the previous value and the 6 past values of the source serie. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it usefull to add the previous value of the time serie ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel like I don't use all the information contained in the curve to predict futures values. But I don't see how it would be possible to use all previous data to predict a value (first, the number of features would be growing trough time...).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the caveats of a 6 month time-window approach ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any paper about differents method of feature selection for time-series ? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Let's assume I'm building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it's a history of which pages each user has viewed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I'd like to explore alternatives to see which method gives the most relevant recommendations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition to my simple traversal, I'm curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I've looked at &lt;a href=&quot;http://snap.stanford.edu/&quot; rel=&quot;nofollow&quot;&gt;SNAP&lt;/a&gt;, it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Recently, I studied a paper called &quot;What Does Your Chair Know About Your Stress Level?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be download at the link below.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;at page 210, Fig.5 (picture 5) mentioned that&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Spectra of the norm of the CoP vector during the experiment (stages 3–7) for the same subject used in Fig. 4.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How did they do to transform Fig.4 to Fig.5? And what do x-axis and y-axis mean in Fig.5?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fig4&#xA;&lt;img src=&quot;https://i.stack.imgur.com/Iq7qc.png&quot; alt=&quot;Fig4&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fig5&#xA;&lt;img src=&quot;https://i.stack.imgur.com/7HPxu.png&quot; alt=&quot;Fig5&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a &lt;strong&gt;dataset of xyz coordinates with a date component&lt;/strong&gt; in a pandas dataframe&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ex:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;date1: $[x_1,y_1,z_1]$,&lt;/li&gt;&#xA;&lt;li&gt;date2: $[x_2,y_2,z_2]$,&lt;/li&gt;&#xA;&lt;li&gt;date3: $[x_3,y_3,z_3]$,&#xA;..&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I would like to &lt;strong&gt;classify a sample of object positions over the period of a week&lt;/strong&gt;&#xA;(using indexes to re-map the classification label back to the date), like this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Week 1: $[x_1,y_1,z_1], [x_2, y_2, z_2], [x_3,y_3,z_3], [x_4,y_4,z_4], [x_5,y_5,z_5], [x_6,y_6,z_6], [x_7,y_7,z_7]$,&lt;/li&gt;&#xA;&lt;li&gt;Week 2: $[x_8,y_8,z_8],[x_9,y_9,z_9],[x_{10},y_{10},z_{10}],[x_{11},y_{11},z_{11}],[x_{12},y_{12},z_{12}],[x_{13},y_{13},z_{13}],[x_{14},y_{14},z_{14}]$,&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;When I try to run KMeans it returns&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;k_means = KMeans(n_clusters=cclasses)&#xA;k_means.fit(process_set.hpc)&#xA;date_classes = k_means.labels_&#xA;&#xA;ValueError: Found array with dim 3. Expected &amp;lt;= 2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?&lt;/li&gt;&#xA;&lt;li&gt;Are there any other methods I could use?&lt;/li&gt;&#xA;&lt;li&gt;Am I doing everything completely backwards and should consider a different approach, any thoughts?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Imagine modeling the &quot;&lt;em&gt;input(plaintext) - output(ciphertext)&quot;&lt;/em&gt; pairs of an encryption algorithm as a data science problem. Very informally, the strength of an encryption scheme is measured by the randomness (unpredictability, or entropy) of the output. This is counter-intuitive to classic regression problems, which are frequently used for prediction. Say, informally, the strength of an encryption scheme is determined by the number of such input-output pairs needed beyond which it becomes predictable. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we model this as data science problem? Given all pairs of two different encryption schemes, can we determine which is stronger just by using the input-output pairs of both the schemes? Is there any other way apart from regression to solve this problem?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;while comparing two different algorithms to feature selection I stumbled upon the follwing question: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a given dataset with a discrete class variable we want to train a naive bayes classifier. We decide to conduct feature selection during preprocessing using naive bayes in a wrapper approach. &#xA;Does this method of feature selection consider the size of the used feature subsets? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/2Vh2Q.png&quot; alt=&quot;Naive Bayes Classification&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When considering how NB classifies a given instance, the size of the feature subset being used for classification only influences the number of parts that the product of the conditional dependencies has but that does not make a difference, or does it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It'd be great if someone could offer a solid explanation since for me it's more of a gut feeling at the moment.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the &lt;strong&gt;captions in the properties&lt;/strong&gt; section.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Create statement:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LOAD CSV WITH HEADERS FROM &quot;File Location&quot; AS row CREATE (:NodeName {DATE_OCCURED: row.Date});&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Query to visualize the relationship between nodes:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MATCH (a)-[:`REL NAME`]-&amp;gt;(b) RETURN a,b LIMIT 25;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This gives me the values the node &quot;a&quot; and random numbers for all the node &quot;b&quot;. I don't know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don't get the actual values of the related nodes.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/xxKVi.png&quot; alt=&quot;Sample output for the relationship query&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;What's the best / easiest way to create a graph from address data?  For example, if I have 100 houses from all across a city is there any easy way to determine the shortest distance between two houses and all that good stuff?  Would this require changing the data into coordinates and using GIS software or can I get away with using Python or R?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm wondering if e-commerce companies where products are offered by users, such as EBay, are using Object Recognition to ensure that an uploaded image corresponds to an specific type of object (clothing, shoes, glasses, etc) either to classify automatically or more importantly to filter undesired images (such as non related or even illegal types).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, which algorithms and/or open platforms could be use for doing so? From what I've looked it seems that HOG+Exemplar SVM might be one of the most accurate methods developed so far (&lt;a href=&quot;http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf&lt;/a&gt;), even having couple of public repo's with Matlab implementations (&lt;a href=&quot;https://github.com/quantombone/exemplarsvm&quot; rel=&quot;nofollow&quot;&gt;https://github.com/quantombone/exemplarsvm&lt;/a&gt;), but I'm still wondering if this is being used in industry.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;This refers to a system described in a book by Nick Lawrence titled &quot;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0975276107&quot; rel=&quot;nofollow&quot;&gt;Correlithm Object Technology&lt;/a&gt;&quot;. The author coined the term &quot;correlithm&quot; as a combination of &quot;correlation&quot; and &quot;algorithm&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Correlithm Objects (&quot;COs&quot;) described in the book are, in the author's view, &quot;primary data tokens&quot; in biological neural systems and are central to &quot;all high-level data representation, storage, and manipulation&quot;. In addition to this the author describes the COs as &quot;important mathematical objects of the statistics of bounded, high-dimensional spaces&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Considering these descriptions it seems like these COs could be useful for AI. What I was curious about is whether they are actually used anywhere in the industry, and if so, in what kind of situations?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am reading up about lambda architecture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It makes sense. we have queue based data ingestion. we have an in-memory store for data which is very new and we have HDFS for old data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we have our entire data set. in our system. very good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but the architecture diagram shows that the merge layer is able to query both the batch layer and the speed layer in one shot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your batch layer is probably a map reduce job or a HIVE query. The speed layer query is probably a scala program which is execution on the spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how will you merge these?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any guidance.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;For an upcoming project, I'm mining textual posts from an online forum, using Scrapy. What is the best way to store this text data? I'm thinking of simply exporting it into a JSON file, but is there a better format? Or does it not matter?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Im doing my academic project. im having the base paper for reference  the paper is IEEE paper &quot;effective and efficient clustering methods for correlated probabilistic graph&quot;. i wish to do this in R tool. in this paper two algorithm are implemented. i like to implement the peedr algorithm in the paper. how can i give the input for that algorithm.? suggest the packgages in R tool&#xA; the paper can be found here&lt;br&gt;&#xA;&lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474&quot; rel=&quot;nofollow&quot;&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474&lt;/a&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to develop my neural network with both early stopping and bayesian regularization (matlab implementation, lm algorithm is used for both).&#xA;Since in bayesian regularization I have not the validation set, how can I compare the generalization capability of the networks obtained with the two methodologies? &#xA;Thanks&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm building a neural network to analyze a business' sales. I'm normalizing all input values to the range &lt;code&gt;{0,1}&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm struggling with the day of the week column. Business days are identified by a number ranging &lt;code&gt;{1-5}&lt;/code&gt; (1=Monday). Normalizing these values to the range &lt;code&gt;{0,1}&lt;/code&gt; is straightforward, but results in a major bias in the final output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason is that the full range of normalized values for the business day column is explored with every week worth of data, whereas other price-related column explore their full range of normalized values infrequently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The business day column ends up being the largest contributor to the final output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I normalize it to make its contribution more in tune with the rest of the inputs?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;It is often pointed out that &lt;em&gt;sample&lt;/em&gt; is an overloaded term in statistics and the sciences being supported by statistics.  In my field (geological sciences) as in most other sciences, the process of collecting meaningful data is critical and discussions about the traps and pitfalls in that process talk about &lt;em&gt;sampling&lt;/em&gt;.  Not far down the road from that, particularly when lab results are back, conversations involving statisticians, data scientists, geomathematicians, GIS analysts and even &lt;em&gt;normal&lt;/em&gt; geologists are likely to attempt to include multiple meanings of &lt;em&gt;sample&lt;/em&gt; in the same sentence!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Q: Have any data scientists (or statisticians) found practical ways to communicate these different meanings?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way is to always add soil, rock, statistical and so on before &lt;em&gt;sample&lt;/em&gt;.  But I was curious if there are any other approaches to effective communication that are in use.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am interested in finding a statistic that tracks the unpredictability of a time series. For simplicity sake, assume that each value in the time series is either 1 or 0. So for example, the following two time series are entirely predictable&#xA;TS1: 1 1 1 1 1 1 1 1&#xA;TS2: 0 1 0 1 0 1 0 1 0 1 0 1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the following time series is not that predictable:&#xA;TS3: 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for a statistic that given a time series, would return a number between 0 and 1 with 0 indicating that the series is completely predictable and 1 indicating the series in completely unpredictable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked at some entropy measures like Kolmogorov Complexity and Shannon entropy, but neither seem to fit my requirement. In Kolmogorov complexity, the statistic value changes depending on the length of the time series (as in &quot;1 0 1 0 1&quot; and &quot;1 0 1 0&quot; have different complexities, so its not possible to compare predictability of two time series with differing number of observations). In Shannon entropy, the order of observations didn't seem to matter. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers on what would be a good statistic for my requirement?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a data set, in excel format, with &lt;strong&gt;account names, reported symptoms, a determined root cause and a date in month year format&lt;/strong&gt; for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Account : &lt;em&gt;X&lt;/em&gt; Symptoms : &lt;em&gt;AB, AD, AB, AB&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Account : &lt;em&gt;Y&lt;/em&gt;  Symptoms : &lt;em&gt;AE, AE, AB, AB, EA&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the sake of this example, let's assume that all the dates are this month. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;O/P: Account : X Symptom: AE&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here both of them have reported &lt;em&gt;AB&lt;/em&gt; 2 or more times. I could fix such number as a threshold to look for probable symptoms. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to do a correlation analysis between inputs and outputs inspecting the data in order to understand which input variables to include. What could be a threshold in the correlation value to consider a variable eligible to be an input for my Neural Network?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is there any way to use package &quot;dplyr&quot; on RStudio having R base 3.0.2 ?  I am not interested  in &quot;plyr&quot; package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&#xA;Navin&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Could anyone recommend a good similarity measure for objects which have multiple classes, where each class is part of a hierarchy?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, let's say the classes look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 Produce&#xA;  1.1 Eggs&#xA;    1.1.1 Duck eggs&#xA;    1.1.2 Chicken eggs&#xA;  1.2 Milk&#xA;    1.2.1 Cow milk&#xA;    1.2.2 Goat milk&#xA;2 Baked goods&#xA;  2.1 Cakes&#xA;    2.1.1 Cheesecake&#xA;    2.1.2 Chocolate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;An object might be tagged with items from the above at any level, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Omelette: eggs, milk (1.1, 1.2)&#xA;Duck egg omelette: duck eggs, milk (1.1.1, 1.2)&#xA;Goat milk chocolate cheesecake: goat milk, cheesecake, chocolate (1.2.2, 2.1.1, 2.1.2)&#xA;Beef: produce (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the classes weren't part of a hierarchy, I'd probably I'd look at cosine similarity (or equivalent) between classes assigned to an object, but I'd like to use the fact that different classes with the same parents also have some similarity value (e.g. in the example above, beef has some small similarity to omelette, since they both have items from the class '1 produce').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it helps, the hierarchy has ~200k classes, with a maximum depth of 5.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a project where we would like to take the ratio of two measurements A/B and subject these ratios to a ranking algorithm. The ratio is normalized prior to ranking (though the ranking/normalization are not that import to my question).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In most cases measurement A (the starting measurement) is a count with values greater than 1000.  We expect an increase for measurement B for positive effects and a decrease in measurement B for negative effects.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the issue, some of our starting counts are nearly zero which we believe is an artifact of experimental preparation.  This of course leads to some really high ratios/scaling issues for these data points.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best way to adjust these values in order to better understand the real role in our experiment?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One suggestion we received was to add 1000 to all counts (from measurement A and B) to scale the values and remove the bias of such a low starting count,  is this a viable option?  Thank you in advance for your assistance, let me know if I am not being clear enough.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I would like to know if you people have some good tutorials (fast and straightforward) about topic models and LDA, teaching intuitively how to set some parameters, what they mean and if possible, with some real examples.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;(Me: Never learned calculus or advanced math and I started Stanford openclasses for machine learning. I know basic matrix calculations.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One chapter of my course is about cost function. I have been trying to find any example calculation of it with numbers. Googling only finds the same formula everytime, and also on Octave. But I want to do the same thing first with pen+paper and without it, I cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I require a cost function calculation example for following sample dataset:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#Rooms = Rent&#xA;1 = 4000&#xA;2 = 10000&#xA;3 = 22000&#xA;4 = 30000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to build a home server/workstation to run my R projects. Based on what I have gathered, it should probably be Linux based. I want to buy the hardware now, but I am confused with the many available options for processors/ram/motherboards. I want to be able to use parallel processing, at least 64GB? of memory and enough storage space (~10TB?). Software wise, Ubuntu?, R, RStudio, PostgreSQL, some NOSQL database, probably Hadoop. I do a lot of text/geospatial/network analytics that are resource intensive. Budget ~$3000US.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My Questions:&lt;/strong&gt;&lt;BR&gt;&#xA;What could an ideal configuration look like? (Hardware + Software)&lt;br&gt;&#xA;What type of processor?&lt;BR&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;BR&gt;&#xA;No, I don't want to use a cloud solution.&lt;BR&gt;&#xA;I know it is a vague question, but any thoughts will help, please?&lt;BR&gt;&#xA;If it is off-topic or too vague, I will gladly delete.&lt;BR&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cheers B&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it's hard to get a bird's eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All these vector space models tend to use cosine-similarity as the similarity function, however I have seen some literature discussing similarity functions which are more exotic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this basically where we currently are in attacking this particular type of problem? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a mysql database with the following format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id      string&#xA;1        foo1...&#xA;2        foo2...&#xA;..       ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are &gt;100k entries in this db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to do is for each string, compare it to each other string and store some metric of the comparison.  Doing this will essentially yield a 2D matrix of size &lt;code&gt;NxN&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is the number of row in the db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My initial thought was creating another db where each index corresponds to the string of the index in the first db and each column is the value from comparing the two strings. For example, id 1 column 2 in the second db would be the value outputted from comparing id1 and id2 in the first db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The format of the second db:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id    col1    col2    col3    ....&#xA;1       1      0.4     0.5    .....&#xA;...    ...     ...      ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This way of creating the second db would result in 100k rows x 100k columns, which is the issue at hand.  What is the best way to handle large data sets like this?  Is storing the data in a text file more efficient (say each text file corresponds to one row in the second db.)&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am currently collecting second by second data regarding buyer vs seller initiated trades for different financial instruments (securities mostly). If there are more buyer initiated trades in a given second, then that second's data point would contain a positive value in the pertinent feature. If there are more seller initiated trades, then there would be a negative value. And if either there is an equal amount of buy vs seller initiated trades OR if there are simply not any trades in a given second, there will be a 0 for the feature in that data point. Along with this feature, there are several other features that are based on what occurred in the preceding seconds (eg if the value discussed above was 12 for the data point immediately preceding the current point, then the second feature for the current data point would be 12 - please let me know if this is not clear) After much troubleshooting, I have concluded that if there are too many data points with too many 0's for features, the classifier simply wont work. When I print out the probabilities of evaluation data points falling into different classes, I simply get&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0:NaN,1:NaN&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;for all model evaluation points I try to classify. (I am using logistic regression from apache-mahout. In total have 183 features, but over 40million data points. There are three categories to which the data point can be classified)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have found that if I set the default value to 1, then I no longer encounter this error, e.g. if there are no trades, the value will be a 1, if there is one seller initiated trade, the value will be 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So with all this in mind, I have two related questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Has anyone else encountered this issue? e.g. if you have a vector with x features, and for a majority of the data points, a majority of the features contain 0's, is this know to give issues?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is shifting all values up by a constant (such as 1) a valid fix to this issue? I assume that if this constant is applied to all values, then it shouldn't skew the data, but I figure it won't hurt to check with the experts.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Also, I'm new to this, so if you believe that my question could use more info please let me know, and if you could give me ideas of what information to include, it would be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;thanks in advance&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am very new in machine learning. I have annotated data with category, aspect, opinion word and sentiment. for example, for the bellow text&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;The apple was really tasty&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have category-&gt;food, aspect-&gt; apple, opinion word -&gt;tasty and sentiment-&gt;positive. I have training data like this format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I train a SVM classifier using this format of training set?&#xA;How to extract features like n-gram, POS and sentiment word to train the classifier?&#xA;Could you please suggest any beginning step for this aspect based sentiment analysis using machine learning algorithms?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have 11 lottery tickets (used) and I have discovered that in each ticket, the 3rd digit's value is +1 of the value of the 6th digit. I have 11 tickets, each ticket is composed of 16 digits. Would someone (anyone) help me find the logic used here, in creating the other digits and their interrelation? I am a complete noob in data analysis, and any help would be greatly appreciated. For those who would like to know, I incidentally discovered the certain pattern, and made me realize these numbers are not totally random and if I could find the underlying pattern I'd be able to predict. The excel file is here &lt;a href=&quot;http://s000.tinyupload.com/index.php?file_id=39010250377074241779&quot; rel=&quot;nofollow&quot;&gt;Excel file&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How can I get information about an entity from DBpedia using Python?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eg: I need to get all DBpedia information about USA  (&lt;a href=&quot;http://dbpedia.org/page/United_States&quot; rel=&quot;nofollow&quot;&gt;http://dbpedia.org/page/United_States&lt;/a&gt;) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PREFIX db: &amp;lt;http://dbpedia.org/resource/&amp;gt;&#xA;SELECT ?p ?o&#xA;WHERE { db:United_States ?p ?o }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But here all DBpedia information is not displaying.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I do this and which all are the possible plugins/api available for python to connect with DBpedia ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also what will be the SPARQL query for generating the above problem result?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Given a dataset that has a binary (0/1) dependent variable and a large collection of continuous and categorical independent variables, is there a process and ideally a R package that can find combinations/subsets/segments of the IVs that are highly correlated with the DV?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple example: &#xA;DV: college education (0/1), and IVs: age (20 to 120), income (0 to 1 million), race (white, black, hispanic etc), gender (0/1), state, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then finding correlations combining IVs and subsets of IVs (e.g. women between 30 and 50, with incomes over 100k are highly positively correlated with the DV), and then being able to compare the combinations (e.g. to find out women between 30 and 40, with incomes over 100k have a higher correlation than women between 40 and 50, with incomes over 100k)&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have data, which looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqH9I.jpg&quot; alt=&quot;Data table&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These data are only for one subject. I will have a lot more.&lt;br&gt;&#xA;These data will be analyzed in R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I'm storing them like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;subject &amp;lt;- rep(1, times = 24)&#xA;measurement &amp;lt;- factor(x = rep(x = 1:3, each = 8), &#xA;                      labels = c(&quot;Distance&quot;, &quot;Frequency&quot;, &quot;Energy&quot;))&#xA;speed &amp;lt;- factor(x = rep(x = 1:2, each = 4, times = 3), &#xA;                labels = c(&quot;speed1&quot;, &quot;speed2&quot;))&#xA;condition &amp;lt;- factor(x = rep(x = 1:2, each = 2, times = 6), &#xA;                    labels = c(&quot;Control&quot;, &quot;Experm&quot;))&#xA;Try &amp;lt;- factor(x = rep(x = 1:2, times = 12), &#xA;              labels = c(&quot;Try1&quot;, &quot;Try2&quot;))&#xA;result &amp;lt;- c(1:8, &#xA;            11:18, &#xA;            21:28)&#xA;&#xA;dt &amp;lt;- data.frame(subject, measurement, speed, condition, Try, result)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the appropriate way to store these data in R (in a data frame)? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the proper name for that? Which are examples of algorithms for the job?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&#xA;More specifically, I want to train audio source separation. The input is a mixed sound (spectrogram) and the output is the sound with some energy removed in certain frequencies. The function needs to recognize some pattern in the input and decide what to remove.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm wondering if there's a way to automatically generate a list of tags for live chat transcripts without domain knowledge.  I've tried applying NLP chunking to the chat transcripts and keep only the noun phrases as tag candidates.  However, this approach would generate too many useless noun phrases.  I could use some rules to prune out some of them, but it would be hard to generalize the rules.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;This question might sound silly. But I have been wondering why do we assume that there is&#xA;a hidden probability distribution between input-output pairs in machine learning setup ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if we want to learn a function $f: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{Y}$, we generally tend to assume a probability distribution $\\\\rho(x,y)$ on $Z=\\\\mathcal{X} \\\\times \\\\mathcal{Y} $ and try to minimize the error &#xA;$$&#xA;\\\\mathcal{E}(f) = \\\\int (f(x)-y)^2 \\\\ d\\\\rho(x,y)&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the probability distribtution $\\\\rho$ inherent to the very nature of $Z$ or depends on $f$ ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone please provide a good intuitive explanation for this ?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am currently working on a recommendation system for daily news. At first, I evaluated all the recommender algorithms and their corresponding settings (e.g., similarities, factorizers, ...etc) implemented in Mahout. Since we want to recommend daily news for users, we use the reading behavior of each user collected two days ago as training set, data of the next day as the testing set. The evaluated RMSE is good, the best recommender is SVD+SGD, so we implemented the recommender on our system for several days of trial run.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the result, the actually recommended news, seems to be not very attractive for real users (&quot;not attractive&quot; here means, the users feel like &quot;why you recommend this to me?&quot;). So we decided another approach: use the tags and categories and their relationship to do the main job of recommendation, the result from CF is for just supporting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This makes me wonder if CF if not appropriate for some kind of content. Because I also worked on movie and music recommendation, CF is a good tool. But for news, it seems not the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain why this happening, and also give some guideline about how to choose appropriate recommendation methods? Thanks:)&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I was wondering whether we could list machine learning winning methods to apply in many fields of interest: NLP, image, vision, medical, deep package inspection, etc. I mean, if someone will get started a new ML project, what are the ML methods that cannot be forgotten?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a data set of video watching records in a 3G network. In this data set, 2 different kind of features are included: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;user-side information, e.g., age, gender, data plan and etc; &lt;/li&gt;&#xA;&lt;li&gt;Video watching records of these users, each of  which associated with a download ratio and some detailed network condition metrics, say, download speed, RTT, and something similar.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, Given this data set, I want to predict the download ratio of each video.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since it is a regression problem, so I use &lt;em&gt;gradient boosting regression tree&lt;/em&gt; as model and run 10-fold cross validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have tried different model parameter configurations and even different models (linear regression, decision regress tree), the best root-mean-square error I can get is 0.3790, which is quite high, because if I don't use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890. There is not obvious difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For this problem, I have some questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Does this high error rate imply that the label in data set is unpredictable? &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Can anybody tell me what is the purpose of feature generation? and why feature space enrichment is needed before classifying an image? Is it a necessary step?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any method to enrich feature space?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;is there any comprehensive framework matching tasks with approaches or methods related to different types of machine learning related problems?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn a simple Gaussian?&lt;/strong&gt; &#xA;Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn a mixture of Gaussians (MoG)?&lt;/strong&gt; Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I learn any density?&lt;/strong&gt; Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l  ́ 2 error; Kernel density estimation (KDE), optimal kernel, KDE theory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I predict a continuous variable (regression)?&lt;/strong&gt; Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I predict a discrete variable (classification)?&lt;/strong&gt; Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which loss function should I use?&lt;/strong&gt; Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which model should I use?&lt;/strong&gt; AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (combined) models?&lt;/strong&gt; Ensemble learning theory; boosting; bagging; stacking&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (nonlinear) models?&lt;/strong&gt; Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I learn fancier (compositional) models?&lt;/strong&gt; Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I reduce or relate features?&lt;/strong&gt; Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I create new features?&lt;/strong&gt; principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I reduce or relate the data?&lt;/strong&gt; Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat time series?&lt;/strong&gt; ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat non-ideal data?&lt;/strong&gt; covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize the parameters?&lt;/strong&gt; Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize linear functions?&lt;/strong&gt; computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I optimize with constraints?&lt;/strong&gt; Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I evaluate deeply-nested sums?&lt;/strong&gt; Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I evaluate large sums and searches?&lt;/strong&gt; Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I treat even larger problems?&lt;/strong&gt; Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I apply all this in the real world?&lt;/strong&gt; Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I want to visualize goal achievment progress.&#xA;This is my first idea:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;use area chart to show progress in current metric&lt;/li&gt;&#xA;&lt;li&gt;use horizontal band to show the goal value&lt;/li&gt;&#xA;&lt;li&gt;colorize areas under/above the band into &quot;positive&quot; and &quot;negative&quot; colors&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/h7fUa.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/EIhmz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is this approach informative enough? Are there better choises?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additional info:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;charts made in Tableau&lt;/li&gt;&#xA;&lt;li&gt;two data sources: metric progress &amp;amp; goals&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am COMPLETELY new to the field of Data Science, mainly because every employer I have worked for, simply COULDN'T sell any customers anything that would use techniques learned in this field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of particular interest to me is machine learning/Predictive Analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have attempted many &quot;test projects&quot; myself, but I seem to NEED some sort of outside &quot;catalyst&quot; to tell me a specific goal, and a specific set of guidelines, when I am trying to learn something.&#xA;Otherwise, I tend to lose focus, and jump from one interesting topic to the next, without ever gaining any experience.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;In SIFT feature extraction how the key points will be generated and how the features will be stored in the database. In image will the bag of visual words be images or text words?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;As there are numerous tools available for data science tasks, and it's cumbersome to install everything and build up a perfect system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a Linux/Mac OS image with Python, R and other open-source data science tools installed and available for people to use right away? An Ubuntu or a light weight OS with latest version of Python, R (including IDEs), and other open source data visualization tools installed will be ideal. I haven't come across one in my quick search on Google.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if there are any or if someone of you have created one for yourself? I assume some universities might have their own VM images. Please share such links.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In the big data world there is lot of talk about implementing an &quot;active archive&quot;. I see cloudera talk about it a lot. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The value prop is that you move the low value (and less used) data from EDW to Hadoop and then save on expensive EDW storage by using Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in my company say we keep 10 years of data on EDW and say we don't use anything below 2 years very actively. So I move 8 years of data from EDW to Hadoop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can setup an Impala (or equivalent) product to query the 8 years of data as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the problem is how do you order, sort a query which requires some data form EDW and some data from Impala?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;because this kind of grouping, sorting ordering etc will have to be done in memory and the apps which queried the EDW were not written for operations and don't have the capacity as well to sort, group and process so much of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how are people implementing the Active Archive?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What happens when we train a basic support vector machine (linear kernel and no soft-margin) on non-linearly separable data? The optimisation problem is not feasible, so what does the minimisation algorithm return?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Due to various &lt;a href=&quot;http://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curses of dimensionality&lt;/a&gt;, the accuracy and speed of many of the common predictive techniques degrade on high dimensional data. What are some of the most useful techniques/tricks/heuristics that help deal with high-dimensional data effectively? For example,&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Do certain statistical/modeling methods perform well on high-dimensional datasets?&lt;/li&gt;&#xA;&lt;li&gt;Can we improve the performance of our predictive models on high-dimensional data by using certain (that define alternative notions of distance) or &lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernels&lt;/a&gt; (that define alternative notions of dot product)?&lt;/li&gt;&#xA;&lt;li&gt;What are the most useful techniques of dimensionality reduction for high-dimensional data?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " ('&lt;p&gt;How does varying the regularization parameter in an SVM change the decision boundary for a non-separable dataset? A visual answer and/or some commentary on the limiting behaviors (for large and small regularization) would be very helpful.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;What are the most useful techniques for learning a binary classifier from a dataset with a high degree of imbalance (i.e., a dataset with the &quot;target&quot; class being much rarer than the &quot;background&quot; class)? For example,&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Should one first down-sample the majority/background class to reduce its frequency and then readjust the probabilities reported by the learning algorithm? How should one do the readjustment?&lt;/li&gt;&#xA;&lt;li&gt;Should one use different approaches for different learning algorithms, i.e., are there different techniques for dealing with imbalance in SVM, random forests, logistic regression, etc.?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I hope this is a question appropriate for SO.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The article in question: &lt;a href=&quot;http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I can tell, the only publicly available data from Google Search is through their Trends API.  The help page states that&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don't represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However in the article, the author reports (absolute) &quot;average monthly searches&quot;.  The source is stated as:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;All monthly search numbers are approximate and derived from anonymous and aggregate web activity.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Source: analysis of Google data by (author)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, how did he get this &quot;anonymous and aggregate web activity&quot;?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am looking for a method to parse semi-structured textual data, i.e. data poorly formatted but usually having a visual structure of a matrix which may vary a lot in content and number of items in it, which may have headers or not, which may be interpreted sometimes column-wise or row-wise, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read about the WHISK information extraction paper : &lt;a href=&quot;https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdf&quot;&gt;https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but unfortunately, it is not very detailed and I have not been able to find a real-system implementing it, or even snippets of code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anybody have an idea where I can find such help? Or suggest an alternative approach which may be suited to my problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance for your reply!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The author built a model for each gesture of each player to guess which gesture would be played next. Calculating over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable for cross-fold validation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that legitimate? That seems like cheating. I would think you would have to exclude the target variables from your test set when calculating features in order to not &quot;cheat&quot;.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;My company provides managed services to a lot of its clients. Our customers typically uses following monitoring tools to monitor their servers/webapps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;OpsView&lt;/li&gt;&#xA;&lt;li&gt;Nagios&lt;/li&gt;&#xA;&lt;li&gt;Pingdom&lt;/li&gt;&#xA;&lt;li&gt;Custom shell scripts&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Whenever any issue is found, an alert mail comes to our Ops team so that they act upon rectifying the issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As we manage thousands of servers, our Ops teams' inbox is flooded with email alerts all the time. Even a single issue which has a cascading effect, can trigger 20-30 emails.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, what I want to do is to implement a system which will be able to extract important features out of an alert email - like server IP address, type of problem, severity of problem etc. and also classify the emails into proper category, like &lt;code&gt;CPU-Load-Customer1-Server2, MySQL-Replication-Customer2-DBServer3&lt;/code&gt; etc. We will then have a pre-defined set of debugging steps for each category, in order to help the Ops team to rectify the problem faster. Also, the feature extractor will provide input data to the team for a problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have been able to train &lt;em&gt;NaiveBayesClassifier&lt;/em&gt; with supervised learning techniques i.e. labeled training data(cluster data), and able to classify new unseen emails into its proper cluster/category. As the emails are based on certain templates, the accuracy of the classifier is very high. But we also get alert emails from custom scripts, which may not follow the templates. So, instead of doing supervised learning, I want to try out unsupervised learning for the same. I am looking into &lt;em&gt;KMeans clustering&lt;/em&gt;. But again the problem is, we won't know the number of clusters beforehand. &lt;strong&gt;So, which algorithm will be best for this use case?&lt;/strong&gt; Right now I am using Python's TextBlob library for classification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, for feature extraction out of an alert email, I am looking into NLTK (&lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nltk.org/book/ch07.html&lt;/a&gt;) library. I tried it out, but it seems to work on proper English paragraphs/texts well, however, for alert emails, it extracted a lot of unnecessary features. &lt;strong&gt;Is there already any existing solution for the same? If not, what will be the best way to implement the same? Which library, which algorithm?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;PS: I am not a Data Scientist.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Sample emails:&lt;/strong&gt;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.10.0.100  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: CRITICAL  Date &amp;amp; Time: Sat Oct 4 07:02:06 UTC 2014    Additional Information:     CRITICAL - load average: 41.46, 40.69, 37.91&#xA;RECOVERY: OK - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.1.1.100  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: OK  Date &amp;amp; Time: Sat Oct 4 07:52:05 UTC 2014    Additional Information:     OK - load average: 0.36, 0.23, 4.83&#xA;PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.100.10.10  Host Group Hierarchy: Opsview &amp;gt; Customer1  - BIG C &amp;gt; Customer1_PROD  State: CRITICAL  Date &amp;amp; Time: Sat Oct 4 09:29:05 UTC 2014    Additional Information:     CRITICAL - load average: 29.59, 26.50, 18.49&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Classifier code:(format of csv - email, &amp;lt;disk/cpu/memory/mysql&amp;gt;)&lt;/strong&gt;    &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from textblob import TextBlob&#xA;from textblob.classifiers import NaiveBayesClassifier&#xA;import csv&#xA;train = []&#xA;with open('cpu.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;// this can be done in a loop, but for the time being let it be&#xA;with open('memory.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;with open('disk.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;with open('mysql.txt', 'r') as csvfile:&#xA;    reader = csv.reader(csvfile, delimiter=',', quotechar='&quot;')&#xA;    for row in reader:&#xA;        tup = unicode(row[0], &quot;ISO-8859-1&quot;), row[1]&#xA;        train.append(tup)&#xA;&#xA;cl = NaiveBayesClassifier(train)&#xA;cl.classify(email)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Feature extractor code taken from:&lt;/strong&gt; &lt;a href=&quot;https://gist.github.com/shlomibabluki/5539628&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/shlomibabluki/5539628&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please let me know if any more information is required here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Do you know of any machine learning add-ins that I could use within Excel? For example I would like to be able to select a range of data and use that for training purposes and then use another sheet for getting the results of different learning algorithms.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I found that Apache-Spark very powerful in Big-Data processing. but I want to know about Dryad (Microsoft) benefits. Is there any advantage for this framework than Spark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why we must use Dryad instead of Spark?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Let's say I'm trying to predict a person's electricity consumption, using the time of day as a predictor (hours 00-23), and further assume I have a hefty but finite amount of historical measurements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I'm trying to set up a linear model akin to &lt;/p&gt;&#xA;&#xA;&lt;p&gt;power_used = hr_of_day * alpha_hr + temperature * beta&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problem: using the hr_of_day as a numerical value is a very bad idea for many reasons, the fact that 23 and 0 are actually quite close values is one problem that can be solved with a simple transformation [1]. The fact that electrical consumption is often bi-modal is another problem which isn't solved by a simple transformation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A possible solution that works rather well is to treat the time of day as a categorical variable. That does the trick, but it suffers from a significant drawback in that there's no information sharing between neighbouring hours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what I'm asking is this: does anyone know of a &quot;soft&quot; version of categorical values? I'm suggesting something quite loosely defined: Ideally I would have some parameter alpha that reduces the regression to numerical regression where alpha = 1 and reduces to categorical regression where alpha = 0, and behaves &quot;in between&quot; if it's some other number. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now the only answer I can think of is to alter the weights in the regression in such a way that they tend towards zero the further away the quasi-categorical value is from the desired value. Surely there are other approaches?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] &#xA;introduce the hour variable as two new variables: Cos(time_of_day/24) and Sin(time_of_day/24)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Both Apache-Spark and Apache-Flink projects claim pretty much similar capabilities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what is the difference between these projects. Is there any advantage in either Spark or Flink?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Twitter is a popular source of data for many applications, especially involving sentiment analysis and the like.  I have some things I'm interested in doing with Twitter data, but here's the issue:  To get all Tweets, you have to get special permission from Twitter (which, as I understand it, is never granted) or pay big bucks to Gnip or the like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OTOH, &lt;a href=&quot;https://dev.twitter.com/streaming/firehose&quot; rel=&quot;nofollow&quot;&gt;Twitter's API documentation&lt;/a&gt; says:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Few applications require this level of access. Creative use of a combination of other resources and various access levels can satisfy nearly every application use case.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the filter api with keyword tracking seems like something that would be a big part of this, but you obviously can't enumerate every keyword.  Using a User stream on many User accounts that follow a lot of people might be an option as well, and I'm not sure if it makes sense to think about using the search API in addition.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here's the question &quot;What combination of other resources and access levels is the best way to get the maximum amount of data from Twitter&quot;?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an array of edges and weights:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[['a', 'b', 4],&#xA; ['a', 'c', 3],&#xA; ['c', 'a', 2], &#xA; ...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have about 100,000 edges and weights are between 1 and 700, most around 100.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking of using Markov Cluster Algorithm however wanted to reach out to see if this is the best to use. What about Affinity Propagation? In either case, what is the workflow? Do you typically have a way to measure how well clustered the results. Is there an equivalent to a silhouette score? Is there a way to visualize the clusters?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I found that Apache-Storm, Apache-Spark, Apache-Flink and TIBCO StreamBase are some powerful frameworks for stream processing. but I don't know which one of them has the best performance and capabilities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know Apache-Spark and Apache-Flink are two general purpose frameworks that support stream processing. but Apache-Storm and TIBCO StreamBase are built for stream processing specially. Is there any considerable advantage between these frameworks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;When considering Support Vector Machine, in an take in multiple inputs. Can each of these inputs be a vector??&#xA;What i am trying to say is, can the input be a 2 dimensional vector??&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I need to simulate for an academical project how the traffic fluxes (input/output with respect to a monitored area, measured in number of cars) of a city area evolves in correspondence of an event (i.e. the opening of a restricted traffic area to decongest the traffic).&#xA;I have some simulated sensors that provide the data: I was thinking to use a combination of a fuzzy system (to assign a membership function to each type of data, e.g. PM10 value and CO2 value) and a markov process: I would need to modify the probability to decrement the number of car in the monitored area (simulating that a car is going out the congested area, towards the new opened area) basing on decisions made by means of a fuzzy system.&#xA;So my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It is a good way to interpret the problem or there are better ideas that I have not taken into account yet?&lt;/li&gt;&#xA;&lt;li&gt;How to implement such a combination of markov chain and fuzzy systems in matlab?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have been working in NLTK for a while using Python. The problem I am facing is that their is no help available on training NER in NLTK with my custom data. They have used MaxEnt and trained it on ACE corpus. I have searched on the web a lot but I could not find any way that can be used to train NLTK's NER. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone can provide me with any link/article/blog etc which can direct me to Training Datasets Format used in training NLTK's NER so I can prepare my Datasets on that particular format. And if I am directed to any link/article/blog etc which can help me TRAIN NLTK's NER for my own data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a question widely searched and least answered. Might be helpful for someone in the future whose working with NER.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am currently working on a multi-class classification problem with a large training set. However, it has some specific characteristics, which induced me to experiment with it, resulting in few versions of the training set (as a result of re-sampling, removing observations, etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to perform pre-processing of the data, that is to scale, center and impute (not much imputation though) values. This is the point where I've started to get confused.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been taught that you should always pre-process the test set in the same way you've pre-processed the training set, that is (for scaling and centering) to measure the mean and standard deviation on the training set and apply those values to the test set. This seems reasonably to me. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what to do in case when you have shrinked/resampled training set? Should one focus on characteristics of the data that is actually feeding the model (that is what would 'train' function in R's caret package suggest, as you can put the pre-processing object in there directly) and apply these to the test set, or maybe one should capture the real characteristics of the data (from the whole untouched training set) and apply these? If the second option is better, maybe it would be worth it to capture the characteristics of the data by merging the training and test data together just for pre-processing step to get as accurate estimates as possible (I've actually never heard of anyone doing that though)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know I can simply test some of the approaches specified here, and I surely will, but are there any suggestions based on theory or your intuition/experience on how to tackle this problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also have one additional and optional question. Does it make sense to center but NOT scale the data (or the other way around) in any case? Can anyone present any example where that approach would be reasonable?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you very much in advance.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a weekly dataset and I have to normalize this data. &#xA;Data is something like this : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. week   50&#xA;2. week   51&#xA;3. week   50&#xA;4. week   54&#xA;5. week   150&#xA;6. week   155&#xA;7. week   ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The important thing is, the difference between week 3 and week 4 (50-54) is not same with week 5 and week 6. And also there is a huge different between week 4 and week 5. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is how can i handle all of this things ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the standard normalization functions(for example scikit normalization) can do it for me and should I normalize this data 0-1 or -1 to 1 ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html&quot; rel=&quot;nofollow&quot;&gt;Sklearn normalization page&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; I am working with python and generally scikit-learn library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I'm trying to dice it quite finely and pick things with cosine similarity 0.1 and above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I'm struggling to see how I implement this practically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I have so far is as follows&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user's score)&lt;/li&gt;&#xA;&lt;li&gt;I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0&lt;/li&gt;&#xA;&lt;li&gt;I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits.&lt;/li&gt;&#xA;&lt;li&gt;Next I look for similar sketches by doing the following&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I split the sketch vector into b bands of r bits&lt;/li&gt;&#xA;&lt;li&gt;Each band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket.&lt;/li&gt;&#xA;&lt;li&gt;I then look in each bucket. Any movies that are in the same bucket are candidate pairs.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The book suggests I can &quot;amplify&quot; my results by adding more AND and OR steps, but I'm at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me understand how to do this?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;We can access HDFS file system and YARN scheduler In the Apache-Hadoop. But Spark has a higher level of coding. Is it possible to access HDFS and YARN in Apache-Spark too?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I found that Apache-Spark has pretty much simple interface and easy to use. But I want to know about other interfaces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me a ranking of Big-Data frameworks in base of simplicity of their interfaces. also this is useful to express most simple and complex interfaces in base of your experiences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. Detailed comparison is so lengthy and this is not my purpose. Just a selection or ranking on your opinions is sufficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Maybe it is a bit general question. I am trying to solve various regression tasks and I try various algorithms for them. For example, multivariate linear regression or an SVR. I know that the output can't be negative and I never have negative output values in my training set, though I could have 0's in it (for example, I predict 'amount of cars on the road' - it can't be negative but can be 0). Rather often I face a problem that I am able to train relatively good algorithm (maybe fit a good regression line to my data) and I have relatively small average squared error on training set. But when I try to run my regression algorithm against new data I sometimes get a negative output. Obviously, I can't accept negative output since it is not a valid value. The question is - what is the proper way of working with such output? Should I think of negative output as a 0 output? Is there any general advice for such cases?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm going to start my degree thesis and I want to do a fault detector system using machine learning techniques. I need datasets for my thesis but I don't know where I can get that data. I'm looking for historical operation/maintenance/fault datasets of any kind of machine in the oil &amp;amp; gas industry (drills, steam injectors etc) or electrical companies (transformators, generators etc).&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am wondering if there is a way to proceed 2 exectuions in 1 step in hive.&#xA;For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT * FROM TABLE1&#xA;SELECT * FROM TABLE2&#xA;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do this in one window, and do not have to open 2 hive windows to execute each line separetly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can it be done on HUE?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Our main use case is object detection in 3d lidar point clouds i.e. data is not in RGB-D format. We are planning to use CNN for this purpose using theano. Hardware limitations are CPU: 32 GB RAM Intel 47XX 4th Gen core i7 and GPU: Nvidia quadro k1100M 2GB. Kindly help me with recommendation for architecture. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking in the lines of 27000 input neurons on basis of 30x30x30 voxel grid but can't tell in advance if this is a good option.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additional Note: Dataset has 4500 points on average per view per point cloud&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Recently I read about path ranking algorithm in a paper (source: &lt;a href=&quot;https://www.cs.cmu.edu/~nlao/publication/2014.kdd.pdf&quot; rel=&quot;nofollow&quot;&gt;Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this paper was a table (Table 3) with facts and I tried to understand how they were calculated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;F1 (harmonic mean of precision and recall) = 0.04&lt;br/&gt;&#xA;P (precision) = 0.03&lt;br/&gt;&#xA;R (recall) = 0.33&lt;br/&gt;&#xA;W (weight given to this feature by logistic regression)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found a formula for F1 via Google which is&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$F1 = 2 * \\\\frac{precision * recall}{precision + recall}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I get the result of 0.055 with this formula, but not the expected result of 0.04.&#xA;Can someone help me to get this part?&#xA;Also, does someone know how 'W' can be calculated?&#xA;Thanks.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've recently become interested in possibly of developing some sort of method for ranking athletes of sports such as American football and determining which players are better than others in terms of specific statistics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My thoughts are that there are two ways to go about doing this. The first would be some sort of mathematical formula which would take in the statistics of a given player and provide some sort of standardized score which could be compared with other players to determine which is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My other idea would be to have some machine learning algorithm go through historical data and determine the patterns which indicate how well a certain combination of statistics would perform in the following week of play by using the patterns it recognizes over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure which approach would be more effective and so I'm hoping that someone has an idea or any advice as to which would be best to look into. Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;With respect to &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;ROC&lt;/a&gt; can anyone please tell me what the phrase &quot;discrimination threshold of binary classifier system&quot; means? I know what a binary classifier is.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am learning about Data Science and I love the Healthcare part. That's why I have started a blog and my third entry is about using Genetic Algorithm for solving NP-Problems. &#xA;This post is &lt;a href=&quot;https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/&quot; rel=&quot;noreferrer&quot;&gt;https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some expertise with GA package solving problems like the TSP, but do you know any most powerful R package?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks so much!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'am trying t create a regression based prediction (like booking website): predict the number of clicks for each hotel.&#xA;I have to generate a .csv file containing two columns: hotel_id, predicted_number_of_clicks for all hotel_ids.&#xA;My first question question is : should I put the Id_hotel as feature in the predictive model. I think that I must to drop it no?&#xA;Second question is : how can write in the csv file only this 2 columns &quot;hotel_id&quot;, &quot;predicted_number_of_clicks&quot; if I drop hotel_id from the model feature?&#xA;Thank you very much for your reply in advance&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to use a particular cost function (based on doubling rate of wealth) for a classification problem, and the solution works well in MATLAB. See &lt;a href=&quot;https://github.com/acmyers/compareCostFXs&quot; rel=&quot;nofollow&quot;&gt;https://github.com/acmyers/compareCostFXs&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try to do this in Python 2.7.6 I don't get any errors, but it only returns zeros for the theta values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the cost function and optimization method I've used in Python:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def costFunctionDRW(theta, X, y):&#xA;&#xA;    # Initialize useful values&#xA;    m = len(y)&#xA;    # Marginal probability of acceptance&#xA;    marg_pA = sum(y)/m&#xA;    # Marginal probability of rejection&#xA;    marg_pR = 1 - marg_pA&#xA;&#xA;    # =============================================================&#xA;    pred = sigmoid(np.dot(X,theta))&#xA;    final_wealth_individual = (pred/marg_pA)*y + ((1-pred)/marg_pR)*(1-y)&#xA;    final_wealth = np.prod(final_wealth_individual)&#xA;    final_wealth = -final_wealth&#xA;&#xA;    return final_wealth&#xA;&#xA;result = scipy.optimize.fmin(costFunctionDRW, x0=initial_theta, \\\\&#xA;                   args=(X_array, y_array), maxiter=1000, disp=False, full_output=True )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any advice would be much appreciated!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y$, which is a continuous value from zero to one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, I have tried different models, &lt;em&gt;e.g.,&lt;/em&gt; decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and &lt;em&gt;etc.&lt;/em&gt; However, all of these models also do not perform well. (RMSE $\\\\approx $0.35, there is not significant difference with linear regression)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: &lt;strong&gt;how can I examine whether it is caused by bad data quality?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, for the size of data set, there are more than 10K data points, each of which associated with 105 features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also tried to investigate importance of each feature by using decision-tree-based regression, it turns out that, only one feature (which should not be the most outstanding feature in my knowledge to this problem) have an importance of 0.2, while the rest of them only have an importance less than 0.1.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a research project that deals with American military casualties during WWII. Specifically, I am attempting to construct a count of casualties for each service at the county level. There are two sources of data here, each presenting their own challenges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1. Army and Air Force data.&lt;/strong&gt; The National Archives hosts lists of Army and Air Force servicemen killed in action by state and county. There are .gif images of the report available online. &lt;a href=&quot;http://media.nara.gov/media/images/29/19/29-1891a.gif&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a sample for several counties in Texas. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I DO NOT need to recover the names or any other information. I simply need to count the number of names (each on its own line, and listed in groups of five) under each County. There are hundreds of these images (50 states - 30-100 for each state). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been unable to find an OCR program that can tackle this problem adequately. How would you suggest I approach this challenge? (I have some programming expertise in Python and Java, but would prefer to use any off-the-shelf solutions that may exist).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2. Navy and Marine Core data.&lt;/strong&gt; This data is organized differently. Each state has long lists of casualties with the address of their next of kin. &lt;a href=&quot;http://media.nara.gov/media/images/27/31/27-3023a.gif&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a sample for Texas again. For these images, I need to BOTH count the number of dead and recover their hometown, which is typically the last word in each entry. I can then match these hometowns to counties and merge with database 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, the usual OCR programs have proved inadequate. Any help on this (admittedly more difficult) problem would be very much appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance experts!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm doing some work trying to extract commonly occurring words from a set of human classified documents and had a couple questions for anyone who might know something about NLP or statistical analysis of text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have a set of a bunch of documents, and users have classified them as either good or bad. What I'd like to do is figure out what words are common to the good documents, but not necessarily the other ones.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could, for example, use the (frequency within good documents / total frequency) which would essentially normalize the effect of a word being generally common. This, unfortunately, gives very high precedence to words that occur in only a few good documents &amp;amp; not at all in the other documents. I could add some kind of minimum threshold for # of occurrences in good docs before evaluating the total frequency, but it seems kind of hacky.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know what the best practice equation or model to use in this case is? I've done a lot of searching and found a lot of references to TF-IDF but that seems more applicable for assessing the value of a term on a single document against the whole set of docs. Here I'm dealing with a set of docs that is a subset of the larger collection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, I'd like to identify which words are uniquely or more important to the class of good documents.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;strong&gt;Example Data&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset (in &lt;code&gt;R&lt;/code&gt; as a &lt;code&gt;data frame&lt;/code&gt;) of race results for athletes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;athlete racedistance time    location tracktype       date    coach&#xA;A          100       10.0       UK     typeA       2014-01-01  carlos&#xA;A          200       20.0       US     typeB       2014-02-01  carla&#xA;A          100        9.5      AUS     typeC       2014-03-01  chris&#xA;B          100       11.0       UK     typeA       2014-01-01  carla&#xA;B          200       21.0       US     typeB       2014-02-01  carlos&#xA;B          400       61.0      AUS     typeC       2014-03-01  carla&#xA;B          100       10.5      GER     typeA       2014-04-01  clive&#xA;C          100        9.5       UK     typeA       2014-01-01  clive&#xA;C          200       21.5       US     typeB       2014-02-01  chris&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an appropriate machine learning algorithm or method that can use the previous results of each athlete as a feature, when trying to predict the &lt;code&gt;time&lt;/code&gt; for an athlete in a future race? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, &lt;code&gt;athlete A&lt;/code&gt; has three races, with one month rest between them. In the third race he performs slightly better than the first race over the same distance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can an algorithm learn that the second race had an effect on the athlete, which meant he performed better in the third race?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I've read on the subject and the training examples I've completed it would appear that each 'row' of data should be independent, is this the case for all ML algorithms? Is there another prediction technique I should be considering to solve this type of problem?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I once knew some Java, but that was close to 10 years ago.  Assuming I can learn a language to get into data analytics.... what language do you recommend?  &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am not certain, if this is the right place to ask the following question. I am looking for some practical scenarios in social networks where the following information propagation model can arise:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/mERQh.jpg&quot; alt=&quot;A toy example of the desired information propagation model&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I want to build a chatbot that serves as a first line customer support on a  retail website. I have a large log of chat sessions between customers and support professionals that I can use. I am wondering what is the best way to go about building this chatbot. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some ideas I have looked into;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The first thing I did was to breakdown the chat logs into Q&amp;amp;A pairs. Treat each question as a document, compute a term-document matrix and use it to retrieve the most similar question when a customer asks sometihng. The idea was to then simply pick the answer given to the question by the human (with some modifications). However, this gives really abysmal results and does not match to previous questions very well. Even if I get this approach to work well, we would be limited to the questions that already been asked. &lt;/li&gt;&#xA;&lt;li&gt;Another thing I thought about was to modify something like ALICE and write some AIML for customer support related QA. However, that would take a lot of very precise AIML writint to work well. This solution would not be able to scale to other languages, which is something I want to do.&lt;/li&gt;&#xA;&lt;li&gt;Another idea I have is to try and cluster the answers given by the customer support staff (they are more likely to be aligned compared to the questions asked by customers). This would give me a sense of what questions are similar. Then I can use something like LSA on the questions and fall back to the first approach.&lt;/li&gt;&#xA;&lt;li&gt;Yet another way is to build an ontology of the domain specific knowledge and given a question decide which category the question falls into. The questions from the chat logs can then be mapped to the ontology and I could train classifiers for mapping questions to different knowledge areas. Once I reach the leaf nodes, I can give back pre-defined answers. However, my reservation is that mapping chat logs to the ontology would be very tedious. Is there a good way to map the existing QA to the ontology?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have 2 data files: the first one is a database, potentially very large; the second one contains queries I want to answer. My program pipeline is processing the database to get some information first, and then use that same information to answer the queries. Although the number of queries is not big, processing each query takes a long time. So I want to give each worker some queries to answer, then combine all the answers together into one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sounds like a MapReduce job. But to answer each query, the worker also needs to use the processed information from the database, and I'm not sure how to do this with MapReduce. I'm new to parallel programming and just heard of MPI and Spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you help me to choose an appropriate one?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please ask if the description is not clear.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1.00 0.88 0.96 0.99 &#xA;0.88 1.00 0.99 0.96 &#xA;0.96 0.99 1.00 0.86 &#xA;0.99 0.96 0.86 1.00 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to implement recommender which, for a set of items, recommends a new set of items.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking about using SVD to reduce the items to n-dimensional space, let's say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a base set of items (which can get quite big), I hope I could calculate an average of their vectors, and use it for search.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this a good idea? What is this procedure called? And can it be done in Mahout?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my code so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ItemSimilarity similarity = new LogLikelihoodSimilarity(model);&#xA;Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);&#xA;// copy similarities to a matrix&#xA;for (int i = 0; i &amp;lt; NUM_ITEMS; i++) {&#xA;        double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));&#xA;        for (int j = 0; j &amp;lt; NUM_ITEMS; j++) {&#xA;            m.setQuick(i, j, similar[j]);&#xA;    }&#xA;}&#xA;Matrix v = new SingularValueDecomposition(m).getV();&#xA;Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is, SVD is taking forever for NUM_ITEMS &gt; 30. I don't know if there is an issue with data, or with SVD implementation I'm using. The matrix m is symmetrical, could that be an issue? I tried googling &quot;demean matrix mahout&quot; with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT 2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem was the matrix contained a few NaN values, that's why SVD was taking infinite time. After replacing these with 0.0 it works fine for 1000 x 1000 matrix. And my recommendations are working like a charm. I'll still need compute SVD of 20x more rows and columns. If anyone knows what's the easiest way to compute (approximate) SVD of 20.000 x 20.000 dense matrix, probably through some cloud parallel service (?), please let me know.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS. Thanks for help!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Hello I work as data scientist for a private company.&#xA;I am interested in working for a nonprofit company, such as a research institute (public or private) or a company that takes care of  issues such as  environment, public health,  social improvements. Even an internet company like Wikipedia can be interesting.&#xA;Does anybody know if nonprofit companies hire data scientists?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;If I have virtually endless training data (it's synthesized) is there still purpose in having epochs? I.e. training on the same samples multiple times?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to calculate maximum values for different groups in a relation in Pig. The relation has three columns patientid, featureid and featurevalue (all int). &#xA;I group the relation based on featureid and want to calculate the max feature value of each group, heres the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grpd = GROUP features BY featureid;&#xA;DUMP grpd;&#xA;temp = FOREACH grpd GENERATE $0 as featureid, MAX($1.featurevalue) as val;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its  giving me  &lt;strong&gt;Invalid scalar projection: grpd&lt;/strong&gt; Exception. I read on different forums that MAX takes in a &quot;bag&quot; format for such functions, but when I take the dump of grpd, it shows me a bag format. Here's a small part of the output from the dump:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(5662,{(22579,5662,1)})&#xA;(5663,{(28331,5663,1),(2624,5663,1)})&#xA;(5664,{(27591,5664,1)})&#xA;(5665,{(30217,5665,1),(31526,5665,1)})&#xA;(5666,{(27783,5666,1),(30983,5666,1),(32424,5666,1),(28064,5666,1),(28932,5666,1)})&#xA;(5667,{(31257,5667,1),(27281,5667,1)})&#xA;(5669,{(31041,5669,1)})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Whats the issue ?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a complete Hadoop platform with &lt;em&gt;HDFS&lt;/em&gt;, &lt;em&gt;MR&lt;/em&gt;, &lt;em&gt;Hive&lt;/em&gt;, &lt;em&gt;PIG&lt;/em&gt;, &lt;em&gt;Hbase&lt;/em&gt;, etc., &lt;em&gt;Python&lt;/em&gt;, &lt;em&gt;R&lt;/em&gt;, &lt;em&gt;Java&lt;/em&gt;. All data sets have a large size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data set A, describing the jobs of people working in a company, is composed of the following fields:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Id Person: a unique alphanumeric identifier per person.&lt;/li&gt;&#xA;&lt;li&gt;Start Date: a date format iso entry in the post  &lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;End Date: iso size release date of the position. If the date is not given, it is the current position&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with &lt;strong&gt;Google&lt;/strong&gt;, Commercial Manager at [missing text] , Manager at &lt;strong&gt;googole&lt;/strong&gt; ...&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you in advance&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking for a design pattern that is relevant to a module that extracts features.&#xA;I want to define a certain number of features over my data points, and then according to the performance and the feature selection, I may want to remove some of them and add others, and also I may want to consider any subsets of them to test.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is a good design pattern to do that? Did I miss something obvious? I am neither an engineer nor a developer, so I never study such things but I understand that it could help me a lot!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help,&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm very new to this community, so please overlook my noobness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a data set with 2948 instances and I tried to remove outliers using InterquartileRange filter in Weka. The issue is that the number of 'YES' instances in ExtremeValues and Outliers takes up to 2947 and 2946 respectively. In other words, all my data are considered outliers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this say about my data set? Or am I not meant to perform IQR on this data, if so, is there other algorithms to identify outliers other than IQR?  And how would one perform regression on such a data set?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are some data analytic package &amp;amp; feature in python which helps do data analytic?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I was not sure about posting this question with mentioning the name of the company, which I quite respect and admire. However, I've figured that a wider exposure might help the team to fix this and similar problems faster as well as increase the quality of the &lt;em&gt;machine learning (ML)&lt;/em&gt; engine of their website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem exposes itself by too many occurrences of a quite trivial &lt;em&gt;misclassification error&lt;/em&gt; on Amazon's book categories &lt;strong&gt;classification&lt;/strong&gt; (which I'm a frequent visitor of). In the following example, the underlying reason of such behavior is quite clear, but in other cases the reasons might be different. I am curious about what could be other potential &lt;em&gt;reasons&lt;/em&gt; for misclassification and what are the &lt;em&gt;strategies/approaches&lt;/em&gt; to avoiding such problems. Without much further ado, here's how the problem appears in real life.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was reviewing some books, related to transitioning from graduate programs (Ph.D., in particular) to work environment in academia. Among several other books, I ran across the following one:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/4LSzV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, so good. However, let's scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out &lt;em&gt;categories&lt;/em&gt;, &lt;strong&gt;relevant&lt;/strong&gt; to the book's &lt;em&gt;discipline, topic and contents&lt;/em&gt;. How surprised was I (and that's an understatement!) to see the following result of Amazon.com's sophisticated ML engine and algorithms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/KXrFh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Clearly, the only fuzzy fact that connects this book with the subject &quot;Audiology and Speech Pathology&quot; (!) is IMHO the author's last name (Boice), which, is &lt;strong&gt;close to&lt;/strong&gt; the word &quot;voice&quot;. If my guess is correct, Amazon's ML engine, for some reason, decided to take into account the book's lexicographical attribute instead of the book's most important and &lt;strong&gt;most relevant attributes&lt;/strong&gt;, such as title, topic and contents. I've seen multiple occurrences of similar absolutely incorrect ML-based decision making on Amazon.com and some other websites. So, hopefully my question makes sense as well as interesting and important enough to spark a discussion: &lt;strong&gt;What could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems?&lt;/strong&gt; (Any related thoughts will also be appreciated.)&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;em&gt;Disclaimer:&lt;/em&gt; although I know some things about big data and am currently learning some other things about machine learning, the specific area that I wish to study is vague, or at least appears vague to me now. I'll do my best to describe it, but this question could still be categorised as too vague or not really a question. Hopefully, I'll be able to reword it more precisely once I get a reaction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some experience with Hadoop and the Hadoop stack (gained via using CDH), and I'm reading a book about Mahout, which is a collection of machine learning libraries. I also think I know enough statistics to be able to comprehend the math behind the machine learning algorithms, and I have some experience with R.&#xA;My ultimate goal is making a setup that would make trading predictions and deal with financial data in real time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if there're any materials that I can further read to help me understand ways of managing that problem; books, video tutorials and exercises with example datasets are all welcome.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am looking for a suitable graph representation where the nodes/vertices are molecules with 2 main variables: structural characteristic and a real number property (for example biological potency). The network is build up by molecules by matching structural properties and it can be rather complex with many clusters of high degree vertices (ie molecules that are structurally similar to each other). However the representation should give a clear picture of potencies, although high detail accuracy is not required: some sort of classification is adequate. I played with JUNG with a graph having edge weights as the change in potency and the ISOM layout thinking that it reserves (as much as possible) the edge weights but with not good results. Something like radial layout could be ideal, as it gives a clear picture of potency distribution, but this obviously would suffer from crowding of vertices that are closer to the center.  As, obviously, my knowledge is restricted, is there some layout that you would suggest and is practical for a non expert to implement? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;A book I'm now reading, &quot;Apache Mahout Cookbook&quot; by Pierro Giacomelli, states that &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;To avoid [this], you need to divide the vector files into two sets called the 80-20 split &amp;lt;...&gt;&#xA;  A good dividing percentage is shown to be 80% and 20%.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is there a strict statistical proof of this being the best percentage, or is it a euristic result?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Can you please show the step by step calculation of Entropy(Ssun)?  I do not understand how 0.918 is arrived at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried but I get the values as 0.521089678, 0.528771238, 0.521089678 for Sunny, Windy, Rainy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was able to calculate the target entropy (Decision) correctly as = -(6/10)*log(6/10) + -(2/10)log(2/10) + -(1/10)log(1/10) + -(1/10)log(1/10) = 1.570950594&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am totally stuck at the next step.  Request your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Reference: &lt;a href=&quot;http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.html&quot; rel=&quot;nofollow&quot;&gt;http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.html&lt;/a&gt;&#xA;Please search for &quot;The first thing we need to do is work out which attribute will be put into the node at the top of our tree:&quot; to reach the line I am referring to.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Context of question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to find semantically similar documents in corpora. For that, I'm first trying Latent Dirichlet Allocation (LDA) with divergences (Hellinger, Kullback-Leibler, Jensen-Shannon) on the per document topic distributions. However, to find # of topics for my corpus ( a 948 document dataset, extracted from larger collection, where docs about the same story are humanly annotated), it was suggested to use HDP. Unfortunately, after MANY tries, I'm still unsure I'm using the packages correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More detailed:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) HDPFASTER continually increases the number of topics (in train.log file).  What is the stopping criterion (e.g. difference in avg. likelihood between successive iterations smaller than x? which x?) and/or how many cycles should I let it run? Afterwords, do I take as the correct # of topics the last line of &quot;train.log&quot; OR should I also test for minimum perplexity (It has also been suggested that Bayesian Information Criterion (BIC) might be a better measure) and choose the smallest? If the latter, should I test for ALL iterations?!  Is HDPFASTER's test feature the appropriate tool for this? Last, should I --sample_hyper? Do I just use a(alpha) as input to LDA's prior alpha, perhaps divided by a number (e.g. # of topics)? What about LDA's prior beta? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) HCA continually decreases  the number of topics (in *.log file). When to stop? Do I accept the final # of topics in .log ( exp.ent =number0) OR do I search for the minimum perplexity of test set (as I assume it appears as number2 in &quot;log_2(perp)=number1,number2&quot; in .log throughout iterations), which ALWAYS appears VERY close to my initial max number of topics? Hyperparameters alpha,beta: do I sample using -D , -E? Which do I put as input to LDA's priors? Is it generally worth it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generally: Is it possible that my dataset is just too small and/or 'biased/fragmented/incomplete', in the sense that each story has only 1-3 examples, while the diversity of the topics these stories discuss can be quite large? Should I just try and augment it with a larger 'homogeneous' dataset? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;is the survival table classification method on the Kaggle Titanic dataset an example of an implementation of Naive Bayes ? I am asking because I am reading up on Naive Bayes and the basic idea is as follows:&#xA;&quot;Find out the probability of the previously unseen instance&#xA;belonging to each class, then simply pick the most probable class&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The survival table &#xA;(&lt;a href=&quot;http://www.markhneedham.com/blog/tag/kaggle/&quot; rel=&quot;nofollow&quot;&gt;http://www.markhneedham.com/blog/tag/kaggle/&lt;/a&gt;)&#xA;seems like an evaluation of the possibilities of survival given possible combinations of values of the chosen features and I'm wondering if it could be an example of Naive Bayes in another name. Can someone shed light on this ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am working on a problem(non competition) from hacker rank &lt;a href=&quot;https://www.hackerrank.com/challenges/predict-missing-grade&quot; rel=&quot;nofollow&quot;&gt;https://www.hackerrank.com/challenges/predict-missing-grade&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically you're given test data of a bunch of students of their scores in other subjects but math and you are to predict their score in math based off all their other test scores. Say you were passed data of&lt;/p&gt;&#xA;&#xA;&lt;p&gt;{&quot;SerialNumber&quot;:1,&quot;English&quot;:1,&quot;Physics&quot;:2,&quot;Chemistry&quot;:3,&quot;ComputerScience&quot;:2}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you go about generating that student's score in mathematics or coming up with a prediction engine to generate the math score? I know that's the whole point of this question but can someone give me a hint or a resource to go to so I can have a chance of figuring this out and actually get started? I really want to learn.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;strong&gt;Jaccard similarity&lt;/strong&gt; and &lt;strong&gt;cosine similarity&lt;/strong&gt; are two very common measurements while comparing item similarities. However, I am not very clear in what situation which one should be preferable than another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can somebody help clarify the differences of these two measurements (the difference in concept or principle, not the definition or computation) and their preferable applications?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm curious if anyone has Python library suggestions for inferential statistics. I'm currently reading &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL/&quot; rel=&quot;nofollow&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, which uses R for the example code, but ideally I'd like to use Python as well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most of my data experience is with Pandas, Matplotlib, and Sklearn doing predictive modeling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I've found &lt;a href=&quot;https://pypi.python.org/pypi/statsmodels&quot; rel=&quot;nofollow&quot;&gt;statsmodels&lt;/a&gt;. Is this what is recommended or is there something else?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Hi I am new to Data analytics.I am planning to learn R by doing some real time projects. How should I stream line (set goals) my time in learning R and also I have not learnt statistics till data. I am planning to learn both side by side. I am mid level data warehouse engineer who has experience in DBMS Data-Integration. I am planning to learn R so that I can bring out useful analysis from the Integrated data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I be specific, I am beginning in R, so what are the basic statistical concepts I should know and implement it in R. If I want to be an expert or above average person in R how should I plan strategically to become one. Say if I can spend 2 hrs a day for 1 year what level I should reach. FYI am working for a SaaS company. What are the way s in which I can utilize R knowledge in a SaaS environment &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am pursing Data Analyst course at Udacity. I came across this video : &lt;a href=&quot;https://www.youtube.com/watch?v=3Z73Wd2T1xE&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=3Z73Wd2T1xE&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Watching it lead me to wonder if &lt;a href=&quot;http://www.ayasdi.com/&quot; rel=&quot;nofollow&quot;&gt;Ayasdi&lt;/a&gt; products would reduce the demand for data scientists. I wish to compete in Kaggle contests but after watching the video, I feel that many of those problems can be easily solved using their platform and that I will be at disadvantage since I do not have access to their tools. Also I feel such tools would reduce the need of experts in data sciences. Now I am worried if I should continue with Nano Degree in Data Analyst at Udacity.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am confused by the definition of the likelihood function in different contexts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of linear and logistic regression, it is defined as y given x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case naive bayes and LDA, it is defined over x and y&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of the EM algorithm (with observed variable x and unobserved variable z), it is defined over x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we know over what it should be defined?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm wondering how other developers are setting up their local environments for working on Spark projects. Do you configure a 'local' cluster using a tool like Vagrant? Or, is it most common to SSH into a cloud environment, such as a cluster on AWS? Perhaps there are many tasks where a single-node cluster is adequate, and can be run locally more easily.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Hi I would appreciate it if someone can point me in the right direction.  I'm looking for an algorithm or mathematical theory which I would use to compute the similarity between two ordered lists, where each list element can have n sub-elements.  I will explain with an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I go to a baseball game and I record the sequence of strikes and balls for each of the first 30 players at bat.  My list looks like this, where P is a player, S is a strike and B is a ball. Order matters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;L1: {P1=(S,S,S)}, {P2=(B,B,S)}, {P3=(B,B,S,S)}, ... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My friend goes to a baseball game and does the same thing.  Later, we meet up and compare our lists.  We find that our lists are almost identical except that I recorded a strike for player 16 where my friend recorded a ball.  What are the chances we were at the same game and one of us made a mistake at player 16?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance...&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the best known Data Science Methodologies today? By methodology I mean a step-by-step phased process that can be used for framing guidance, although I will be grateful for something close too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Given some clusters created from similarity measures between items, is there a recommended way to assign a new item to an existing cluster based on similarity alone? (i.e. avoiding re-clustering)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Measuring the similarity of a new item to all other items is fairly cheap, so I'm looking for a way of using this to assign it to the cluster it's most likely to belong to. It's also important for it to take cluster size into account (i.e. doesn't unfairly weight towards or against larger clusters).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I'm trying to sacrifice some clustering accuracy in exchange for avoiding a complete re-clustering when the occasional new item is added.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;This is in continuation to a previos question. I would like to know where in practice will the following M-ary tree-structured social network propagation model arise. I am looking for some concrete examples in social media (twitter and youtube etc.). Eventually, I want to do some statistical analysis on such social networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/x14St.jpg&quot; alt=&quot;Example of a binary tree-structured social network&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'd like to see the top N results for a RandomForestClassifier prediction, ordered by descending probability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The answer may be predict_proba, but I have no idea how to interpret the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Help appreciated!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;&lt;em&gt;Hadoop&lt;/em&gt; is a buzzword now. A lot of start-ups use it (or just say, that they use it), a lot of widely known companies use it. But when and what is the border? When person can say: &quot;Better to solve it without Hadoop&quot;?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have the following data&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^1_i$, $y^1_i$) for $i=1,2,...N_1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^2_i$, $y^2_i$) for $i=1,2,...N_2$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;($x^m_i$, $y^m_i$) for $i=1,2,...N_m$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to train a neural net to produce some $y_k$ where $k&amp;lt;=min(N)$ given a input ${x_1, x_2, ..., x_{k-1}}$?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so any suggestion of documentation/ library I can look at (preferably python)?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have 22 classes of objects but they have very skewed distributions where max class has 100.000 images and the min class has 1600 images. In that setting I would like to hear some possible solutions to this balance problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried followings so far;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;multiply number of instances in the lower classes up to the max class by replicating the instances, possibly adding some noise as well.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;changing the learning regarding the class distribution in the given minibatch of the next epoch. (no implemented but in my mind)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What are your suggestions?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have used the same methods/parameters to create two decision trees. The trees classify the presence or absence of a medical condition using the presence or absence of various symptoms. There is a tree for Medical Condition #1 and another tree for Medical Condition #2. Both trees are based on the same set of symptoms, rated by patients. If Medical Condition #1 resulted in a much simpler tree than Medical Condition #2, can that suggest Medical Condition #2 is a more complex disease? If so, can anyone point me to a reference that states the complexity/depth of a tree can be representative of a complex condition?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The goal of my analysis is separate from this question. I am trying to figure out all the conclusions I can draw or suggest from my analysis. Yes, I am interested in saying something about the complexity of condition A to B. &#xA;When is a condition complex ? When there are many symptoms as the disease is diagnosed by the symptoms.&#xA;If it's hard to diagnose ? YES&#xA;if the symptoms are severe ? NO&#xA;can s.o. have both conditions ? NO&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently trying to predict a continuous variable using KNN. Instead of treating each neighbor equally I would like to use the weights to create a weighted average. The weights by themselves are not ideal, as the closer a neighbor the more I would like that neighbor to influence the final results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This lead me to consider the inverse of each of the distances, but this doesn't handle the case where an instance is the exact same -&gt; with a distance of 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any recommendations on how to properly set the weights of each neighbor relative to their distance? Similar to how the inverse would handle this, but one that allows for 0 values.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have code to generate feature vectors in the style of the bag-of-words model, you can see it on my &lt;a href=&quot;https://github.com/h1395010/perceptron/blob/master/src/file_dict_createur/FileDictCreateur.java&quot; rel=&quot;nofollow noreferrer&quot;&gt;github page&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It renders output of the form:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/data/train/politics/p_0.txt, [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]&#xA;/data/train/science/s_0.txt, [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]&#xA;/data/train/atheism/a_0.txt, [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]&#xA;/data/train/sports/s_1.txt, [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to move on to the next step of the perceptron but I'm confused about what to do next. I guess the next thing I need to do is generate a feature vector for the 'test' data, isn't it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, in &lt;a href=&quot;http://en.literateprograms.org/index.php?title=Special:DownloadCode/Perceptron_(Java)&amp;amp;oldid=19184&quot; rel=&quot;nofollow noreferrer&quot;&gt;this example&lt;/a&gt; what is 'teachingOutput' supposed to be, training data vectors? Such as the ones I have above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried poking around in &lt;a href=&quot;https://stackoverflow.com/questions/21738277/perceptron-learning-most-important-feature&quot;&gt;this implementation&lt;/a&gt; in an attempt to figure out what to do but, alas, to no avail. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I often get the problem when this or that alias name is already used somewhere, and I can't easily find that variable or aggregation to release the name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/kK9mM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there some place in Tableau where I can view/edit/reset full list of aliases?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am a PhD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know &lt;code&gt;svn&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find &lt;code&gt;git&lt;/code&gt; also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn't fun, and cost me a lot of time and effort.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that others in science seem to have similar problems, yet I couldn't find a good solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use the storage facilities of my institute, so I need something that can use a &quot;dumb&quot; server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So, I need a tool that can handle more than one remote location.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have evaluated a lot of different solutions, but none seem to fit the bill:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://subversion.apache.org/&quot;&gt;svn&lt;/a&gt; is somewhat inefficient and needs a smart server&lt;/li&gt;&#xA;&lt;li&gt;hg &lt;a href=&quot;http://mercurial.selenic.com/wiki/BigfilesExtension&quot;&gt;bigfile&lt;/a&gt;/&lt;a href=&quot;http://mercurial.selenic.com/wiki/LargefilesExtension&quot;&gt;largefile&lt;/a&gt; can only use one remote&lt;/li&gt;&#xA;&lt;li&gt;git &lt;a href=&quot;https://github.com/beenje/git-bigfile&quot;&gt;bigfile&lt;/a&gt;/&lt;a href=&quot;https://github.com/alebedev/git-media&quot;&gt;media&lt;/a&gt; can also use only one remote, but is also not very efficient&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://attic-backup.org/&quot;&gt;attic&lt;/a&gt; doesn't seem to have a log, or diffing capabilities&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://bup.github.io/&quot;&gt;bup&lt;/a&gt; looks really good, but needs a &quot;smart&quot; server to work&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've tried &lt;code&gt;git-annex&lt;/code&gt;, which does everything I need it to do (and much more), but it is very difficult to use and not well documented. I've used it for several days and couldn't get my head around it, so I doubt any other coworker would be interested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do researchers deal with large datasets, and what are other research groups using?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me that almost everyone should have this problem, yet I don't know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I need to draw a decision tree about this subject :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The research and development manager in an old oil company, which is&#xA;  considering making some changes, lists the following courses of action&#xA;  for the company:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(i) Adopt a process developed by another oil company. This would cost&#xA;  €7 million in royalties and yield a net €20 million profit (before&#xA;  paying the royalty).&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(ii) Carry out one or two (not simultaneously) alternative research&#xA;  projects :&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(R1) the more expensive one has a 0.8 chance of success; net profit&#xA;  €16 million and a further €6 million in royalties. If it fails there&#xA;  will be a net loss of €10 million. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(R2) the alternative research programme is less expensive but only has&#xA;  a 0.7 chance of success with a net profit of €15 million and a further&#xA;  €5 million in royalties. If it fails a net loss of €6 million will be&#xA;  incurred.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;(iii) Make no changes. After meeting current operating costs the&#xA;  company expects to make a net €15 million profit from its existing&#xA;  process. Failure of one research program would still leave open all&#xA;  remaining courses of action (including the other research programme).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I need also to indicate the different payoffs. This is what I've done so far :&#xA;&lt;img src=&quot;https://i.stack.imgur.com/BLTJ6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to be sure that I'm going in the right direction since I'm a beginner with decision trees. And then I need to decide the best course of action using Bayes, Maximax and Maximin rules.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm quite new to Data Science, but I would like to do a project to learn more about it.&#xA;My subject will be Data Understanding in Public Health.&#xA;So I want to do some introductory research to public health.&#xA;I would like to visualize some data with the use of a tool like Tableau.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which path would you take to develop a good understanding of Data Science? I imagine taking some online courses, eg. Udacity courses on data science, but which courses would you recommend?&#xA;Where can I get real data (secondary Dummy Data) to work with?&#xA;And are there any good resources on research papers done in Data Science area with the subject of Public Health?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions and comments are welcome.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was wondering which language can I use: R or Python, for my internship in fraud detection in an online banking system: I have to build machine learning algorithms (NN, etc.) that predict transaction frauds.&#xA;Thank you very much for your answer.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Hello and thanks in advance!  I'd like some advice on a scalability issue and the best way to resolve.  I'm writing an algorithm in R to produce forecasts for several thousand entities.  One entity takes about 43 seconds to generate a forecast and upload the data to my database.  That equates to about 80+ hours for the entire set of entities and that's much too long.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought about running several R processes in parallel, possibly many on a few different servers, each performing forecasts for a portion of total entities.  Though that would work, is there a better way?  Can Hadoop help at all?  I have little experience with Hadoop so don't really know if it can apply.  Thanks again!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;A usual way to find association rules in R is the &quot;arules&quot; package, which easily let's use calculate some rules based on the apriori algorithm. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, for the data i'm using, I have a lot of NULL cases (baskets where no product A o B is present). This means that I need to calculate some null-invariant measures (kulcynski, for instance). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of any package or workable code that let's me implement this as opposed to writing from scratch the entire algorithm?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Are there commonly accepted ways to visualize the results of a multivariate regression for a non-quantitative audience? In particular, I'm asking how one should present data on coefficients and T statistics (or p-values) for a regression with around 5 independent variables.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;This question is likely somewhat naive.  I know I (and my colleagues) can install and use Python on local machines. But is that really a best practice?  I have no idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there value in setting up a Python &quot;server&quot;?  A box on the network where we develop our data science related Python code.  If so, what are the hardware requirements for such a box?  Do I need to be concerned about any specific packages or conflicts between projects?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Most literature focus on either explicit rating data or implicit (like/unknown) data. Are there any good publications to handle like/dislike/unknown data? That is, in the data matrix there are three values, and I'd like to recommend from unknown entries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And are there any good open source implementations on this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a pandas dataframe with a salary column which contains values like:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;£36,000 - £40,000 per year plus excellent bene...,&#xA;  £26,658 to £32,547 etc&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I isolated this column and split it with the view to recombining into the data frame later via a column bind in pandas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I now have an object with columns like the below. The columns I split the original data frame column I think are blank because I didn't specify them (I called &lt;code&gt;df['salary']=df['salary'].astype(str).str.split()&lt;/code&gt;&#xA;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my new object contains this type of information:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[£26,658, to, £32,547],&#xA;[Competitive, with, Excellent, Benefits]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to do is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create three columns called minvalue and maxvalue and realvalue&lt;/li&gt;&#xA;&lt;li&gt;List items starting with £ (something to do with &lt;code&gt;&quot;^£&quot;&lt;/code&gt;?&lt;/li&gt;&#xA;&lt;li&gt;Take till the end of the items found ignoring the £ (get the number out) (something to do with &lt;code&gt;(substr(x,2,nchar(x)))&lt;/code&gt;?&lt;/li&gt;&#xA;&lt;li&gt;If there are two such items found, call the first number &quot;minvalue&quot; and call the second number &quot;maxvalue&quot; and put it below the right column. If there is only one value in the row, put it below the realvalue column.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I am very new to pandas and programming in general, but keen on learning, your help would be appreciated. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;As an example. If you are tying to classify humans from dogs. Is it possible to approach this problem by classifying different kinds of animals (birds, fish, reptiles, mammals, ...) or even smaller subsets (dogs, cats, whales, lions, ...) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then when you try to classify a new data set, anything that did not fall into one of those classes can be considered a human.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this is possible, are there any benefits into breaking a binary class problem into several classes (or perhaps labels)? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Benefits I am looking into are: accuracy/precision of the classifier, parallel learning.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am performing Named Entity Recognition using Stanford NER. I have successfully trained and tested my model. Now I want to know:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) What is the general way of measuring accuracy of NER model ?? For example what techniques or approaches are used ??&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is there any built-in method in STANFORD NER for evaluating the accuracy ??&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;We are currently developing a customer relationship management software for SME's. What I'd like to structure for our future CRM is developing CRM with a social-based approach (Social CRM). Therefore we will provide our users (SME's) to integrate their CRM into their social network accounts. Also CRM will be enhance intercorporate communication of owner company.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All these processes I've just indicated above will certainly generate lots of unstructured data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering how can we integrate &lt;i&gt;big data&lt;/i&gt; and &lt;i&gt;data-mining&lt;/i&gt; contepts for our project; especially for the datas generated by social network? I am not the expert of these topics but I really want to start from somewhere.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Basic capabilities of CRM (Modules)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;-Contacts: People who you have a business relationship.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Accounts: Clients who you've done a business before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Leads: Accounts who are your potential customers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Oppurtunites: Any business opportunity for an account or a lead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Sales Orders&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Calendar&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Tasks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What kind of unstructured data or the ways (ideas) could be useful for the modules I've just wrote above? If you need more specific information please write in comments.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am starting to play around in datamining / machine learning and I am stuck on a problem that's probably easy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have a report that lists the url and the number of visits a person did. So a combination of ip and url result in an amount of visits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to run the k-means clustering algorithm on this so I thought I could approach it like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;url      ip    visits&#xA;&#xA;abc.be   123   5&#xA;abc.be/a 123   2&#xA;abc.be/b 123   2&#xA;abc.be/b 321   4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I would turn in into a feature vector/matrix like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc.be  abc.be/a   abc.be/b   impressions&#xA;   1       0          0          5&#xA;   0       1          0          2&#xA;   0       0          1          2&#xA;   0       0          1          4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I am stuck on how to transform my data set to a feature matrix. Any help would be appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I would like to use a neural network for image classification.  I'll start with pre-trained CaffeNet and train it for my application.  &lt;/p&gt;&#xA;&#xA;&lt;h1&gt;How should I prepare the input images?&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don't know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn't be really the same scale (maybe 1.5x variation).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Dataset augmentation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I've heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am new to &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot; rel=&quot;noreferrer&quot;&gt;machine learning&lt;/a&gt;!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now I am doing some problems on an application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, &lt;a href=&quot;http://scikit-learn.org&quot; rel=&quot;noreferrer&quot;&gt;scikit-learn&lt;/a&gt; takes only numbers as parameters, but I want to inject the strings as well as they carry a significant amount of knowledge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I handle such a scenario?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can convert a string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your support!&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am struggling to choose a right data prediction method for the following problem.&#xA;Essentially I am trying to model a scheduler operation, trying to predict its scheduling without knowing the scheduling mechanism and having incomplete data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) There are M available resource blocks that can carry data, N data channels that must be scheduled every time instance i&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Inputs into the scheduler:  &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Matrix $X_i$ size M by N, consisting of N column vectors from each data source.    Each of M elements is index from 1 to 32 carrying information about quality of data channel for particular resource block. 1 - really bad quality, 32 - excellent quality.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Data which contains type of data to be carried (voice/internet etc)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Scheduler prioritizes number of resource blocks occupied by each channel every time instant i.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;I CAN see resource allocation map every time instant&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I DO have access to matrix $X_i$  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I DON'T know the algorithm of scheduler and&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I dont have access to the type of data to be scheduled. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I want to have a best guess (prediction) how the data will be scheduled based on this incomplete information i.e, which resource block will be occupied by which data channel. What is the best choice of prediction/modelling algorithm?&#xA;Any help appreciated!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;There are several metrics for the quality of a graph clustering, e.g. Newman modularity. These enable you to compare two candidate clusterings of the same graph. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know a metric that will answer the question &quot;how modular is this graph&quot;? For example the first of these two graphs is more modular than the second:&#xA;    o===o-----o====o    o----o===o-----o&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be possible to choose a clustering algorithm, run it, and compute your preferred modularity metric for the best clustering found. But this is only a lower bound, so it doesn't seem very satisfactory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question matters. For example, the work of life scientists will be easier if the molecular organisation of life is modular than if it is not. It would be good to have a robust test - some of the discussion so far seems to involve wishful thinking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My best attempt at this is:&#xA;- a tree is more modular if the edges near leaves are higher weight&#xA;- the modularity of a graph is the modularity of its min cut spanning tree&#xA;Does anyone know of an established answer to this question?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I produced association rules by using the arules package (apriori). I'm left with +/- 250 rules. I would like to test/validate the rules that I have, like answering the question: How do I know that these association rules are true? How can I validate them? What are common practice to test it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought about cross validation (with training data and test data) as I read that it's not impossible to use it on unsupervised learning methods..but I'm not sure if it makes sense since I don't use labeled data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If someone has a clue, even if it's not specifically about association rules (but testing other unsupervised learning methods), that would also be helpful to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I uploaded an example of the data that I use here in case it's relevant: &lt;a href=&quot;https://www.mediafire.com/?4b1zqpkbjf15iuy&quot; rel=&quot;nofollow&quot;&gt;https://www.mediafire.com/?4b1zqpkbjf15iuy&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Please I want to know if there is any SVM R package that can handle more than one response variable (y) at a time. that is to train one model for predicting more than one response variable. it could be regression or multi class classification problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have been working in the last years with statistics and have gone pretty deep in programming with R. I have however always felt that I wasn't completely grasping what I was doing, still understanding all passages and procedures conceptually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wanted to get a bit deeper into the math behind it all. I've been looking online for texts and tips, but all texts start with a very high level. Any suggestions on where to start?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more precise, I'm not looking for an exaustive list of statistical models and how they work, I kind of get those. I was looking for something like &quot;Basics of statistical modelling&quot;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Many of us are very familiar with using R in reproducible, but very much targeted, ad-hoc analysis. Given that R is currently the best collection of cutting-edge scientific methods from world-class experts in each particular field, and given that plenty of libraries exist for data io in R, it seems very natural to extend its applications into production environments for live decision making.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;did someone of you go into production with pure R (I know of shiny, yhat etc, but would be very interesting to hear of pure R);&lt;/li&gt;&#xA;&lt;li&gt;is there a good book/guide/article on the topic of building R into some serious live decision-making pipelines (such as e.g. credit scoring);&lt;/li&gt;&#xA;&lt;li&gt;I would like to hear also if you think it's not a good idea at all;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I start with a data.frame (or a data_frame) containing my dependent Y variable for analysis, my independent X variables, and some &quot;Z&quot; variables -- extra columns that I don't need for my modeling exercise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I would like to do is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an analysis data set without the Z variables;&lt;/li&gt;&#xA;&lt;li&gt;Break this data set into random training and test sets;&lt;/li&gt;&#xA;&lt;li&gt;Find my best model;&lt;/li&gt;&#xA;&lt;li&gt;Predict on both the training and test sets using this model;&lt;/li&gt;&#xA;&lt;li&gt;Recombine the training and test sets by rows; and finally&lt;/li&gt;&#xA;&lt;li&gt;Recombine these data with the Z variables, by column.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It's the last step, of course, that presents the problem -- how do I make sure that the rows in the recombined training and test sets match the rows in the original data set? We might try to use the row.names variable from the original set, but I agree with Hadley that this is an error-prone kludge (my words, not his) -- why have a special column that's treated differently from all other data columns?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One alternative is to create an ID column that uniquely identifies each row, and then keep this column around when dividing into the train and test sets (but excluding it from all modeling formulas, of course). This seems clumsy as well, and would make all my formulas harder to read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This must be a solved problem -- could people tell me how they deal with this? Especially using the plyr/dplyr/tidyr package framework?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Terms like 'data science' and 'data scientist' are increasingly used these days.&#xA;Many companies are hiring 'data scientist'. But I don't think it's a completely new job.&#xA;Data have existed from the past and someone had to deal with data. &#xA;I guess the term 'data scientist' becomes more popular because it sounds more fancy and 'sexy'&#xA;How were data scientists called in the past?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want to turn reviews of up to 5 stars and the number of reviews into upvotes. What's a good algorithm for doing this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A venue with 10 reviews total with a 5-star average rating should obviously get more upvotes than a venue with 10 reviews total with a 3-star average rating. Also, a venue with 60 ratings and a 4-star rating should probably get more upvotes than the one with 10 reviews and a 5-star rating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need this rating to be based off of the total number of reviews and the average star rating, but I would also like the number to stay below a variable number (for example, say upvotes stay below 100, but I can also plug in 200 and it would stay below 200).&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm struggling with for loop in R. I have a following data frame with sentences and two dictionaries with pos and neg words:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(stringr)&#xA;library(plyr)&#xA;library(dplyr)&#xA;library(stringi)&#xA;library(qdap)&#xA;library(qdapRegex)&#xA;library(reshape2)&#xA;library(zoo)&#xA;&#xA;# Create data.frame with sentences&#xA;sent &amp;lt;- data.frame(words = c(&quot;great just great right size and i love this notebook&quot;, &quot;benefits great laptop at the top&quot;,&#xA;                         &quot;wouldnt bad notebook and very good&quot;, &quot;very good quality&quot;, &quot;bad orgtop but great&quot;,&#xA;                         &quot;great improvement for that great improvement bad product but overall is not good&quot;, &quot;notebook is not good but i love batterytop&quot;), user = c(1,2,3,4,5,6,7),&#xA;               number = c(1,1,1,1,1,1,1), stringsAsFactors=F)&#xA;&#xA;# Create pos/negWords&#xA;posWords &amp;lt;- c(&quot;great&quot;,&quot;improvement&quot;,&quot;love&quot;,&quot;great improvement&quot;,&quot;very good&quot;,&quot;good&quot;,&quot;right&quot;,&quot;very&quot;,&quot;benefits&quot;,&#xA;          &quot;extra&quot;,&quot;benefit&quot;,&quot;top&quot;,&quot;extraordinarily&quot;,&quot;extraordinary&quot;,&quot;super&quot;,&quot;benefits super&quot;,&quot;good&quot;,&quot;benefits great&quot;,&#xA;          &quot;wouldnt bad&quot;)&#xA;negWords &amp;lt;- c(&quot;hate&quot;,&quot;bad&quot;,&quot;not good&quot;,&quot;horrible&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And now I'm gonna to create replication of origin data frame for big data simulation:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Replicate original data.frame - big data simulation (700.000 rows of sentences)&#xA;df.expanded &amp;lt;- as.data.frame(replicate(100000,sent$words))&#xA;    sent &amp;lt;- coredata(sent)[rep(seq(nrow(sent)),100000),]&#xA;    sent$words &amp;lt;- paste(c(&quot;&quot;), sent$words, c(&quot;&quot;), collapse = NULL)&#xA;rownames(sent) &amp;lt;- NULL&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For my further approach, I'll have to do descending ordering of words in dictionaries with their sentiment score (pos word = 1 and neg word = -1).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Ordering words in pos/negWords&#xA;wordsDF &amp;lt;- data.frame(words = posWords, value = 1,stringsAsFactors=F)&#xA;wordsDF &amp;lt;- rbind(wordsDF,data.frame(words = negWords, value = -1))&#xA;wordsDF$lengths &amp;lt;- unlist(lapply(wordsDF$words, nchar))&#xA;wordsDF &amp;lt;- wordsDF[order(-wordsDF[,3]),]&#xA;wordsDF$words &amp;lt;- paste(c(&quot;&quot;), wordsDF$words, c(&quot;&quot;), collapse = NULL)&#xA;rownames(wordsDF) &amp;lt;- NULL&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have a following function with for loop. 1) matching exact words 2) count them 3) compute score 4) remove matched words from sentence for another iteration:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scoreSentence_new &amp;lt;- function(sentence){&#xA;  score &amp;lt;- 0&#xA;  for(x in 1:nrow(wordsDF)){&#xA;    sd &amp;lt;- function(text) {stri_count(text, regex=wordsDF[x,1])} # count matched words&#xA;    results &amp;lt;- sapply(sentence, sd, USE.NAMES=F) # count matched words&#xA;    score &amp;lt;- (score + (results * wordsDF[x,2])) # compute score&#xA;    sentence &amp;lt;- str_replace_all(sentence, wordsDF[x,1], &quot; &quot;) # remove matched words from sentence for next iteration&#xA;  }&#xA;  score&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I call that function&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SentimentScore_new &amp;lt;- scoreSentence_new(sent$words)&#xA;    sent_new &amp;lt;- cbind(sent, SentimentScore_new)&#xA;    sent_new$words &amp;lt;- str_trim(sent_new$words, side = &quot;both&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it resulted into desired output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                                                                             words user     SentimentScore_new&#xA;                             great just great right size and i love this notebook    1                  4&#xA;                                                 benefits great laptop at the top    2                  2&#xA;                                               wouldnt bad notebook and very good    3                  2&#xA;                                                                very good quality    4                  1&#xA;                                                             bad orgtop but great    5                  0&#xA; great improvement for that great improvement bad product but overall is not good    6                  0&#xA;                                       notebook is not good but i love batterytop    7                  0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In real I'm using dictionaries with pos/neg words about 7.000 words and I have 200.000 sentences. When I used my approach for 1.000 sentences it takes 45 mins. Please, could you anyone help me with some faster approach using of vectorization or parallel solution. Because of my beginner R programming skills I'm in the end of my efforts :-( Thank you very much in advance for any of your advice or solution&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering about something like that:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 1:nrow(wordsDF)&#xA;score &amp;lt;- 0&#xA;&#xA;try_1 &amp;lt;- function(ttt) {&#xA;sd &amp;lt;- function(text) {stri_count(text, regex=wordsDF[ttt,1])}&#xA;results &amp;lt;- sapply(sent$words, sd, USE.NAMES=F)&#xA;    score &amp;lt;- (score + (results * wordsDF[ttt,2])) # compute score (count * sentValue)&#xA;    sent$words &amp;lt;- str_replace_all(sent$words, wordsDF[ttt,1], &quot; &quot;)&#xA;score&#xA;}&#xA;&#xA;a &amp;lt;- unlist(sapply(n, try_1))&#xA;apply(a,1,sum)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But doesn't work :-(&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Does reinforcement learning always need a grid world problem to be applied to?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me any other example of how reinforcement learning can be applied to something which does not have a grid world scenario?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I took on a project to predict the outcome of soccer matches but it turned out to be a very challenging task.&#xA;I tried out different models but I only got 50-54% accuracy on my test dataset. Some of the models were created in such&#xA;a way that a certain model would predict if a team will win, draw, or loose a match. That same model would also predict if&#xA;the opponent of that team will win, draw, or loose the match. Each model predicting with an accuracy of about 50% on each team distinctively. The second set of models I tried, takes the combination of data from both teams and predicts which class the match belongs to (home win, away win, draw). In the system,&#xA;only 10 matches are given everyday to be predicted. Meaning, if I predict the 10 matches using the second model, I have a chance of predicting 5 &#xA;correctly. In this project, I only need to predict 3 matches correctly out of the 10 matches given in a day. Is there a system&#xA;of knowing the 3 matches which my models have the best chance of predicting correctly? I only need to get 3 correct, I usually&#xA;get 5 correctly but I don't know how to select my 3 best matches. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The first type of models use about 50 features for prediction while the second uses 101. I've tried ensembles, they still give &#xA;me ~50% accuracy. I'm still about to setup a system that selects matches where the prediction for the home team does not &#xA;contradict the prediction for the away team using the first type of model.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The project I am working on allows users to create Stock Screeners based on both technical and fundamental criteria.  Stock Screeners are then &quot;backtested&quot; by simulating the results of applying in over the last 10 years using Point-in-Time data. I get back the list of trades and overall graph of performance.&#xA;(If that is unclear, I have an overview &lt;a href=&quot;https://www.equitieslab.com/features/stock-screener/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.equitieslab.com/wiki/QuickStart/StockScreener&quot; rel=&quot;nofollow&quot;&gt;there&lt;/a&gt; with more details).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now a common problem is that users create overfitted stock screeners. I would love to give them a warning when the screen is likely to be over-fitted.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fields I have to work with&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All trades made by the Stock Screener&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stock, Start Date, Start Price, End Date, End Price&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;S&amp;amp;P 500 performance for the same time frame&lt;/li&gt;&#xA;&lt;li&gt;Market Cap, Sector, and Industry of each Stock&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;',),\n",
       " ('&lt;p&gt;The training set contains $p$ papers. Each paper is annotated as &lt;em&gt;research&lt;/em&gt; or &lt;em&gt;non-research&lt;/em&gt;. To develop the research paper filter, we consider the $W$ most frequent phrases in a paper. The research paper filter will use the presence/absence of these $W$ phrases to decide if the paper is indeed a research paper or not.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;How many possible hypothesis are there?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have $2^W$ possible hypothesis, but if we are including conjunctive hypothesis symbols, then I would say $4^W$. Is my logic correct?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How many of them are consistent?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For this question, I have no idea where to begin. Any insight?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We learned normalization last week and it seems like you're supposed to normalize data when they have very different values. For decision trees, is the case the same?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure about this but would normalization affect the resulting decision tree from the same data set? It doesn't seem like it should but...&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was given a target function to design neural network and train: (y = (x1 ∧ x2) ∨ (x3 ∧ x4))&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of input and number of output seems obvious (4 and 1). And the training data can use truth table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in order to train as a multilayer artificial neural network, I need to choose number of hidden units. May I know where can I find some general guideline for this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I want to generate an $N\\\\times N$ matrix $A$ so as to target an $N$ vector of row sums and simultaneously all column sums should sum to 1. In addition to this, I have a prefixed number of elements which are set to zero. &#xA;For example, beginning with:&#xA;$$&#xA;  \\\\left[\\\\begin{array}{rrr}&#xA;    0 &amp;amp; 1 &amp;amp; 1 \\\\\\\\&#xA;    1 &amp;amp; 0 &amp;amp; 0 \\\\\\\\&#xA;    1 &amp;amp; 1 &amp;amp; 0&#xA;  \\\\end{array}\\\\right]&#xA;$$&#xA;and the row sum vector $[1.5, 0.25, 1]^{T}$, I want to end up with&#xA;$$&#xA;  \\\\left[\\\\begin{array}{rrr}&#xA;    0 &amp;amp; a_{12} &amp;amp; a_{13} \\\\\\\\&#xA;    a_{21} &amp;amp; 0 &amp;amp; 0 \\\\\\\\&#xA;    a_{31} &amp;amp; a_{32} &amp;amp; 0&#xA;  \\\\end{array}\\\\right]&#xA;$$&#xA;under the following conditions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{12} + a_{13}  = 1.5$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$ a_{21} = 0.25$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{31}+a_{32} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{21}+a_{31} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{12}+a_{32} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$a_{13} = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While this is simplistic, in general, I have $2N$ equations in $N^{2}-Z$ unknowns, where $Z$ is the number of elements fixed to zero. So, this system of equations could be overdetermined or underdetermined, but I would like to be able to generate matrices like this such that all nonzero elements lie in $(0,1]$.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm a Newbie to Hadoop!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By web search and after going through hadoop guides, it is clear that hdfs doesn't allow us to edit the file but to append some data to existing file, it will be creating a new instance temporarily and append the new data to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1.would like to know whether the &lt;em&gt;new file&lt;/em&gt; or &lt;em&gt;temp file&lt;/em&gt; is created in the &lt;em&gt;same block&lt;/em&gt; or in a &lt;em&gt;different block&lt;/em&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2.What will happen if the revised file &lt;strong&gt;exceeds&lt;/strong&gt; the previous  allocated block size?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be greatly appreciated!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Any suggestion on what kind of dataset lets say $n \\\\times d$ ($n$ rows, $d$ columns) would give me same eigenvectors?.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe it should be the one with same absolute values in each cell. Like alternating +1 and -1. But it seems to work otherwise. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am using twitteR package to retrievie timeline data. My request looks as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;tweets &amp;lt;- try(userTimeline(user , n=50),silent=TRUE)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and this worked quite well for a time, but now I receive this error message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in function (type, msg, asError = TRUE)  : easy handle already used in multi handle&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In a related question on Stackoverflow one answer is to use Rcurl directly but this does not seem to work with twitteR package. Anybody got an idea on this?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a huge collection of objects from which only a tiny fraction are in a class of interest. The collection is initially unlabelled, but labels can be added using an expensive operation (for example, by human).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I use the simple generic machine learning strategy:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Use hand-crafted rules to select a smaller subset of objects (thus leaving out a fraction of interesting ones).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Label part of the smaller subset, and use these for training and choosing a classification algorithm and its parameters.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Classify the remaining objects in the smaller set (and also perhaps in the big set).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This has two drawbacks:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The labeller still needs to see a huge number of uninteresting objects, and therefore is able to label only a very small fraction of interesting ones. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The objects not in the smaller set are completely ignored in the learning phase, resulting in a loss of some information (the classification algorithm might not work well on this complement).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It seems that it would be better to use online learning: i.e., select the objects to show to the labeller based on the previous labels. But then it becomes no longer obvious that the result of classification algorithm retains the nice theoretical properties (i.e., statistical consistency).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a general framework for active object detection which works either theoretically or practically (or both)? I could not get the complete picture from the Wikipedia article &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_learning_%28machine_learning%29&quot; rel=&quot;nofollow&quot;&gt;active learning&lt;/a&gt;. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;This is an interview question&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;How are neural nets related to Fourier transforms?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I could find papers that talk about methods to process the Discrete Fourier Transform (DFT) by a single-layer neural network with a linear transfer function. Is there some other correlation that I'm missing?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I need to implement an algorithm for multiple extended string matching in text. Algorithms to match regular expression would be perhaps too slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Extended&lt;/strong&gt; means the presence of wildcards (any number of characters instead of a star), for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc*def //matches abcdef, abcpppppdef etc.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Multiple&lt;/strong&gt; means that the search is going on simultaneously for multiple string patterns (not a separate search for each pattern), for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abc*def&#xA;abc&#xA;whatever&#xA;some*string&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;QUESTION:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the fast algorithm that can do multiple extended string matching?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Preferably, optimized for SIMD instructions and multicore implementation. Open source implementation (C/C++/Python) would be great as well. I'm interested in 10 Gbps+ performance on a single core of a modern CPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am attempting to work with a very large data-set (~1.5mil lines) for the first time in SAS and I am having some difficulty. The data-set I have is formatted as a &quot;long&quot; .txt file as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;'cat1/: Topic1_Variable1'&#xA;'cat2/: Topic1_Variable2'&#xA;'cat3/: Topic1_Variable3'&#xA;'cat4/: Topic1_Variable4'&#xA;&#xA;'cat1/: Topic2_Variable1'&#xA;'cat2/: Topic2_Variable2'&#xA;'cat3/: Topic2_Variable3'&#xA;'cat4/: Topic2_Variable4'&#xA;&#xA;'cat1/: Topic3_Variable1'&#xA;'cat2/: Topic3_Variable2'&#xA;'cat3/: Topic3_Variable3'&#xA;'cat4/: Topic3_Variable4'&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fro analysis and sharing with others, I really would like to see it formatted as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat1              cat2              cat3              cat4&#xA;Topic1_Variable1  Topic1_Variable2  Topic1_Variable3  Topic1_Variable4&#xA;Topic2_Variable1  Topic2_Variable2  Topic2_Variable3  Topic2_Variable4&#xA;Topic3_Variable1  Topic3_Variable2  Topic3_Variable3  Topic3_Variable4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think that this may be easier in R, but I honestly am drawing a complete blank in SAS. I've even played with MS Access to try to get it to look the way I want but the program crashes every time (due to the size?). At any rate, I have looked into some of the statements in PROC TRANSPOSE and PROC SQL but it seems that most functions within those procedures are utilized to combine duplicate 'Topics'. In the data I have been provided, each &quot;group&quot; represents an individual response to a question with several thousand individuals repeated, I want to retain the independence of each occurrence and not perform a UNION as defined in PROC SQL. At this point, I feel like I am over-thinking this but I just can't get around the mental block and actually do what I am working toward. &#xA;Any help or guidance is much appreciated. I'm open to trying all suggestions or ideas, I think I have access to most statistical computing programs. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying my first 'project' concerning machine learning and I am a bit stuck.&#xA;However, I am not sure if it's even possible but here goes my question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to achieve is clustering user groups based on the amount of visits a user does on a certain website.&#xA;So I started out with this feature matrix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;USER    abc.be  abc.be/a    abc.be/b    xyz.be  xyz.be/a&#xA;123      0        0           0            0      1&#xA;456      1        0           1            0      0&#xA;789      2        3           1            0      0&#xA;321      1        0           1            0      1&#xA;654      1        1           1            1      1&#xA;987      0        1           0            3      0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I got in this example 5 features (my 5 different websites).&#xA;So then I used PCA to come to 2 dimensions, so I could plot it and see how it went.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My feature matrix (in my example) is 5 columns * 6 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My PCA matrix is 2 columns * 6 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I came to this plot (please note that this plot uses different data then the example but the idea is the same)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Tl0Qv.png&quot; alt=&quot;PCA points and k-means centroids&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green points are my PCA points&#xA;The red circles are my K-Means centroids.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the part I am struggling with is this: so I got my clusters (red circles) but how can I use this to say:&quot;Looks like most users go to either site A or site B)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I couple my clusters to a feature label from my feature matrix?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or how does one approach this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated :)&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Problem: I want to know methods to perform an effective sampling from a database.&#xA;The size of the database is about &lt;code&gt;250K&lt;/code&gt; text documents and in this case each text document is related to some majors (Electrical Engineering, Medicine and so on). So far I have seen some simple techniques like &lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_random_sample&quot; rel=&quot;nofollow&quot;&gt;Simple random sample&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Stratified_sampling&quot; rel=&quot;nofollow&quot;&gt;Stratified sampling&lt;/a&gt;; however, I don't think it's a good idea to apply them for the following reasons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;In the case of simple random sample, for instance, there are a few documents in the database that talk about majors like Naval Engineering or Arts. Therefore I think they are less likely to be sampled with this method but I would like to have some samples of every major as possible.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In the case of stratified sampling, most of the documents talk about more than one major so I cannot divide the database in subgroups because they will not be mutually exclusive, at least for the case in which each major is a subgroup.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Finally, I cannot use the whole database due to expensive computational cost processing. So I would really appreciate any suggestions on other sampling methods. Thanks for any help in advance.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am a newbie. I would like to do visualization on twitter data : top trends based on country (over map) and time variations (for each months in 1 year or for each year) . Can someone tell me where can I get the twitter data set and any advice on how to start proceeding would be really help full.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am training a neural network with 1 sigmoid hidden layer and a linear output layer. The network simply approximates a cosine function. The weights are initiliazed according to Nguyen-Widrow initialization and the biases are initialized to 1. I am using MATLAB as a platform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Running the network a number of times without changing any parameters, I am getting results (mean squared error) which range from 0.5 to 0.5*10^-6. I cannot understand how the results can even vary that much, I'd imagine there would at least be a narrower and more consistent window of errors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What could be causing such a large variance?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;In a assignment we are given macro economic indicators like GDP, Consumer price index, Producer Price index and Industrial production index. Also we are given Crude oil, Sugar prices and FM-CG Sales. We are required to forecast future quarter sales and give a model. As I'm new to this subject, I don't know where to start with it, or what to read. Can anyone provide me with some examples of what to do, or any PDFs which might be helpful. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Given a sentence like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Complimentary gym access for two for the length of stay ($12 value per person per day)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What general approach can I take to identify the word gym or gym access?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;For example, there are 4 objects. Each object has some elements. There are 3 clusters. Each element of each object belongs to some cluster. Clustering was done before, and we know only how many elements of each object belong to some cluster.&#xA;How we can calculate the porbability of similarity of two objects?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;         Cluster1  Cluster2  Cluster3&#xA;Object1    500       300        200&#xA;Object2    200       200        100&#xA;Object3    250       2500       1250       &#xA;Object4    190       210        300&#xA;&lt;/pre&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am new to python programming. As part of a data analysis project, I am trying to get a scatter plot of Salary vs Wins for each year of 4 consecutive years (so I am trying to get 4 different scatter plots, one for each year). I am using the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;teamName = 'OAK'&#xA;years = np.arange(2000,2004)&#xA;for yr in years:&#xA;    df = joined[joined['yearID']==yr]&#xA;    plt.scatter(df['salary']/1e6,df['W'])&#xA;    plt.title('Wins vs Salaries in year' + str(yr))&#xA;    plt.xlabel('Total Salary (in millions)')&#xA;    plt.ylabel('Wins')&#xA;    plt.xlim(0,180)&#xA;    plt.ylim(30,130)&#xA;    plt.grid()&#xA;    plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I am only getting one plot corresponding to 2003. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone point out the mistake ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;The optimisation problem for support vector regression is (see &lt;a href=&quot;http://alex.smola.org/papers/2003/SmoSch03b.pdf&quot; rel=&quot;nofollow&quot;&gt;http://alex.smola.org/papers/2003/SmoSch03b.pdf&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;minimise:&#xA;\\\\begin{align*}&#xA;C\\\\sum_{i=0}^{l}(\\\\xi_{i} +\\\\xi^{*}_{i})+ \\\\frac{1}{2}\\\\lVert w \\\\rVert^{2}&#xA;\\\\end{align*}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;subject to the constraints:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;\\\\begin{align*}&#xA;&amp;amp; y_{i} - &amp;lt;w,x_{i}&amp;gt; - b \\\\leq \\\\epsilon + \\\\xi_{i} \\\\\\\\&#xA;&amp;amp; &amp;lt;w,x_{i}&amp;gt; + b - y_{i} \\\\leq \\\\epsilon + \\\\xi^{*}_{i} \\\\\\\\&#xA;&amp;amp; \\\\xi_{i}, \\\\xi^{*}_{i} \\\\geq 0&#xA;\\\\end{align*}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand where the $\\\\lVert w \\\\rVert^{2}$ comes from. I understand how the $\\\\lVert w \\\\rVert^{2}$ is derived in the case of support vector classification (by maximizing the margin), but not in the case of regression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper says that the &quot;goal is to find a function $f(x)$ that has at most $\\\\epsilon$ deviation from the actually obtained targets $y_{i}$ for all the training data, and at the same time is as flat as possible. In other words, we do not care about errors as long as they are less than $\\\\epsilon$, but will not accept any deviation larger than this.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I am not sure what &quot;flat&quot; means.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does someone know?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have implemented an interactive visualization with d3.js, javascript to explore the frequency and various combinations of co-occurring item sets. I want to complement the interactive exploration with some automated options.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does someone know an efficient javascript implementation of the association rules mining ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My typical scenario will have just up to 30 different items.&#xA;There are some good web site with implementations of frequent item set mining (improvements from the initial apriori algorithm): &lt;a href=&quot;http://www.borgelt.net/apriori.html&quot; rel=&quot;nofollow&quot;&gt;http://www.borgelt.net/apriori.html&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is greatly appreciated.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to read a csv file as input from user in Shiny and assign it to a variable in global.r file.The code I have in ui.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fileInput('file1', 'Choose CSV File',&#xA;                     accept=c('text/csv', 'text/comma-separated-values,text/plain', .csv'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code I have in main Panel of server.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;textOutput('contents')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code I have currently in server.R is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;output$contents &amp;lt;- renderText({    &#xA;      if(is.null(input$file1))return()&#xA;      inFile &amp;lt;- input$file1&#xA;      data&amp;lt;-read.csv(inFile$datapath)&#xA;  print(summary(data))&#xA;&#xA; })&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to assign this input$file1 to a variable called 'data' in global.r. Please let me know if this is possible. &#xA;Thanks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am from information security field, and have some introductory level understanding of machine learning field. My problem is to identify behavioural patterns from network traffic. If I use supervised learning ( classification ) the results are very promising but I encounter of a problem of missing out a pattern with slight change in behaviour. And If I use clustering then i need to do a much work on manual tuning of centroids position to get better clusters. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking to ensemble classification and clustering and use classification output to tune cluster centroids. Is this is a good idea to address the problem?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Jaccard_index&quot; rel=&quot;nofollow&quot;&gt;Jaccard coefficient&lt;/a&gt; measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had 100 sets all of same cardinality. By mistake I calculated the similarity measures as the ratio of intersection with total elements in a set (i.e 100).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This gives different similarity values than the original Jaccard formula.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering if the original formula has considered the union of two sets to handle cases where there might be sets with different cardinalities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think though numerically my values are different, they repersent the same idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anybody could verify/disverify what I am trying to do ?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am subsetting some data frames and am sure there is a better way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially, I have two data frames.  The first is the actual data.  The second has some meta data and importantly a flag on whether or not the row is in the subset I am interested in.  All I would like to do is pull out the subset and write a file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, here is my subset data frame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     head(temp[,c(1,4:8)])&#xA;     ensembl_gene_id   FGF Secreted  SigP Receptors    TF&#xA;1 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;2 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;3 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;4 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;5 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;6 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is my actual data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Expt1_C1_EXONS_dxd_res_wDesc[1:5,1:5]&#xA;                                   groupID featureID exonBaseMean dispersion         stat&#xA;ENSMUSG00000000001:E001 ENSMUSG00000000001      E001    624.80240 0.04271781  1.255734504&#xA;ENSMUSG00000000001:E002 ENSMUSG00000000001      E002     30.92281 0.02036015  0.514038911&#xA;ENSMUSG00000000001:E003 ENSMUSG00000000001      E003     41.61413 0.01546023 10.105615831&#xA;ENSMUSG00000000001:E004 ENSMUSG00000000001      E004    137.47209 0.03975305  0.281105120&#xA;ENSMUSG00000000001:E005 ENSMUSG00000000001      E005     85.97116 0.05753662  0.005482149`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I was doing is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;write.table(Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,],&#xA;            &quot;Expt1_C1_EXONS_dxd_res_wDesc_FGF.tab&quot;,col.names=T,row.names=F,quote=F)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is taking 20+ min per subset just to subset and write the data.  The data frame is 370,000 rows by 27 variables, so large but not huge.  I have about 30 of these to do.  Is there a more efficient way?  Note, the groupID does NOT equal the first column in my subset data frame.  In some instances the groupID contains a concatenated set of ensembl ids.  So I have preprocessed to get the temp data frame to have what I want in the same row order.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Bob&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to write an ANN in python for handwriting recognition by mouse movements. ( like identify characters we draw in paint app n convert it to text) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question might seem that I haven't done my homework, but I am *not * looking for image datasets for handwritten characters.( at least that's what I've figured out so far)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any database for such (array or co-ordinates corresponding to each character) inputs? If not, how can I create this database from existing ones like MNIST?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, I have written a script to generate my own database because I could not find any. But it seems a very tedious task to generate a good database. &#xA;I am a beginner in ML. Could somebody point me in &lt;em&gt;right direction&lt;/em&gt; upon how else can we do it?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;If I prune a decision tree, does that make the resulting decision tree always more general than the original decision tree?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there examples where this is not the case?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm struggling to find a solution to produce a line chart which shows a trend from a Time Series metric. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I try to make my line chart look similar to the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/hZaa2.png&quot; alt=&quot;Time series&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to make use of relative change rates, but I struggle to find a calculation which makes a visualization as seen above possible. The graph should always be relative to a specific time window, meaning that I can query my data dynamically with a start and end timestamp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My (incomplete) sample data set is the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;timestamp,value,change_rate,change_rate_absolute_sum,change_rate_delta,change_rate_sum&#xA;1426587900000,778;0.00129,0.00129;0.00129,0.00129&#xA;1426588200000,778;0,0.00129;-0.00129,0&#xA;1426588500000,1189;0.52828,0.52957;0.52828,0.52828&#xA;1426588800000,1195;0.00505,0.53462;-0.52323,0.00505&#xA;1426589100000,1195;0,0.53462;-0.00505,0&#xA;1426589400000,1196;0.00084,0.53546;0.00084,0.00084&#xA;1426589700000,1286;0.07525,0.61071;0.07441,0.07525&#xA;1426590000000,1290;0.00311,0.61382;-0.07214,0.00311&#xA;1426590300000,1294;0.0031,0.61692;-0.00001,0.0031&#xA;1426590600000,1296;0.00155,0.61847;-0.00155,0.00155&#xA;1426590900000,1356;0.0463,0.66477;0.04475,0.0463&#xA;1426591200000,1358;0.00147,0.66624;-0.04483,0.00147&#xA;1426591500000,1358;0,0.66624;-0.00147,0&#xA;1426591800000,1360;0.00147,0.66771;0.00147,0.00147&#xA;1426592100000,1408;0.03529,0.703;0.03382,0.03529&#xA;1426592400000,1390;-0.01278,0.69022;-0.04807,-0.01278&#xA;1426592700000,1391;0.00072,0.69094;0.0135,0.00072&#xA;1426593000000,1410;0.01366,0.7046;0.01294,0.01366&#xA;1426593300000,1414;0.00284,0.70744;-0.01082,0.00284&#xA;1426593600000,1410;-0.00283,0.70461;-0.00567,-0.00283&#xA;1426593900000,1414;0.00284,0.70745;0.00567,0.00284&#xA;1426594200000,1420;0.00424,0.71169;0.0014,0.00424&#xA;1426594500000,1417;-0.00211,0.70958;-0.00635,-0.00211&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any ideas warmly appreciated. Thanks a lot in advance!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm going to describe the procedure for gradient descent using the language of English:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; For all of the files (e.g. documents) in our training corpus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;--&gt; commence, now we are considering our first document&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; For all the features in the feature vector associated with that training example (document)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; Take the dot product of the vector of weight values * and the vector of features specific to the document under consideration&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* &lt;em&gt;vector of weight values is applicable to all features, though all features will not be present in all documents, indeed only some features will be present in each individual document&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;4)&lt;/strong&gt; Take the value resulting from step 3 and apply this to our loss function. I'm using the Hinge Loss function, i.e. the formula&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/tVvmW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that equation saying that the hinge loss objective function should be the sum of the misclassification error for every file in our training set?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is that equivalent to the java code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Math.max(0, 1 - y * value_from_step_three)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With &lt;code&gt;y = {-1, 1}&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;^ THIS IS MY FIRST POINT OF CONFUSION&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess that is inaccurate but I'm not sure, according to my mental model it should be right. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;5)&lt;/strong&gt; For every weight, update it as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;new weight value&lt;sub&gt;j&lt;/sub&gt; = old weight value&lt;sub&gt;j&lt;/sub&gt; - learning rate * &lt;em&gt;value from step 4&lt;/em&gt; * value of feature&lt;sub&gt;j&lt;/sub&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm a bit confused about what &lt;em&gt;value of feature&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; should be, since I guess this would correspond to a vector, that has values for every weight under consideration, i.e.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Document 1 = [&quot;I&quot;, &quot;am&quot;, &quot;awesome&quot;]&#xA;Document 2 = [&quot;I&quot;, &quot;am&quot;, &quot;great&quot;, &quot;great&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Dictionary is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[&quot;I&quot;, &quot;am&quot;, &quot;awesome&quot;, &quot;great&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the documents as a vector would look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Document 1 = [1, 1, 1, 0]&#xA;Document 2 = [1, 1, 0, 2]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There will be weights:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Weight&lt;sub&gt;&quot;I&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;am&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;awesome&quot;&lt;/sub&gt;, Weight&lt;sub&gt;&quot;great&quot;&lt;/sub&gt;]&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I use RStudio for R programming. I remember about solid IDE-s from other technology stacks, like Visual Studio or Eclipse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What other IDE-s than RStudio are used (please consider providing some brief description on them).&lt;/li&gt;&#xA;&lt;li&gt;Does any of them have noticeable advantages over RStudio?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I mostly mean debug/build/deploy features, besides coding itself (so text editors are probably not a solution).&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm not sure which word to use to differentiate a self-organizing map (SOM) training procedure in which updates for the entire data set are aggregated before they are applied to the network from a training procedure in which the network is updated with each data point individually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of other algorithms I'd say &lt;code&gt;stochastic gradient descent&lt;/code&gt;, but I'm not sure &lt;code&gt;gradient descent&lt;/code&gt; is correct for SOM learning.  To my knowledge, SOM learning does not follow any energy function exactly, so that should mean it doesn't do gradient descent exactly, right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another term would be &lt;code&gt;online learning,&lt;/code&gt; but `online' sort of implies I train my SOM in the real world with data points streaming in, eg. from a set of sensors.  It may also imply that I use every data point only once, which is not what I want to say.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm an &lt;code&gt;R&lt;/code&gt; language programmer. I'm also in the group of people who are considered Data Scientists but who come from academic disciplines other than CS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works out well in my role as a Data Scientist, however, by starting my career in &lt;code&gt;R&lt;/code&gt; and only having basic knowledge of other scripting/web languages, I've felt somewhat inadequate in 2 key areas:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Lack of a solid knowledge of programming theory.&lt;/li&gt;&#xA;&lt;li&gt;Lack of a competitive level of skill in faster and more widely used languages like &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt; and &lt;code&gt;Java&lt;/code&gt;, which could be utilized to increase the speed of the pipeline and Big Data computations as well as to create DS/data products which can be more readily developed into fast back-end scripts or standalone applications.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The solution is simple of course -- go learn about programming, which is what I've been doing by enrolling in some classes (currently C programming). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, now that I'm starting to address problems #1 and #2 above, I'm left asking myself &quot;&lt;em&gt;Just how viable are languages like &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;C++&lt;/code&gt; for Data Science?&lt;/em&gt;&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, I can move data around very quickly and interact with users just fine, but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So. can &lt;code&gt;C&lt;/code&gt; do the job -- what tools are available for advanced statistics, ML, AI, and other areas of Data Science?&lt;/strong&gt; Or must I loose most of the efficiency gained by programming in &lt;code&gt;C&lt;/code&gt; by calling on &lt;code&gt;R&lt;/code&gt; scripts or other languages?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best resource I've found thus far in C is a library called &lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/index.html&quot; rel=&quot;noreferrer&quot;&gt;Shark&lt;/a&gt;, which gives &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;C++&lt;/code&gt; the ability to use Support Vector Machines, linear regression (not non-linear and other advanced regression like multinomial probit, etc) and a shortlist of other (great but) statistical functions.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What approaches are there to not display a search result that a user is not supposed to see?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose we have the following situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;High-level view&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In an enterprise, there are different repositories like websites, databases, filesystem folder etc. And there is a search engine that crawls all those repositories and creates an index. The user can then use the search engine UI to perform searches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say the search engine was &lt;a href=&quot;https://lucene.apache.org/core/&quot; rel=&quot;nofollow&quot;&gt;Apache Lucene&lt;/a&gt; and the encompassing indexing and retrieval system &lt;a href=&quot;http://lucene.apache.org/solr/&quot; rel=&quot;nofollow&quot;&gt;Apache Solr&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;User permissions are managed via &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_Directory&quot; rel=&quot;nofollow&quot;&gt;Microsoft Active Directory&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The content in the different repositories are managed with all kinds of applications like MS Word, custom-made applications, database management tools etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Search-index and user permissions&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The index contains the documents A1, A2, B1 and B2.&lt;/li&gt;&#xA;&lt;li&gt;User A has only permission to see documents of type A*.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Performing a search&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Now, user A performs a search for &lt;code&gt;A* AND B*&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Displaying the results&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A system administrator with permission to see everything should see all four documents in the search results: A1, A2, B1 and B2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;User A however, who only has permissions to see documents of type A*, should/will only see two documents in the search results: A1 and A2. So user A shouldn't even know that documents B1 and B2 exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Elaboration on the question&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What ways are there to implement that requirement that users only see documents they are allowed to see?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suppose it would be a bad idea to add the information to the index whether a user may see a document. I think that because of this use case: suppose all of a sudden user A may only see document A1 and not A2 anymore; those permissions are now set for example on the filesystem. Now it takes a while until the permissions in the index are updated and during that time user A would still be able to see document A2 in the search results even if he can't click on the result and access it anymore.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, the above approach probably would break down when there are hundreds of millions of documents and thousands of employees.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where and how would the security aspect be implemented in a content indexing and retrieval system in the above scenario?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm fairly new to machine learning, but I'm doing my best to learn as much as possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious about how predicting athlete performance (runners in particular) in a race of a specific starting lineup. For instance, if RunnerA, RunnerB, RunnerC, and RunnerD are all racing a 400 meter race, I want to best predict whether &lt;strong&gt;RunnerA&lt;/strong&gt; will beat &lt;strong&gt;RunnerB&lt;/strong&gt; based on past race result information (which I have at my disposal). However, I have many cases where &lt;strong&gt;RunnerA&lt;/strong&gt; has never raced against &lt;strong&gt;RunnerB&lt;/strong&gt;; yet I do have data showing &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; in the past, and &lt;strong&gt;RunnerC&lt;/strong&gt; has beat &lt;strong&gt;RunnerB&lt;/strong&gt; in the past. This logic extends deeper as well. So, it would seem that &lt;strong&gt;RunnerA&lt;/strong&gt; &lt;em&gt;should&lt;/em&gt; beat &lt;strong&gt;RunnerB&lt;/strong&gt;, given this information. My real concern is when it gets more complicated than this as I add more features (multiple runners, different distances, etc), and so I'm turing to ML algorithms to help my predictions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am having difficulty figuring out how to include this in my row data that I can train (after all, correctly formatting data is 99% of proper machine learning), and I am hoping that someone here might have thought along the same lines in the past and might be able to shed some light. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently trying to include &lt;strong&gt;RunnerX&lt;/strong&gt;-&lt;strong&gt;RunnerY&lt;/strong&gt; past race data by counting all the races that &lt;strong&gt;RunnerX&lt;/strong&gt; and &lt;strong&gt;RunnerY&lt;/strong&gt; have run together and normalizing them on a scale from &lt;code&gt;-1&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;; &lt;code&gt;-1&lt;/code&gt; indicating &lt;strong&gt;RunnerX&lt;/strong&gt; lost all past races against RunnerY; and &lt;code&gt;+1&lt;/code&gt; indicating that RunnerX has  won all past races against RunnerY; and &lt;code&gt;+1&lt;/code&gt; indicating. And &lt;code&gt;0&lt;/code&gt; indicating an equal number of wins and losses (or no past races against each other).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, if &lt;strong&gt;RunnerA&lt;/strong&gt; is racing &lt;strong&gt;RunnerB&lt;/strong&gt;, and &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerB&lt;/strong&gt; in the past, then I want the algorithm to know that (denoted by a &lt;code&gt;+1&lt;/code&gt; on the &lt;strong&gt;RunnerB&lt;/strong&gt; column of row &lt;strong&gt;RunnerA&lt;/strong&gt;); same for vice versa. Taking it another step further, If &lt;strong&gt;RunnerA&lt;/strong&gt; is racing &lt;strong&gt;RunnerC&lt;/strong&gt; (but the two have never raced each other in the past), and &lt;strong&gt;RunnerA&lt;/strong&gt; has beat &lt;strong&gt;RunnerD&lt;/strong&gt; in a past race, and &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; in a past race, then I want the algorithm to learn that &lt;strong&gt;RunnerA&lt;/strong&gt; should beat &lt;strong&gt;RunnerC&lt;/strong&gt;. I say &lt;em&gt;beat&lt;/em&gt; here, but I mean an &quot;average beat&quot; for any &lt;strong&gt;RunnerX&lt;/strong&gt;-&lt;strong&gt;RunnerY&lt;/strong&gt; combinations when data for more than 1 past race is available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have set my data up as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name     track   surface  distance  age    RunnerA   RunnerB   RunnerC   RunnerD&#xA;RunnerA  Home    2        400       11     0         1         0         1&#xA;RunnerC  Away    2        400       12     0         0         0         -1&#xA;RunnerD  Home    2        400       10     0         0         1         0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which shows that RunnerA has beat &lt;strong&gt;RunnerB&lt;/strong&gt; and &lt;strong&gt;RunnerD&lt;/strong&gt; in the past. RunnerC has lost to &lt;strong&gt;RunnerD&lt;/strong&gt;. And &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I don't really think this is a correct display of the information for an ML algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;From what I understand, ML data should be row independent. And this data isn't because row 1 (&lt;strong&gt;RunnerA&lt;/strong&gt;) has beat &lt;strong&gt;RunnerD&lt;/strong&gt;, yet the data indicating &lt;strong&gt;RunnerD&lt;/strong&gt; has beat &lt;strong&gt;RunnerC&lt;/strong&gt; is in row 3.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any ideas how I might be able to incorporate this past win-percentage-for-runner-pair-combination data??? I'm totally stuck here. I've read a lot about some algorithms that estimate the win loss by simply totaling win statistics, but those don't say anything about the actual probability of a particular runner to beat another particular runner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers would be super helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!!!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I think there are numerous posts regarding which one to use: R or Python. However, I'm curious about how their architecture differences yield differences in speed performance, &lt;strong&gt;not&lt;/strong&gt; which one to use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This &lt;a href=&quot;http://alstatr.blogspot.com/2014/01/python-and-r-is-python-really-faster.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog post&lt;/a&gt; performs a small test between R and python to show that the (optimized) python code was 2x faster than R code.* And I've read in &lt;a href=&quot;https://datascience.stackexchange.com/questions/59/what-are-rs-memory-constraints&quot;&gt;this post&lt;/a&gt; that R tends to put everything in memory, which is why computations on large datasets is generally slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what makes python's low level memory management so much different than R, which helps it yield these benchmarks?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*Though python was 2x faster in this test than R, I'm not saying that python is generally 2x faster than R.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a large dataset with characters and 90000 intances and I have the error &lt;em&gt;ValueError: array is too big&lt;/em&gt; when I have the following code before the plot_kmeans_digits.py code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data2=list(csv.DictReader(open('C:\\\\diabeticdata.csv', 'rU')))&#xA;vec = DictVectorizer()&#xA;data = vec.fit_transform(data2).toarray()&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know how I can solve this error?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am struggling with a conceptual problem related to feature scaling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume I am building a classifier (e.g., a NN) and let's assume I rely on future scaling for the input features of my model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this context I will normalise the training set using its mean and its std and I would do the same with the testing set using the testing mean and std.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let us also assume I succeed in building my classifier and I move to production where I try to classify new inputs.  However for such new inputs the mean and std are unknown! How can I scale them appropriately before processing with my model? May be I could use the mean and std from training+testing.....&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I really don't know which is the correct practice here....any hint?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your help!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am investigating whether to use theano or caffe for convnets. I would like to know which one provides a better debug environment. In caffe, it seems you don't write any code just a config in protobuf - how do you debug the convnet then?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a complex dataset with more than 16M rows coming from pharmaceutical industry. Regarding the data, it is saved in a sql server with multiple (more than 400) relational tables. Data got several levels of hierachies like province, city, postal code, person, and antigens measures, etc. I would like to create many dashboards in order to observe the changes &amp;amp; trends happening. I can use Pentaho, R (shiny) or Tableau for this purpose. But the problem is data is so huge, and it take so long to process it with dashboard softwares. I have a choice of making cube and connect it to dashboard. My question here is whether there are  any other solutions that I can use instead of making a cube? I don't want to go through the hassle of making &amp;amp; maintaining a cube. I would like to use a software where I specify relationships between tables, so the aggregation/amalgamation happens smoothly and output processed tables that can connect  to dashboards. I hear Alteryx is one software that can do it for you (I haven't tried it myself, and it is an expensive one!). I understand this task needs two or more softwares/tools. Please share your input &amp;amp; experience. Please mention what tools do you use, size of your data, and how fast/efficient is the entire system, and other necessary details. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm testing a SVD-based collaborative filter on my data set, in which the label, $r_{ij}$, is a real value from 0 to 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like the many papers suggested, to have a better performance, instead of using $ \\\\hat{R} = U \\\\cdot V^T $ directly, I use $\\\\hat{R} = \\\\mu + B_u + B_v + U \\\\cdot V^T $, where $\\\\mu$ is the average rating, $B_u$ is the bias of user, and $B_v$ is the bias of item.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, this model corresponds to a loss function:&#xA;$\\\\min_{B_u, B_v, U, V}  = ||I\\\\circ(R-\\\\mu-Bu-B_v-U\\\\cdot V^T)||_F^2 + \\\\lambda (||B_u||_F^2 + ||B_v||_F^2 + ||U||_F^2 + ||V||_F^2)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where I is the masking matrix in which $I_{ij} = 1$ if $R_{ij}$ is known, and $||\\\\cdot||_F$ is the frobenius norm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, I solve this by gradient descent, it seems to work fine, and the test RMSE is 0.25. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I investigate the contribution of each part in predict function $\\\\hat{R} = \\\\mu + B_u + B_v + U \\\\cdot V^T $, I notice that, $\\\\mu$ is about 0.5, $b_u$ and $b_i$ are about $\\\\pm0.3$, but the part of $ U \\\\cdot V^T $ is quite small, normally about $\\\\pm 0.01$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why does this part contribute so small? Since this part is the actual part where the collaborative filter works, I expect it to contribute more in prediction.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have one clarification - &lt;/p&gt;&#xA;&#xA;&lt;p&gt;First the definitions- &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;User-based&lt;/strong&gt;: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Item-based&lt;/strong&gt;: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed offline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the question - in item based CF the similarity between items are tracked via user behavior - since user behavior is changing will it not impact the similarity between items?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Why would you want to decorrelated data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I am reading about PCA and whitening on image data for DNN, I wonder what is the purpose of achieving the identity covariance matrix in your data is?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it to remove interaction between variables, thus allow simpler models to express interactions without having to compute x1*x2?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have been looking at Stochastic Single Value Decomposition in Mahout for implementing a distributed LSA algorithm. However, I am having trouble finding the best way to set k and p such that k+p &amp;lt; min(m,n). Is there an optimal way to set k and p? I know that p should not exceed 10% of k and that k is the rank (typically from 20 to 200 according to the documents). Can I relate it to the number of dependent vectors?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am a beginner studying &lt;strong&gt;social network analysis&lt;/strong&gt;.&#xA;I installed python 3 just 2 weeks ago.&#xA;There are a lot of books for python and social network analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I couldn't choose one of them.&#xA;I found one book named &quot;Mining the Social Web (Analyzing Data from Facebook, Twitter, Linkedln, and Other Social Media Sites)&quot; written by Matthhew A. Russell.&#xA;This book looks very interesting and fits in my purpose, but it is based on python 2. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any good books with &lt;strong&gt;python 3&lt;/strong&gt;? I usually use &lt;strong&gt;Twitter, Facebook, or Blog data&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition, could you recommend any good book for &lt;strong&gt;nodeXL and UCINET?&lt;/strong&gt;  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;This may be too broad of a question with heavy opinions, but I really am finding it hard to seek information about running various algorithms using SQL Server Analysis Service Data Mining projects versus using R. This is mainly because all the data science guys I work with don't have any idea about SSAS because no one seems to use it. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Database Guy&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Before I start, let me clarify. I am a database guy and not a data scientist. I work with people who are data scientist who mainly use R. I assist these guys with creating large data sets where they can analyze and crunch data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My objective here is to leverage a tool that came with SQL Server that no one is really leveraging because no one seems to have a clue about how it works in comparison to other methods and tools such as R, SAS, SSPS and so forth in my camp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;SSAS&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have never really used SQL Server Analysis Services (SSAS) outside of creating OLAP cubes. Those who know SSAS, you can also perform data mining tasks on cubes or directly on the data in SQL Server. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;SSAS Data Mining comes with a range of algorithm types:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Classification algorithms&lt;/strong&gt; predict one or more discrete variables,&#xA;based on the other attributes in the dataset.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Regression algorithms&lt;/strong&gt; predict one or more continuous variables, such&#xA;as profit or loss, based on other attributes in the dataset.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Segmentation algorithms&lt;/strong&gt; divide data into groups, or clusters, of&#xA;items that have similar properties.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Association algorithms&lt;/strong&gt; find correlations between different attributes&#xA;in a dataset. The most common application of this kind of algorithm&#xA;is for creating association rules, which can be used in a market&#xA;basket analysis.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sequence analysis algorithms&lt;/strong&gt; summarize frequent sequences or episodes&#xA;in data, such as a Web path flow.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Predicting Discrete Columns&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With these different algorithm options, I can start making general predictions from the data such as finding out simply who is going to buy a bike based on a predictable column, Bike Buyers, against an input column, Age. The histogram shows that the age of a person helps distinguish whether that person will purchase a bicycle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/X64Y1.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Predicting Continuous Columns&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When the Microsoft Decision Trees algorithm builds a tree based on a continuous predictable column, each node contains a regression formula. A split occurs at a point of non-linearity in the regression formula. For example, consider the following diagram.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/6MkTt.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With some of that said, it seems I can run a range of algorithms on the data and also have various functions available to me in SSAS to run against the data. It also seems I can develop my own algorithms in Visual Studio and deploy them to SSAS (if I'm not mistaken).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what am I missing here in regards to languages and tools from R? Is it just that they have more flexibility to deploy and edit complex algorithms versus SSAS etc?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using k-means clustering to processes running on machines. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Dataset sample : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;machine name, process&#xA;m1,java&#xA;m2,tomcat&#xA;m1,word&#xA;m3,excel&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Build a matrix of associated counts : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   java,tomcat,word,excel&#xA;m1,1,0,1,0&#xA;m2,0,1,0,0&#xA;m3,0,0,0,1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then run k-means against this dataset (have tried Euclidean and Manhattan distance functions) &#xA;The dataset is extremely sparse which I think is causing the generated clusters to not make much sense as many machines get grouped into the same cluster(as they are very similar) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to achieve clusters where each cluster contains approx equal number of points ? Or perhaps this is not possible due to the sparseness of the data and instead I should try to cluster on a different attributes of dataset ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am doing some research on Logistic regression and SVM using different parameters using HOG features. I am facing a bit of problem while understanding each classifier with combination of different parameters and different HOG features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My findings and confusions are given below, &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(1,1)&#xA;Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=209,Wrong Prediction=7&#xA;    , &#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(1,1)&#xA;Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5&#xA;Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5&#xA;Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6            &#xA;&#xA;-------------------------------------------------------------------------            &#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(9,9), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=213,Wrong Prediction=3&#xA;&#xA;&#xA;&#xA;For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;&#xA;&#xA;For Hog:    orientations=18, pixelsPerCell=(9,9), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=3&#xA;&#xA;&#xA;For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(3,3)&#xA;Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')&#xA;Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For the above results we can conclude that &quot;C&quot; of SVM and orientation of HOG doesn't matter much for the combinations, so we do not have the issue of highbias or variance and bin size. What matters is the value of gamma and combination of HOG with &quot;cellsPerBlock&quot;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;From my analysis, I found&#xA;1. Using &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(1,1)&quot; doesn't give proper    outcome.&#xA;2. The &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(1,1)&quot; works best with gamma=0.1&#xA;3. The &quot;rbf&quot; SVM with HOG &quot;cellsPerBlock=(3,3)&quot; and different combination of &quot;pixelspercells&quot; works best with gamma=1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've also tried pixelspercell = (3,3) but all the different combinations of &quot;rbf&quot; kernels fails to classify properly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;&quot; The problem I face is understanding this discrepancy&quot;&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Confusion even eggravates when I use Logistic regression for the Combinations of HOG surprisingly Regularized Logistic Regression works better with cellsPerBlock=(1,1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below are my findings with 2nd order Regularized Logistic Regression: I used 0.9 as my threshold level, i.e below 0.9 would be classifies as 0 and above 0.9 would be classified as 1&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Q:   Why did I put 0.9 as threshold- &#xA;Sol: Because the test data that had the class(labels)=1, 99% of them output probability &amp;gt;0.9&#xA;     However the test data that had the class(labels)=0, many instances outputprobability &amp;gt;0.5&#xA;   So inorder to filter out many test data instances that gave probability &amp;gt;0.5 A threshold of 0.9 was used.&#xA;&#xA;1.  orientations=9,    &#xA;pixelsPerCell=(6,6)&#xA;cellsPerBlock=(3,3)&#xA;Output:     Total dataset=216, Correct prediction=187, Wrong Prediction=29&#xA;&#xA;2.  orientations=9,    &#xA;pixelsPerCell=(9,9)&#xA;    cellsPerBlock=(3,3)&#xA;    Output:     Total dataset=216, Correct prediction=202, Wrong Prediction=14&#xA;&#xA;3.  orientations=9,    &#xA;pixelsPerCell=(9,9)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=211, Wrong Prediction=5&#xA;&#xA;4.  orientations=9,    &#xA;pixelsPerCell=(6,6)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=206, Wrong Prediction=10&#xA;&#xA;5.  orientations=9,    &#xA;pixelsPerCell=(3,3)&#xA;    cellsPerBlock=(1,1)&#xA;    Output:     Total dataset=216, Correct prediction=203, Wrong Prediction=13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;E.g 1: &quot;The speed of the car is 35 km/h and speed limit is 50 km/h&quot; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g 2: &quot;The car is crossing the current speed limit by driving at 90 km/h&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g.3: &quot;Driving at 50 km/h your car will hit the speed limit.&quot;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I very new in neural networks and in NLP too. Good in perl and in writing parsers for web pages. NLP is asking me my code to learn english and word meanings. I wish to try differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to train my first neural network classification algo using offsets to words. I can train that to parse speed: in e.g1 the 35 is true and 50 is false. That in e.g2 90 is true. and e.g3 50 is false.&#xA;Suppose I have a collection of lot of such sentences, so I can use my offset based training for my algorithm. There will be very confusing sentences too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way my algo would work is to use offets of all words which are near the text which is to be classified. In sentence one it would output 11 parameters, is = -1, km/h=+1, car=-2, and=+2,the=-3,speed=+3,of=-4,limit=+4,speed=-5,is=+5,The=-6,km/h=+6. Same thing would be done for all sentences. the algo will train with these offsets and the result. There are too many words and they all will become parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please suggest how durable is my proposed learning algorithm to parse out actual car driven speed from sentences? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I've only worked with ML with .csv formats. I've worked with image formats too but only premade imagesets (MNIST,etc). If I were to create an imageset from scratch, how are the class labels typically formated? Would I have to manually title the image of a jpeg? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best,&#xA;Jeremy&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to implement Streaming Naive Bayes in a distributed system. What are the best approach to choose framework. Should I choose:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Storm alone and implement streaming naive bayes on my own in storm topology.&lt;/li&gt;&#xA;&lt;li&gt;Storm + TridentML&lt;/li&gt;&#xA;&lt;li&gt;Storm + SAMOA&lt;/li&gt;&#xA;&lt;li&gt;Spark Streaming + MLlib&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What is the best framework set to choose and start working on. Any suggestion will be of great help.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Generally, the machine learning model is built on datasets. I'd like to know if there is any way to generate synthetic dataset using such trained machine learning model preserving original dataset characteristics ?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;[original data --&gt; build machine learning model --&gt; use ml model to generate synthetic data....!!!]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible ? Please point me to related resource if possible.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm having a having a hard time understanding the difference between an isomorphism in graphs and canonical graphs. I have read through the Wikipedia articles, but it still isn't clicking.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can somebody explain the difference, perhaps with an example?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have watched an explanation of Big O Notations from this eloquent YouTube video: &lt;a href=&quot;https://www.youtube.com/watch?v=V6mKVRU1evU&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=V6mKVRU1evU&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, he does not mention algorithms with two asterisk signs. How does the algorithm with the complexity of &lt;code&gt;O(N**3)&lt;/code&gt; work?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am working on a data set where the categorical variables have lots of empty spaces (not &quot;NA&quot; but &quot;&quot;). For example, one variable has 14587 empty spaces out of 14644 observations. There are many such variables where most of the observations are empty.In fact it is a survey dataset where the participant just chose to ignore a particular question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have never handled similar dataset. I am looking for advise as to how best to handle such datasets before any modeling is done. Deleting the rows or the variables with lots of empty spaces doesn't seem feasible. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to see if there is a conventional term for this concept to help me in my literature research and writing.  When a machine learning model causes an action to be taken in the real world that affects future instances, what is that called?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about something like a recommender system that recommends one given product and doesn't recommend another given product.  Then, you've increased the likelihood that someone is going to buy the first product and decreased the likelihood that someone is going to buy the second product.  So then those sales numbers will eventually become training instances, creating a sort of feedback loop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a term for this?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Does anyone know, where I can learn about applying data science to win a political campaign? I know the Obama campaign had 12 data scientists in 2008 and 165 data scientists in 2012. In 2012, they ran over 65,000 simulations every night, for 14 months. They correctly predicted every state within 0.5% and Florida within 0.05%. How did they do this? And where can I find the data they used?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I would consider myself a journeyman data scientist.  Like most (I think), I made my first charts and did my first aggregations in high school and college, using Excel.  As I went through college, grad school and ~7 years of work experience, I quickly picked up what I consider to be more advanced tools, like SQL, R, Python, Hadoop, LaTeX, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are interviewing for a data scientist position and one candidate advertises himself as a &quot;senior data scientist&quot; (a very buzzy term these days) with 15+ years experience.  When asked what his preferred toolset was, he responded that it was Excel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I took this as evidence that he was not as experienced as his resume would claim, but wasn't sure.  After all, just because it's not my preferred tool, doesn't mean it's not other people's. &lt;strong&gt;Do experienced data scientists use Excel?  Can you assume a lack of experience from someone who does primarily use Excel?&lt;/strong&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;with regards to the Logistic Regression cost function of:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lBaI2.png&quot; alt=&quot;Logistic Regression Cost Function&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And hypothesis:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/ISUJW.png&quot; alt=&quot;Hypothesis&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to tell the +/- of the error for how &quot;confident&quot; the hypothesis is?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g. if the +/- of the error was 0.1, I would know that if my hypothesis predicted 0.4 it could be 0.1 greater (0.5) or 0.1 less (0.3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is for binary classification&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have around 1,000 job ads in the filed of IT (in excel file). I want to find the skills which are mentioned in each of ads. and then find the similar jobs based on skills.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My method: I created 12 categories Such as programming skills,  testing skills,  communication skills, network skills, ... . Each advertisement may belong to 3-4 categories. In this case, some said multi-variate classification or Multi label classification is useful. But I don't know how to do this kind of classification in RapidMiner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1- Does anyone know how to do multi-variate classification or Multi label classification in RapidMiner? or is there another way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2- Do you recommend &quot;classification&quot; in order to analysis required job skills? or another technique? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;3- Is there any better way to classify the skills which are stated in job ads?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm new in the field of text mining. Please let me know if you have any idea. Thanks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How to write &quot;OR&quot; in this example?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DATA a1;&#xA;set a;&#xA;if var1=1 OR 2;&#xA;run;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. va1 is the categorial (with categories: 1, 2, 3)&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;If I parse following sentence:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;He is playing cricket in ground with grass.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with stanford parser, the result is: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(ROOT&#xA;  (S&#xA;    (NP (PRP He))&#xA;    (VP (VBZ is)&#xA;      (VP (VBG playing)&#xA;        (NP&#xA;          (NP (NN cricket))&#xA;          (PP (IN in)&#xA;            (NP (NN ground))))&#xA;        (PP (IN with)&#xA;          (NP (NN grass)))))&#xA;    (. .)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any parser who can correct the result on the basis of probability as the probability of appearing grass with ground is higher than cricket?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or parser which do chunking before parsing like this:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;He is playing cricket |in ground| |with grass|.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and calculate probability with all combinations before generating the parse tree.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;He is playing cricket |in ground| |with grass|.&lt;/li&gt;&#xA;&lt;li&gt;He is playing cricket |with grass| |in ground|.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Obviously most employers, when hiring a data scientist, would prefer experience with big data and/or data science.  But what can one safely assume they will acknowledge as experience?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say someone often launches software on a computing cluster that typically generates an amount of data.  I'm not sure what the best measure of this data is for data science.  I'll call it one or two thousand rows, 200k or 300k points per row...certainly under 500k.  Then for each point, let's call it 25 or 30 values.  This amounts to 30 or 40 gig of data.  300 or 400 times of this and you can call it a study - maybe one or two studies per year.  I'm under the impression that this is much smaller than a data scientist at Google or Facebook would be used to, but it's certainly too big for my home computing systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If someone's been working with this for years (some people at this company have been doing it since before &lt;em&gt;data science&lt;/em&gt; was coined/before social media existed), is it fair for them to claim big data experience?  According to &lt;a href=&quot;https://datascience.stackexchange.com/a/37/8953&quot;&gt;this answer&lt;/a&gt;, it's not the amount of data but what needs to be done with the data that matters - is that a universally accepted opinion?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, working with this data entails manipulation/cleaning of the data with some proprietary languages, shell scripts, and a lot of Python.  A little bit of R but that's a more recent thing.  It involves tons of data visualization, drawing conclusions, and presenting to management/convincing decision makers.  Some of it involves trend determination, extrapolation, and comparisons made between data sets that aren't related in a straightforward way, so it sounds data science-ish to me.  But I will be the first to admit that I have a limited understanding of what data science currently is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;...bonus points if you can tell me whether or not this is an Easter egg or is the actual answer total for this site at the moment:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/w6haD.png&quot; alt=&quot;1337&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll try to clarify.  What do data science employers acknowledge as experience with big data/data science?  Does the size of the data above qualify experience with it?  Or is it universally accepted among those in the field that it's not at all the size of the data, but what you need to do with the data?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to understand the procedure of building a static local website using &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; and &lt;code&gt;Rmarkdown&lt;/code&gt;. I am aware of a &lt;a href=&quot;http://rmarkdown.rstudio.com/html_document_format.html#creating-a-website&quot; rel=&quot;nofollow&quot;&gt;Rmarkdown&lt;/a&gt; website where the procedure is outlined, but unfortunately I do not understand the steps. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody here have some experience in building a static local website and would be so kind as to describe the procedure in more detail?   &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;In recent methods for community detection in dynamic networks, LFR benchmark is used as dynamic dataset generator, but I thought it is for static community based data generation. For example in paper &lt;a href=&quot;http://www.cise.ufl.edu/~mythai/files/mobicom_CS.pdf&quot; rel=&quot;nofollow&quot;&gt;Overlapping Communities in Dynamic Networks: Their Detection and Mobile Applications&lt;/a&gt;, LFR is used. But I don't know how this dataset is generated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://pages.towson.edu/npnguyen/Databases/AFOCS.zip&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the code of this paper and ground truth included is only for one snapshot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question: How is the dataset generated?&lt;/strong&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Is there a recommended approach for storing processed data for testing new data products?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I'd like to have a system where a data scientist or an analyst could think of a new data product to present to users, do the data processing to create it, and then put it in a data store that our application can then access easily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I'm not sure about is what kind of data store would be good for this type of &quot;testing&quot; use case.  Since it would need to be flexible enough to handle different types of data products, like aggregates, windowed data, etc.  And ideally it wouldn't require a huge instrumentation process to try out new things.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;a matrix X with N lines&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;a vector Y&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've computed the Euclidean distance with Y for each line of X.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I get is a vector of distances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want is a vector of scores between 0 and 1, 1 meaning &quot;very&quot; high correlation, 0 meaning &quot;no&quot; correlation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here what I did :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I divided the vector of distances by the max distance inside it.&#xA;I get vector D. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;1 - D&lt;/em&gt; is the final result with values between 0 and 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that I get many values (75%) too close to 1.&#xA;Do you think what I did is correct ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would you get a better result ?&#xA;(Between 0 and 1 but not everything too close to 1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For now, I tried to take the square of the result. (To stay between 0 and 1 but to minimize the values)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here a picture of the distance values I want to turn in a score&#xA;&lt;img src=&quot;https://i.stack.imgur.com/36LDh.png&quot; alt=&quot;distance values I want to turn in a score&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've made a logistic regression to combine two independent variables in &lt;code&gt;R&lt;/code&gt;, using &lt;code&gt;pROC&lt;/code&gt; package and I obtain this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; summary(fit)&#xA;&#xA;Call: glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;Deviance Residuals: &#xA;  Min       1Q     Median     3Q      Max  &#xA;-1.5751  -0.8277  -0.6095   1.0701   2.3080  &#xA;&#xA;Coefficients:&#xA;             Estimate  Std. Error z value Pr(&amp;gt;|z|)    &#xA;(Intercept) -0.153731   0.538511  -0.285 0.775281    &#xA;X           -0.048843   0.012856  -3.799 0.000145 ***&#xA;Y            0.028364   0.009077   3.125 0.001780 ** &#xA;---&#xA;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xA;&#xA;(Dispersion parameter for binomial family taken to be 1)&#xA;&#xA;Null deviance: 287.44  on 241  degrees of freedom&#xA;Residual deviance: 260.34  on 239  degrees of freedom&#xA;AIC: 266.34&#xA;&#xA;Number of Fisher Scoring iterations: 4&#xA;&#xA;&amp;gt;     fit&#xA;&#xA;Call:  glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;Coefficients:&#xA;  (Intercept)       X            Y  &#xA;   -0.15373     -0.04884      0.02836  &#xA;&#xA;Degrees of Freedom: 241 Total (i.e. Null);  239 Residual&#xA;Null Deviance:      287.4 &#xA;Residual Deviance:  260.3        AIC: 266.3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I need to extract some information from this data and I'm not sure about how to do it. First, I need the model equation: suppose that fit is a combined predictor called &lt;code&gt;CP&lt;/code&gt;; could it be &lt;code&gt;CP=-0.15-0.05X+0.03Y&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, the resulting combined predictor from the regression should present a median value, so that I can compare median from the two groups &lt;code&gt;Case&lt;/code&gt; and &lt;code&gt;Controls&lt;/code&gt; which I used to make the regression (in other words, my &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; variables are N-dimensional with &lt;code&gt;N = N1+N2&lt;/code&gt;, where &lt;code&gt;N1 = Number of Controls&lt;/code&gt;, for which &lt;code&gt;Case=0&lt;/code&gt;, and &lt;code&gt;N2 = Number of Cases&lt;/code&gt;, for which &lt;code&gt;Case=1&lt;/code&gt;).&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working for a logistics firm and there are approx. 750+ customers who avail our services. I am in the process of building and generating some insight for the business based on the payments made by these customers for the last 1 year. Some make payment on time whereas some are late &amp;amp; some are extremely late. Could you please advice which modeling technique or statistical approach would be best in this case. &#xA;I can think of creating clusters based on the payment history and highlights &amp;amp; placing all defaulters in one cluster where our company can focus. &#xA;PLease suggest. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I understand that by filtering out the instances with labels that Random Forest trees are uncertain upon with their decisions, and model these with another classifier could give a better overall result. My question is, how can I &quot;combine&quot; two (or more) classifiers' classification on a single unlabeled dataset?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to apply classification algorithms to KDD Cup 2012 track2 data using R&#xA;&lt;a href=&quot;http://www.kddcup2012.org/c/kddcup2012-track2&quot; rel=&quot;nofollow&quot;&gt;http://www.kddcup2012.org/c/kddcup2012-track2&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems not possible to work with this 10GB training data on my local system with 4GB RAM.&#xA;Can anyone work on this data using this kind of a local system ? Or is using a cluster the norm ?&lt;br&gt;&#xA;It would be great if anyone could provide me with any guidance on how to get started with working on a cluster and the normally used type of cluster for such tasks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am interested in any data, publications, etc about what is the smallest neural network that can achieve a certain level of classification performance.  By small I mean &lt;em&gt;few parameters&lt;/em&gt;, not &lt;em&gt;few arithmetic operations&lt;/em&gt; (=fast).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested primarily in convolutional neural networks for vision applications, using something simple like CIFAR-10 without augmentation as the benchmark.  Top-performing networks on CIFAR in recent years have had anywhere between 100 million and 0.7 million parameters (!!), so clearly small size is not (always) a bad thing.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Small networks are also in general faster to train and overfit less.  Moreover, recent work on &lt;a href=&quot;http://arxiv.org/abs/1503.02531&quot; rel=&quot;nofollow&quot;&gt;Knowledge Distillation&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6550&quot; rel=&quot;nofollow&quot;&gt;FitNets&lt;/a&gt;, etc show ways of making smaller networks from large networks while preserving most of the performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another question is, what is the best performance achievable with a network no larger than a fixed size?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples of especially small networks that get good performance (100k parameters with 10% on CIFAR, anyone?) or systematic studies of the size vs performance tradeoff would be appreciated.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a project for comparison between clustering techniques using the data set of SSA for birth names from 1910-2013 years for the different states.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have finished applying my clustering techniques on my data set and the output of the clusters were the clusters of the states for each year.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I can know from my results; which states are close to each other with the birth names and which were not. By looking &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to have interesting results to make my project report interesting, Which states are similar in birth names are not enough to make the read be excited about my project. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;any ideas of what can be learned from my project?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How can I show the comparison between the clustering techniques? anyway other than seeing how Homogeneous the clusters are?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " ('&lt;p&gt;PROC means data=d mean; &#xA;var a;&#xA;class b; var a;&#xA;run; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to perform the &quot;PROC means&quot; for continuous &quot;var a&quot;:&#xA;1) in general and&#xA;2) by classes.&#xA;But it performed by the classes only.&#xA;How to make procedure for &quot;var a&quot; here in general too?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. SAS WARNING: Analysis variable &quot;a&quot; was defined in a previous statement, duplicate definition will be ignored.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a new data point and want to classify it into the existing classes.I can calculate pairwise distance for the new point to all existing points(in the existing classes). I know using KNN would be a straightforward to classify this point. Is there a way I could randomly sampling existing classes and then correlated the new point to a potential classes without calculating all pairwise distances? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Assume the training data is &lt;code&gt;fruit&lt;/code&gt;, which I am going to use for prediction in a CART model in R:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fruit = data.frame(color=c(&quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;yellow&quot;,&#xA;                           &quot;orange&quot;, &quot;green&quot;, &quot;pink&quot;, &quot;red&quot;, &quot;red&quot;),&#xA;                   isApple=c(TRUE, TRUE, FALSE, FALSE, FALSE,&#xA;                             FALSE, FALSE, FALSE, TRUE, FALSE))&#xA;mod = rpart(isApple ~ color, data=fruit, method=&quot;class&quot;, minbucket=1)&#xA;prp(mod)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could anyone explain what is exactly the role of &lt;code&gt;minbucket&lt;/code&gt; in plotting CART tree for this example if we are going to use &lt;code&gt;minbucket = 2, 3, 4, 5&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say &lt;code&gt;fruit&lt;/code&gt; is my data frame, I'm finding whether the outcome is apple or not? I have 5 red apples (4 TRUE, 1 FALSE), one FALSE value is tomato here, so what ever is red need not be an apple. But if I give &lt;code&gt;minbucket&lt;/code&gt;=5 or 4 here, there is no split at all. Only for &lt;code&gt;minbucket&lt;/code&gt; 1 to 3 there is a split beyond 3 there is no split. But I have more than 3 observation in my leaf node.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm conjecturing that with &lt;a href=&quot;http://en.wikipedia.org/wiki/Complete-linkage_clustering&quot; rel=&quot;nofollow&quot;&gt;Complete-linkage clustering&lt;/a&gt; two elements from the same cluster will always be closer to each other some other element from another cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In more formal terms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $C$ be a clustering.&#xA; $\\\\not\\\\exists z \\\\in C_j$ s.t. $\\\\bigtriangleup(x, z) &amp;lt; \\\\bigtriangleup(x, y)$ where $x,y \\\\in C_i$, $C_i \\\\neq C_j$ and $C_i, C_j \\\\in C$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't been able to prove the conjecture yet, thus I'm wondering whether I'm right or wrong. If this is indeed the case, I would much appreciate a sketch a proof. I'm pretty sure I can work my way from there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side note (not that I think it makes a difference), I'll be applying the clustering algorithm on a one-dimensinal dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your input is much appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a pandas DataFrame containing a time series column. The years are shifted in the past, so that I have to add a constant number of years to every element of that column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best way I found is to iterate through all the records and use&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;x.replace(year=x.year + years)  # x = current element, years = years to add&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is cythonized as below, but still very slow (proofing)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cdef list _addYearsToTimestamps(list elts, int years):&#xA;    cdef cpdatetime x&#xA;    cdef int i&#xA;    for (i, x) in enumerate(elts):&#xA;        try:&#xA;            elts[i] = x.replace(year=x.year + years)&#xA;        except Exception as e:&#xA;            logError(None, &quot;Cannot replace year of %s - leaving value as this: %s&quot; % (str(x), repr(e)))&#xA;    return elts&#xA;&#xA;def fixYear(data):&#xA;    data.loc[:, 'timestamp'] = _addYearsToTimestamps(list(data.loc[:, 'timestamp']), REAL_YEAR-(list(data[-1:]['timestamp'])[0].year))&#xA;    return data&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm pretty sure that there is a way to change the year without iterating, by using Pandas's Timestamp features. Unfortunately, I don't find how. Could someone elaborate?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I don't know, if I can ask this here, but I'm interested to know what kind of &lt;em&gt;abstract data type (ADT)&lt;/em&gt; does Twitter use to &lt;strong&gt;model relations&lt;/strong&gt; between profiles and why. I'm just starting to learn ADTs, so I would like to learn how they work in the real world as well as their applications.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to learn web scraping using Python by myself as part of an effort to learn data analysis. I am trying to scrape imdb webpage whose url is the following: &lt;a href=&quot;http://www.imdb.com/search/title?sort=num_votes,desc&amp;amp;start=1&amp;amp;title_type=feature&amp;amp;year=1950,2012&quot; rel=&quot;noreferrer&quot;&gt;http://www.imdb.com/search/title?sort=num_votes,desc&amp;amp;start=1&amp;amp;title_type=feature&amp;amp;year=1950,2012&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using BeautifulSoup module. Following is the code I am using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;r = requests.get(url) # where url is the above url    &#xA;bs = BeautifulSoup(r.text)&#xA;for movie in bs.findAll('td','title'):&#xA;    title = movie.find('a').contents[0]&#xA;    genres = movie.find('span','genre').findAll('a')&#xA;    genres = [g.contents[0] for g in genres]&#xA;    runtime = movie.find('span','runtime').contents[0]&#xA;    year = movie.find('span','year_type').contents[0]&#xA;    print title, genres,runtime, rating, year&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the following outputs:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;The Shawshank Redemption [u'Crime', u'Drama'] 142 mins. (1994)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using this code, I could scrape title, genre, runtime,and year but I couldn't scrape the imdb movie id,nor the rating. After inspecting the elements (in chrome browser), I am not being able to find a pattern which will let me use similar code as above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody help me write the piece of code that will let me scrape the movie id and ratings ? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using the &lt;a href=&quot;http://cran.r-project.org/web/packages/biglm/biglm.pdf&quot; rel=&quot;nofollow&quot;&gt;biglm&lt;/a&gt; R package for linear regression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;click/impression is the output required. But the test data does not contain click and impression. &#xA;The &lt;code&gt;predict&lt;/code&gt; function of &lt;code&gt;biglm&lt;/code&gt; gives the error&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error in eval(expr, envir, enclos) : object 'click' not found&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I assume this is because &lt;code&gt;predict&lt;/code&gt; tries to compute the standard errors also.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a method to just obtain the predictions ? I tried assigning values to &lt;code&gt;se.fit&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt; attributes, but I get the same error. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I noticed a difference between R and Excel when it comes to trend lines.  Basically I am automating a simple process that someone does.  He/she takes web traffic and finds the best trend line by looking at the R squared to predict what the web traffic will be a few months from now to make sure we will have the servers to handle it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In R, one can do: lm(log(y)~x,data) but you must &quot;remember&quot; to do &lt;b&gt;exp&lt;/b&gt;(predict(...)).  Is there a better predict function that remembers the left hand operations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also the R-squared reported for log(y)~x is the r squared for x vs that y's log.  The R-squared against the original y was better.  Is there a nifty function that calculates that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see from this output, what summary reports as the rsquared is not &quot;correct&quot; while my rsq data accurately shows the best rmse. (See source code below.)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;[1] &quot;rmse is 305221.416535481 rsq is 0.749525713697497 summary$r.squared 0.705953722113025&quot;&#xA;[1] &quot;rmse is 304961.752311906 rsq is 0.749815047530537 summary$r.squared 0.706163612870978&quot;&#xA;[1] &quot;rmse is 318083.254498832 rsq is 0.723564406971294 summary$r.squared 0.723564406971402&quot;&#xA;[1] &quot;rmse is 317352.614485029 rsq is 0.724832898371528 summary$r.squared 0.724832898371531&quot;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to learn the best way to do this in R...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;&#xA;rmse=function(x,y){&#xA;  return( sqrt(sum((x-y)^2)/length(x)));&#xA;}&#xA;rsq=function(actual,pred){    &#xA;  return( cor(actual,pred,method=c(&quot;pearson&quot;))^2);&#xA;}&#xA;&#xA;&#xA;findBestFormula=function(data,yvar,xvar){&#xA;  #This figures out the best Excel-like trend function based on lowest rmse&#xA;  data$y=data[,yvar];&#xA;  data$x=data[,xvar];  &#xA;  models=list(lm(log(y)~log(x),data),&#xA;              lm(log(y)~x,data),&#xA;             lm(y~log(x),data),&#xA;              lm(y~x,data));&#xA;  preds=list(exp(predict(models[[1]],data)),&#xA;    exp(predict(models[[2]],data)),&#xA;    (predict(models[[3]],data)),&#xA;    (predict(models[[4]],data)));&#xA;  rs=c();&#xA;  for(i in (1:4)){&#xA;    #say(names(models[[i]]))&#xA;    rs[i]=rmse(data$y,preds[[i]]);&#xA;    print(paste(&quot;rmse is&quot;,rs[i],&quot;rsq is&quot;,rsq(data$y,preds[[i]]),&#xA;                &quot;summary$r.squared&quot;,summary(models[[i]])$r.squared));&#xA;  }&#xA;  best=min(rs)&#xA;  #say(best);&#xA;  for(i in (1:4)){&#xA;    if(best==rs[i]){&#xA;      return(list(model=models[[i]],pred=preds[[i]],modelNum=i,rmse=best));  &#xA;    }&#xA;  }&#xA;&#xA;}&#xA;&#xA;&#xA;summary(findBestFormula(weekDayDf,'daily','d')$model)&#xA;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any tips on niftier code to write the above?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a question about two different methods from different libraries which seems doing same job. I am trying to make linear regression model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the code which I using statsmodel library with OLS :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.3, random_state=1)&#xA;&#xA;x_train = sm.add_constant(X_train)&#xA;model = sm.OLS(y_train, x_train)&#xA;results = model.fit()&#xA;&#xA;print &quot;GFT + Wiki / GT  R-squared&quot;, results.rsquared&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This print out &lt;strong&gt;GFT + Wiki / GT  R-squared 0.981434611923&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the second one is scikit learn library Linear model method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model = LinearRegression()&#xA;model.fit(X_train, y_train)&#xA;&#xA;predictions = model.predict(X_test)&#xA;&#xA;print 'GFT + Wiki / GT R-squared: %.4f' % model.score(X_test, y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This print out &lt;strong&gt;GFT + Wiki / GT R-squared: 0.8543&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is the both method prints our R^2 result but one is print out 0.98 and the other one is 0.85. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my understanding, OLS works with training dataset. So my questions, &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is there a way that work with test data set with OLS ?&lt;/li&gt;&#xA;&lt;li&gt;Is the traning data set score gives us any meaning(In OLS we didn't use test data set)? From my past knowledge we have to work with test data.&lt;/li&gt;&#xA;&lt;li&gt;What is the difference between OLS and scikit linear regression. Which one we use for calculating the score of the model ? &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am working on image classification tasks and decided to use Lasagne + Nolearn for neural networks prototype.&#xA;All standard examples like MNIST numbers classification run well, but problems appear when I try to work with my own images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use 3-channel images, not grayscale.&#xA;And there is the code where I'm trying to get arrays from images:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; img = Image.open(item)&#xA; img = ImageOps.fit(img, (256, 256), Image.ANTIALIAS)&#xA; img = np.asarray(img, dtype = 'float64') / 255.&#xA; img = img.transpose(2,0,1).reshape(3, 256, 256)   &#xA; X.append(img)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the code of NN and its fitting:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X, y = simple_load(&quot;new&quot;)&#xA;&#xA;X = np.array(X)&#xA;y = np.array(y)&#xA;&#xA;&#xA;net1 = NeuralNet(&#xA;    layers=[  # three layers: one hidden layer&#xA;        ('input', layers.InputLayer),&#xA;        ('hidden', layers.DenseLayer),&#xA;        ('output', layers.DenseLayer),&#xA;        ],&#xA;    # layer parameters:&#xA;    input_shape=(None, 65536),  # 96x96 input pixels per batch&#xA;    hidden_num_units=100,  # number of units in hidden layer&#xA;    output_nonlinearity=None,  # output layer uses identity function&#xA;    output_num_units=len(y),  # 30 target values&#xA;&#xA;    # optimization method:&#xA;    update=nesterov_momentum,&#xA;    update_learning_rate=0.01,&#xA;    update_momentum=0.9,&#xA;&#xA;    regression=True,  # flag to indicate we're dealing with regression problem&#xA;&#xA;&#xA;       max_epochs=400,  # we want to train this many epochs&#xA;        verbose=1,&#xA;        )&#xA;&#xA;  net1.fit(X, y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I recieve exceptions like this one:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;las_mnist.py&quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    net1.fit(X[i], y[i])&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 266, in fit&#xA;    self.train_loop(X, y)&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 273, in train_loop&#xA;    X, y, self.eval_size)&#xA;  File &quot;/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py&quot;, line 377, in train_test_split&#xA;    kf = KFold(y.shape[0], round(1. / eval_size))&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So, in which format do you &quot;feed&quot; your networks with image data?&lt;/strong&gt;&#xA;Thanks for answers or any tips!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm having trouble figuring out a way to analyze some simple data. When graphed, the data I have make a somewhat sinusoidal curve. What I want to do is to find the x-values of the maximum peaks of the sinusoidal curve. I then want to subtract each of these x-values from the last peak found and average these differences to obtain an average distance between peaks. Is there an easy way to do this with Excel, Mathematica, or MatLab (the programs I have available to me?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!!!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I would like to know more on fraud/anomaly detection. I am looking for good source or survey article/book etc out there which will give me some preliminary idea of the area. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestion is greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have an application that tracks people making mentions of various topics. We've used a Bayes algorithm to do some simple classification (users give a thumbs up/thumbs down) to pick the people that they believe are the best fit for their project. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our intention was to use this data to help us sort and order the influencers based on &quot;fit&quot; to the customer's needs.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, we've got some trained data, and now all we can do is say are they similar to the &quot;thumbs up&quot; group, or the &quot;thumbs down&quot; group.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What algorithm should we have used for this instead? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, the ideal is to have a score.. and the biggest, smallest based on the trained data is the one that gets shown first. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thoughts? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even better if its in Ruby. &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am looking towards a solution where classification algorithms produce output with some confidence value. but I am confused whether classification algorithms are able to produce results with percentage of confidence? &#xA;Thanks&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm working on a problem where frequency analysis applies (decomposition of a signal into frequencies, that is), but it's noisy and the samples are unevenly spaced.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically: given a list of items purchased at a bar/restaurant, try to estimate the number of guests on the check based on distinct &quot;frequencies&quot; of purchase. The logic is that if there are N guests on a check, then it's reasonable to see N frequencies of drinks being purchased, one person buying every 10 minutes, another every 15, etc. (Plenty of other properties of the check should be included, but here I'm focusing specifically on estimating distinct frequencies).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So more formally: given a noisy, unevenly spaced time series, find the smallest number of frequencies which reproduce the signal while minimizing the error (... for some sensible definition of how to minimize both the error and the number of frequencies simultaneously).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is more a machine learning problem than signal processing. I realize it's also an open question, but can anyone point me in the right direction? Is there a particular method or algorithm that applies here?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am analyzing a dataset in Python for strictly learning purpose.&#xA;In the code below that I wrote, I am getting some errors which I cannot get rid off. Here is the code first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')&#xA;plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)&#xA;plt.xlabel('Year')&#xA;plt.ylabel('Rating')&#xA;remove_border()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the following errors:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. TypeError: 'str' object is not callable&#xA;2. NameError: name 'remove_border' is not defined&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, the label='Decade Average' is not showing up in the plot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What confuses me most is the fact that in a separate  code snippet for plots (see below), I didn't get the 1st error above, although &lt;code&gt;remove_border&lt;/code&gt; was still a problem.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;plt.hist(movieDF.rating, bins = 5, color = 'blue', alpha = 0.3)&#xA;plt.xlabel('Rating')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any explanations of all or some of the errors would be greatly appreciated. Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following the comments, I am posting the data and the traceback below:&#xA;decade_mean is given below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;year&#xA;1970    8.925000&#xA;1980    8.650000&#xA;1990    8.615789&#xA;2000    8.378947&#xA;2010    8.233333&#xA;Name: rating, dtype: float64&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;traceback:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;TypeError                                 Traceback (most recent call last)&#xA;&amp;lt;ipython-input-361-a6efc7e46c45&amp;gt; in &amp;lt;module&amp;gt;()&#xA;      1 plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')&#xA;      2 plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)&#xA;----&amp;gt; 3 plt.xlabel('Year')&#xA;      4 plt.ylabel('Rating')&#xA;      5 remove_border()&#xA;&#xA;TypeError: 'str' object is not callable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have solved remove_border problem. It was a stupid mistake I made. But I couldn't figure out the problem with the 'str'.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is it possible to learn the weights for a logistic regression classifier using EM (Expectation Maximization)algorithm? Is there any instance reference?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Undergraduate researcher here. I worked at many traditional scientific research labs, ranging from cancer biology, to radiation medicine, and to supercapacitors. I'm thinking of switching to Statistics and Computer Science from Computational Biology to join the exciting field of Data Science. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What ways can I contribute to Humanity without a technical background in basic science? I don't know how I will feel about making better clickbaits. I've thought about continuing in scientific research but I will lack the science background. I've thought about working for the United Nations. What other ways can I contribute? Inspire me!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I understand how recurrent neural networks work, however I'm trying to build a deep intuitive understanding of their behavior which is difficult for me because they exhibit such complex behaviors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most difficult thing for me to understand is how a Reccurrent Neural Network can exhibit an oscillatory behavior along with the notion of exploding weights. For one, I'm guessing that oscillatory behaviors are only possible for certain activation functions and configurations. Here are my following questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Can sigmoid activation functions exhibit oscillatory behavior? I've convinced myself that they don't since they have a positive range between 0 and 1 which doesn't allow for negative derivatives, but maybe I'm wrong. Is there a formal proof out there for whether it can or cannot?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Which activation functions can exhibit oscillatory behaviors and are there proofs out there for them? I believe tanh might have this behavior but I'm not sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) What are exploding weight derivatives and how do they occur? There is polar opposite of exploding weight's which seems to be where the weights do not learn and stay stagnate. What causes these issues? I imagine this is dependent upon the activation function as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I began reading this article:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CCsQFjAB&amp;amp;url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FChristian_OReilly%2Fpublication%2F52004955_Permanent_oscillations_in_a_3-node_recurrent_neural_network_model%2Flinks%2F0c96052464be171d68000000.pdf&amp;amp;ei=X_E1Vf7ULNGsogSo9YHIBQ&amp;amp;usg=AFQjCNE0fex0s7hY2w_upMNkmaxIseUKww&amp;amp;bvm=bv.91386359,d.cGU&quot; rel=&quot;nofollow&quot;&gt;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CCsQFjAB&amp;amp;url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FChristian_OReilly%2Fpublication%2F52004955_Permanent_oscillations_in_a_3-node_recurrent_neural_network_model%2Flinks%2F0c96052464be171d68000000.pdf&amp;amp;ei=X_E1Vf7ULNGsogSo9YHIBQ&amp;amp;usg=AFQjCNE0fex0s7hY2w_upMNkmaxIseUKww&amp;amp;bvm=bv.91386359,d.cGU&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but got confused around page 10 because the author omit's explaining opaque assumptions such as why we are defining the matrix measure as is and so forth(it's a good paper nonetheless). Is there a more transparent paper or lecture out there that can shed light on this subject, or maybe simple explanation for what the author is trying to get at?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am currently working with the forest cover type prediction from Kaggle, using classification models with scikit-learn. My main purpose is learning about the different models, so I don't pretend to discuss about which one is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When working with logistic regression, I wonder if I need the 'penalty' parameter (where I can choose L1 or L2 regularization). Based on what I found, these regularization terms are useful to avoid over-fitting, specially when the parameter values are extreme (by extreme I understand the range of some parameter values are very large compared to other parameters, Correct me if I am wrong. In this case, wouldn't it be enough to apply log-scale or normalization to these values?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main questions are: as the number of parameters is large, are there visualization techniques and tools in scikit-learn which can help me to find parameters with extreme values? is there any statistical function/tool which returns how extreme the values of parameters are?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Has there been a successful implementation of Nash-Equilibrium in big data problems like suggesting a best buy in a stock market, in traffic monitoring systems or crowd control systems.&#xA;All the above mentioned scenarios have competitive environments and one needs to get the best possible solution in them, which should suit well for Nash-Equilibrium cases.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have scraped the rating that a customer gave in the following categories namely overall rating score, value for money, seat comfort,staff service, catering and entertainment from an airline forum. I would like to know what quality information I can deduce from such ratings about an airline. I have rating of nearly 300 customers who traveled in an airline.  &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I had this basic query on ML and would like to get basic ideas on modelling prediction models using ML and Python.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say I have a training data of 1000 items as&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Item_name, Attrib_1, Attrib_2, Attrib_3,....Attrib_N, Cost&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And my aim is to create a model to predict cost for a new item given the attributes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So where should I start and what are the different ways to prediction and solve this problem ? Also how to evaluate different methods ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;First of all, I hope I'm in the right StackExchange here. If not, apologies!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently working with huge amounts of feature-value vectors. There are millions of these vectors (up to 20 million possibly). They contain some linguistic/syntactic features and their values are all strings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because most classifiers do not handle string data as values, I convert them to binary frequency values, so an attribute looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@attribute 'feature#value' numeric&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And per row, the value is either 1 or it is absent (so note it's a &lt;em&gt;sparse&lt;/em&gt; ARFF file). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The thing is, with 250K rows, there are over 500K attributes and so, most algorithms have a hard time with this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a lot of algorithms. I'm really curious as to what you would consider a suitable one (preferably unsupervised, but anything works), and if you even have some ideas how I could improve performance. I can train on small subsets of data, but the results only get better when using large amounts of data (at least 7 million events). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For now, I've been using NaiveBayes variations (Multinomial and also DMNBText) and those are really the only ones that are able to chew up data with acceptable speed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot. If you need more information, please let me know.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cheers.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Does anyone know if it's possible to import a large dataset into Amazon S3 from a URL?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to avoid downloading a huge file and then reuploading it to S3 through the web portal. I just want to supply the download URL to S3 and wait for them to download it to their filesystem. It seems like an easy thing to do, but I just can't find the documentation on it.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a dataset with differents attributes which don't have the same range on their values which is a problem when we need to compute distance beetween objects. After some research i found that i can do the regularisation job with this formula : (value-min)/(max-min)  where min and max are respectively the minimum and maximum value in the domain of val attribute.&#xA;The question is that one, does it exist other ways ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your help.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Using scikit-learn, why would you use bfgs optimization which is non-linear for a linear classifier as logistic regression? I am confused. Does the optimization method finds the optimum of the chosen score function? if so, which one? I can't choose it when defining the estimator. does the linearity or non-linearity of the score function depend on the model (whether it is linear or non-linear)?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I used clustering on my dataset. Now when I'm trying to use a LASSO with cv to predict a response, one of the variables it takes into consideration is which cluster a new point is classified into.(I included the cluster variable as a predictor to see if being in a particular group affects the response) &#xA;Since the information on all variables is already captured by the cluster variable,using it again in the Lasso model with some other variables,does it become redundant/biased?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a text classification problem in which i need to classify an answer to a message as either relevant or not. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the first phase of my calculations, I have already used a SVM to determine if the original message was relevant or not, deciding whether a message contains a hint or question if somebody's twitter account has been hacked.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;example:&#xA;&quot;Hey @foobar, have you been hacked?&quot;   &amp;lt;-- relevant&#xA;&quot;My bank account has just been hacked&quot; &amp;lt;-- not relevant&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, when I want to classify whether the answer is relevant, I would want to have both the original message and the answer as input, right? An answer is relevant in my case if it, in any way, responds to the original message. Is this approach possible using a SVM or any other machine learning tool? I'm using python with the scikit-learn library.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;example:&#xA;&quot;Hey @foobar, have you been hacked?&quot;&#xA;&quot;@barfoo it seems so, thx for suggesting&quot; &amp;lt;-- relevant&#xA;&#xA;&quot;Hey @foobar, have you been hacked?&quot;&#xA;&quot;Lose 20 pounds quickly! http://blabla.com&quot; &amp;lt;-- not relevant&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not very experienced in this field, so any input would be very appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am currently trying to develop a classifier in python using Naive Bayes technique. I need a dataset so that I can train it. My classifier would classify a new document given to it into one of the four categories : Science and technology, Sports, politics, Entertainment. Can anybody please help me find a dataset for this. I've been stuck on this problem for quite some time now. Any help would be greatly appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm currently studying Chapter 7 (&quot;Modeling with Decision Trees&quot;) of the book &quot;Programming Collective intelligence&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find the output of the function mdclassify() p.157 confusing. The function deals with missing data. The explanation provided is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In the basic decision tree, everything has an implied weight of 1,&#xA;  meaning that the observations count fully for the probability that an&#xA;  item fits into a certain category. If you are following multiple&#xA;  branches instead, you can give each branch a weight equal to the&#xA;  fraction of all the other rows that are on that side.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;From what I understand, an instance is then split between branches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, I simply don't understand how we can obtain:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{'None': 0.125, 'Premium': 2.25, 'Basic': 0.125}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;as &lt;code&gt;0.125+0.125+2.25&lt;/code&gt; does not sum to 1 nor even an integer. How was the new observation split?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/arthur-e/Programming-Collective-Intelligence/blob/master/chapter7/treepredict.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/arthur-e/Programming-Collective-Intelligence/blob/master/chapter7/treepredict.py&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the original dataset, I obtain the tree shown here: &#xA;&lt;a href=&quot;http://mattscodecave.com/media/tree3.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://mattscodecave.com/media/tree3.jpg&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone please explain me precisely what the numbers precisely mean and how they were exactly obtained?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS : The 1st example of the book is wrong as described on their errata page but just explaining the second example (mentioned above) would be nice.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Assume that &quot;1,2,3&quot; are the ids of users, active means that person visited the stackoverflow in last one month (0=passive, 1=active), and there are positive and negative votes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id  question       votes                 active&#xA; 1     1        -1, +1, -1, -1, -1         0&#xA; 1     2        -1, +1, -1, -1, +1         0&#xA; 2     1        +1, +1, -1, -1             0&#xA; 3     1        +1, +1, +1, -1, +1         1&#xA; 3     2        +1, +1, -1, +1, +1, +1     1&#xA; 3     3        -1, +1                     1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to know what makes the users stop using stackoverflow. Think that, I have already calculate the how many times did they get negative votes, total vote, average vote for each question...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder what kind of information could I get from these sequences. I want to find something like this: these users who are passive have two negative votes sequentially. For example, one positive vote after two negative votes in the second question of user 1, doesn't prevent the user churn. User 3 doesn't have any 2 negative votes sequentially in any of his questions. Hence he is still active.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for something like PrefixSpan Algorithm but order is important for me. I mean, I can't write the sequences like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;(-1 +1 -1 -1 -1) (-1 +1 -1 -1 +1 )&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;(-1) (+1) (-1) (-1) (-1) (-1) (+1) (-1) (-1) (+1 )&amp;gt;. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Because the first one loses the order, and the second one jumbled the questions together. Is there any algorithm to find these sequences which is common for churners?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are Hybrid classifiers used for sentiment analysis? How are they built? Please suggest good tutorial/book/link for reference. Also how are they different from other classifiers like SVM and Naive Bayes? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm doing some cluster analysis on the &lt;code&gt;MLTobs&lt;/code&gt; from the &lt;code&gt;LifeTables&lt;/code&gt; package and have come across a tricky problem plotting frequency of the &lt;code&gt;Year&lt;/code&gt; variable in the &lt;code&gt;mlt.mx.info&lt;/code&gt; dataframe. &lt;code&gt;Year&lt;/code&gt; contains the period that the life table was measured over, in intervals. Here's a table of the data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    1751-1754 1755-1759 1760-1764 1765-1769 1770-1774 1775-1779 1780-1784 1785-1789 1790-1794 &#xA;        1         1         1         1         1         1         1         1         1 &#xA;1795-1799 1800-1804 1805-1809 1810-1814 1815-1819 1816-1819 1820-1824 1825-1829 1830-1834 &#xA;        1         1         1         1         1         2         3         3         3 &#xA;1835-1839 1838-1839 1840-1844 1841-1844 1845-1849 1846-1849 1850-1854 1855-1859 1860-1864 &#xA;        4         1         5         3         8         1        10        11        11 &#xA;1865-1869 1870-1874 1872-1874 1875-1879 1876-1879 1878-1879 1880-1884 1885-1889 1890-1894 &#xA;       11        11         1        12         2         1        15        15        15 &#xA;1895-1899 1900-1904 1905-1909 1908-1909 1910-1914 1915-1919 1920-1924 1921-1924 1922-1924 &#xA;       15        15        15         1        16        16        16         2         1 &#xA;1925-1929 1930-1934 1933-1934 1935-1939 1937-1939 1940-1944 1945-1949 1947-1949 1948-1949 &#xA;       19        19         1        20         1        22        22         3         1 &#xA;1950-1954 1955-1959 1956-1959 1958-1959 1960-1964 1965-1969 1970-1974 1975-1979 1980-1984 &#xA;       30        30         2         1        40        40        41        41        41 &#xA;1983-1984 1985-1989 1990-1994 1991-1994 1992-1994 1995-1999 2000-2003 2000-2004 2005-2006 &#xA;        1        42        42         1         1        44         3        41        22 &#xA;2005-2007 &#xA;       14 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, some of the intervals sit within other intervals. Thankfully none of them overlap. I want to simplify the intervals so intervals such as &lt;code&gt;1992-1994&lt;/code&gt; and &lt;code&gt;1991-1994&lt;/code&gt; all go into &lt;code&gt;1990-1994&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An idea might be to get the modulo of each interval and sort them into their new intervals that way but I'm unsure how to do this with the interval data type. If anyone has any ideas I'd really appreciate the help. Ultimately I want to create a histogram or barplot to illustrate the nicely.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;For word2vec with negative sampling, the cost function for a single word is the following according to &lt;a href=&quot;http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf&quot; rel=&quot;nofollow&quot;&gt;word2vec&lt;/a&gt;:&#xA;$$&#xA;E = - log(\\\\sigma(v_{w_{O}}^{'}.u_{w_{I}})) - \\\\sum_{k=1}^K log(\\\\sigma(-v_{w_{k}}^{'}.u_{w_{I}}))&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$v_{w_{O}}^{'}$ = hidden-&gt;output word vector of the output word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$u_{w_{I}}$ = input-&gt;hidden word vector of the output word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$v_{w_{k}}^{'}$ = hidden-&gt;output word vector of the negative sampled word&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\sigma$ is the sigmoid function&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And taking the derivative with respect to $v_{w_{O}}^{'}.u_{w_{j}}$ is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$&#xA;\\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} = \\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) * (\\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) - 1)&#xA;$ $ if w_j = w_O $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$&#xA;\\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} = \\\\sigma(v_{w_{j}}^{'}.u_{w_{I}}) * \\\\sigma(-v_{w_{j}}^{'}.u_{w_{I}})&#xA;$ $ if w_j = w_k \\\\ for \\\\ k = 1...K$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we can use chain rule to get &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$ \\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}} = \\\\frac{\\\\partial E}{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}} * \\\\frac{\\\\partial v_{w_{j}}^{'}.u_{w_{I}}}{\\\\partial v_{w_{j}}^{'}} $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is my reasoning and derivative correct? I am still new to ML so any help would be great! &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want to measure the correlation between the survival time which is a time to event data and the patient's activity count which is measured on continuous scale. What type of correlation coefficient is available to measure the strength of these two variables?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Suppose classifier trained with 5 class, and input query content does not belong to any of the trained class data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Naive bayes provides and random class as a result here. Which classifier deals best in such scenario?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have setup R-Studio Server on an Ubuntu EC2 instance for the first time and successfully started r-studio server in my browser. I also have putty ssh client. How do I set path in r-studio server to my mounted EBS volume and why do I not see the contents of my EBS volume in the r-studio files area (bottom right side? ) . Also, I had a file in an s3 bucket. I passed this command to bring it from s3 to my ebs volume: &lt;code&gt;s3cmd get s3://data-analysis/input-data/filename.csv&lt;/code&gt; . I assume this command downloads the file from s3 into the ebs volume. But I can't find it in RStudio Server! I have scoured the internet looking for help on this but not able to solve my problem.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;It is said that before an earthquake happens, a viewer experiences disturbances in DTH TV transmission in the form of distorted images on the screen which automatically correct after a few seconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to identify patterns of such disturbances by continuously monitoring TV images so that earthquakes can potentially be predicted at least few minutes in advance and many lives could be saved?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have one small list of entities, such as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Russia&#xA;Vladimir&#xA;Moscow&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have a massive database of JSON indices. For each entry there are multiple alpha-numeric identifiers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for instance, for &lt;code&gt;Russia&lt;/code&gt; there might only be one. But for &lt;code&gt;Vladimir&lt;/code&gt; maybe there will be 100. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;They're stored in JSON but I read them into my java program like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        // GET JSON DATA&#xA;        File f = new File(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json&quot;);&#xA;        String jsonTxt = null;&#xA;&#xA;        if (f.exists())&#xA;        {&#xA;            InputStream is = new FileInputStream(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json&quot;);&#xA;            jsonTxt = IOUtils.toString(is);&#xA;        }&#xA;        //reformat&#xA;        jsonTxt = ( jsonTxt.substring(1, jsonTxt.length()-1) ).replace(&quot;\\\\\\\\&quot;,&quot;&quot;);&#xA;&#xA;        Gson json = new Gson();&#xA;        Map&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt; mast_Q_storage_map = new HashMap&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt;();&#xA;        mast_Q_storage_map = (Map&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt;) json.fromJson(jsonTxt, mast_Q_storage_map.getClass());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to get all of the values associated with the entities from the small list. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I need to search the big list and retrieve their values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I want to try to determine if there is a relationship between any of the entities in the sentence, as in I want to check if &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Russia   X Vladimir&#xA;Russia   X Moscow&#xA;Moscow   X Vladimir&#xA;Moscow   X Russia&#xA;Vladimir X Russia&#xA;Vladimir X Moscow&lt;/p&gt;&#xA;&#xA;&lt;p&gt;will result in a relationship, I've been trying to do it like this, but I'm running into big problems:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        // Read in all the sentences, that are in files, in this folder&#xA;        final File folder = new File(&quot;/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/&quot;);&#xA;&#xA;        for (final File fileEntry : folder.listFiles()) &#xA;        {&#xA;            BufferedReader br = new BufferedReader(new FileReader(fileEntry));&#xA;            try &#xA;            {&#xA;                //Store the filename&#xA;                //System.out.println(fileEntry.getName());&#xA;                StringBuilder sb = new StringBuilder();&#xA;                String line = br.readLine();&#xA;&#xA;                while (line != null) &#xA;                {   &#xA;                    sb.append(line);&#xA;                    sb.append(System.lineSeparator());&#xA;                    line = br.readLine();&#xA;                }&#xA;                String everything = sb.toString();&#xA;                //System.out.println(everything);&#xA;&#xA;                Document doc = Jsoup.parse(everything);&#xA;&#xA;                Elements contents = doc.getElementsByTag(&quot;sentence&quot;);&#xA;                for (Element content : contents) &#xA;                {&#xA;                    //store the sentence number&#xA;                    String number = content.select(&quot;sentence&quot;).text();&#xA;                    number = number.substring(0, number.indexOf(&quot; &quot;)); &#xA;                    System.out.println(number);&#xA;&#xA;                    //get all the entities in this sentence&#xA;                    Elements pers = content.select(&quot;PERSON&quot;);&#xA;                    Elements locs = content.select(&quot;LOCATION&quot;);&#xA;                    Elements orgs = content.select(&quot;ORGANIZATION&quot;);&#xA;&#xA;                    //collect all the elements to a list, all the elements of one sentence&#xA;                    List&amp;lt;String&amp;gt; list = new ArrayList&amp;lt;String&amp;gt;();&#xA;&#xA;                    for (Element per : pers) &#xA;                    {&#xA;                        list.add(per.text().trim());&#xA;                    }&#xA;                    for (Element loc : locs) &#xA;                    {&#xA;                        list.add(loc.text().trim());&#xA;                    }&#xA;                    for (Element org : orgs) &#xA;                    {&#xA;                        list.add(org.text().trim());&#xA;                    }&#xA;&#xA;&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println(&quot;This is list of sentence elements:&quot;);&#xA;                    for (String s : list)&#xA;                        System.out.println(s);&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;                    System.out.println();&#xA;&#xA;&#xA;                    List&amp;lt;String&amp;gt; Q_value_list = new ArrayList&amp;lt;String&amp;gt;();&#xA;&#xA;                    // for the list of Q values to keys&#xA;                    for (Entry&amp;lt;String, HashSet&amp;lt;String&amp;gt;&amp;gt; e : mast_Q_storage_map.entrySet()) &#xA;                    {&#xA;                        for (String s : list)&#xA;                        {&#xA;                            if (e.getKey().contains(s)) &#xA;                            {&#xA;                                //List&amp;lt;String&amp;gt; Q_value_list = new ArrayList&amp;lt;String&amp;gt;(e.getValue());&#xA;&#xA;                                System.out.println(e.getKey() + &quot; :: &quot; + Q_value_list.toString());&#xA;                            }&#xA;                        }&#xA;                    }&#xA;&#xA;&#xA;&#xA;                                //czeher&#xA;                                for (String home:Q_value_list) &#xA;                                {&#xA;                                  for (String away:Q_value_list) &#xA;                                  {&#xA;&#xA;&#xA;                                    String URL_czech = &quot;http://milenio.dcc.uchile.cl/sparql?default-graph-uri=&amp;amp;query=PREFIX+%3A+%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0D%0ASELECT+*+WHERE+%7B%0D%0A+++%3A&quot; &#xA;                                                       + home + &quot;+%3FsimpleProperty+%3A&quot; &#xA;                                                       + away + &quot;%0D%0A%7D%0D%0A&amp;amp;format=text%2Fhtml&amp;amp;timeout=0&amp;amp;debug=on&quot;;&#xA;&#xA;&#xA;                                    URL wikidata_page = new URL(URL_czech);&#xA;                                    HttpURLConnection wiki_connection = (HttpURLConnection)wikidata_page.openConnection();&#xA;                                    InputStream wikiInputStream = null;&#xA;&#xA;&#xA;                                        try &#xA;                                        {&#xA;                                            // try to connect and use the input stream&#xA;                                            wiki_connection.connect();&#xA;                                            wikiInputStream = wiki_connection.getInputStream();&#xA;                                        } &#xA;                                        catch(IOException error) &#xA;                                        {&#xA;                                            // failed, try using the error stream&#xA;                                            wikiInputStream = wiki_connection.getErrorStream();&#xA;                                        }&#xA;                                    // parse the input stream using Jsoup&#xA;                                    Document docx = Jsoup.parse(wikiInputStream, null, wikidata_page.getProtocol()+&quot;://&quot;+wikidata_page.getHost()+&quot;/&quot;);&#xA;&#xA;&#xA;&#xA;                                    Elements link_text = docx.select(&quot;table.sparql &amp;gt; tbody &amp;gt; tr:nth-child(2) &amp;gt; td &amp;gt; a&quot;);&#xA;                                    //link_text.text();&#xA;                                    for (Element l : link_text) &#xA;                                    {&#xA;                                        String output = l.text();&#xA;                                        System.out.println( output );&#xA;                                    }&#xA;&#xA;&#xA;                                  }&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;                    }&#xA;&#xA;                }&#xA;            }&#xA;            finally &#xA;            {&#xA;                br.close();&#xA;            }&#xA;&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I would like to find different patterns recognition algorithm to detect different type of fraud. I have 1 million unstructured text documents about the clients' information with metadata about the client name, viewers, location in the cloud. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the patterns I was thinking of and the related fraud that i would like to detect:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Numeric Patterns - fictitious invoice numbers, fictitiously-generated transaction amounts. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Time Patterns - transactions occurring too regularly, activity at unusual times or dates. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Name Patterns - similar and alerted name and addresses. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Geographic Patterns - Proximity relationships between apparently unrelated entities. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What technique can I use ? any keywords? thx.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Does anyone know where I might be able to find a list of the most common typing errors and their corrections? This is separate from more complicated considerations concerning general spelling checking (which can have very many candidates in relation to the correct spelling of the word in question); rather I am looking for a similar list as used by Microsoft Word (for instance correcting &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;teh&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;the&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;becaise&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;with &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;because&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Not only can the manner in which these sort of errors are fixed be hard-coded, their frequent occurrence in text provides significant dividends in textual mining (provided that such a list of errors and corrections can be obtained, of course).&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;What is the difference between data mining approaches: frequent itemsets and item-based collaborative filtering in the area of recommender systems?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to analyze the movie database using python, downloaded from imdb. While trying to generate some plots, I am running into errors which confuses me.&#xA;I am trying to generate a matrix of small figures which can show me any hidden pattern etc. Here is the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fig, axes = plt.subplots(nrows=4, ncols=6, figsize=(12, 8), &#xA;                         tight_layout=True)&#xA;&#xA;bins = np.arange(1950,2012,3)&#xA;for ax, genre in zip(axes.ravel(), movieGenre):&#xA;    ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')&#xA;    ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')&#xA;    ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)&#xA;    ax.xaxis.set_ticks(np.arange(1950, 2013, 30))&#xA;    ax.set_yticks([])&#xA;    ax.set_xlabel('Year')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The first &lt;code&gt;hist&lt;/code&gt; isn't working, but the second one is working when I am commenting out the first one. Here is the traceback:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; KeyError                                  Traceback (most recent call last)&#xA;&amp;lt;ipython-input-158-c2e7c2737372&amp;gt; in &amp;lt;module&amp;gt;()&#xA;      4 bins = np.arange(1950,2012,3)&#xA;      5 for ax, genre in zip(axes.ravel(), movieGenre):&#xA;----&amp;gt; 6     ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')&#xA;      7     ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')&#xA;      8     ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/axes.pyc in hist(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)&#xA;   8247         # Massage 'x' for processing.&#xA;   8248         # NOTE: Be sure any changes here is also done below to 'weights'&#xA;-&amp;gt; 8249         if isinstance(x, np.ndarray) or not iterable(x[0]):&#xA;   8250             # TODO: support masked arrays;&#xA;   8251             x = np.asarray(x)&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)&#xA;    477     def __getitem__(self, key):&#xA;    478         try:&#xA;--&amp;gt; 479             result = self.index.get_value(self, key)&#xA;    480 &#xA;    481             if not np.isscalar(result):&#xA;&#xA;/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)&#xA;   1169 &#xA;   1170         try:&#xA;-&amp;gt; 1171             return self._engine.get_value(s, k)&#xA;   1172         except KeyError as e1:&#xA;   1173             if len(self) &amp;gt; 0 and self.inferred_type == 'integer':&#xA;&#xA;&#xA;&#xA;KeyError: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the first few columns of the data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     imdbID     title                      rating    vote      runtime  year    genre&#xA;0   tt0111161   The Shawshank Redemption    9.3      1,439,277  142    1994 [Crime, Drama]&#xA;1   tt0468569   The Dark Knight             9.0      1,410,124  152    2008 [Action, Crime, Drama]&#xA;2   tt1375666   Inception                   8.8      1,209,159  148    2010 [Action, Mystery, Sci-Fi, Thriller]&#xA;3   tt0137523   Fight Club                  8.9      1,123,462  139    1999 [Drama]&#xA;4   tt0110912   Pulp Fiction                8.9      1,117,193  154    1994 [Crime, Drama]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;movieGenre is basically collecting all the different genres from 'genre' column with duplicates removed: &lt;code&gt;movieGenre = set(movieDF.genre.sum())&lt;/code&gt; . I then added a single column to movieDF data frame for each genre such that if a particular movie belong to that genre, then that cell is &lt;code&gt;True&lt;/code&gt; otherwise it is &lt;code&gt;False&lt;/code&gt;.  So for example, for the movie Inception, the &lt;code&gt;Action&lt;/code&gt; column is marked &lt;code&gt;True&lt;/code&gt; but &lt;code&gt;Crime&lt;/code&gt; column is marked &lt;code&gt;False&lt;/code&gt; and so forth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I was going through a paper comparing glove and word2vec. I came across the pound notation shown below. What does it mean when used like this?&#xA;&lt;img src=&quot;https://i.imgur.com/WQoNTdG.png&quot; alt=&quot;equation&quot;&gt;&#xA;The link for paper is &lt;a href=&quot;http://arxiv.org/pdf/1411.5595v2.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am looking for a package that does gradient descent parameter estimation in R, maybe with some bootstrapping to get confidence intervals. I wonder if people call it something different here as I get almost nothing on my searches, and the one article I found was from someone who rolled their own. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is not that hard to implement, but I would prefer to use something standard.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I would like to post a paper in International Conference on Soft Computing. I want to know whether the journal is a reputed journal. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Something that I often see in papers (&lt;a href=&quot;http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt;) about large-scale learning is that click-through rate (CTR) problems can have up to a billion of features for each example. In &lt;a href=&quot;http://research.google.com/pubs/pub41159.html&quot; rel=&quot;nofollow&quot;&gt;this Google paper&lt;/a&gt; the authors mention:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The features used in our system are drawn from a variety of sources,&#xA;  including the query, the text of the ad creative, and various&#xA;  ad-related metadata.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I can imagine a few thousands of features coming from this type of source, I guess through some form of feature hashing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: how does one get to a billion features? How do companies translate user behavior into features in order to reach that scale of features?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;When a random initialization of centroids is used, different runs of K-means produce different total SSEs. And it is crucial in the performance of the algorithm. &#xA;What are some effective approaches toward solving this problem? Recent approaches are appreciated.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am not in the data science field, but I would like to examine in depth this field and, particularly, I would like to start from the analysis of the social networks data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to find some good references, both paper, websites and books, in order to start learning about the topic. Browsing on the internet, one can find a lot of sites, forum, papers about the topic, but I'm not able to discriminate among good and bad readings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am an R, Matlab, SAS user and I know a little bit of python language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you suggest any references from which I could start studying and deepen the industry?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've got survey data that resembles:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-------------| Q1a | Q1b | Q1c | Q2a | Q2b | Q2c | Classification&#xA;| Respondent  | 1   | 0   | 0   | 1   | 0   | 0   | Red&#xA;| Respondent  | 0   | 0   | 1   | 1   | 0   | 0   | Green&#xA;| Respondent  | 0   | 1   | 0   | 0   | 0   | 1   | Yellow&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to predict the classification for new respondents. Currently I'm using a Naive Bayes, and getting pretty bad accuracy (~20%). I don't have much training data, and the training data is hand scraped from non-standard sources (internal company procedures are a mess here).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for other ways to predict the classification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking about assigning weights to each question, and magically predicting the result based on those, somehow. Although I don't really know where to start learning about how to do that, and whether it's appropriate for this data. I have very little background in this :(&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas or tips on predicting the classification column with no training data?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a large dataset that I need to split into groups according to specific parameters.  I want the job to process as efficiently as possible.  I can envision two ways of doing so&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt; - Create map from original RDD and filter&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def customMapper(record):&#xA;    if passesSomeTest(record):&#xA;        return (1,record)&#xA;    else:&#xA;        return (0,record)&#xA;&#xA;mappedRdd = rddIn.map(lambda x: customMapper(x))&#xA;rdd0 = mappedRdd.filter(lambda x: x[0]==0).cache()&#xA;rdd1 = mappedRdd.filter(lambda x: x[1]==1).cache()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt; - Filter original RDD directly&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def customFilter(record):&#xA;    return passesSomeTest(record)&#xA;&#xA;rdd0 = rddIn.filter(lambda x: customFilter(x)==False).cache()&#xA;rdd1 = rddIn.filter(customFilter).cache()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The fist method has to itterate over all the records of the original data set 3 times, where the second only has to do so twice, under normal circumstances, however, spark does some behind the scenes graph building, so I could imagine that they are effectively done in the same way.  My questions are:&#xA;a.) Is one method more efficient than the other, or does the spark graph building make them equivalent&#xA;b.) Is it possible to do this split in a single pass&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have two series of values, a and b as inputs and I want to create a score, c, which reflects both of them equally.  The distribution of a and b are below&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/TSKfA.png&quot; alt=&quot;Distribution of a&quot;&gt;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/WivxT.png&quot; alt=&quot;Distribution of b&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In both cases, the x-axis is just an index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I go about creating an equation c = f(a,b) such that a and b are (on average) represented equally in c?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: c = (a+b)/2 or c = ab will not work because c will be too heavily weighted by a or b.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need a function, f, where c = f(a,b) and c' = f(a + stdev(a),b) = f(a, b + stdev(b))&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;In Support Vector Machines, when used for sentiment analysis, text gets converted into a set of data points. How does this happen, usually?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a question about MLlib in Spark.(with Scala)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to understand how LogisticRegressionWithLBFGS and LogisticRegressionWithSGD work. I usually use SAS or R to do logistic regressions but I now have to do it on Spark to be able to analyze Big Data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How is the variable selection done? Is there any try of different variable combinations in LogisticRegressionWithLBFGS or LogisticRegressionWithSGD? Something like a test of significance of variable one by one? Or a correlation calculation with the variable of interest? Is there any calculation of BIC, AIC to choose the best model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because the model only returns weights and intercept...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I understand those Spark functions and compare to what I'm used to with SAS or R ?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;What are the tools, practices and algorithms used in automated text writing?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, lets assume that I have access to wikipedia/wikinews and similar websites API and I would like to produce article about &quot;Data Science with Python&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe that this task should be divided into two segments. First would be &lt;strong&gt;text mining&lt;/strong&gt; and second would be &lt;strong&gt;text building&lt;/strong&gt;. I'm more or less aware how text mining is performed and there are lots of materials about it in Internet. However, amount of materials related to automated text building seems to be lower. There are plenty of articles which says that some companies are using it, but there is lack of details. Are there any common ideas about such text building?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've used &lt;code&gt;scikit-learn&lt;/code&gt; in Python to compare results of naive Bayes and SVM. I've found that naive Bayes is quicker than SVM. Could anyone shed some light on reasons for such finding?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'd like to classify the data on coordinate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are 2 example data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data1 = [(1,1), (2,2), &lt;strong&gt;(3, 3), (4, 2), (5, 3)&lt;/strong&gt;, (6, 0)]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;data2 = [(1,1), (2,2), &lt;strong&gt;(3, 10), (4, 9), (5, 10)&lt;/strong&gt;, (6, 0)]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bold part have the same wave in above data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The length are all different in my data set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way that I can find the similar wave in many data like this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;First of all, I am new in this field we call &lt;em&gt;big data&lt;/em&gt;, so my questions may be naive. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to build an application, which deals with geolocation data, which could be : &lt;em&gt;latitude and longitude coordinates&lt;/em&gt; and &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/cc280766.aspx&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Geography SQL Server&lt;/em&gt;&lt;/a&gt; column types.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to have the following elements made easy:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scalability : be prepared to receive huge amount of data, adding servers to the system have to be easy&lt;/li&gt;&#xA;&lt;li&gt;Proximity requests : in example, how much points are in a circle (at meter scale). &lt;/li&gt;&#xA;&lt;li&gt;Data must be accessible rapidly after being written. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've been looking around for existing solutions, which are &quot;Hadoop friendly&quot; (Hortonworks, Cloudera) and available DBMS, like Cassandra. &#xA;I have found some interesting information, but I still think it's hard to decide, which one to choose. &#xA;It also need drivers for &lt;em&gt;NodeJS&lt;/em&gt; &amp;amp; &lt;em&gt;.NET&lt;/em&gt; (Hadoop with Cassandra seem to be OK with that). &#xA;I've also looked around the MongoDB ecosystem, but, again, I feel that it is hard to know where to look at. By (little) experience with Mongoose, MongoDB can be disqualified by the third point because data writes are slow. But my model could certainly be improved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do any of you have some recent experiences, manipulating massive amount of &lt;em&gt;geolocation data&lt;/em&gt;? I would appreciate sharing them here as well as any quality and recent literature on the subject.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;A commonly heard sentence in unsupervised Machine learning is&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;High dimensional inputs typically live on or near a low dimensional&#xA;  manifold&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is a dimension? What is a manifold? What is the difference?&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Can you give an example to describe both?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Manifold&lt;/strong&gt; from Google/Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In mathematics, a manifold is a topological space that resembles&#xA;  Euclidean space near each point. More precisely, each point of an&#xA;  n-dimensional manifold has a neighbourhood that is homeomorphic to the&#xA;  Euclidean space of dimension n.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dimesion&lt;/strong&gt; from Google/Wikipedia:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In physics and mathematics, the dimension of a mathematical space (or&#xA;  object) is informally defined as the minimum number of coordinates&#xA;  needed to specify any point within it.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What does the Google/Wikipedia even mean in layman terms? It sounds like some bizarre definition like most machine learning definition?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;They are both spaces, so what's the difference between a Euclidean space (i.e. Manifold) and a dimension space (i.e. feature-based)?&lt;/strong&gt; &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Could you tell me, are there any techniques for building neural networks with non-negative weights?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;One of the problems I often encounter is that of poor data provenance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I do research I continuously make modifications to my code and rerun experiments. Each time I'm faced with a number of questions, such as: do I save the old results somewhere, just in case? Should I include the parameter settings in the output filenames or perhaps save them in a different file? How do I know which version of the script was used to produce the results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've recently stumbled upon &lt;a href=&quot;https://pythonhosted.org/Sumatra&quot; rel=&quot;nofollow&quot;&gt;Sumatra&lt;/a&gt;, a pretty lightweight Python package that can capture Code, Data, Environment (CDE) information that can be used to track data provenance. I like the fact that it can be used both from the command line and from within my Python scripts and requiring no GUI. The downside is that the project seems inactive and perhaps there's something better out there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: what is a good lightweight data provenance solution for my research? I'm coding small projects mostly in Python in the terminal on a remote server over SSH, so a command line solution would be perfect for me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I have stuck with Sumatra. When I posted this question I didn't look into the web interface yet, but that turns out to be a unique selling point. It displays a very detailed overview of the experiments, capturing not only the state of the data and code, but also the Python environment (package dependencies and versions!) and platform information (architecture and kernel version!).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I've updated the subject of my question to emphasize that I'm mostly concerned about  provenance.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Is the Semantic Web dead? Are ontologies dead?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am developing a work plan for my thesis about &lt;em&gt;&quot;A knowledge base through a set ontology for interest groups around wetlands&quot;&lt;/em&gt;. I have been researching and developing ontologies for it but I am still unclear about many things. What is the modeling language for ontologies?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which methodology for ontologies is better? &lt;a href=&quot;http://semanticweb.org/wiki/OTK_methodology&quot; rel=&quot;nofollow&quot;&gt;OTK&lt;/a&gt; or &lt;a href=&quot;http://semanticweb.org/wiki/METHONTOLOGY&quot; rel=&quot;nofollow&quot;&gt;METHONTOLOGY&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any program that does  as does &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.hd.uib.no/AcoHum/abs/Mejia.htm&quot; rel=&quot;nofollow&quot;&gt;Cratilo&lt;/a&gt; is a software for analyzing of textual corpora and for extraction of specific terms of the domain of study (it is developed by professors Jorge Antonio Mejia, Francisco Javier Alvarez and John Albeiro Sánchez, Institute of Philosophy the University of Antioquia). It enables lexical analysis of texts, identifying the words that appear their frequency and location in the text. Through a process of recognition, Cratylus identifies all the words in the text and builds a database becomes the draft analysis of the work. Are there other similar tools?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can the terms found by Cratilo be used to create a knowledge base?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the existing open semantic frameworks that can be used for such things? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there software that automatically creates RDF, OWL, and XML? How does Tails work? Jena? Sesame? &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;Referring to the Stanford course notes on &lt;a href=&quot;http://cs231n.github.io/neural-networks-1/#actfun&quot; rel=&quot;noreferrer&quot;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;, a paragraph says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Unfortunately, ReLU units can be fragile during training and can&#xA;  &quot;die&quot;. For example, a large gradient flowing through a ReLU neuron&#xA;  could cause the weights to update in such a way that the neuron will&#xA;  never activate on any datapoint again. If this happens, then the&#xA;  gradient flowing through the unit will forever be zero from that point&#xA;  on. That is, the ReLU units can irreversibly die during training since&#xA;  they can get knocked off the data manifold. For example, you may find&#xA;  that as much as 40% of your network can be &quot;dead&quot; (i.e. neurons that&#xA;  never activate across the entire training dataset) if the learning&#xA;  rate is set too high. With a proper setting of the learning rate this&#xA;  is less frequently an issue.&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What does dying of neurons here mean? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you please provide an intuitive explanation in simpler terms.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am redesigning some of the classical algorithms for Hadoop/MapReduce framework. I was wondering if there any established approach for denoting Big(O) kind of expressions to measure time complexity?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, hypothetically, a simple average calculation of n (=1 billion) numbers is O(n) + C operation using simple for loop, or O(log) I am assuming division to be a constant time operation for the sake for simplicity. If i break this massively parallelizable algorithm for MapReduce, by dividing data over k nodes, my time complexity would simply become O(n/k) + C + C'. Here, C' can be assumed as the job planning time overhead. Note that there was no shuffling involved, and reducer's job was nearly trivial.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am interested in a more complete analysis of algorithm with iterative loops over data and involve heavy shuffling and reducer operations. I want to incorporate, if possible, the I/O operations and network transfers of data.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have 40000 rows of text data of health care domain. Data has one column for text (2-5 sentences) and one column for its category.&#xA;I want to classify that into 300 categories. Some categories are independent while some are somewhat related. Distribution of data among categories is not uniform either i.e some of the categories(around 40 of them) have less data about 2-3 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am attaching log probablity of each class/categories. (OR distribution of classes) here.&#xA;&lt;img src=&quot;https://i.stack.imgur.com/nko61.png&quot; alt=&quot;Class prior logarithm of probabilities (log class distribution of data)&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I've compared the logistic regression models on R (&lt;code&gt;glm&lt;/code&gt;) and on Spark (&lt;code&gt;LogisticRegressionWithLBFGS&lt;/code&gt;) on a dataset of 390 obs. of 14 variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The results are completely different in the intercept and the weights.&#xA;How to explain this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the results of Spark (LogisticRegressionWithLBFGS) :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model.intercept  : &#xA; 1.119830027739959&#xA;model.weights :&#xA; GEST    0.30798496002530473&#xA; DILATE  0.28121771009716895&#xA; EFFACE  0.01780105068588628&#xA; CONSIS -0.22782058111362183&#xA; CONTR  -0.8094592237248102&#xA; MEMBRAN-1.788173534959893&#xA; AGE    -0.05285751197750732&#xA; STRAT  -1.6650305527536942&#xA; GRAVID  0.38324952943210994&#xA; PARIT  -0.9463956993328745&#xA; DIAB   0.18151162744507293&#xA; TRANSF -0.7413500749909346&#xA; GEMEL  1.5953124037323745&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the result of R :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;             Estimate Std. Error z value Pr(&amp;gt;|z|)   &#xA;(Intercept)  3.0682091  3.3944407   0.904 0.366052    &#xA;GEST         0.0086545  0.1494487   0.058 0.953821    &#xA;DILATE       0.4898586  0.2049361   2.390 0.016835 *  &#xA;EFFACE       0.0131834  0.0059331   2.222 0.026283 *  &#xA;CONSIS       0.1598426  0.2332670   0.685 0.493196    &#xA;CONTR        0.0008504  0.5788959   0.001 0.998828    &#xA;MEMBRAN     -1.5497870  0.4215416  -3.676 0.000236 ***   &#xA;AGE         -0.0420145  0.0326184  -1.288 0.197725    &#xA;STRAT       -0.3781365  0.5860476  -0.645 0.518777    &#xA;GRAVID       0.1866430  0.1522925   1.226 0.220366    &#xA;PARIT       -0.6493312  0.2357530  -2.754 0.005882 **  &#xA;DIAB         0.0335458  0.2163165   0.155 0.876760    &#xA;TRANSF      -0.6239330  0.3396592  -1.837 0.066219 .  &#xA;GEMEL        2.2767331  1.0995245   2.071 0.038391 *  &#xA;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Beyond the immediate suspects defined in the &lt;a href=&quot;https://spark.apache.org/docs/latest/tuning.html#memory-tuning&quot; rel=&quot;nofollow&quot;&gt;spark documentation&lt;/a&gt;, what are some ways to profile, tune and boost performance of an Apache Spark application? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;RBF kernel using SVM depends on two parameters C and gamma. If the equation of the kernel RBF as the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$K(X,X')= \\\\exp(\\\\gamma||X-X'||^2)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the equation I can see where can I use gamma, but I can't find the C parameter.&#xA;So, can enybody tell me please?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance,&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on an information extractor specifically purposed with parsing relationships between entities such as movies, directors, and actors. NLTK appears to provide the necessary tools to construct such a system. However, it is not clear how one would go about adding custom labels (e.g. actor, director, movie title).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, &lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow&quot;&gt;Chapter 7 of the NLTK Book&lt;/a&gt; discusses information extraction using a named entity recognizer, but it glosses over labeling details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have two questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would I add custom labels?&#xA;If I have bare lists of relevant named entities (e.g. movies, actors, etc.), how can I include them as features? It appears that I would need to use IOB format, but I am unsure about how to do this when I only have lists of named entities.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;This is just for finding overfitting gap.&lt;br&gt;&#xA;After initial research, I can only find method to draw learning curve using evaluation of test set.  However, I could not evaluate on training set and over the two learning curves.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to work using Amazon machine learning, but the data set that I have is small. The model I want to build is for regression based predictions and the domain I am aiming for the data set to belong is financial, say product price prediction, price and demand prediction based on macro/micro economic factor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for a data set that contains factors that lead to variations in value of a product or commodity. For example, I would like to predict the value of 1 unit of polyester yarn after 1 year.  The factors which influence the yarn price are say - prices of crude oil, GDP of country,figures of IIP, inflation etc. So I would like a data set that contains the quotes of these factors on which the final price depends.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find difficulty assembling this data myself because I don't know all the factors that contribute to a certain predictive price. Does anyone know of a dataset I can start with that sounds like it might contain these factors?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm new to all this and am putting together a learning project. I've decided on finding similarities between users in a data set such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Enron_Corpus&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://en.wikipedia.org/wiki/Enron_Corpus&lt;/a&gt;. After doing a bit of research, I also came across &lt;a href=&quot;https://datascience.stackexchange.com/questions/641/dataset-for-named-entity-recognition-on-informal-text/5397#5397&quot;&gt;Dataset for Named Entity Recognition on Informal Text&lt;/a&gt;. So I'm not short of data or a goal, I need to understand high-level techniques to get there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One valuable comment noted that this question appears too broad. What I was hoping to find with this question was the breadth of techniques I should focus research on, not answers that are immediately implementable. Please consider vague answers as entirely appropriate!!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Expanding on the goal, I am hoping to discover which authors might have affinity toward each other, or conversely do not care much for each other. So I will definitely need to start with Named Entity Recognition and build a means to organize the documents against those entities. Beyond that, I am not so sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What high level concepts should I be looking at? Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are some Python libraries which can convert a (X,Y) tuple to strings? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1.23,4.56) yields strings “1_4”, “12_45”, “123_456”.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am looking for a good free existing tool which visualizes geographical data (let's say in the form of coordinates) by plotting them on a map. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be a library (see &lt;a href=&quot;https://stackoverflow.com/questions/9018607/library-for-map-visualization&quot;&gt;this question on StackOverflow&lt;/a&gt;, which suggests a Python library called basemap, which is interesting but not dynamic enough, namely it does not allow for interactivity) or a complete toolkit. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Existing things I found are oriented towards realizing web pages, which are not my ultimate goal (see &lt;a href=&quot;http://www.simile-widgets.org/exhibit/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exhibit&lt;/a&gt; or &lt;a href=&quot;http://modestmaps.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Modest Maps&lt;/a&gt;). I'd like something to feed with data which spits out an interactive map where you can click on places and it displays the related data. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I found a data set called &lt;a href=&quot;http://www.cs.cmu.edu/~./enron/&quot;&gt;Enron Email Dataset&lt;/a&gt;. It is possibly the only substantial collection of &quot;real&quot; email that is public. I found some prior analysis of this work:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A paper describing the Enron data was presented at the 2004 CEAS conference.&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Some experiments associated with this data are described on Ron Bekkerman's home page&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.parakweet.com/&quot;&gt;Parakweet&lt;/a&gt; has released an open source set of Enron sentence data, labeled for speech acts.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Work at the University of Pennsylvania includes a query dataset for email search as well as a tool for generating spelling errors based on the Enron corpus. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm looking for some interesting current trend topics to work with.please give me some suggestions.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;When do we feel need to go through non-linear transformation like kernel PCA ? Please share an example&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am looking to do k-means clustering on a set of 10-dimensional points.  The catch: &lt;strong&gt;there are 10^10 points&lt;/strong&gt;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking for just the center and size of the largest clusters (let's say 10 to 100 clusters); I don't care about what cluster each point ends up in.  Using k-means specifically is not important; I am just looking for a similar effect, any approximate k-means or related algorithm would be great (minibatch-SGD means, ...).  Since GMM is in a sense the same problem as k-means, doing GMM on the same size data is also interesting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this scale, subsampling the data probably doesn't change the result significantly: the odds of finding the same top 10 clusters using a 1/10000th sample of the data are very good.  But even then, that is a 10^6 point problem which is on/beyond the edge of tractable.  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an Excel sheet with &lt;code&gt;n&lt;/code&gt; columns, these columns contain info about the students. For admissions we have the score of test scores in multiple subject areas, scores from an interview, and scores of a written test and comprehension test. There is a column which contains student's academic level (High, M.High, Middle, M.Low, Low). I want to compare the last column with the others variables and see whether there are common features that passing students have in common.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there software for this? If this can be done with excel, how can I do it? Does SPSS provide this kind of analysis?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'll set the question up with an example. You are analysing news coverage text data from 2014, and find that a term appears less often in the third quarter of 2014 than the final quarter (let's imagine it's the term &quot;Christmas&quot;). Unfortunately, there are also far less news articles in the third quarter than in the second (due to the lack of news in the summer). So how do we accurately compare the counts in each quarter? We assume that there will be a greater number of occurrences in the fourth quarter, but how much does the magnitude of this difference depend on the change in size of the underlying text?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Heaps%27_law&quot; rel=&quot;nofollow&quot;&gt;Heap's law&lt;/a&gt; shows the relationship between text size and number of unique terms. It's non-linearity implies that the rate of new, unique words introduced by the text decreases as you increase the size of the text, and the proportion of the text taken up by each existing word therefore increases. This applies given documents taken from the same 'distribution' of text, in other words the underlying zipfian distribution of word ranks is identical (see &lt;a href=&quot;http://en.wikipedia.org/wiki/Zipf%27s_law&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my example above this is obviously not the case, since the underlying topics, and resultant term distribution, will change between summer and winter, especially with regards to the term &quot;Christmas&quot;. But take the same term count but over the whole of 2013 and 2014; you would reasonably expect the general underlying term distribution to be the same in each period, so Heap's law applies, but what if the volume of text has changed? Simply normalising by the size of the text, or the number of documents, does not, as far as I can tell, account for the relative change in expected value of the term count.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a hunch that this might be a simple application of Heap's or Zipf's laws, but I can't see how to apply them to this particular question. Appreciate any insight.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I know that there are a number of predictive models (generized linear ones, trees, neural network, support vector machines, knn, Naive Bayes, ...) that have been proposed to perform various analytical tasks. Now I am striving to find appropriate references about their performance when the data becames &quot;Big&quot;. In other words, how is their performance when the data becames really big. Does the training time increase more than linear? Is there any comparative benchmark between computational time and precision when the data becames high (for the various predictive models).&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm a new bee for data analysis . i need to work on a research project in big data analysis. first of all i search for a dataset and i found interest in s&lt;a href=&quot;https://archive.org/details/stackexchange&quot; rel=&quot;nofollow&quot;&gt;tack exchange  data dump&lt;/a&gt;. however i browse for researches i found a lot .And whatever i thought about a idea based on this dataset , its already done by someone else . &#xA;please help me out with a new and useful idea for my research .&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Users tend to click on results ranked highly by search engines much more often than those ranked lower. How do you train a search engine using click data / search logs without this bias? I.e. you don't want to teach the search engine that the results that are currently ranked highly should necessarily continue to be ranked highly just because they were frequently clicked.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Building out a system that tries to apply zero or more predefined labels to text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each label, we've:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;built out a reasonably good vocabulary of high-value words/features&lt;/li&gt;&#xA;&lt;li&gt;developed a corpus containing thousands of labeled entries&lt;/li&gt;&#xA;&lt;li&gt;trained a NaiveBayesClassifier for each topic that does a good job of classifying valid vs noisy content&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The problem seems to be that the individual classifier is great at differentiating between valid &amp;amp; noisy content &lt;strong&gt;WITHIN&lt;/strong&gt; a topic:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&quot;the green energy bill will revolutionize...&quot; (green = &quot;green energy&quot;)&lt;/li&gt;&#xA;&lt;li&gt;&quot;the green bay packers went on to lose their...&quot; (green != &quot;green energy&quot;) &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;...but when classifying content that shouldn't match ANY topic it has a very high rate of false positives. There's no &quot;everything else&quot; label!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;tl;dr it's good at subtle, in-topic differentiation, but terrible at broad topic labeling&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any algorithms that help you classify into N categories, but allow for &quot;everything else&quot; which might not fit into ANY of the categories?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Could somebody please recommend a good R package for doing logit and probit regression? I have tried to find an answer by searching on Google but all the links I find go into lengthy explanations about what logit regression is, which I already know, but nobody seems to recommend an R package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Jerome Smith&lt;/p&gt;&#xA;',),\n",
       " ('&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.ensemble import RandomForestClassifier&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; clf = RandomForestClassifier(n_estimators=10, random_state=1)&#xA;&amp;gt;&amp;gt;&amp;gt; Y=[0,1]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,2,1,0], [7,6,5,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[0, 1, 2,3], [4,5,6,7]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,2,1,0], [7,6,5,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; X = [[3,1,2,0], [7,5,6,4]]&#xA;&amp;gt;&amp;gt;&amp;gt; clf = clf.fit(X, Y)&#xA;&amp;gt;&amp;gt;&amp;gt; print clf.feature_importances_&#xA;[ 0.2  0.1  0.1  0. ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Assume the features have names.&#xA;When I shuffle/change the listed order of features specified in the training data set, the importance for each feature changes.&lt;br&gt;&#xA;That means the resulted random forest classifier also changes.&#xA;Note that I have rule out the effect of randomness, by fixing the random seed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why does the listed order of features specified in the data set matter to the random forest classifier, given that the random seed is fixed? Thanks.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I dropped out of college but am interested in a career in data analysis. Now I am self-studying approximately 10 hours per day.  Browsing through job postings on Linkedin has allowed me to compose a rough curriculum.  It would be of great help to me if you would either add a subject I have omitted or eliminate a subject that is not necessary for success in the market place.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Curriculum (in 3-subject groupings):&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 1&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Single-variable calculus&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Intro to python&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;SQL&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 2&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Multi-variable calculus/linear algebra&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Discrete math&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Data structures and algorithms&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 3&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Calculus-based statistics and probability&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Hadoop stack&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Differential equations&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Group 4&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Statistical learning/predictive modelling&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Python data analysis techniques/Statistical programming in R&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Fundamentals of machine learning&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;All the while I plan to practice using any data sets I can find online.  Will this be sufficient to land a job in data analysis?  Of course I plan to learn far more than just this, but is this foundation solid enough to land an entry level data engineering/science position?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I recently discovered a &lt;a href=&quot;http://www.r-bloggers.com/analyze-linkedin-with-r/&quot;&gt;new R package&lt;/a&gt; for connecting to the LinkedIn API. Unfortunately the LinkedIn API seems pretty limited to begin with; for example, you can only get basic data on companies, and this is detached from data on individuals. I'd like to get data on all employees of a given company, which you can do &lt;a href=&quot;https://www.linkedin.com/vsearch/p?keywords=stack%20exchange&amp;amp;f_CC=974353&amp;amp;sb=People%20who%20work%20at%20Stack%20Exchange&amp;amp;trk=tyah&amp;amp;trkInfo=clickedVertical%3Asuggestion%2Cidx%3A1-1-1%2CtarId%3A1431584515143%2Ctas%3Astack%20exchange&quot;&gt;manually on the site&lt;/a&gt; but is not possible through the API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://import.io/&quot;&gt;import.io&lt;/a&gt; would be perfect if it &lt;a href=&quot;http://blog.import.io/post/tips-tricks&quot;&gt;recognised the LinkedIn pagination&lt;/a&gt; (see end of page).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know any web scraping tools or techniques applicable to the current format of the LinkedIn site, or ways of bending the API to carry out more flexible analysis? Preferably in R or web based, but certainly open to other approaches.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a data set of tweets regarding vaccines.  They have been collected from an API because they have keywords like &quot;flu, measles, MMR, vaccine&quot; etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to find tweets specifically about measles and the outbreak that occurred in California this past February.  It isn't enough to search the data set for words like &quot;California&quot; and &quot;Measles&quot; because tweets like &quot;MMR vaccination rates in Palo Alto on the rise&quot; are about measles and California, but wont be captured by a naive search.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any unsupervised algorithms that could help me out?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm not quite sure what &quot;latent&quot; refers to in this context. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://ijcai.org/papers07/Papers/IJCAI07-259.pdf&quot; rel=&quot;nofollow&quot;&gt;Computing Semantic Relatedness using&#xA;Wikipedia-based Explicit Semantic Analysis&lt;/a&gt; they say &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;''Our semantic analysis is explicit in the sense that we manipulate &#xA; manifest concepts grounded in human cognition, rather than &#xA; 'latent concepts' used by Latent Semantic Analysis''.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What does that mean in simple terms?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had a look at the related Wikipedia articles, ( &lt;code&gt;Latent&lt;/code&gt; (wikipedia.org/wiki/Latent_semantic_analysis), &lt;code&gt;Explicit&lt;/code&gt; wikipedia.org/wiki/Explicit_semantic_analysis) ), and I wasn't able to make heads or tails of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Perhaps someone with a better appreciate of the nuance involved here might be able to provide me with a clear indication of the similarities and differences, the pros and cons between these two methods for accessing document/text fragment relatedness to a particular concept.  &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a training data set distributed in two files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;File 1&lt;/strong&gt;: This contains actual classification for each X1. X1 is unique in this file. X1 has one-to-one relationship with X2, i.e. X2 is also unique. Y is binary.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X2 | Y  | &#xA;| 1  | 4  | 0  | &#xA;| 3  | 5  | 1  | &#xA;...&#xA;| 8  | 9  | 1  | &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;File 2&lt;/strong&gt;: This contains the real 'observations' of the experiment. X1 can appear multiple times. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X3 | X4 | &#xA;| 3  | 4  | 5  | &#xA;| 3  | 1  | 2  | &#xA;...&#xA;| 1  | 4  | 8  | &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here I can combine the two tables to have a structure like below and use them as observations:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| X1 | X2 | X3 | X4 | Y |&#xA;| 3  | 5  | 4  | 5  | 1 |&#xA;| 3  | 5  | 1  | 2  | 1 |&#xA;...&#xA;| 1  | 4  | 4  | 8  | 0 |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For test data I have similar structure, just the Y column is missing in File 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have multiple concerns here:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;X1 and X2 has one-to-one dependency in the data, i.e. X1 = f(X2) and X2 = f(X1)&lt;/li&gt;&#xA;&lt;li&gt;Y = f'(X1) or f'(X2)&lt;/li&gt;&#xA;&lt;li&gt;Frequency distribution of X1,X2 and Y changes dramatically in the new joined data set.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Does this kind of transformation of data leads to any insights?&lt;/li&gt;&#xA;&lt;li&gt;Does regression and ensemble learning techniques are capable of capturing these internal relationships?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I found in many sources that Hidden Markov Models are linear-chain networks(e.g. in Predicting Structured Data book by MIT). However, as I understand it, HMMs can have any edges in its graph. Even simple example of HMM in wikipedia has non-linear graph: &lt;img src=&quot;https://i.stack.imgur.com/JU1YV.png&quot; alt=&quot;enter image description here&quot;&gt; .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, the question is: what is the formal definition linear-chain structure and in which case forward-backward and Viterbi algorithms can give precise results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also taken into consideration this picture, taken from CRF tutorial, which says, that linear-chain CRF is &quot;generative-discriminative pair&quot; to HMM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/9CA8H.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;HI I am currently trying to apply various algorithms to a classification problem to assess which could be better and then try to fine tune the bests of the first approach. I am a beginner so I use Weka for now. I have basic ML concept understanding but am not in the details of algorithms yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I observed that on my problem, RBF networks performed vastly worse than IBK and other K methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I read about RBF networks, &quot;it implements a normalized Gaussian radial basisbasis function network. It uses the k-means clustering algorithm to provide the basis functions and learns either a logistic regression (discrete class problems) or linear regression (numeric class problems) on top of that. Symmetric multivariate Gaussians are fit to the data from each cluster. If the class is nominal it uses the given number of clusters per class.It standardizes all numeric attributes to zero mean and unit variance.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So basically, it also use k means to classify at first. But for some reason, I get the worst results with it using my metrics (ROC), while K methods are among the bests. Can I deduce from that fact something important about my data, like the fact that it has not a gaussian distribution, or is not fitted for logistic regression, or whatever I can't figure out?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also observed that random forests get similar results to K methods, and that adding a filter to reduce dimensionality improved these RF, random projection being better than PCA?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can this last point means that there is much randomness in my data so random dimension reduction is better than &quot;ruled&quot; dimension reduction like PCA? What can I deduce from the fact that RF perform equally to K methods?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel there is some signification here, but I am not skilled enough to understand what, and I would be very glad for any insights. Thanks by advance.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have location data of taxis moving around the city sourced from: &lt;a href=&quot;http://research.microsoft.com/apps/pubs/?id=152883&quot; rel=&quot;nofollow&quot;&gt;Microsoft Research&lt;/a&gt;&lt;br&gt;&#xA;Overall it has around 17million data points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have converted the data to JSON and filled up mongo. A sample looks like this:&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84349, 'timestamp': '2008-02-08 17:38:10', 'lon': 116.33986, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84441, 'timestamp': '2008-02-08 17:38:15', 'lon': 116.33995, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.8453, 'timestamp': '2008-02-08 17:38:20', 'lon': 116.34004, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84615, 'timestamp': '2008-02-08 17:38:25', 'lon': 116.34012, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84705, 'timestamp': '2008-02-08 17:38:30', 'lon': 116.34022, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.84891, 'timestamp': '2008-02-08 17:38:40', 'lon': 116.34039, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;code&gt;{'lat': 39.85083, 'timestamp': '2008-02-08 17:38:50', 'lon': 116.3406, 'ID': 1131}&lt;/code&gt;&lt;br&gt;&#xA;&lt;br&gt;&#xA;It consists of a taxiID - ID field, timestamp of its latitude and longitude combination.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: I want to use this data to calculate estimated time of arrival(ETA)  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I am doing it a crude way by querying mongoDB with aggregation. It is totally inefficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking at some sort of learning algorithm where the historical data can be used to train it. In the end, given two points, the algorithm should traverse the possible route by referring historical data and give an estimate of time.&#xA;Calculating time estimate is not a problem at all if I get the array of JSON documents between the points. But, getting those right arrays is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers in this direction will be very helpful. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Apologies if this isn't the correct place to ask - I'm not sure if this fits best with Stats or Data Science.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using analytics to help marketers identify attributes of their users correspond to successful conversions (such as someone buying a product, signing up for a newsletter, or subscribing to a service). Attributes could be things like which site they came from (referrer), their location, time/day of week, device type, browser, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I'd like to say (although I'm not certain it's possible) is to isolate differences in conversion rate to an individual attribute, something like, '11% of your users from Facebook converted whereas only 3% of non-Facebook users converted', which would mean that the attribute 'referrer' and the level of the attribute 'Facebook' are responsible for driving conversions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that I may have 100s of quasi-independent variables, is it even possible to isolate the effect to one variable and one level of that variable? As opposed to a combination of them that is more likely to be driving the difference? If so, what technique or conceptual paradigm do I use to identify which variable-level is responsible for the greatest lift in my dependent variable, conversion rate?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to figure out how to use NLTK's cascading chunker as per &lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Chapter 7 of the NLTK book&lt;/a&gt;. Unfortunately, I'm running into a few issues when performing non-trivial chunking measures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's start with this phrase:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&quot;adventure movies between 2000 and 2015 featuring performances by daniel craig&quot;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to find all the relevant NPs when I use the following grammar:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;grammar = &quot;NP: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&quot;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am not sure how to build nested structures with NLTK. The book gives the following format, but there are clearly a few things missing (e.g. How does one actually specify multiple rules?):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grammar = r&quot;&quot;&quot;&#xA;  NP: {&amp;lt;DT|JJ|NN.*&amp;gt;+}          # Chunk sequences of DT, JJ, NN&#xA;  PP: {&amp;lt;IN&amp;gt;&amp;lt;NP&amp;gt;}               # Chunk prepositions followed by NP&#xA;  VP: {&amp;lt;VB.*&amp;gt;&amp;lt;NP|PP|CLAUSE&amp;gt;+$} # Chunk verbs and their arguments&#xA;  CLAUSE: {&amp;lt;NP&amp;gt;&amp;lt;VP&amp;gt;}           # Chunk NP, VP&#xA;  &quot;&quot;&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my case, I'd like to do something like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grammar = &quot;MEDIA: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&#xA;           RELATION: {&amp;lt;V.*&amp;gt;}{&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN.*&amp;gt;+}&#xA;           ENTITY: {&amp;lt;NN.*&amp;gt;}&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It occurs to me that a CFG might be a better fit for this, but I only became aware of NLTK's support for this function about 5 minutes ago (from &lt;a href=&quot;https://stackoverflow.com/questions/14692489/chunking-with-nltk&quot;&gt;this question&lt;/a&gt;), and it does not appear that much documentation for the feature exists.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, assuming that I'd like to use a cascaded chunker for my task, what syntax would I need to use? Additionally, is it possible for me to specify specific words (e.g. &quot;directed&quot; or &quot;acted&quot;) when using a chunker?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I prefer this model in R&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are capturing sales data by time series (Month by month). Some of items have commissions and some have Discounts and others have both commissions and discounts. Is it Commissions or Discounts or commissions + Discounts have impact on my sales growth? Or is it my sales are growing because of those commissions or discounts or discounts +commissions Can you suggest me best model to solve my use case? I am thinking multiple regression. But I want to double check with experts like you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your all your help&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Data set: (5 variables)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Year-Month -Product -Sales  -Commission -Discounts&#xA;2013-01 Milk    300 No  Yes&#xA;2013-02 Milk    400 No  Yes&#xA;2013-03 Milk    200 No  Yes&#xA;2013-04 Milk    150 No  Yes&#xA;2013-05 Milk    500 No  Yes&#xA;2013-01 Bread   800 Yes No&#xA;2013-02 Bread   879 Yes No&#xA;2013-03 Bread   790 Yes No&#xA;2013-04 Bread   459 Yes No&#xA;2013-05 Bread   600 Yes No&#xA;2013-01 Cheese  400 Yes Yes&#xA;2013-02 Cheese  350 Yes Yes&#xA;2013-03 Cheese  600 Yes Yes&#xA;2013-04 Cheese  590 Yes Yes&#xA;2013-05 Cheese  720 Yes Yes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a data frame (a csv file) with dimensions 100x6 and I need only the columns c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;) and the rows in which the value of &quot;X1&quot; is greater than 30. So I did:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  data_frame &amp;lt;- read.csv (&quot;data_frame&quot;)&#xA;  data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The column subset problem is solved but now I need to subset rows from data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)] where the values of &quot;X1&quot; is greater than 30. I tried:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  data_frame [c(&quot;X1&quot; &amp;gt; 30), c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)] &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it returned the same data frame as data_frame [c(&quot;X1&quot;, &quot;X2&quot;, &quot;X4&quot;)].&#xA;Also tried using the function subset() with the same approach but got the same results.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have recently completed an MSc in Control Systems from a top university. It seems to me that control theory must have an application within data science. I would like to apply my degree within this domain, but I want to be sure that it is relevant to the role of a data scientist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The topics which I have particular interest and experience in are State Space Control, Systems Identification, Model Predictive Control and Optimal Control. I imagine that effective management of any large dataset must involve modelling of the system in terms of transfer functions/state space models (based on large sets of historical input/output data). These models could then be used to predict the evolution of a market/variable over time, and therefore optimise a given cost function such as profit, risk etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this kind of role exists within data science/ other areas, can you please give me more information/ ideas of job roles/ industries to research.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to investigate price-setting behavior of airlines -- specifically how airlines react to competitors pricing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I would say my knowledge about more complex analysis is quite limited I've done mostly all basic methods to gather a overall view of the data. This includes simple graphs which already help to identify similar patterns. I'am also using SAS Enterprise 9.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I'am looking for a more number based approach.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Data Set&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The (self) collected data set I'am using contain around ~54.000 fares.&#xA;All fares were collected within a 60 day time window, on a daily basis (every night at 00:00).&lt;img src=&quot;https://i.stack.imgur.com/q5bcT.png&quot; alt=&quot;Collection Method&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, every fare within that time window occurs $n$ times subject to the availability of the fare as well as the departure date of the flight, when it is passed by the collection date of the fare.&#xA;&lt;em&gt;(You cant collect a fare for a flight when the departure date of the flight is in the past)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The unformatted that looks basically like this: (fake data)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| requestDate        | price| tripStartDeparture | tripDestinationDeparture | flightCarrier |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 725.32    | 16APR2015:10:50:02 | 23APR2015:21:55:04       | XA            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 966.32    | 16APR2015:13:20:02 | 23APR2015:19:00:04       | XY            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;| 14APR2015:00:00:00 | 915.32    | 16APR2015:13:20:02 | 23APR2015:21:55:04       | XH            |&#xA;+--------------------+-----------+--------------------+--------------------------+---------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&quot;DaysBeforeDeparture&quot; is calculated via $I=s-c$ where&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I &amp;amp; interval (days before departure)&lt;/li&gt;&#xA;&lt;li&gt;s &amp;amp; date of the fare (flight departure)&lt;/li&gt;&#xA;&lt;li&gt;c &amp;amp; date of which the fare was collected&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here is a example of grouped data set by I (DaysBeforeDep.) (fake data!):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----------------+------------------+------------------+------------------+------------------+&#xA;| DaysBefDeparture | AVG_of_sale | MIN_of_sale | MAX_of_sale | operatingCarrier |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 880.68           | 477.99           | 2,245.23         | DL           |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 904.89           | 477.99           | 2,534.55         | DL           |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;| 0               | 1,044.39         | 920.99           | 2,119.09         | LH               |&#xA;+-----------------+------------------+------------------+------------------+------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;What I came up with so far&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Looking at the line graphs I can already estimate that several lines will have a high correlation factor. Hence, I tried to use correlation analysis first on the grouped data. But is that the correct way? Basically I try now to make correlations on the averages rather then on the individual prices?&#xA;Is there an other way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'am unsure wich regression model fits here, as the prices do not move in any linear form and appear non-linear. Would I need to fit a model to each of price developments of an airline&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: This is a long text-wall. If I need to clarify anything let me know. I'am new to this sub.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone a clue? :-)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have several sets of text files on hdfs that are exports from relations.  Unfortunately I do not know the structure of the table is, but I do know that each has a multi-part key that defines a row uniquely.  I know through domain knowledge that the key is multi-part (e.g. reporting-date, and item number) and I can identify some columns that are clearly not in the key (e.g. revenue from sale).  What is an effective way to identify potential sets of columns that are natural keys, because they are not duplicated in the observed data?  I can get several days of logs in a few Gig, so python or sql could work.  This seems like an great application for a dictionary, but I am not sure how to approach this.     &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have some basic features which I encoded in a &lt;strong&gt;one-hot vector&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Length of the feature vector equals to 400.&#xA;It is &lt;strong&gt;sparse&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw that conv nets is applied to a dense feature vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any problems to apply conv nets to a sparse feature vectors?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm wondering if anyone might have some novel insights as to the best way to analyze the following data.  It's a problem I've been thinking about in the back of my mind for a while, so I thought that I'd ask here. I have data that look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     day event actor recipient&#xA;995    8   128     G         J&#xA;996    8   129     G         K&#xA;997    8   130     G         B&#xA;998    8   131     B         G&#xA;999    8   132     H         G&#xA;1000   8   133     G         H&#xA;1001   8   134     E         G&#xA;1002   8   135     G         J&#xA;1003   8   136     B         H&#xA;1004   8   137     G         H&#xA;1005   8   138     G         H&#xA;1006   8   139     B         J&#xA;1007   9     1     D         J&#xA;1008   9     2     A         J&#xA;1009   9     3     A         J&#xA;1010   9     4     H         J&#xA;1011   9     5     A         J&#xA;1012   9     6     D         H&#xA;1013   9     7     A         F&#xA;1014   9     8     D         J&#xA;1015   9     9     A         H&#xA;1016   9    10     D         J&#xA;1017   9    11     A         J&#xA;1018   9    12     F         J&#xA;1019   9    13     F         J&#xA;1020   9    14     F         H&#xA;1021   9    15     F         G&#xA;1022   9    16     F         H&#xA;1023   9    17     C         F&#xA;1024   9    18     C         G&#xA;1025   9    19     D         H&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What you see here is an extract of a R dataframe.  The first column being the rownumber of the df, then the four variables.    These data start at day1 and end at day 22.  There are between 13 and 215 'events' on each day - each event is a separate behavioral event. Higher number events occur later in time than earlier numbered events.  Individuals are in the 'actor' and 'recipient' variables.   The data are available in csv format &lt;a href=&quot;https://github.com/jalapic/exampledata/blob/master/demodf.csv&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are 11 individuals (A - K). One thing you'll notice is that recipients tend to be lower down the alphabet, and actors tend to be higher up the alphabet.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A key question I'm interested in working out a methodology to address is to see if the likelihood of becoming an actor increases if an individual has recently been a recipient. You can see this on line 997 that G-B and then B-G occur followed by H-G and G-H.    An individual recipient doesn't have to appear on the next line to count as having an increased likelihood of appearing as an actor - I'm interested in the decay in the probability of this occurring over events (but not continuing to the next day).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further, I don't think this will be true for all individuals, so I am keen to test which individuals it is true for.     &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I am interested to know if an individual who has recently been a recipient but is now an actor is paired with more often an individual of a higher or lower letter than themselves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that these questions are clear and make sense.   I obviously don't expect a full analysis.  But I would be keen to hear of ideas for this type of analysis.  I believe that examining some Markov processes may be useful but I am interested in hearing about other ideas.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a set of strings, each also has soem categorical information associated with it. The categorical information isn't always great though, so I need to cluster the messages based on the text content &amp;amp; the categories. What is the best way to do this? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am looking to change careers and would appreciate some advice. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an undergraduate degree in English Literature and a JD. Needless to say, these were not the best decisions and I would like to change my career. I have always enjoyed math and science, and after months of research and self study, I have decided that I would like to pursue statistics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is basically this : would it be better to get another Bach. Degree in statistics or should I take calculus 1-3, linear algebra, probably and statistics and some computer science courses at a community college then try to get into a grad program instead? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am afraid that without the bachelors degree I will not have the required knowledge for a masters even after taking the courses at my local c.c. However, another bachelors degree may be a worthless waste of time. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please share your thoughts. Also, sorry if this is not the right place for this question. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I was informed of 5 java NLP libraries.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Apache cTAKES™&lt;/li&gt;&#xA;&lt;li&gt;MetaMap&lt;/li&gt;&#xA;&lt;li&gt;LexEVS (&lt;a href=&quot;http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/web/download.html&quot; rel=&quot;nofollow&quot;&gt;http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/web/download.html&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Apache OpenNLP&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I also plan to parallelize an NLP library via map-reduce with hadoop.&#xA;However, I'm new to natural language processing, so I don't know how to approach the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The goal is to download a set of clinical trials on cancer from www.clinicaltrials.gov and parse eligibility criteria (both inclusion and exclusion criteria), identify the ECOG scores and annotate each CTA with the allowed ECOG scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, in the following document, if ECOG is specified in inclusion criteria, it is not negated. If ECOG is specified in exclusion criteria, it is negated. If both exclusion criteria and inclusion criteria are not mentioned, then ECOG is not negated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Document: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Eligibility criteria of CTA &quot;NCT01572038&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Inclusion Criteria:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;Patients must have histological proof of a primary non-small cell lung cancer&#xA;  (bronchoalveolar carcinomas presenting as a discrete solitary radiological mass or&#xA;  nodule are eligible)&lt;/li&gt;&#xA;  &lt;li&gt;Patients must be classified post-operatively as stage IB, II or IIIA on the basis of&#xA;  pathologic criteria&lt;/li&gt;&#xA;  &lt;li&gt;At the time of resection a complete mediastinal lymph node resection or at least&#xA;  lymph node sampling should have been attempted; if a complete mediastinal lymph node&#xA;  resection or lymph node sampling was not undertaken, any mediastinal lymph node which&#xA;  measured 1.5 cm or more on the pre-surgical computed tomography (CT)/magnetic&#xA;  resonance imaging (MRI) scan or any area of increased uptake in the mediastinum on a&#xA;  pre-surgical positron emission tomography (PET) scan must have been biopsied; note: a&#xA;  pre-surgical PET scan is not mandatory&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;The nodal tissue must be labelled according to the recommendations of the&#xA;  American Thoracic Society; surgeons are encouraged to dissect or sample all&#xA;  accessible nodal levels; the desirable levels for biopsy are:&lt;/li&gt;&#xA;  &lt;li&gt;Right upper lobe: 4, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Right middle lobe: 4, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Right lower lobe: 4, 7, 9, 10&lt;/li&gt;&#xA;  &lt;li&gt;Left upper lobe: 5, 6, 7, 10&lt;/li&gt;&#xA;  &lt;li&gt;Left lower lobe: 7, 9, 10&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Surgery may consist of lobectomy, sleeve resection, bilobectomy or pneumonectomy as&#xA;  determined by the attending surgeon based on the intraoperative findings; patients&#xA;  who have had only segmentectomies or wedge resections are not eligible for this&#xA;  study; all gross disease must have been removed at the end of surgery; all surgical&#xA;  margins of resection must be negative for tumor&lt;/li&gt;&#xA;  &lt;li&gt;No more than 16 weeks may have elapsed between surgery and randomization; for&#xA;  patients who received post-operative adjuvant platinum-based chemotherapy, no more&#xA;  than 26 weeks may have elapsed between surgery and randomization&lt;/li&gt;&#xA;  &lt;li&gt;Patient must consent to provision of and investigator(s) must agree to submit a&#xA;  representative formalin fixed paraffin block of tumor tissue at the request of the&#xA;  Central Tumor Bank in order that the specific EGFR correlative marker assays may be&#xA;  conducted&lt;/li&gt;&#xA;  &lt;li&gt;The patient must have an Eastern Cooperative Oncology Group (ECOG) performance status&#xA;  of 0, 1 or 2&lt;/li&gt;&#xA;  &lt;li&gt;Leukocytes &amp;gt;= 3.0 x 10^9/L or &amp;gt;= 3000/ul&lt;/li&gt;&#xA;  &lt;li&gt;Absolute granulocyte count &amp;gt;= 1.5 x 10^9/L or &amp;gt;= 1,500/ul&lt;/li&gt;&#xA;  &lt;li&gt;Platelets &amp;gt;= 100 x 10^9/L or &amp;gt;= 100,000/ul&lt;/li&gt;&#xA;  &lt;li&gt;Total bilirubin within normal institutional limits&lt;/li&gt;&#xA;  &lt;li&gt;Alkaline phosphatase =&amp;lt; 2.5 x institutional upper limit of normal; if alkaline&#xA;  phosphatase is greater than the institutional upper limit of normal (UNL) but less&#xA;  than the maximum allowed, an abdominal (including liver) ultrasound, CT or MRI scan&#xA;  and a radionuclide bone scan must be performed prior to randomization to rule out&#xA;  metastatic disease; if the values are greater than the maximum allowed, patients will&#xA;  not be considered eligible regardless of findings on any supplementary imaging&lt;/li&gt;&#xA;  &lt;li&gt;Aspartate aminotransferase (AST) (serum glutamic oxaloacetic transaminase [SGOT])&#xA;  and/or alanine aminotransferase (ALT) (serum glutamate pyruvate transaminase [SGPT])&#xA;  =&amp;lt; 2.5 x institutional upper limit of normal; if AST (SGOT) or ALT (SGPT) are greater&#xA;  than the institutional upper limit of normal (UNL) but less than the maximum allowed,&#xA;  an abdominal (including liver) ultrasound, CT or MRI scan must be performed prior to&#xA;  randomization to rule out metastatic disease; if the values are greater than the&#xA;  maximum allowed, patients will not be considered eligible regardless of findings on&#xA;  any supplementary imaging&lt;/li&gt;&#xA;  &lt;li&gt;Patient must have a chest x-ray done within 14 days prior to randomization; patient&#xA;  must have a CT or MRI scan of the chest done within 90 days prior to surgical&#xA;  resection if at least one of the following was undertaken:&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;A complete mediastinal lymph node resection; or&lt;/li&gt;&#xA;  &lt;li&gt;Biopsy of all desired levels of lymph nodes - as specified above; or&lt;/li&gt;&#xA;  &lt;li&gt;A pre-surgical PET scan within 60 days prior to surgical resection If none of&#xA;  the above was undertaken then the CT or MRI scan of the chest must have been&#xA;  performed within 60 days prior to surgical resection Note: a pre-surgical PET&#xA;  scan is not mandatory&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Patient must have an electrocardiogram (EKG) done within 14 days prior to&#xA;  randomization&lt;/li&gt;&#xA;  &lt;li&gt;Women of childbearing age and men must agree to use adequate contraception (hormonal&#xA;  or barrier method of birth control) prior to study entry and while taking study&#xA;  medication and for a period of three months after final dose; should a woman become&#xA;  pregnant or suspect she is pregnant while she or her male partner are participating&#xA;  in this study, she should inform her treating physician immediately&lt;/li&gt;&#xA;  &lt;li&gt;Patients may receive post-operative radiation therapy; patients must have completed&#xA;  radiation at least 3 weeks prior to randomization and have recovered from all&#xA;  radiation-induced toxicity; patients who have received radiation therapy should also&#xA;  be randomized within 16 weeks of surgery&lt;/li&gt;&#xA;  &lt;li&gt;Patient consent must be obtained according to local institutional and/or University&#xA;  Human Experimentation Committee requirements; it will be the responsibility of the&#xA;  local participating investigators to obtain the necessary local clearance, and to&#xA;  indicate in writing to either the National Cancer Institute of Canada (NCIC) Clinical&#xA;  Trials Group (CTG) study coordinator (for NCIC CTG centers) or the Cancer Trials&#xA;  Support Unit (CTSU) (for all other investigators), that such clearance has been&#xA;  obtained, before the trial can commence in that center; a standard consent form for&#xA;  the trial will not be provided, but a sample form is given; this sample consent form&#xA;  has been approved by the National Cancer Institute (NCI) Central Institutional Review&#xA;  Board (IRB) and must be used unaltered by those CTSI centers which operate under CIRB&#xA;  authority; for NCIC CTG centers, a copy of the initial full board Research Ethics&#xA;  Board (REB) approval and approved consent form must be sent to the NCIC CTG central&#xA;  office; please note that the consent form for this study must contain a statement&#xA;  which gives permission for the government agencies, NCI, NCIC CTG and monitoring&#xA;  agencies to review patient records&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;NCIC-CTG Centers: the patient must have the ability to understand and the&#xA;  willingness to sign a written informed consent document; the patient must sign&#xA;  the consent form prior to randomization&lt;/li&gt;&#xA;  &lt;li&gt;CTSU Centers: the patient, or in the case of a mentally incompetent patient his&#xA;  or her legally authorized and qualified representative, must have the ability to&#xA;  understand and the willingness to sign a written informed consent document; the&#xA;  consent form must be signed prior to randomization&lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;li&gt;Patients must be accessible for treatment and follow-up; investigators must assure&#xA;  themselves that patients registered on this trial will be available for complete&#xA;  documentation of the treatment administered, toxicity and follow-up&lt;/li&gt;&#xA;  &lt;li&gt;Initiation of protocol treatment must begin within 10 working days of patient&#xA;  randomization&lt;/li&gt;&#xA;  &lt;li&gt;Patients may have received post-operative adjuvant platinum-based chemotherapy;&#xA;  patients must have completed chemotherapy at least 3 weeks prior to randomization and&#xA;  have recovered from all chemotherapy-induced toxicity; patients who have received&#xA;  adjuvant chemotherapy should also be randomized within 26 weeks of surgery&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;Exclusion Criteria:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;Prior or concurrent malignancies; patients who have had a previous diagnosis of&#xA;  cancer, if they remain free of recurrence and metastases five years or more following&#xA;  the end of treatment and, in the opinion of the treating physician do not have a&#xA;  substantial risk of recurrence of the prior malignancy, are eligible for the study;&#xA;  patients who have been adequately treated for non-melanomatous skin cancer or&#xA;  carcinoma in situ of the cervix are eligible irrespective of when that treatment was&#xA;  given&lt;/li&gt;&#xA;  &lt;li&gt;A combination of small cell and non-small cell carcinomas or a pulmonary carcinoid&#xA;  tumor&lt;/li&gt;&#xA;  &lt;li&gt;More than one discrete area of apparent primary cancer (even if within the same lobe,&#xA;  T4, IIIB)&lt;/li&gt;&#xA;  &lt;li&gt;Clinically significant or untreated ophthalmologic (e.g. Sjogren's etc.) or&#xA;  gastrointestinal conditions (e.g. Crohn's disease, ulcerative colitis)&lt;/li&gt;&#xA;  &lt;li&gt;Any active pathological condition that would render the protocol treatment dangerous&#xA;  such as: uncontrolled congestive heart failure, angina, or arrhythmias, active&#xA;  uncontrolled infection, or others&lt;/li&gt;&#xA;  &lt;li&gt;A history of psychiatric or neurological disorder that would make the obtainment of&#xA;  informed consent problematic or that would limit compliance with study requirements&lt;/li&gt;&#xA;  &lt;li&gt;Patient, if female, is pregnant or breast-feeding&lt;/li&gt;&#xA;  &lt;li&gt;Neoadjuvant chemotherapy or immunotherapy for NSCLC; however, patients may have&#xA;  received pre-operative limited field, low dose (less than 1000 cGy) external beam&#xA;  radiation therapy or endobronchial brachytherapy or laser therapy for short term&#xA;  control of hemoptysis or lobar obstruction; full dose pre-operative radiotherapy of&#xA;  curative intent is a cause for exclusion; patients may have received post-operative&#xA;  adjuvant platinum-based chemotherapy however non-platinum-based chemotherapy is a&#xA;  cause for exclusion&lt;/li&gt;&#xA;  &lt;li&gt;History of allergic reactions attributed to compounds of similar chemical or biologic&#xA;  composition to the agents used on this trial; patients with ongoing use of phenytoin,&#xA;  carbamazepine, barbiturates, rifampicin, or St John's Wort are excluded&lt;/li&gt;&#xA;  &lt;li&gt;Incomplete healing from previous oncologic or other major surgery&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I want to find documents with a query as below.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;'Patients with Eastern cooperative Oncology Group (ECOG) performance status &gt; 2' will be excluded&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Output &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CTA: NCT01572038 &#xA;Labels: ECOG 0, ECOG 1, ECOG 2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the simplest way to approach this problem?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a problem I would like to solve using machine learning. I would like to use some sort of classification to know if a just added change in a tree data structure is &quot;good&quot; or is &quot;bad&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say I have this tree:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        (A) &#xA;        / \\\\&#xA;       /   \\\\&#xA;     (B)   (C)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I apply a change to it (a &quot;good&quot; change, so the algorithm should associate this change with the &quot;good&quot; changes). The updated tree would be like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;       (A)&#xA;       / \\\\&#xA;      /   \\\\&#xA;    (D)   (C)&#xA;    /&#xA;   /&#xA; (B)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Added a certain node (D) above another node (B) would be classified as a &quot;good&quot; change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So when I have the learner with the correct data, the algorithm should be able to know that if I add a node of type D above a node of type B, it is a &quot;good&quot; change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to work with XML files that keeps the tree structure, a simple classifier like a naive bayes would not work, because it wouldn't be able to recognise if a node is added above another one, it only would be able to know that a node has been added.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know how which algorithm/technique I should use and I don't know how I should pass the data to the learner, because the context in this scenario is important.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best technique/algorithm to compare trees changes?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a dataset about users purchasing product from website. The attributes I have are  user id, region(state) of the user, the categories id of product, keywords id of product, keywords id of website, sales amount spent of the product. The goal is to use the information of product and website to identity who the users are, such as &quot;male young gamer&quot;;&quot;stay at home mom&quot;. I attached a sample picture as below.&lt;img src=&quot;https://i.stack.imgur.com/S7OSa.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;There are totally 1940 unique categories and 13845 unique keywords for products. For the website, there are 13063 unique keywords. The whole dataset is huge as that's the daily logging data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking of clustering, as those are unsupervised, but those id are ordered number having no numeric meaning, then I don't know how to apply the algorithm. I also think of classification if I add a column of class based on the sales amount of product purchased. I think clustering is more preferred. I don't know what algorithm I should use for this case as the dimensions of the keywords id could be more than 10000 (each product could have many keywords, so does website). I need to use Spark for this project. Can anyone help me out with some ideas,suggestions? Thank you so much!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm looking for a spatial index that can efficiently find the most extreme n points in a certain direction, i.e. for a given w, find x[0:n] in the dataset where x0 gives the largest value of w.x and x1 the second largest value of w.x, etc... . Is there a name for this type of query? What would be an efficient data structure to use? x might have around 20 dimensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thankyou!&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am interested in modeling startup companies failure and success rates to describe what is the representative startup. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 40 companies in a dataset. Each company is represented as a list of all the investment financing rounds it has gone through. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Company 1: Seed Round, Series A Round&#xA;Company 2: Seed Round, Series A Round, Series B Round&#xA;Company 3: Seed Round, Series A Round, Series B Round&#xA;Company 4: Seed Round, Series A Round, Series B Round, Series C Round&#xA;Company 5: Seed Round&#xA;Company 6: Series A Round, Series B Round&#xA;Company 6: Series A Round&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can see that each round can be represented as a state in a Markov Chain, and transitions are only allowed from earlier stages to later stages. I can go from Seed to Series A, and from Series A to Series B, but not from Series B to Series A.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we have N-order markov chains (production data has N &amp;lt;= 4).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output I'm looking for is a binary tree chart showing each stage as a node and each node can either transition to the next node or to a final state meaning the company has failed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This problem can also be seen as a real options model...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas on how to implement this model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can code in Python or Ruby (but I am no expert).&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have completely followed the machine learning course on &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot; rel=&quot;nofollow&quot;&gt;coursera Machine Learning by professor Andrew Ng&lt;/a&gt; &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to put my knowledge to action. Some ideas that I have include : &lt;/p&gt;&#xA;&#xA;&lt;p&gt;-Voice synthesis&#xA; -Voice recognition&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But since the course did not focus specifically on application of machine learning in these domains, could some one point me to some other course or books that can get me started.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am looking for a practical guide/tutorial preferably in R to show how to do gerrymandering. (I was looking for it also in CRAN but didn't find such package) Gerrymandering is the manipulation of the boundaries of electorial districts in order to gain political advantage for one party. If there is an analogy/similar process in another area it would be also interesting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have detailed historical  election results. I can see the results from each &quot;voting office&quot; and also the geographical area they cover. These results are aggregated to electorial district level. I'd like to see how moving voting offices to different electorial districts can influence the results of the election and how this process can be optimized for one party. Certainly there should be some constraints not to create really weird shaped districts at the end.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I went through &lt;a href=&quot;http://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared&quot; rel=&quot;nofollow&quot;&gt;this comparison of analytic disciplines&lt;/a&gt; and &lt;a href=&quot;http://www.win-vector.com/blog/2010/10/a-personal-perspective-on-machine-learning/&quot; rel=&quot;nofollow&quot;&gt;this perspective of machine learning&lt;/a&gt;, but I am not finding any answers on the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How is Data Science related to Machine learning? &lt;/li&gt;&#xA;&lt;li&gt;How is it &lt;strong&gt;not&lt;/strong&gt; related to Machine Learning? &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What's a good book to start learning Artificial Intelligence? &#xA;What field to learn first? What are the prerequisites? &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Training a basic multilayer perceptron neural network boils down to minimizing some kind of error function. Often the sum of squared errors is chosen as a this error function, but where does this function come from?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I always thought this function was chosen because it makes sense intuitively. However, recently I learned that this is only partly true and there is more behind it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bishop wrote in one of his &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/downloads/bishop%20-%20nna%20-%20rsi94.pdf&quot; rel=&quot;nofollow&quot;&gt;papers&lt;/a&gt; that the sum of squared errors function can be derived from the principle of &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;maximum likelihood&lt;/a&gt;. Furthermore he wrote that the squared error therefore makes the assumption that the noise on the target value has a Gaussian distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not sure what he means with that. How does the sum of squared errors relate to the maximum likelihood principle in the context of neural networks?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;All distributions in the &lt;code&gt;gbm&lt;/code&gt; package in R are associated with a loss function. For example, when we set &lt;code&gt;distribution = 'binomial'&lt;/code&gt;, the loss function chosen internally is the logistic loss function. Can anyone explain how multi-class classification works with &lt;code&gt;gbm&lt;/code&gt; and the loss function that is being used for it i.e. when we set &lt;code&gt;distribution='multinomial'&lt;/code&gt;? Is it using one-vs-all or all-vs-all under the hood for doing its multi-class classification? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have customer data since 2013 and there is a file which has the customer unique id, a timestamp, and the reason for the call (a drop down from the person who handled the call).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some cumulative counts based on customer ID and the timestamp and I saw that one customer called in over 1000 times alone. What's the best way to make sense of the call driver data when I'm looking at millions of rows and around 200 categories of call types?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a broader topic which looks into 'downstream'  issues or predicting the probability of future calls or events?  The end goal would be to visualize these calling patterns and focus on reducing the call backs. This is a specific problem but it seems like it should be common and I can learn about addressing it in a bigger picture manner. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am working on a  classification problem. I have 1000+ features in this dataset. I don't know how to select the right variables/ features that can actually contribute to predicting the output. What are the different methods through which I can identify the important variables that can be used out of these 1000+ variables.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;While I was studying, few years ago , one of the most interesting topic was evolution, genetic algorithms and neural networks. Many of the problems I faced could be solved by using that knowledge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume that since that time world has found more interesting algorithms, do you recommend some books worthy of reading in that domain ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mainly I am looking a way to find patterns in huge amount of data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ex. Having energy consumption for few years for one building.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lets have an algorithm that is trying to find all possible repeatings in many variations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously at the beginning it should find that at the weekend energy consumption is less then average between Mon and Fri, but is that possible that an algorithm would tell me sth like this ? :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every Friday at third week in even months user sleeps for 3 hours and in uneven months 5 hours ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or the algorithm finds it self that user like to save energy so if previous month he sees that he spent more then usual , next month he is trying to spent less, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;or lets assume that user eats breakfast at work but if he eats it at home then he will stay whole day at home, then lets assume that he has off day, then check energy usage after breakfast if is high, which means user is preparing to leave or is small which means that he wants to sleep and basically stays home. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I was wondering if this possible to auto detect this kind of patterns ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am enthusiast of c# and interested in R.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm using &lt;a href=&quot;https://www.npmjs.com/package/brain&quot; rel=&quot;nofollow noreferrer&quot;&gt;Brain&lt;/a&gt; to train a neural network on a feature set that includes both positive and negative values. But Brain requires inputs that are between 0 and 1. What's the best way to normalize my data?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Consider a talent pool in which each member has some set of skills. Some of these talent are submitted to orders as potential candidates of which one is selected. It is reasonable to assume that the submitted talent have some dominant thing in common in their skill sets (let's call it a segment) that qualifies them for the order. Example segments are &quot;front end web-designer&quot; or &quot;brochure / sprint designer&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the total set of skills over all of the talent submitted to an order (like 2-5 with say 10 skills each, so 20 - 50 skills total), I am looking for the dominant segment. Then, I am looking for the dominant segment for each individual talent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My plan is to use latent Dirichlet allocation (LDA) such that the skills of all the talent submitted for an order are a &quot;document&quot; that contains some segments or &quot;topics&quot; with some probability. Likely, there will be one or two dominant topics depending on the total topic number. I will then use this model to predict the dominant segment for each talent where the individual talent skill set is a &quot;document&quot; with some segments or &quot;topics&quot; within.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am curious if anyone has feedback about my use of LDA or other ideas about how I might go about discovering these segments?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am looking for some hints on how to curate a list of stopwords. Does someone know / can someone recommend a good method to extract stopword lists from the dataset itself for preprocessing and filtering?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Data:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;a huge amount of human text input of variable length (searchterms and whole sentences (up to 200 characters) ) over several years. The text contains a lot of spam (like machine input from bots, single words, stupid searches, product searches ... ) and only a few % of seems to be useful. I realised that sometimes (only very rarely) people search my side by asking really cool questions. These questions are so cool, that i think it is worth to have a deeper look into them to see how people search over time and what topics people have been interested in using my website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My problem:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;is that i am really struggling with the preprocessing (i.e. dropping the spam). I already tried some stopword list from the web (NLTK etc.), but these don't really help my needs regarding this dataset. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your ideas and discussion folks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How important is linear algebra to being a data scientist? Are we talking college postgraduate level?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have data of the form :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Id1      A_Id2      B_Id2       C_Id2       D_Id2      E_Id2      F_Id2&#xA;1         6           3           9           23         20         5&#xA;1         4           7           8           9          11         56                                                   &#xA;1         2           36          98          73         2          4     &#xA;1         9           5           2           7          32         24           &#xA;1         14          7           5           9          12         5                                                   &#xA;2         34          4           7           10          7         12                                                       &#xA;2         5           57          23          91          4         6                                                    &#xA;2         7           .           .           .           .         .&#xA;2         3           .           .           .           .         .&#xA;2         .           .           .           .           .         .&#xA;.&#xA;.&#xA;100      .            .           .           .           .         .            &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Basically, I want to build a model such that I can get a best match of Id1, given top 5 Id2 matches of each attribute(A_Id2, B_Id2,...,F_Id2). Now every match should be computed keeping in mind A has highest priority, followed by B and C, followed by D and least priority to E and F. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the output will look like this :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Id1    Match_Id2&#xA;1        9&#xA;2        7&#xA;3        .&#xA;4        .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I hope the problem is clear, if not please ask.&#xA;How should I go about building a Machine Learning model for this? I was wondering if ranking algorithm will help ?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am not sure if &quot;minimize correlation&quot; is the right title for this issue but I could not find a better sentence to describe what I would like to achieve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say that I have a black box with multiple inputs and a single output. I know one of the inputs and the output and I have multiple example recordings of both. This known input modifies the output in a way that it is not desired, therefore, I would like to get rid of this &quot;noise&quot; caused by the known input. The transfer function for this input can be safely assumed as linear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I am doing right now, it is to loop through the example recordings, creating a linear regression model to predict the unwanted outcome and subtracting it from the real measured output signal, for each example. Afterwards, I compute the average of all the fixed output signals to reveal meaningful data beyond noise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This strategy seems to work according to the following plot:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/oAdyb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;X axis is the known input signal, Y axis is the output signal, blue and green dots represent the averaged data before and after applying the linear regression algorithm, respectively. Lines are the best fit for each data set. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see that the green line (&quot;cleaned&quot; dataset) has the smallest slope, meaning that the output variable is considerably less linearly correlated with the input than it was previously. Therefore, I assume that the regression technique explained before is working as expected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question, looking at the plot, is there any mathematical procedure to directly &quot;project&quot; the original dataset in a way that the correlation between the input and output variables is minimized? Is there any math trick to avoid the use of the regression technique on all the example datasets to obtain a similar result? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My written expression is not the best so please feel free to comment the question if you need further explanations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any code is welcomed but python (pandas, numpy, etc.) and Matlab are preferred. Theoretical explanations are also very welcomed. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;How do you assess the quality of your data? In data scientists' world, we come across several data. We often crunch numbers without formally assessing its quality due to various reasons. One such reason is we need to meet deadlines for reports &amp;amp; publications. I am wondering if anyone has adopted or come across a method/guidelines that help to find issues within the data (time-saving tips), so we can analyze the data efficiently. Please share your experience, tips, etc. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm using Google Analytics on my mobile app to see how different users use the app. I draw a path based on the pages they move to. Given a list of paths for say a 100 users, how do I go about clustering the users. Which algorithm to use? By the way, I'm thinking of using sckit learn package for the implementation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My dataset (in csv) would look like this :  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DeviceID,Pageid,Time_spent_on_Page,Transition.&amp;lt;br&amp;gt; &#xA;ABC,Page1, 3s, 1-&amp;gt;2.&amp;lt;br&amp;gt;&#xA;ABC,Page2, 2s, 2-&amp;gt;4.&amp;lt;br&amp;gt;&#xA;ABC,Page4,1s,4-&amp;gt;1.&amp;lt;br&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the path, here is 1-&gt;2-&gt;4-&gt;1, where 1,2,4 are Pageids.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;While running rattle in my system I am getting this error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rattle()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error: attempt to apply non-function&#xA;In addition: Warning message:&#xA;In method(obj, ...) : Unknown internal child: selection&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using R version 3.1.0 (2014-04-10)&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am searching for pointers to algorithms for feature detection. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: all the answers helped me a lot, I cannot decide which one I should accept. THX guys!&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What I did:&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;For discrete variables (i.e. $D_i, E$ are finite sets) $X_i : \\\\Omega \\\\to D_i$ and a given data table &#xA;$$ &#xA;\\\\begin{pmatrix}{}&#xA;  X_1 &amp;amp; ... &amp;amp; X_n &amp;amp; X_{n+1} \\\\\\\\&#xA;  x_1^{(1)} &amp;amp; ... &amp;amp; x_n^{(1)} &amp;amp; x_{n+1}^{(1)} \\\\\\\\&#xA;  ... \\\\\\\\&#xA;  x_1^{(m)} &amp;amp; ... &amp;amp; x_n^{(m)} &amp;amp; x_{n+1}^{(m)} \\\\\\\\&#xA;\\\\end{pmatrix}&#xA;$$&#xA;(the last variable will be the 'outcome', thats why I stress it with a special index) and $X, Y$ being some of the $X_1, ..., X_{n+1}$ (so if $X=X_a, Y=X_b$ then $D=D_a, E=D_b$) compute&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$H(X) = - \\\\sum_{d \\\\in D} P[X=d] * log(P[X=d])$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$H(Y|X) = - \\\\sum_{d \\\\in D} {&#xA;             P[X=d] * \\\\sum_{e \\\\in E} {&#xA;               P[Y=e|X=d] * log(P[Y=e|X=d])&#xA;             }&#xA;           }$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where we estimate&#xA;  $$P[X_a=d] = |\\\\{j \\\\in \\\\{1, ..., m\\\\} : x_a^{(j)} = d\\\\}|$$&#xA;and analogously&#xA;  $$P[X_a=d \\\\cap X_b=e] = |\\\\{j \\\\in \\\\{1, ..., m\\\\} : x_a^{(j)} = d ~\\\\text{and}~ x_b^{(j)}=e\\\\}|$$&#xA;and then&#xA;  $$I(Y;X) = \\\\frac{H(Y) - H(Y|X)}{\\\\text{log}(\\\\text{min}(|D|, |E|))}$$&#xA; which is to be interpreted as the influence of $Y$ on $X$ (or vice versa, its symmetric).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: A little late now but still:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is wrong:&#xA;&lt;s&gt;Exercise for you: show that if $X=Y$ then $I(X,Y)=1$.&lt;/s&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is correct:&#xA;Exercise for you: show that if $X=Y$ then $I(X,X)=H(X)/log(|D|)$ and if $X$ is additionally equally distributed then $I(X,X)=1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For selecting features start with the available set $\\\\{X_1, ..., X_n\\\\}$ and a set 'already selected'$ = ()$ [this is an ordered list!]. We select them step by step, always taking the one that maximizes   $$\\\\text{goodness}(X) = I(X, X_{n+1}) - \\\\beta \\\\sum_{X_i ~\\\\text{already selected}} I(X, X_i)$$ for a value $\\\\beta$ to be determined (authors suggest $\\\\beta = 0.5$). I.e. goodness = influence on outcome - redundancy introduced by selecting this variable. After doing this procedure, take the first 'few' of them and throw away the ones with lower rank (whatever that means, I have to play with it a little bit). This is what is described in &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.7629&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For computing the $I$ for continuous variables one needs to bin them in some way. More concretely, the inventors of 'I' suggest to take the maximal value over binning $X$ into $n_x$ bins, $Y$ into $n_y$ bins and $n_x \\\\cdot n_y &amp;lt;= m^{0.6}$, i.e.  compute&#xA;  $$ \\\\text{MIC}(X;Y) = \\\\text{max}_{n_X \\\\cdot n_Y \\\\leq m^{0.6}} \\\\left( \\\\frac{I_{n_X, n_Y}(X;Y)}{log(\\\\text{min}(n_X, n_Y)} \\\\right)$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $I_{n_X, n_Y}(X;Y)$ means: compute the $I$ precisely as you did for discrete variables by treating $X$ as a discrete random variable after binning it into $n_X$ bins and analogously with $Y$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;===&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;ORIGINAL QUESTION&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;More precisely: I have a classification problem for one boolean variable, let's call this variable &lt;code&gt;outcome&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have lots of data and lots of features (~150 or so) but these features are not totally 'meaningless' as in image prediction (where every x and y coordinate is a feature) but they are of the form gender, age, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I did until now: from these 150 features, I guessed the ones that 'seem' to have some importance for the outcome. Still, I am unsure which features to select and also how to measure their importance before starting the actual learning algorithm (that involves yet more selection like PCA and stuff). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, for a feature &lt;code&gt;f&lt;/code&gt; taking only finitely many values &lt;code&gt;x_1, ..., x_n&lt;/code&gt; my very naive approach would be to compute some relation between&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;P(outcome==TRUE | f==x_1)&lt;/code&gt;, ..., &lt;code&gt;P(outcome==TRUE | f==x_n)&lt;/code&gt; and &lt;code&gt;P(outcome==TRUE)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(i.e. the feature is important when I can deduce more information about the coutcome from it than without any knowledge about the feature).&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;em&gt;Concrete question(s): Is that a good idea? Which relation to take? What to do with continuous variables?&lt;/em&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I'm sure that I'm not the first one ever wondering about this. I've read about (parts of) algorithms that do this selection in a sort-of automated way. Can somebody point me into the right direction (references, names of algorithms to look for, ...)?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a multi year dataset. Each time frame  of the data has different predictor importance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say for example, I am slicing the data into two partions as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;a dataset for the year 2014 (whole year)&lt;/li&gt;&#xA;&lt;li&gt;a 2015 Jan. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;When i look for the predictor importance, the predictor variables are different for both the partions. (1) Hence i am not able to arrive at one  unique decision tree which can explain the model better. (2). I am not able to train a model which can predict the new data correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there anything I am going wrong here.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I tried finding about exception lists in wordnet lemmatizers. &quot;Morphy() uses inflectional ending rules and exception lists to handle different possibilities&quot; which I read from &lt;a href=&quot;http://www.nltk.org/howto/wordnet.html&quot; rel=&quot;nofollow&quot;&gt;http://www.nltk.org/howto/wordnet.html&lt;/a&gt; . Can you explain what is an exception list. Thank you.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to connect to hive from java but getting error. I searched in google but not got any helpfull solution. I have added all jars also.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is:-&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package mypackage;&#xA;import java.sql.SQLException;&#xA;import java.sql.Connection;&#xA;import java.sql.ResultSet;&#xA;import java.sql.Statement;&#xA;import java.sql.DriverManager;&#xA;&#xA;&#xA;public class HiveJdbcClient {&#xA;&#xA;&#xA;private static String driver = &quot;org.apache.hadoop.hive.jdbc.HiveDriver&quot;;&#xA;&#xA;public static void main(String[] args) throws SQLException, &#xA;       ClassNotFoundException {&#xA; Class.forName(&quot;org.apache.hadoop.hive.jdbc.HiveDriver&quot;);&#xA; try {&#xA; Class.forName(driver);&#xA; } catch (ClassNotFoundException e) {&#xA; e.printStackTrace();&#xA;  System.exit(1);&#xA;}&#xA;&#xA;Connection connect = DriverManager.getConnection(&quot;jdbc:hive://master:10000 /default&quot;, &quot;&quot;, &quot;&quot;);&#xA;Statement state = connect.createStatement();&#xA;String tableName = &quot;mytable&quot;;&#xA;state.executeQuery(&quot;drop table &quot; + tableName);&#xA;ResultSet res=state.executeQuery(&quot;ADD JAR /home/hadoop_home/hive/lib /hive-serdes-1.0-SNAPSHOT.jar&quot;);&#xA;res = state.executeQuery(&quot;create table tweets (id BIGINT,created_at     STRING,source STRING,favorited BOOLEAN,retweet_count INT,retweeted_status STRUCT&amp;lt;text:STRING,user:STRUCT&amp;lt;screen_name:STRING,name:STRING&amp;gt;&amp;gt;,entities STRUCT&amp;lt;urls:ARRAY&amp;lt;STRUCT&amp;lt;expanded_url:STRING&amp;gt;&amp;gt;,user_mentions:ARRAY&amp;lt;STRUCT&amp;lt;screen_name:STRING,name:STRING&amp;gt;&amp;gt;,hashtags:ARRAY&amp;lt;STRUCT&amp;lt;text:STRING&amp;gt;&amp;gt;&amp;gt;,text STRING,user  STRUCT&amp;lt;screen_name:STRING,name:STRING,friends_count:INT,followers_count:INT,statuses_count:INT,verified:BOOLEAN,utc_offset:INT,time_zone:STRING&amp;gt;,in_reply_to_screen_name STRING) ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe' LOCATION '/user/flume/tweets'&quot;);&#xA;&#xA;&#xA;String show = &quot;show tables&quot;;&#xA;System.out.println(&quot;Running show&quot;);&#xA;res = state.executeQuery(show);&#xA;if (res.next()) {&#xA;  System.out.println(res.getString(1));&#xA;}&#xA;&#xA;&#xA;String describe = &quot;describe &quot; + tableName;&#xA;System.out.println(&quot;Running describe&quot;);&#xA;res = state.executeQuery(describe);&#xA;while (res.next()) {&#xA;  System.out.println(res.getString(1) + &quot;\\\\t&quot; + res.getString(2));&#xA;}&#xA;&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting these errors:-&lt;/p&gt;&#xA;&#xA;&lt;p&gt;run:&#xA;SLF4J: Class path contains multiple SLF4J bindings.&#xA;SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: Found binding in [jar:file:/home/hadoop/lib/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: Found binding in [jar:file:/home/GlassFish_Server/glassfish/modules/weld-osgi-bundle.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: See &lt;a href=&quot;http://www.slf4j.org/codes.html#multiple_bindings&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.slf4j.org/codes.html#multiple_bindings&lt;/a&gt; for an explanation.&#xA;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/io/Writable&#xA;    at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:198)&#xA;    at org.apache.hadoop.hive.jdbc.HiveStatement.execute(HiveStatement.java:132)&#xA;    at org.apache.hadoop.hive.jdbc.HiveConnection.configureConnection(HiveConnection.java:133)&#xA;    at org.apache.hadoop.hive.jdbc.HiveConnection.(HiveConnection.java:122)&#xA;    at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:106)&#xA;    at java.sql.DriverManager.getConnection(DriverManager.java:571)&#xA;    at java.sql.DriverManager.getConnection(DriverManager.java:215)&#xA;    at dp.HiveJdbcClient.main(HiveJdbcClient.java:35)&#xA;Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Writable&#xA;    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)&#xA;\\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)&#xA;    at java.security.AccessController.doPrivileged(Native Method)&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)&#xA;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)&#xA;    ... 8 more&#xA;Java Result: 1&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a dataset which contains information about when do people enter and leave a premise. I have the following information in the dataset : &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Person Id&lt;/li&gt;&#xA;&lt;li&gt;Time of Entry&lt;/li&gt;&#xA;&lt;li&gt;Time of Leaving&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The dataset has around 50 unique persons. Each person will have multiple entries corresponding to multiple visits. The data spans over a year so I have quite a lot of entries (around 1 million).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These people can be classified on the basis of the department they work under (2 departments - mutually exclusive) or on basis of  their role (4 possible roles - all mutually exclusive)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering what kind of data analysis can be done with this kind of dataset. I am not looking for straight-forward things like &quot;who spent the most time in building&quot;. However things like finding correlation between visits of 2 people would be interesting. So if person A visits the premise, what is the probability that person B would also visit. Since I have only around 50 unique visitors, I think such an analysis is feasible. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another line of thought was to apply some interval-pattern mining techniques but I am not much familiar with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone give me some pointers/ideas about what kind of data products can be build using this or what kind of techniques can be used with such data.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit : As discussed in comments, I call it a product in the sense that I do not want some simple or trivial analysis. And I am not looking for any commercially viable idea - just some cool fun idea :)&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am doing load forecasting using SVR(kernel='rbf').How can I understand which is the best value for parameters C, epsilon and gamma?Thanks.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am currently selecting features of products by using LDA to group 6000 keywords of product into topics. &#xA;Here is the sample of my dataset after being organized into list of keywords for each product id.&#xA;&lt;img src=&quot;https://i.stack.imgur.com/H6Fhh.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;I consider each id as a &quot;document&quot; and each keywords as the &quot;word&quot; in a &quot;document&quot; for the case of LDA.&#xA;It didn't work out as I expected as each topic have many identical keywords with different weight. I removed 100 most common keywords but there are still some identical keywords in the topics. Here is the sample output:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/kOR8W.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I deal with the identical keywords in my topics? and also how to deal with the 100 most common keywords I removed?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the graph showing the frequency of the words in all &quot;documents&quot;&#xA;&lt;img src=&quot;https://i.stack.imgur.com/gbMUC.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;Each word only presents once in each document, but may present in different documents. I updated a new graph of the frequency of the words&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you so much. Any suggestion is appreciated.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Problem:&#xA;Lets say we have an irreducible Markov chain.&#xA;Given a failure state or non desirable state F and a current state S&#xA;Is it possible to find how far we are from the failure state F.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: if we try to model an engine failure using Markov process and using historic data we have a state transition matrix P, one of which is a state where the engine was failed .&#xA;Now , in a dynamic system, if the current state S and a state transition matrix are given, can we calculate how far we are from failure state F.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;x(n+1) = (x)P^n&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right hand side of the equation gives us a vector v which consists of probabilities of transitioning to different states at stage n.&#xA;With different value of n, the values in this vector v will change&#xA;one of the values in v will be the probability of going to state F.&#xA;I want to find for what n , transitioning probability to F is maximum.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have no idea whether this is the right StackExchange flavor to post this question in, but here goes:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an agent (an IRC bot that listens to an event stream) that I would like to add &quot;intelligent notifications&quot; to. Let me give a specific example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bot monitors the results from our continuous integration system, so it basically sees a stream of test results associated with changes (and the changes are associated with users.) If I ask it to notify me of interesting events, then for each event it sees, it decides whether to tell me about it (or rather, about the current results so far.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like it to decide based on how interesting something is for me. Something is interesting if (1) it is a result of a change that I pushed to the continuous integration system, (2) it is a test failure, and (3) the information conveyed by that failure is a significant indicator of whether my change was bad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tests have a baseline probability of failing for no good reason (&quot;intermittent failures&quot;). If one test fails, it does not necessarily mean my change was bad. It might just be a flaky test. (Any given push tends to result in a couple bogus failures, so this isn't some obscure edge case.) But the baseline probability of an intermittent failure varies according to the test suite running (we have several dozen different test suites that fire off for every change pushed.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I don't want to be bothered with reports of failing tests that are probably just noise. And if my push &lt;em&gt;is&lt;/em&gt; bad, I don't want to be flooded with notifications for every failed test. (If I break something, it'll probably show up in multiple test suites. So if the agent sees two failures and 20 successes, maybe it won't bother me, but if it then sees another failure or two it should.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am imagining I could look at the change between the prior and posterior probabilities and use that to estimate the entropy of that new test result, and perhaps also compute my personal entropy (if that makes any sense -- as in, the bot should assume I don't know what any of the results are until it notifies me, at which point it should assume I am aware of the current full set of results and not bother me again until enough additional results have come in to substantially change the probability estimate of my change being bad.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or something like that. I'm really looking for the right mathematical framework to compute things like this. I know next to nothing about machine learning or... well, mathematics in general.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(The above is simplified; I would additionally like to do crazy things like take into account &quot;labeling&quot;, where a third party looks at the test failures and decides whether they are real failures or not. But the time between the failure coming in and when that person labels it for me can also be modeled by a distribution, and I'd like to hold off notifications for interesting results if there's a good chance that this other person may tell me that the result is not interesting after all. But only if that's going to happen &quot;soon&quot;. Also, we have an automated system for guessing whether something &lt;em&gt;might&lt;/em&gt; be an intermittent vs real failure, and it'd be nice to take its opinion into account as well.)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Suppose I have a set of data(with  2 diemensional feature space), and I want to obtain clusters from them. But I do not know how many clusters will be formed.&lt;br&gt;&lt;br&gt; Yet I want separate clusters(The number of clusters is more than 2). &lt;br&gt;&lt;br&gt;I figured that k means of k medoid cannot be used in this case. Nor can I use hierarchical clustering. Also since there is no training set hence cannot use Knn classifier to any others(supervised learning cannot be used as no training set). I cannot use OPTICS algorithm as I do not want to specify the radius(I dont know the radius)&lt;br&gt;&lt;br&gt; Is there any machine learning technique that would give me multiple clusters(distance based clustering) that deals well with outlier points too? &lt;img src=&quot;https://i.stack.imgur.com/dAimG.png&quot; alt=&quot;enter image description here&quot;&gt;&#xA;This should be the output&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Each observation in my data was collected with a difference of 0.1 seconds. I don't call it a time series because it don't have a date and time stamp. In the examples of clustering algorithms (I found online) and PCA the sample data have 1 observation per case and are not timed. But my data have hundreds of observations collected every 0.1 seconds per vehicle and there are many vehicles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note: I have asked this question on quora as well.&lt;/em&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;How widely is Theano used in deep learning research? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is Theano a good start to learn the implementation of machine learning algorithms?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Will learning the implementation of something like a feed forward network really help? Do graduate students implement neural networks or other algorithms at least once during their college days?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a reasonable idea about feed forward and recurrent networks, backpropagation, the general pipeline for a machine learning problem and the necessary mathematics. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I need to do some experimenting with Brown clustering, graph partitioning, agglomerative clustering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Are there Python/Matlab libraries for that? I know sklearn.cluster but it doesn't have algorithms I need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is it possible to install graphical interface for Cluto on Mac OS? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) Overall, are there useful tutorials on using Cluto?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) Other software for clustering that I could learn within a couple of hours?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to paste a &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;int&lt;/code&gt; from &lt;code&gt;map&lt;/code&gt; in &lt;code&gt;Hive&lt;/code&gt; to an &lt;code&gt;array&lt;/code&gt;.&#xA;For now, record looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;string1&quot;:1,&quot;string2&quot;:1,&quot;string3&quot;:15}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a way to convert it to an array like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[&quot;string1:1&quot;,&quot;string2:1&quot;,&quot;string3:15&quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " ('&lt;p&gt;We are developing a classification system, where the categories are fixed, but many of them are inter-related. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, we have a category called, &quot;&lt;code&gt;roads&lt;/code&gt;&quot; and another one called &quot;&lt;code&gt;traffic&lt;/code&gt;&quot;. We believe that the model will be confused by the text samples, which could be in &lt;code&gt;roads&lt;/code&gt; category and also in &lt;code&gt;traffic&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of our text samples are suitable for multi class labelling too.  For example, &quot;There is a garbage dump near the footpath. The footpath is broken completely&quot;. This text could be categorized into &lt;code&gt;garbage&lt;/code&gt; bucket or &lt;code&gt;footpath&lt;/code&gt; bucket. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are going to build a training set for this classifier, by manually annotating the text. So, can we put multiple labels for one issue? How should we deal with  text with multiple labels for it? Should they be added into all categories to which it is tagged to, as training sample ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, &quot;There is a garbage dump near the footpath. The footpath is broken completely&quot;. This text could be categorized into &lt;code&gt;garbage&lt;/code&gt; bucket or &lt;code&gt;footpath&lt;/code&gt; bucket. So, should this text be added as a training sample for &lt;code&gt;garbage&lt;/code&gt; and &lt;code&gt;footpath&lt;/code&gt;? How should we consider the labels?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you please give your insights?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am solving a rare event cum classification problem. I have come across  a package called &lt;strong&gt;SMOTEBoost&lt;/strong&gt; which oversamples the rare event and boost the results. But I'm not sure is that supported in R. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you please help me how can I use SMOTEBoost in R? Any examples?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I want to learn how a spam email detector is done. I'm not trying to build a commercial product, it'll be a serious learning exercise for me. Therefore, I'm looking for resources, such as existing projects, source code, articles, papers etc that I can follow. I want to learn by examples, I don't think I am good enough to do it from scratch. Ideally, I'd like to get my hand dirty in Bayesian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there anything like that that? Programming language isn't a problem for me.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Given some dataset for prediction, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;for eg say I have different housing price prediction dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  1 : 100 training and 100 testing sample, 50 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  2 : 100 training and 100 testing sample, 120 feature &lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  3 : 1000 training and 1000 testing sample, 50 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dataset  4 : 1000 training and 1000 testing sample, 5000 feature&lt;/p&gt;&#xA;&#xA;&lt;p&gt;how should I choose the best methods for estimating the unknown parameters ( predict price) in a linear regression model from the following for each of these dataset?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Ordinary least squares&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Stepwise regression&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Principal component regression &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Partial least squares regression&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Should I experiment with each of these one by one and compare the results or is there any rule of thump on when to use each of them based on the dataset ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;sorry if this question is out of place. I'm a begginer to machine learning, and I have use for a technique, and I don't even know where to look. The problem is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I have 5 features which are real valued (Parameters in a deterministic simulation). &lt;/li&gt;&#xA;&lt;li&gt;This features determine two aspects of the instance (model solution). Its feasibility (binary) and some measure of likelihood given certain experimental data (only for the instances that achieved feasibility). &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Since I want to avoid generating &quot;infeasible&quot; combination of features, what I devised was an algorithm that iteratively does the following:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generate Nc candidate feature vectors&lt;/li&gt;&#xA;&lt;li&gt;Evaluate Feasibility and Likelihood for each&lt;/li&gt;&#xA;&lt;li&gt;Find linear combination of features that involves a compromise between least amount of features / holds largest cluster of feasibility. Add this as constraints to the feature vector generation. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In short, it detects and iteratively refines &quot;simple&quot; constraints that once added to the feature vector generation &quot;guarantee&quot; its feasibility to save computational time evaluating combination of parameters that lead to infeasible models. Afterwards, they could be tested by inverting them and looking for other &quot;regions&quot; (if any) of the feature vector where the model is feasible.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any name of techniques and references I might look for ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am trying to use a Random Forest Model (Regression Type) as a substitute of logistic regression model. I am using R - randomForest Package. I want to understand the meaning of Importance of Variables (%IncMSE and IncNodePurity) by example. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I have a population of 100 employees out of which 30 left the company. &#xA;Suppose in a particular decision tree, population is split by an attribute (say location) into two nodes. One node contains 50 employees out of which 10 left the company and other contains 50 employees from which 20 left the company. Can someone demonstrate me a calculation of %IncMSE and IncNodePurity. (if Required for averages etc., please consider another decision tree)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may look like a repeated question but I could not find a worked out example.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to determine which site in our organization is in greater need of upgrades to SEP 12, so when I run a query to count, I get these type of numbers&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Group       Windows_SEP_11  Mac_SEP_11  Windows_SEP_12  Mac_SEP_12&#xA;Arizona\\\\A   417                  29              219         6&#xA;Arizona\\\\B   380                  20              282        15&#xA;Arizona\\\\C   340                  30              383        507&#xA;Arizona\\\\D   310                  104             186        857&#xA;Arizona\\\\E   307                  74              403        243&#xA;Arizona\\\\F   285                  171             522        14&#xA;Arizona\\\\G   269                  1               559        41&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, when I find percentages, I get these numbers&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Group          Win_Sep_11_%   Mac_SEP_11_%  Windows_SEP_12_%    Mac_SEP_12_%&#xA;Boston/Site 1   100               0                0               0&#xA;Boston/Site 2   100               0                0               0&#xA;Boston/Site 3   94                0                0               5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And obviously, percentage isn't good indicator because Boston/Site 1 has only 3 computers, Boston/Site 2 only has 4 computers, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best way to analyze data? I ultimately need a visual of sites that have&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;many computers, and&lt;/li&gt;&#xA;&lt;li&gt;a great need for upgrades to SEP 12, i.e. if there are more computers with SEP 11 than SEP 12.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Please point me in the right direction.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am trying to train an artificial neural network with two convolutional layers (c1, c2) and two hidden layers (c1, c2). I am using the standard backpropagation approach. In the backward pass I calculate the error term of a layer (delta) based on the error of the previous layer, the weights of the previous layer and the gradient of the activation in respect to the activation function of the current layer. More specifically the delta of layer l looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;delta(l) = (w(l+1)' * delta(l+1)) * grad_f_a(l)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am able to compute the gradient of c2, which connects into a regular layer. I just multiply the weights of h1 with it's delta. Then I reshape that matrix into the form of the output of c2, multiply it with the gradient of the activation function and am done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I have a the delta term of c2 - Which is a 4D matrix of size (featureMapSize, featureMapSize, filterNum, patternNum). Furthermore I have the weights of c2, which are a 3D matrix of size (filterSize, filterSize, filterNum). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;With these two terms and the gradient of the activation of c1 I want to calculate the delta of c1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Long story short:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the delta term of a previous convolutional layer and the weights of that layer, how do I compute the delta term of a convolutional layer?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am having 'hour' field as my attribute, but it takes a cyclic values. How could I transform the feature to preserve the information like '23' and '0' hour are close not far. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way I could think is to do transformation: &lt;code&gt;min(h, 23-h)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Input: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]&#xA;&#xA;Output: [0 1 2 3 4 5 6 7 8 9 10 11 11 10 9 8 7 6 5 4 3 2 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any standard to handle such attributes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: I will be using superviseed learning, to train random forest classifier!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to create and train a model which classifies a new text content into finance, programming, analytics, design etc. Where can I get a relevant dataset to train my models? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;TIA. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am new to practicing NLP and most topics related, but I want to make a program that can gather and extract data for me on its own.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more specific, I want to tell the program &quot;I want more information on this topic(i.e heart attacks)&quot;, and then the program shall find, gather and extract meaningful texts on the topic from around the www.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I happen to live in Norway, which means that most interesting data will be in English, but I also want to fetch interesting data found in Norwegian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One challenge is the differences in stop words. For instance, &quot;are&quot; and &quot;and&quot; are both stop words in English and subjects in Norwegian.&#xA;Other challenges are also likely to occur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So my question is: Would I need to create separate algorithms for every natural language to be interpreted?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a large dataset with 9m JSON objects at ~300 bytes each. They are posts from a link aggregator: basically links (a URL, title and author id) and comments (text and author ID) + metadata.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They could very well be relational records in a table, except for the fact that they have one array field with IDs pointing to child records.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What implementation looks more solid?&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;JSON objects on a PostgreSQL database (just one large table with one column, namely the JSON object)&lt;/li&gt;&#xA;&lt;li&gt;JSON objects on a MongoDB&lt;/li&gt;&#xA;&lt;li&gt;Explode the JSON objects into columns and use arrays on PostgreSQL&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I want to maximize performance in joins, so I can massage the data and explore it until I find interesting analyses, at which point I think it will be better to transform the data into a form specific to each analysis.&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a data set of questions belonging to ten different categories namely (definitions, factoids, abbreviations, fill in the blanks, verbs, numerals, dates, puzzle, etymology and category relation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The categories are briefly described as follows: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Definition – A question that contains a definition of the answer. &lt;/li&gt;&#xA;&lt;li&gt;Category Relation – The answer has a semantic relation to the question where the relation is specified in the category.&lt;/li&gt;&#xA;&lt;li&gt;FITB – These are generic fill in the blank questions – some of them ask for the completion of a phrase. &lt;/li&gt;&#xA;&lt;li&gt;Abbreviation – The answer is an expansion of an abbreviation in the question. &lt;/li&gt;&#xA;&lt;li&gt;Puzzle – These require derivation or synthesis for the answer. &lt;/li&gt;&#xA;&lt;li&gt;Etymology – The answer is an English word derived from a foreign word. &lt;/li&gt;&#xA;&lt;li&gt;Verb – The answer is a verb. &lt;/li&gt;&#xA;&lt;li&gt;Number – The answer is a numeral. &lt;/li&gt;&#xA;&lt;li&gt;Date – The question asks for a date or a year. &lt;/li&gt;&#xA;&lt;li&gt;Factoid – A question is a factoid if its answer can be found on Wikipedia. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I used the Stanford core NLP package called shiftreducer to find out the Part-Of-Speech (POS) values for each question in a category. I thought of using this POS pattern as a discriminant among the classes but it turned out to be generalized since: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All the classes follow a similar pattern&lt;/li&gt;&#xA;&lt;li&gt;Nouns top the POS count followed by Determinants, Prepositions, Adjectives, Plural nouns and finally verbs.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What could be the other ways in which I could differentiate among the question categories? Or as my question was in its first place, &quot;What kind of features do I select for efficient categorization?&quot;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I want to make a 3D scatter plot of multiple data selections on a single plot (i.e same axes). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that in 2D this is possible by using par() function like so: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;plot(6:25,rnorm(20),type=&quot;b&quot;,xlim=c(1,30),ylim=c(-2.5,2.5),col=2)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;par(new=T)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;plot(rnorm(30),type=&quot;b&quot;,axes=F,col=3)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;par(new=F)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;(source: &lt;a href=&quot;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I do something like that on a 3D plot, preferably an interactive 3D plot, like the ones created using plot3D from 'rgl' package? &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am new to R Programming and just learned basics through codeschool.com&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our network spans the city, and it is divided into districts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to create a map that assigns a value (based on ratio of outdated software and new software) to each district.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This website has sample of 3-D maps that were created by R Programming, and I see one I am very interested in replicating, but for our city only. &lt;img src=&quot;https://i.stack.imgur.com/Q1HTK.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.rforscience.com/portfolio/koppen-geiger/&quot; rel=&quot;nofollow noreferrer&quot;&gt;But when I see the source code&lt;/a&gt;, I don't see any mention of latitude or longitude. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My head is spinning, trying to figure out how I will input this, i.e latitude and longitude of a district in our city, versus an assigned ratio, which I believe will be read from a spreadsheet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any guidance.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am looking into creating a model to predict whether an item is &quot;Very Good&quot;, &quot;Good&quot;, &quot;Bad&quot; or &quot;Very Bad&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After I fit the training data to the models, comparing the accuracy of the models during test stump me: should it matter if a model misclassified a G to VG while the other G to VB? What about a model that has two misclassifications of one level away versus another model with only one misclassification but three levels away (eg VG to VB)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any guideline on what is the common approach? Also, my thinking at the moment is that this should be a regression problem, but I'm happy to be corrected if I should approach this labeling of datasets more as a classification problem.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;What are the general assumptions of a Random Forest Model? I could not find by searching online. For example, in a linear regression model, limitations/assumptions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;It may not work well when there are non-linear relationship between dependent and independent variables. &lt;/li&gt;&#xA;&lt;li&gt;It may not work if the dependent variables considered in the model are linearly related. Therefore one has to remove correlated variable by some other technique. &lt;/li&gt;&#xA;&lt;li&gt;It assumes that model errors are uncorrelated and uniform (No hetroscedasticity). &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Are there any assumptions/limitations on similar lines.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I just started using machine learning and was wondering if anybody have cool ideas for a Startup project, I've seen this website &lt;a href=&quot;http://treato.com&quot; rel=&quot;nofollow&quot;&gt;Treato&lt;/a&gt;  and was amazed by it. &#xA;Thanks.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have an algorithm which have as an input about 20-25 numbers. Then in every step it uses some of these numbers with a random function to calculate the local result which will lead to the final output of A, B or C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since every step has a random function, the formula is not deterministic. This means that with the same input, I could have either A, B or C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first thought was to take step by step the algorithm and calculating mathematically the probability of each output. However, it is really difficult due to the size of the core.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My next thought was to use machine learning with supervised algorithm. I can have as many labeled entries as I want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I have the following questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How many labeled inputs should I need for a decent approach of the probabilities? Yes, I can have as many as I want, but it needs time to run the algorithm and I want to estimate the cost of the simulations to gather the labeled data.&lt;/li&gt;&#xA;&lt;li&gt;Which technique do you suggest that works with so many inputs that can give the probability of the three possible outputs?&lt;/li&gt;&#xA;&lt;li&gt;As an extra question, the algorithm run in 10 steps and there is a possibility that some of the inputs will change in one of the steps. My simple approach is to not include this option on the prediction formula, since I have to set different inputs for some of the steps. If I try the advanced methods, is there any other technique I could use?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;',),\n",
       " ('&lt;p&gt;&quot;Knowledge&quot; is crucial within several fields like Knowledge Discovery, Knowledge Distraction, Natural Language Processing, Data Mining, Big Data, etc etc etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is the definition of knowledge within these fields?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there 1 common definition, or does it depend on the exact context?&lt;/strong&gt;&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am working on a stock market decision system. I have currently centered on gradient boosting as the likely best machine learning solution for the problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have 2 fundamental issues with my data owing to it being from the stock market having to do with it not being IID. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, because of the duration of average in some indicators use, some data-points are highly correlated. For example, the 2-year trailing return of a stock is not very different if measured a month ago. My understanding is that this requires a sampling (for ensembles) where I choose datapoints that are &quot;far away&quot; in time to make trees more independent. From what I can tell so far, Matlab does not have functionality to pick a random subspace with this criteria. When I was previously thinking of using simple bagging, I figured I would just build the trees myself from custom subspaces and aggregate them into an ensemble, but this won’t work if I want to do gradient boosting. Now, on this point I am not totally sure that it is so critical to have samples “far away.” My intuition is that it is better if they are, but even if they are not perhaps by right-sizing the percent of data sampled and having enough trees it gives the same result. I would love any insight on that issue and how I might be able to use LSboost in matlab on custom samples.  (One idea I have is to just to create a small number, like 5-10 custom samples, use LS-Boost on each, and then average them.) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second fundamental problem is that data from a given stock is correlated/related to itself. I realized after thinking about it that this is of critical importance. Consider, it would likely be better, if there is enough data, to make a prediction for stock A from training data only or mostly from stock A than to use the entire market. Thus, I had been thinking of a “system” where I train on stock-specific data, stock-group data (where I use a special algorithm to group stocks), and the entire market, and then use a calculation (I can elaborate if interested) that determines which of these models is more likely to give the better result. If the input looks very different from the stock-specific training data, for example, then it will use the group or entire market. I am pretty convicted that some form of taking into account which stock the system is looking at is important to optimizing performance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, on the second issue the question is what is the best way to organize this. Thinking naively, it would be great to simply feed categories to the predictor that indicate what stock it is looking at. However, my belief here from what I know about these algorithms is that this will have poor results on new data, because this predictor will assume that it has seen the full universe of potential outcomes for each stock, when many times this isn’t the case. (Say there is a stock with only a one year history with a big rally – the system will think the rally will continue regardless of how different the new data looks). So I feel like I have to do something like in the previous paragraph. I don’t know if there is some way for the system to “automatically” recognize when new data is sufficiently similar to stock-specific data to focus on a stock-specific prediction vs. when it is different and it should go to the default system with multiple stocks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have any insights on these issues and/or how to address them in Matlab or otherwise, I would very much appreciate. Thanks in advance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best,&#xA;Mike&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have few points in $S^n$, i.e., the $n$-dimensional unit sphere embedded in $\\\\mathbb{R}^{n+1}$, and I would like to project them down to $S^2$, i.e., the 2-dimensional unit sphere (embedded in $\\\\mathbb{R}^3$) to visualize it with the constraint that neighboring points should be close by. I spent some time playing with t-sne but of course, the points no longer lie on $S^2$. I normalized the projections but that introduces weird distortions, for instance, if the variance of one dataset is very small in $S^n$ as compared to other, I expect the same to hold in their $S^2$ projections; that is not the case upon normalizing t-sne. Any ideas? I would really like something that makes the previous statement hold.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;The Vowpal Wabbit (VW) apparently supports sequence tagging functionality via &lt;a href=&quot;http://www.umiacs.umd.edu/~hal/searn/&quot; rel=&quot;nofollow noreferrer&quot;&gt;SEARN&lt;/a&gt;. The problem is that I cannot find anywhere detailed parameter list with explanations and with some examples. The best I could find is &lt;a href=&quot;http://zinkov.com/posts/2013-08-13-vowpal-tutorial/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Zinkov's blog entry&lt;/a&gt; with a very short example. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Vowpal_Wabbit&quot; rel=&quot;nofollow noreferrer&quot;&gt;main wiki page&lt;/a&gt; barely mentions SEARN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the checked out source code I found demo folder with some NER sample data. Unfortunately, the script running all the tests does not show how to run on this data. At least it was informative enough to see what is the expected format: almost the same as standard VW data format, except that entries are separated by blank lines (this is important).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current understanding is to run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat train.txt | vw -c --passes 10 --searn 25 --searn_task sequence \\\\&#xA;--searn_passes_per_policy 2 -b 30 -f twpos.vw&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn 25&lt;/code&gt; - the total number of NER labels (?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn_task sequence&lt;/code&gt; - sequence tagging task (?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--searn_passes_per_policy 2&lt;/code&gt; - not clear what it does&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other parameters are standard to VW and need no additional explanation. Perhaps there are more parameters specific to SEARN? What is their importance and impact? How to tune them? Any rules of thumb?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any pointers to examples will be appreciated.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a number of weather stations, and I know their positions. I would like to interpolate measurements from them for various other positions as a weighted average of these stations, but of course I need weights for this. I am thinking the most logical choice here from a physics point of view will be weighting by the inverse of the distance to the station, but I haven't quite convinced myself that that is right, and am not sure if I should maybe be using the distance squared, or maybe something in between.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any comments? Are there any other reasonable alternatives I should be considering?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm not sure if this is more appropriate for SO or DS in Stack Exchange since technically it's not about coding: in &lt;code&gt;caret&lt;/code&gt; package for training in R, it's possible to train the model using &lt;code&gt;rpart&lt;/code&gt; or &lt;code&gt;rpart2&lt;/code&gt; as the method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that &lt;code&gt;rpart&lt;/code&gt; is an implementation of CART. What is &lt;code&gt;rpart2&lt;/code&gt; and how is it different from &lt;code&gt;rpart&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My eventual aim is actually to compare the difference between the tree generated by &lt;code&gt;rpart&lt;/code&gt; and &lt;code&gt;rpart2&lt;/code&gt;, because my result seems to imply &lt;code&gt;rpart2&lt;/code&gt; has better accuracy for my dataset, but I have no clue how to view the &lt;code&gt;rpart2&lt;/code&gt; tree.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have a dataset of clients (their city, name, age, gender, number of children) and another dataset about the products that they have bought. i have been asked to do:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;extract knowledge about client profiles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know what knowledge should I extract.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;we are studying clustering and classification so they should connect to the question. what i thought about is to make clusters of clients. but i don't know what criterisas should I depend on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;should I just use a clustering algorithm like k means and let it give me the clusters ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;any suggestion would be appreciated&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Update&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;These are the dataset that I have:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Card: CardID, City, Region, PostalCode, CardStartDate, Gender, DateOfBirth, MaritalStatus, HasChildren, NumChildren, YoungestChild&lt;/li&gt;&#xA;&lt;li&gt;Item: ItemCode, ItemDescription, CategoryCode, SubCategoryCode, BrandCode, UpmarketFlag&lt;/li&gt;&#xA;&lt;li&gt;Transaction: Store, Date, Time, TransactionID, CardID, PaymentMethod&lt;/li&gt;&#xA;&lt;li&gt;Category: CategoryCode, CategDescription&lt;/li&gt;&#xA;&lt;li&gt;Transaction_Item: Store, Date, Time, TransactionID, ItemNumber, ItemCode, Amount&lt;/li&gt;&#xA;&lt;li&gt;SubCategory: SubCategoryCode, SubCategDescrip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The teacher said that we &lt;strong&gt;should&lt;/strong&gt; categories the clients and then indicate which products best suit for each customer.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a Rare event classification problem. I Have 95% of the data as a majority class and 5% of the data as the minority class. I use classification trees algorithm. I am measuring the goodness of the model using confusion matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the i have the minority class just 5% of the total data, even though my prediction performance of minority class is close to 70%, the total number of errors are high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, here is my confusion matrix.&#xA;             0           1&#xA;     0     213812      7008&#xA;     1     29083       16877&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though the Minority class(class 1) has predicted 16877 times correctly(70% and the misclassifcation is just 30%, but the absolute value of the misclassifcation is very high(29083) comparing to the correctly predicted minotriy class (16877). Which makes the solution less usable for the business. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any idea on handling these kind of  issues in such rare event modelling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Kind note: I have  balanced the target variable using the SMOTE algorithm before applying Classification tree.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;What are xml datasets? Is it possible to convert them to csv files?&#xA;I'm working on a Java program and I sometimes download datasets wich are in a binary format, are those xml? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a classification problem. I have a dataset containing equal number of categorical variables and continuous variables. How will i know what technique to use? between a decision tree and a logistic regression?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it right to assume that logistic regression will be more suitable for continuous variable and decision tree will be more suitable for continuous + categorical variable?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a qualitative variable, e.g. &lt;code&gt;userId&lt;/code&gt;, which could take around 30,000 different coded values ($k$). I would like to represent this variable as a dummy variable. Coding this into a vector of size $k$ doesn't seem to be a good approach. Is there a more compact method for coding for this variable?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm wondering if programmers tend to use AI APIs. And if so, what are they like? And where can I find a nice one for Java?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm not a NLP guy and I have this question. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a text dataset containing terms which go like, &quot;big data&quot; and &quot;bigdata&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my purpose both of them are the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I detect them in NLTK (Python)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or any other NLP module in Python?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am a beginner on Machine Learning.&#xA;In SVM, the separating hyperplane is defined as $y = w^T x + b$.&#xA;Why we say vector $w$ orthogonal to the separating hyperplane?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;New to R. In my example, my customers have restricted allocation of budget for Milk. I have more than 5 brands of milk in my store. Here my objective is how I know my customer is shifting from one brand to other? (Example: Customer is replacing Brand 1 with Brand 2 in my time series data). I would like to compute that shifting pattern every quarter and observe the trend quarter by quarter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Quarter Data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Date    Milk-Brand1 Milk-Brand2 Milk-Brand3 Milk-Brand4 Milk-Brand5&#xA;&#xA;1/1/2015    200 140 190 220 150&#xA;1/2/2015    204 138 195 226 144&#xA;1/3/2015    208 136 200 232 126&#xA;1/4/2015    212 134 205 238 108&#xA;2/2/2015    216 132 210 244 90&#xA;1/6/2015    220 130 215 250 72&#xA;1/7/2015    224 128 220 256 54&#xA;1/8/2015    228 126 225 262 36&#xA;1/9/2015    232 124 230 268 18&#xA;3/1/2015    236 122 235 274 0&#xA;3/2/2015    240 120 240 280 13&#xA;3/3/2015    244 118 245 286 33&#xA;3/4/2015    248 116 250 292 15&#xA;20/3/2015   252 114 255 298 33&#xA;20/3/2015   256 112 260 304 15&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you suggest compute correlation between each 'brand of milk' and compare those correlations from one quarter to other quarter? Or Cross-correlation? Or others? I am open.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your advice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the data via &lt;code&gt;dput&lt;/code&gt; for anyone wanting it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;structure(list(Date = structure(c(1420070400, 1422748800, 1425168000, &#xA;1427846400, 1422835200, 1433116800, 1435708800, 1438387200, 1441065600, &#xA;1420243200, 1422921600, 1425340800, 1428019200, 1426809600, 1426809600&#xA;), tzone = &quot;UTC&quot;, class = c(&quot;POSIXct&quot;, &quot;POSIXt&quot;)), Milk.Brand1 = c(200L, &#xA;204L, 208L, 212L, 216L, 220L, 224L, 228L, 232L, 236L, 240L, 244L, &#xA;248L, 252L, 256L), Milk.Brand2 = c(140L, 138L, 136L, 134L, 132L, &#xA;130L, 128L, 126L, 124L, 122L, 120L, 118L, 116L, 114L, 112L), &#xA;    Milk.Brand3 = c(190L, 195L, 200L, 205L, 210L, 215L, 220L, &#xA;    225L, 230L, 235L, 240L, 245L, 250L, 255L, 260L), Milk.Brand4 = c(220L, &#xA;    226L, 232L, 238L, 244L, 250L, 256L, 262L, 268L, 274L, 280L, &#xA;    286L, 292L, 298L, 304L), Milk.Brand5 = c(150L, 144L, 126L, &#xA;    108L, 90L, 72L, 54L, 36L, 18L, 0L, 13L, 33L, 15L, 33L, 15L&#xA;    )), .Names = c(&quot;Date&quot;, &quot;Milk.Brand1&quot;, &quot;Milk.Brand2&quot;, &quot;Milk.Brand3&quot;, &#xA;&quot;Milk.Brand4&quot;, &quot;Milk.Brand5&quot;), row.names = c(NA, -15L), class = &quot;data.frame&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am very new to machine learning.&#xA;I have a text classification problem in hand. I have a tagged dataset of around 750 documents( short texts), categorized manually into 16 buckets. I want to train a classifier on this data. I know that there should be a training set and a test set (an option could be 80-20 ). In my understanding, this should be for the complete set( 80% of my 750 documents- training, 20% of 750 documents - testing ). &#xA;1. They should be randomly generated or is there some condition for category? ie. if category A constitutes 60%,category B 5%, C 7% etc. how to choose the training set?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have some data of form showed below to squezze through some neural network.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Customer Step Var0  ... VarN&#xA;0        0    some  ... stuff&#xA;0        1    again ... stuff&#xA;0        2    foo   ... bar&#xA;0        3    bla   ... blub&#xA;1        0    other ... stuff&#xA;1        1    and   ... ongoing&#xA;2        0    and   ... so on&#xA;.        .    .     ... .&#xA;.        .    .     ... .&#xA;.        .    .     ... .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As the neural network has a fixed input layer size, but the customers step count can differ at each customer, I have following question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best practise (if exists) to rearange the given data for using as input of a NN? &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am interested in clustering $N$ time series of $T$ 'values' each. These values are distributions (which can be represented by their cumulative distribution functions (cdf), or their probability density functions (pdf), or more convenient forms &lt;a href=&quot;http://projects.csail.mit.edu/atemuri/wiki/images/f/fe/SrivastavaJermynJoshiCVPR2007.pdf&quot; rel=&quot;nofollow&quot;&gt;such as square-root pdfs yielding a simple spheric geometry&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For comparing given distributions, there is an extensive literature on statistical distances (KL, Hellinger, Wasserstein, and so on), but for comparing given time series of distributions, I am not sure whether there is any literature at all?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Such distances should somehow take into account dynamics information besides the distribution proximity at time t. Ideally, I wish I could have a kind of &lt;a href=&quot;http://arxiv.org/pdf/1506.00976v1.pdf&quot; rel=&quot;nofollow&quot;&gt;information factorization similar to this result&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering if such distances already exist and whether this kind of problem has already been formulated in the literature?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;--&#xA;edit for further precisions and answer to comments:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your answer, but dynamic time warping does not suit to my need. This dp technique only captures a rough similarity of shapes by allowing non-linear time distortion. But, it does not amount for the whole information in these time series, e.g. what about the distribution of distortions? Do the distributions of a given time series vary smoothly through time or violently? DTW is not always the solution, for instance, when working with random walks, it does not make sense to use a DTW since there are no time patterns! In this case, the only information is &quot;correlation&quot; and &quot;distribution&quot; (cf. Sklar's theorem in Copula Theory), and the paper cited above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- edit 2 Here are the papers that are somehow related to my question:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://pub.ist.ac.at/~chl/erc/papers/lampert-cvpr2015.pdf&quot; rel=&quot;nofollow&quot;&gt;Predicting the Future Behavior of a Time-Varying Probability Distribution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/BanerjeeDGS05.pdf&quot; rel=&quot;nofollow&quot;&gt;Clustering on the unit hypersphere using von Mises-Fisher distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://users.cis.fiu.edu/~lzhen001/activities/KDD2011Program/docs/p636.pdf&quot; rel=&quot;nofollow&quot;&gt;Unsupervised clustering of multidimensional distributions using earth mover distance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.machinelearning.org/archive/icml2009/papers/538.pdf&quot; rel=&quot;nofollow&quot;&gt;Hilbert space embeddings of conditional distributions with applications to dynamical systems&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am thinking of preprocessing techniques for the input data to a convolutional neural network (CNN) using sparse datasets and trained with SGD. In Andrew Ng's coursera course, &lt;a href=&quot;https://www.coursera.org/course/ml&quot; rel=&quot;nofollow&quot;&gt;Machine Learning&lt;/a&gt;, he states that it is important to preprocess the data so it fits into the interval $ \\\\left[ 3, 3 \\\\right] $ when using SGD. However, the most common preprocessing technique is to standardize each feature so $ \\\\mu = 0 $ and $ \\\\sigma = 1 $. When standardizing a highly sparse dataset many of the values will not end up in the interval.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am therefore curious - would it be better to aim for e.g. $ \\\\mu = 0 $ and $ \\\\sigma = 0.5 $ in order for the values be closer to the interval $ \\\\left[ 3, 3 \\\\right] $? Could anyone argue based on a knowledge of SGD on whether it is most important to aim for $ \\\\mu = 0 $ and $ \\\\sigma = 1 $ or $ \\\\left[ 3, 3 \\\\right] $?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Suppose I have data in the form of  Query/Document Pairs, along with corresponding relevance scores (or class labels). Is there a way to use topic modeling to devise a model so that later given a query and a document, we can predict its relevance score?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I am interested in parsing semi-structured text.&#xA;Assume that I have a text with labels of the kind: year_field, year_value, identity_field, identity_value, ..., address_field, address_value, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These fields and their associated values can be everywhere in the text, but usually they are near to each other, and more generally the text in organized in a (very) rough matrix, but rather often the value is just after the associated field with eventually some non-interesting information in between. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of different format can be up to several dozens, and is not that rigid (do not count on spacing, moreover some information can be added and removed).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am looking toward machine learning techniques to extract all those (field,value) of interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think metric learning and/or conditional random fields (CRF) could be of a great help, but I have not practical experience with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have already encounter a similar problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestion or literature on this topic?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have a dataset like this. The data has been collected through a questionnaire and I am going to do some exploratory data analysis.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;windows &amp;lt;- c(&quot;yes&quot;, &quot;no&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;)&#xA;sql     &amp;lt;- c(&quot;no&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;no&quot;,&quot;no&quot;)&#xA;excel  &amp;lt;- c(&quot;yes&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;)&#xA;salary &amp;lt;- c(100,200,300,400,500 )&#xA;&#xA;test&amp;lt;- as.data.frame (cbind(windows,sql,excel,salary),stringsAsFactors=TRUE)&#xA;test[,&quot;salary&quot;] &amp;lt;- as.numeric(as.character(test[,&quot;salary&quot;] ))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have an outcome variable (salary) in my dataset and a couple of input variables (tools). How can I visualize a horizontal box plot like this:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/CNLC1.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;So, I want to create a Player Profile &lt;strong&gt;Radar Chart&lt;/strong&gt; something like this:&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/neeVU.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/neeVU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Not only the scale of each variable different, but also I want a reversed scale for some statistics like the 'dispossessed' stat, where less actually means good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One solution for the variable scale for each statistic maybe is setting a benchmark and then calculating a score on a scale of 100? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But, How do I display the actual numbers on the chart then? Also, how do I get the reversed scale for some of the statistics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently working in Excel. What is the most powerful tool to create a complex chart like this?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Suppose I have five sets I'd like to cluster. I understand that the SimHashing technique described here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://moultano.wordpress.com/2010/01/21/simple-simhashing-3kbzhsxyg4467-6/&quot;&gt;https://moultano.wordpress.com/2010/01/21/simple-simhashing-3kbzhsxyg4467-6/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;could yield three clusters (&lt;code&gt;{A}&lt;/code&gt;, &lt;code&gt;{B,C,D}&lt;/code&gt; and &lt;code&gt;{E}&lt;/code&gt;), for instance, if its results were:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A -&amp;gt; h01&#xA;B -&amp;gt; h02&#xA;C -&amp;gt; h02&#xA;D -&amp;gt; h02&#xA;E -&amp;gt; h03&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Similarly, the MinHashing technique described in the Chapter 3 of the MMDS book:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://infolab.stanford.edu/~ullman/mmds/ch3.pdf&quot;&gt;http://infolab.stanford.edu/~ullman/mmds/ch3.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;could also yield the same three clusters if its results were:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A -&amp;gt; h01 - h02 - h03&#xA;&#xA;B -&amp;gt; h04 - h05 - h06&#xA;      |&#xA;C -&amp;gt; h04 - h07 - h08&#xA;                  |&#xA;D -&amp;gt; h09 - h10 - h08&#xA;&#xA;E -&amp;gt; h11 - h12 - h13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(Each set corresponds to a MH signature composed of three &quot;bands&quot;, and two sets are grouped if at least one of their signature bands is matching. More bands would mean more matching chances.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I have several questions related to these:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) Can SH be understood as a &lt;em&gt;single band&lt;/em&gt; version of MH?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Does MH necessarily imply the use of a data structure like Union-Find to build the clusters?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(3) Am I right in thinking that the clusters, in both techniques, are actually &quot;pre-clusters&quot;, in the sense that they are just sets of &quot;candidate pairs&quot;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(4) If (3) is true, does it imply that I still have to do an $O(n^2)$ search inside each &quot;pre-cluster&quot;, to partition them further into &quot;real&quot; clusters? (which might be reasonable if I have a lot of small and fairly balanced pre-clusters, not so much otherwise)&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a dataset that I want to classify as fraud/not fraud and I have many weak learners. My concern is that there is much more fraud than not fraud, so my weak learners perform better than average, but none perform better than 50% accuracy in the complete set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is whether I should set up testing and training sets that are half fraud and half not fraud or if I should just use a representative sample.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Let's say there is a function $f$ such that $y = f(x)$. However, if $f$ is a piecewise function such that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$y = \\\\begin{cases} 0 \\\\quad x \\\\leq 0 \\\\\\\\ 1 \\\\quad x &amp;gt;0\\\\end{cases} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I fit $f$ in that case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many thanks, guys.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am working on generating restaurant ratings automatically and I &lt;strong&gt;have various feature values&lt;/strong&gt; like delivery time, cost estimate, etc. I want to generate a rating for each restaurant between 0 to 5. But &lt;strong&gt;I don't have any training data or ground truth to validate&lt;/strong&gt;. This rating might vary with user. Most of the related work, mostly related to the Yelp data challenge, have some relevance score as training data. I though of using &lt;em&gt;reinforcement learning&lt;/em&gt; to learn the rating with user feedback, but not sure how to do that. Can anyone please suggest a relevant technique or algorithm for this problem?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I want to find the variables (and its values) used to build a  classification model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I came to know that the following code does the same&lt;/p&gt;&#xA;&#xA;&lt;p&gt;get_all_vars(model_built, dataset_used_for_building_a_model)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I would not be able to use the above code as I am going to use the &quot;model_built&quot; alone in the R code and i will not be having the &quot;dataset_used_for_building_a_mode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simply put i want to use this &quot;Model_built&quot; alone and fetch the variables used along with the values. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am trying to write the code of a Bernoulli block mixture model in matlab, but am facing an error every time I run the function. In particular, I'm having a problem with how to relate the distribution parameter $\\\\alpha$ to the latent variables $Z$ and $W$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, \\\\mathbf{Z} follows a multinomial distribution of parameter $\\\\pi$, such that $\\\\dim(\\\\pi)=(1, g)$, where $g$ is the number of clusters of rows and $\\\\sum \\\\pi=1$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$W$ follows a multinomial distribution of parameter $\\\\rho$ such that $\\\\dim(\\\\rho)=(1,m)$, where $m$ is the number of column clusters and $\\\\sum \\\\rho=1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\\\\mathbf{Z}$ is a matrix of $\\\\dim(N,g)$, and $\\\\mathbf{W}$ is a matrix of $\\\\dim(d,m)$, where $N$ is the number of observations, $p$ is the number of variables, $g$ is the number of row clusters, and $m$ is the number of column clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$X$ follows a bernoulli distribution of parameter $\\\\alpha_{ZW}$, where the notation $\\\\alpha_{ZW}$ denotes the values of $\\\\alpha$ depends on $\\\\mathbf{Z}$ and $\\\\mathbf{W}$, and $\\\\alpha$ is a $\\\\dim(g,m)$ matrix. Please find attached a graphical representation of the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I write the code of $\\\\alpha_{ZW}$ in order to get generate the model?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/btXXp.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/btXXp.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I am looking for a simple way to sample from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution&quot; rel=&quot;nofollow&quot;&gt;multivariate von Mises-Fisher&lt;/a&gt; distribution in Python. I have looked in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.vonmises.html&quot; rel=&quot;nofollow&quot;&gt;the stats module in scipy&lt;/a&gt; and the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.vonmises.html#numpy.random.vonmises&quot; rel=&quot;nofollow&quot;&gt;numpy module&lt;/a&gt; but only found the univariate von Mises distribution. Is there any code available? I have not found yet.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- edit. Apparently, Wood (1994) has designed an algorithm for sampling from the vMF distribution according to &lt;a href=&quot;http://idg.pl/mirrors/CRAN/web/packages/movMF/movMF.pdf&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;, but I can't find the paper.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm quite new to the NLTK package of Python and to NLP too (I usually work in R but for NLP purposes and scraping maybe Python is more able). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I scrap articles from Hungarian newsportals and want to make a wordcloud out of it to show what are the current trending news topics. First I filter out stopwords and then stem the remaining words. (nltk has Hungarian stemmer) So I'm able to make a frequency table which can be the base of the wordcloud. My problem comes afterwards because stems are usually meaningless chunks (and not lemmas) of real words. I want to somehow complete the stem to a real word.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first idea was to assign the most common word or the shortest one (or some combination of this 2 rules) to the stem and represent that in the wordcloud.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better solution for stem completion or should I follow a different workflow?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;Assuming I can collect the demand of the purchase of a certain product that are of different market tiers. Example: Product A is low end goods. Product B is another low end goods. Product C and D are middle-tier goods and product E and F are high-tier goods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have collected data the last year on the following&#xA;1. Which time period (season - festive? non-festive?) does the different tier product reacts based on the price set? Reacts refer to how many % of the product is sold at certain price range&#xA;2. How fast the reaction from the market after marketing is done? Marketing is done on 10 June and the products are all sold by 18 June for festive season that slated to happened in July (took 8 days at that price to finish selling)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can data science benefit in terms of recommending&#xA;1. If we should push the marketing earlier or later?&#xA;2. If we can higher or lower the price? (Based on demand and sealing rate?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I understanding it right that data science can help a marketer in this aspect? Which direction should I be looking into if I am interested to learn about it.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I've trained an AWS Machine Learning model with the training data from here : &lt;a href=&quot;https://www.kaggle.com/c/titanic/data&quot; rel=&quot;nofollow&quot;&gt;https://www.kaggle.com/c/titanic/data&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm now trying to run a batch prediction with the test data from the same source but I get the following error when I try to load the data : &quot;  The schema in this data file must match the datasource used to create the ML model ml-xxxxxxxxx. Ensure that the data file you are using matches the schema structure.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The schema, as far as I can see, is identical. I have tried it with and without the 'survived' column which is the value I'm trying to predict. I even tried it with the same training set which obviously has an identical schema and got the same error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I recently read &lt;a href=&quot;http://arxiv.org/abs/1411.4038&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt; by Jonathan Long, Evan Shelhamer, Trevor Darrell. I don't understand what &quot;deconvolutional layers&quot; do / how they work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The relevant part is&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;3.3. Upsampling is backwards strided convolution&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Another way to connect coarse outputs to dense pixels&#xA;  is interpolation. For instance, simple bilinear interpolation&#xA;  computes each output $y_{ij}$ from the nearest four inputs by a&#xA;  linear map that depends only on the relative positions of the&#xA;  input and output cells.&lt;br/&gt;&#xA;  In a sense, upsampling with factor $f$ is convolution with&#xA;  a fractional input stride of 1/f. So long as $f$ is integral, a&#xA;  natural way to upsample is therefore backwards convolution&#xA;  (sometimes called deconvolution) with an output stride of&#xA;  $f$. Such an operation is trivial to implement, since it simply&#xA;  reverses the forward and backward passes of convolution.&lt;br/&gt;&#xA;  Thus upsampling is performed in-network for end-to-end&#xA;  learning by backpropagation from the pixelwise loss.&lt;br/&gt;&#xA;  Note that the deconvolution filter in such a layer need not&#xA;  be fixed (e.g., to bilinear upsampling), but can be learned.&#xA;  A stack of deconvolution layers and activation functions can&#xA;  even learn a nonlinear upsampling.&lt;br/&gt;&#xA;  In our experiments, we find that in-network upsampling&#xA;  is fast and effective for learning dense prediction. Our best&#xA;  segmentation architecture uses these layers to learn to upsample&#xA;  for refined prediction in Section 4.2.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I don't think I really understood how convolutional layers are trained. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I think I've understood is that convolutional layers with a kernel size $k$ learn filters of size $k \\\\times k$. The output of a convolutional layer with kernel size $k$, stride $s \\\\in \\\\mathbb{N}$ and $n$ filters is of dimension $\\\\frac{\\\\text{Input dim}}{s^2} \\\\cdot n$. However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if my understanding of convolutional layers is correct, I have no clue how this can be reversed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anybody please help me to understand deconvolutional layers?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I used pretrained GoogleNet from &lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&quot; rel=&quot;nofollow&quot;&gt;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&lt;/a&gt; and finetuned it with my own data (~ 100k images, 101 classes). After one day training I achieved 62% in top-1 and 85% in top-5 classification and try to use this network to predict several images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I just followed example from &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/classification.ipynb&quot; rel=&quot;nofollow&quot;&gt;https://github.com/BVLC/caffe/blob/master/examples/classification.ipynb&lt;/a&gt;,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my Python code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import caffe&#xA;import numpy as np&#xA;&#xA;&#xA;caffe_root = './caffe'&#xA;&#xA;&#xA;MODEL_FILE = 'caffe/models/bvlc_googlenet/deploy.prototxt'&#xA;PRETRAINED = 'caffe/models/bvlc_googlenet/bvlc_googlenet_iter_200000.caffemodel'&#xA;&#xA;caffe.set_mode_gpu()&#xA;&#xA;net = caffe.Classifier(MODEL_FILE, PRETRAINED,&#xA;               mean=np.load('ilsvrc_2012_mean.npy').mean(1).mean(1),&#xA;               channel_swap=(2,1,0),&#xA;               raw_scale=255,&#xA;               image_dims=(224, 224))&#xA;&#xA;def caffe_predict(path):&#xA;        input_image = caffe.io.load_image(path)&#xA;        print path&#xA;        print input_image&#xA;        prediction = net.predict([input_image])&#xA;&#xA;&#xA;        print prediction&#xA;        print &quot;----------&quot;&#xA;&#xA;        print 'prediction shape:', prediction[0].shape&#xA;        print 'predicted class:', prediction[0].argmax()&#xA;&#xA;&#xA;        proba = prediction[0][prediction[0].argmax()]&#xA;        ind = prediction[0].argsort()[-5:][::-1] # top-5 predictions&#xA;&#xA;&#xA;        return prediction[0].argmax(), proba, ind&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my deploy.prototxt I changed the last layer only to predict my 101 classes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;layer {&#xA;  name: &quot;loss3/classifier&quot;&#xA;  type: &quot;InnerProduct&quot;&#xA;  bottom: &quot;pool5/7x7_s1&quot;&#xA;  top: &quot;loss3/classifier&quot;&#xA;  param {&#xA;    lr_mult: 1&#xA;    decay_mult: 1&#xA;  }&#xA;  param {&#xA;    lr_mult: 2&#xA;    decay_mult: 0&#xA;  }&#xA;  inner_product_param {&#xA;    num_output: 101&#xA;    weight_filler {&#xA;      type: &quot;xavier&quot;&#xA;    }&#xA;    bias_filler {&#xA;      type: &quot;constant&quot;&#xA;      value: 0&#xA;    }&#xA;  }&#xA;}&#xA;layer {&#xA;  name: &quot;prob&quot;&#xA;  type: &quot;Softmax&quot;&#xA;  bottom: &quot;loss3/classifier&quot;&#xA;  top: &quot;prob&quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the distribution of softmax output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[ 0.01106235  0.00343131  0.00807581  0.01530041  0.01077161  0.0081002&#xA;   0.00989228  0.00972753  0.00429183  0.01377776  0.02028225  0.01209726&#xA;   0.01318955  0.00669979  0.00720005  0.00838189  0.00335461  0.01461464&#xA;   0.01485041  0.00543212  0.00400191  0.0084842   0.02134697  0.02500303&#xA;   0.00561895  0.00776423  0.02176422  0.00752334  0.0116104   0.01328687&#xA;   0.00517187  0.02234021  0.00727272  0.02380056  0.01210031  0.00582192&#xA;   0.00729601  0.00832637  0.00819836  0.00520551  0.00625274  0.00426603&#xA;   0.01210176  0.00571806  0.00646495  0.01589645  0.00642173  0.00805364&#xA;   0.00364388  0.01553882  0.01549598  0.01824486  0.00483241  0.01231962&#xA;   0.00545738  0.0101487   0.0040346   0.01066607  0.01328133  0.01027429&#xA;   0.01581303  0.01199994  0.00371804  0.01241552  0.00831448  0.00789811&#xA;   0.00456275  0.00504562  0.00424598  0.01309276  0.0079432   0.0140427&#xA;   0.00487625  0.02614347  0.00603372  0.00892296  0.00924052  0.00712763&#xA;   0.01101298  0.00716757  0.01019373  0.01234141  0.00905332  0.0040798&#xA;   0.00846442  0.00924353  0.00709366  0.01535406  0.00653238  0.01083806&#xA;   0.01168014  0.02076091  0.00542234  0.01246306  0.00704035  0.00529556&#xA;   0.00751443  0.00797437  0.00408798  0.00891858  0.00444583]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It seems just like random distribution with no sense.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for any help or hint and best regards, Alex&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm studying machine learning and I feel there is a strong relationship between the concept of VC dimension and the more classical (statistical) concept of degrees of freedom.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain such a connection?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;Similar questions have been asked in this context but this one seems a bit different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Aim]&lt;/strong&gt;&lt;br&gt;&#xA;We would like to find out what the probability is of a person purchasing a 4USD column D (see below) price given that he has purchased C-2, B-2, A&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/mLlaR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Context]&lt;/strong&gt;&lt;br&gt;&#xA;We have a sizeable dataset of about 500,000 observations of this pattern. E.g. customer decides to buy A), bought B-1, then C-2, D-3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;[Issue]&lt;/strong&gt;&lt;br&gt;&#xA;We don't know how deep the events reach. E.g. it can be to column E, F, G. That's why we would like to auto-generate a probability tree in preferably SPSS but are open to other solutions as well.&#xA;How does this look like mathmatically? Can we just extend bayes conditional probability? If so, how does it look? Any potential issues?&#xA;Any other comments/ ideas are always welcome :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a large sequence of vectors of length N. I need some unsupervised learning algorithm to divide these vectors into M segments. E.g.:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/O1hyC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;k-means is not suitable, because it puts similar elements from different locations into a single cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;real data looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/VyVCU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see here 3 clusters: [0..50], [50..200], [200..250]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPD 2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use modified k-means and get this acceptable result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/WBlZn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;borders of clusters: [0, 38, 195, 246]&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am using the below R code to convert text to lower case:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;movie_Clean &amp;lt;- tm_map(movie_Clean, content_transformer(tolower))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I end up getting the below error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error in FUN(content(x), ...) :    invalid input 'I just wanna watch&#xA;  Jurassic World í ½í¸«' in 'utf8towcs'.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Please help how to overcome this error.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;With all the hoopla around Data Science, Machine Learning, and all the success stories around, there are a lot of both justified, as well as overinflated, expectations from Data Scientists and their predictive models.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question to practicing Statisticians, Machine Learning experts, and Data Scientists is - how do you manage expectations from the businesspeople in you company, particularly with regards to predictive accuracy of models? To put it trivially, if your best model can only achieve 90% accuracy, and upper management expects nothing less than 99%, how do you handle situations like these?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;[Apologies if this post sounds naive, I'm fairly new to the world of data science/big data and very unsure where I'm heading career-wise]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently an undergraduate MMath [integrated master's] Mathematics student in the UK who has finished the third year of the course [out of four years].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I have been considering the possibility of doing further research in Mathematics/Statistics/Operational Research/Data Science, I have decided to stay on and complete the Master's component of the course [as it is the only Master's course I can get funding for at this stage]. After the Master's I may continue on and do a PhD.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are currently two projects that appeal to me that seem to have relevant applications. The first one is on improved MCMC [Markov Chain Monte Carlo] methods, in particular MCMC using Hamiltonian Dynamics. There is scope for some big data applications here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other project that I could take part in is one on the centrality/communities detection of networks within network science. This could possibly be useful with applications in Operational Research.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have an idea as to which project will be more relevant to data science/analytics?&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I have an excel file that contains details related to determining the quality of a wine and I want to implement the linear model concept using the function &lt;strong&gt;sklearn.linear_model.SGDClassifier(SVM =&gt; Hinge loss) and (Logarithmic regression =&gt;log loss)&lt;/strong&gt; using python. I learned the basics about these function through the &lt;em&gt;scikit learn&lt;/em&gt; website and I am not able to implement the model using excel file. I am very new to python and machine learning and I finding it hard to implement the model. I opened the excel file in python and tried to take two columns [randomly] from the file and use that as an input to call the &lt;strong&gt;fit&lt;/strong&gt; function available in the model. But, I got an error stating &lt;strong&gt;Unknown label type: array&lt;/strong&gt;. I tried a couple of other methods too, but, nothing worked. Can someone guide me with the implementation process? &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from xlrd import open_workbook&#xA;from sklearn import linear_model&#xA;i = 0&#xA;fa = []&#xA;ph = []&#xA;&#xA;book = open_workbook('F:/BIG DATA/winequality.xlsx')&#xA;sheet = book.sheet_by_name('Sheet1')&#xA;num_rows = sheet.nrows - 1&#xA;num_cols = sheet.ncols - 1&#xA;curr_row = 0&#xA;while curr_row &amp;lt;num_rows:&#xA;    curr_row += 1&#xA;    cell_val = sheet.cell_value(curr_row,0)&#xA;    cell_val1 = sheet.cell_value(curr_row,10)&#xA;&#xA;    fa.append([float(cell_val),float(cell_val1)])&#xA;    cell_val2 = sheet.cell_value(curr_row,8)&#xA;    ph.append(float(cell_val2))&#xA;&#xA;model = linear_model.SGDClassifier()&#xA;print(model.fit(fa,ph))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/I9J8u.png&quot; alt=&quot;Screenshot&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The error message screenshot:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/lCJTu.png&quot; alt=&quot;ERROR&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a rare event (unbalanced target variable) classification problem using decision trees. My dataset comprises of 95% non-event and 5% minority (events) class. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I used decision tree over logistic regression because I had many categorical variables comparing to continuous variables. I get a  good performance for training data with the decision tree C5.0. However I get poor results for the new data. I use the confusion matrix as a measure of performance. Training model is over-fitting. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did pruning to reduce the over-fitting caused by the decision tree.  I used the following code to build the model&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Classifi_C5.0 &amp;lt;- C5.0(TARGET ~., , data = training_data_SMOTED, trails = 500,&#xA;                      control = C5.0Control(minCases = mincases_count,&#xA;                                            noGlobalPruning = FALSE))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I balanced the minority and majority class using the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;training_data_SMOTED &amp;lt;- SMOTE(TARGET ~ ., training_data,&#xA;                              perc.over = 100, k = 5, perc.under = 200)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any sort of advice will be helpful. &lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have several samples (C2, C4, C5) and want to check if they are at a certain stage. I included some known samples (D0 - D77) which were generated at different stages by another lab. In the PCA plot, my samples cluster together on the left and the known samples are dispersed on the right. I think the major difference among all the samples is different experimental protocols (PC1) and the second is different stages (PC2). So my samples are at the same stage. Is that right? And can we say my samples are at a stage between D12 and D19 (when projected to known samples, my samples are located between D12 and D19)? I have no strong mathematical background. Hope someone with math background can give some explanation. Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[ &lt;strong&gt;UPDATE1&lt;/strong&gt; ]&#xA;I did this analysis using &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;prcomp&lt;/a&gt; in R. The input is a 2D numeric matrix with 17436 rows and 42 columns. Each row represents a gene and each column represents a sample. The number is the gene expression level for a gene in a sample. The gene expression level is normalized using DESeq2 and thus the numbers are comparable across genes (rows) and samples (columns). For the 42 columns, 18 are from my experiments and the rest from published datasets. Besides different protocols, it is possible there are other differences. Generally, the table is a combination of two sources of data. In my data , C2, C4 and C5 are three cell lines which we processed in parallel in experiments. In published datasets, they sampled the cells at different time points (Day 0 to Day 77).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[ &lt;strong&gt;UPDATE2&lt;/strong&gt; ] R Code for PCA and plotting&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# normCounts: normalized count from DESeq2, with 17436 rows and 42 columns&#xA;normCounts0 &amp;lt;- normCounts[ rowSums(normCounts) &amp;gt; 0, ]&#xA;tab &amp;lt;- t(normCounts0)&#xA;pca &amp;lt;- prcomp(tab, scale = TRUE)&#xA;tmp.x &amp;lt;- as.data.frame(pca$x)&#xA;tmp.x$sample&amp;lt;-c(rep(&quot;C2_Con&quot;,3),rep(&quot;C2_KD&quot;,3),rep(&quot;C5_Con&quot;,3),rep(&quot;C5_KD&quot;,3),rep(&quot;C4_Con&quot;,3),rep(&quot;C4_KD&quot;,3), rep(&quot;D0&quot;,4),rep(&quot;D7&quot;,4),rep(&quot;D12&quot;,2),rep(&quot;D19&quot;,4),rep(&quot;D26&quot;,2),rep(&quot;D33&quot;,2),rep(&quot;D49&quot;,2),rep(&quot;D63&quot;,2),rep(&quot;D77&quot;,2))&#xA;&#xA;require(&quot;ggplot2&quot;)&#xA;p &amp;lt;- ggplot(tmp.x, aes(x=PC1, y=PC2, color=sample))&#xA;p + geom_point() + scale_color_discrete(breaks=c(&quot;C2_Con&quot;,&quot;C2_KD&quot;,&quot;C5_Con&quot;,&quot;C5_KD&quot;,&quot;C4_Con&quot;,&quot;C4_KD&quot;, &quot;D0&quot;,&quot;D7&quot;, &quot;D12&quot;,&quot;D19&quot;,&quot;D26&quot;,&quot;D33&quot;,&quot;D49&quot;,&quot;D63&quot;,&quot;D77&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/LuU0y.jpg&quot; alt=&quot;PCA plot&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;Detailed Question Explanation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose say our application X is processing huge logs (size varying from MBs to GBs) and giving insight results in these logs(NOT A Social Data logs or Security Logs)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;now this logs are in format say log.y with different variety, using C++ as Engine to process these huge logs.(It generates imp. insights about data but need to be processed using our application X only and we don't want to change core way processing of application X)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this processing happens on some server it under or over utilizes resources (That I already know).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use cloud computing for this processing we get that processing power with optimum usage. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do we see help of BIG data analytics in this particular sort of usage?&#xA;Any help or suggestion is very deeply appreciated  &lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a csv file with characters in Persian and I cannot view them in R correctly. Also, I cannot subset based on Persian characters values. Here is a sample code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;list1 &amp;lt;- c(&quot;x&quot;,&quot;y&quot;)&#xA;list2 &amp;lt;- c(&quot;ب&quot;,&quot;الف&quot;)&#xA;&#xA;list1 &#xA;list2 ##OK-readable&#xA;&#xA;writedf &amp;lt;- as.data.frame(cbind(list1,list2),encoding=&quot;UTF-8&quot;)&#xA;write.csv(writedf,&quot;test.csv&quot;)&#xA;testdf &amp;lt;- read.csv(&quot;test.csv&quot;,encoding=&quot;UTF-8&quot;)&#xA;&#xA;testdf  &#xA;## not readable&#xA;&#xA;testdf[,testdf$list2==&quot;ب&quot;]&#xA;## data frame with 0 columns and 2 rows  ???&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;',),\n",
       " ('&lt;p&gt;I have an excel file containing a long text in column A. I am looking for the words starting by &quot;popul&quot; such as popular and populate . I can find these cells by the formula:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    =SEARCH(&quot;popul&quot;,A1,1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want a function that returns the whole words starting by popul such as popular and populate.&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm using the HMMLearn python package for hidden markov models.  That implementation is build on multivariate gaussian distributions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have a string of features.  How sensitive are gaussians to vastly different feature scales?  Will it be really skewed if one feature is scaled between 0 and 1, and another is scaled between 0 and 1e8?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on a project that aims to retrieve a large data-set (i.e., tweet data which is a couple of days old) from Twitter using the twitteR library on R.  have difficulty storing tweets because my machine has only 8 GB of memory. It ran out of memory even before I set it to retrieve for one day. Is there a way where I can store the tweets straight to my disk without storing into RAM? I am not using the streaming API as I need to get old tweets. &lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I am really new to data science. Please don't mark me down as this website is my only hope of progress.&#xA;I have set of data I obtained from NASA website. When I saved it, it saved as &quot;tsv' file. (Tab separated values). I want to open it on Matlab as a Matrix as I have a code to run on that matrix. &#xA;Basically I want to import that file to matlab and start running the code on it. &#xA;Can someone please help me or guide me in the right direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried various things such as import data, tdfread but so far nothing has worked for me. I was first trying to export the tsv file to MS Excel and then go from Excel to Matlab. That too I don't know how to do. I will give you the link of my data which I want to import on to Matlab. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The link for my data is the following.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please take a look at &lt;a href=&quot;http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=J%2FApJS%2F209%2F31&quot; rel=&quot;nofollow&quot;&gt;http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=J%2FApJS%2F209%2F31&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you just click on submit at the lower right corner, one will see the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;a = importdata('J_ApJS_209_31_table3-150618.tsv') [This the command I used].&#xA;Error message on matlab is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ??? Error using ==&amp;gt; importdata at 136&#xA;    Unable to open file.&#xA;    Error in ==&amp;gt; data at 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I wrote a script.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    tdfread(J_ApJS_209_31_table3-150618,'\\\\t')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error message I get is &#xA;        ??? Undefined function or variable 'J_ApJS_209_31_table3'.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Error in ==&amp;gt; data at 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;What are the best method/library/data available to extract named entities [Names and Location] from Twitter data ? [Other than dictionary lookup]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried with Python-Stanford NER, But it seems to fail when named entities is not capitalized. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried to predict NER after converting text to upper case &#xA;eg : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; text = &quot;david beckham played for england&quot;&#xA;&#xA; stanford.NERTagger.tag(text)&#xA; [(u'david', u'PERSON'), (u'beckham', u'PERSON'), (u'played', u'O'), (u'for', u'O'), (u'england', u'O')]&#xA;&#xA; stanford.NERTagger.tag(text.upper())&#xA; output : [(u'DAVID', u'PERSON'), (u'BECKHAM', u'PERSON'), (u'PLAYED', u'O'), (u'FOR', u'O'), (u'ENGLAND', u'LOCATION')]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\",),\n",
       " ('&lt;p&gt;A question crossed my mind not so long ago: I am doing experiments on Language Model with RNN (always with the same network topology: 50 hidden units, and 10M &quot;directs connections&quot; that are emulating N_grams models) and different fraction of corpus (10,25,50,75,100%) (9M words).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I noticed that while perplexity seems to decrease when the training data become more abundant, certain times it does not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Last example : 143 118 109 106 112&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first thought was network initialization, so I began testing with a smaller corpus and 20 hidden units (for technical reasons. Even with 10% corpus, learning can take up to 30h, which is problematic for me), and I found after 50 tries that all nets converged on values within 3% of each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I thought that maybe the importance of this initialization is a function of the number of hidden units? I mean the more hidden units the more parameters to tune.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also maybe my stop criterion is too sensitive (It stops if evolution of perplexity between two iterations is inferior to a certain number).&#xA;Do you think it would make an impact to allow it to run one of two iterations after the criterion was met to see if it was just a local thing ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Marc&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;h3&gt;Problem&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I have tried using Naive bayes on a labeled data set of crime data but got really poor results (7% accuracy). Naive Bayes runs much faster than other alogorithms I've been using so I wanted to try finding out why the score was so low. &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Research&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;After reading I found that Naive bayes should be used with balanced datasets because it has a bias for classes with higher frequency. Since my data is unbalanced I wanted to try using the Complementary Naive Bayes since it is specifically made for dealing with data skews. In the paper that describes the process, the application is for text classification but I don't see why the technique wouldn't work in other situations. You can find the paper I'm referring to &lt;a href=&quot;http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. In short the idea is to use weights based on the occurences where a class doesn't show up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After doing some research I was able to find an implementation in Java but unfortunately I don't know any Java and I just don't understand the algorithm well enough to implement myself.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Question&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;where I can find an implementation in python? If that doesn't exist how should I go about implementing it myself?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called &lt;strong&gt;spark-submit&lt;/strong&gt; command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/bin&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After that, in cmd prompt: spark-submit script.py&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Run3y.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;It's well known that science has given us large amount of free accessible data, such as &lt;a href=&quot;http://www.1000genomes.org&quot; rel=&quot;nofollow&quot;&gt;http://www.1000genomes.org&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/genbank&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/genbank&lt;/a&gt;. How can we play around with the data and apply data science/machine learning to it? What could be some ideas?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My own ideas:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Biological data visualisation&lt;/li&gt;&#xA;&lt;li&gt;Gene prediction using hidden-markov-model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any more?&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am working on Rstudio on a server which has 250GB ram. But its taking too much time to handle a 2GB data file. how should i speed up my work?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I have a 7 giga confidential dataset which I want to use for a machine learning application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every package recommanded for efficient dataset management in R like :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;data.table, &lt;/li&gt;&#xA;&lt;li&gt;ff &lt;/li&gt;&#xA;&lt;li&gt;and sqldf with no success. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Data.table needs to load all the data in the memory from what I read, so it's obvious that it will not work since my computer has only 4g RAM. Ff leads to a memory error too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I decided to turn to sgdb and I tried :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Mysql which managed to load my dataset in 2 hours and 21'. Then I began my requests (I have a few requests to do to prepare my data before I export a smaller set in R for machine learning application), and then I had to wait for hours before I got the following message &quot;The total number of locks exceeds the lock table size&quot; (my request was just an update to extract the month from a date for each tuple). &lt;/li&gt;&#xA;&lt;li&gt;I read that postgre was similar to mysql in performance so I didn't try&lt;/li&gt;&#xA;&lt;li&gt;I read that redis was really performant but not at all adapted to massive importation like I want to do here so I didn't try&lt;/li&gt;&#xA;&lt;li&gt;I tried mongoDb, the nosql upraising solution that I heard everywhere about. Not only I find rather disturbing that mongoimport is so limited in options (I had to change all semi-colon in commas using sed before I can import the data), but It seems to be less performant that mysql since I launched the loading yesterday and it is still running. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What I can't try : data are confidential so I don't really want to rent some space on Azure or Amazon clouding solution. I am not sure that it is that big that I have to turn to Hadoop solution but maybe I am wrong about that. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an open-source performant solution that I didn't try that you would recommend to perform some sql-like requests on a biggish dataset ?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Edit : &#xA;Some more details about what I want to do with these data for you to visualize. These are events with a timestamp and a geolocalisation. I have 8 billions of lines. One example of what I want to do : &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;standardize series identified by geolocalisation (I need to compute mean grouping by geolocalisation for example), &lt;/li&gt;&#xA;&lt;li&gt;compute average count of events by type of season, day... (usual group by sql request)... &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Edit &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a beginning of answer for those who have limited hardware like me, rSQLite seems to be a possibility. I am still interested in other people experiences. &lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I've been coding a Neural Network for recognizing and tagging parts of speech in English (written in Java). The code itself has no 'errors' or apparent flaws. Nevertheless, it is not learning -- the more I train it does not change its ability to predict the testing data. The following is information about what I've done, please ask me to update this post if I left something important out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wrote the neural network and tested it on several different problems to make sure that the network itself worked. I trained it to learn how to double numbers, XOR, cube numbers, and learn the sin function to a decent accuracy. So, I'm fairly confident that the actual algorithm is working.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The network using using the sigmoid activation function. The learning rate is .3, Momentum is .6. The weights are initialized to rng.nextFloat() -.5) * 4&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then got the Brown Corpus data-set and simplified the tagset to 'universal' with NLTK. I used NLTK for generating and saving all the corpus and dictionary data. I cut the last 15,000 sentences out of the corpus for testing purposes. I used the rest of the corpus (about 40,000 sentences of tagged words) for training. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The neural network layout is as follows: There is an input neuron for each Tag. Output Layer: There is one output neuron for each tag. The network is taking inputs for 3 words: first: the word coming before the word we want to tag, second: the word that needs to be tagged, third: the word that follows the second word. So, total number of inputs are 3x(total number of possible tags). The input values are numbers between 0 and 1. Each of the 3 words being fed into the input layer is searched for in a dictionary (made up by the 40,000 corpus, the same corpus that is used for training). The dictionary holds the number of times that each word has been tagged in the corpus as what part of speech. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For instance, the word 'cover' is tagged as a noun 1 time and a verb 3&#xA;  times.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Percentages of being tagged are computed for each part of speech that the word is associated as, and this is what is fed into the network for that particular word. So, the input neuron designated as NOUN would receive .33 and VERB would receive .66. The other input neurons that hold tags for that word receive an input of 0.0. This is done for each of the 3 words to be inputted. If a word is the first word of a sentence, the first group of tags are all 0. If a word is the last word of a sentence, the final group of input neurons that hold the tag probabilities for the following word are left as 0s.&#xA;I've been using 10 hidden nodes (I've read a number of papers and this seems to be a good place to start testing with)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;None of the 15,000 testing sentences were used to make the 'dictionary.' So, when testing the network with this partial corpus there will be some words the network has never seen. Words that are not recognized have their suffix stripped, and their suffix is searched for in another 'dictionary.' Whatever is most probable for that word is then used as inputs for it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my set-up, and I started trying to train the network. I've been training the network with all 40,000 sentences. 1 epoch = 1 forward and backpropagation of every word in each sentence of the 40,000 training-set. So, just doing 1 epoch takes quite a few seconds. Just by knowing the word probabilities the network did pretty well, but the more I train it, nothing happens. The numbers that follow the epochs are the number of correctly tagged words divided by the total number of words.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First run 50 epochs: 0.928218786&lt;/p&gt;&#xA;&#xA;&lt;p&gt;100 epochs:        0.933130661&lt;/p&gt;&#xA;&#xA;&lt;p&gt;500 epochs:       0.928614499 took around 30 minutes to train this                   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tried 10 epochs:         0.928953683 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using only 1 epoch had results that pretty much varied between .92 and .93&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, it doesn't appear to be working...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then took 55 sentences from the corpus and used the same dictionary that had probabilities for all 40,000 words. For this one, I trained it in the same way I trained my XOR -- I only used those 55 sentences and I only tested the trained network weights on those 55 sentences. The network was able to learn those 55 sentences quite easily. With 120 epochs (taking a couple seconds) the network went from tagging 3768 incorrectly and 56 correctly (on the first few epochs) to tagging 3772 correctly and 52 incorrectly on the 120th epoch. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is where I'm at, I've been trying to debug this for over a day now, and haven't figured anything out.&lt;/p&gt;&#xA;\",),\n",
       " (\"&lt;p&gt;I'm trying to think of a data set that is essentially topologically spherical. It's easier to think of cylindrical datasets (two dimensions, one periodic) or toroidal datasets (two dimensions, both periodic). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obvious candidates are geographical and astronomical, ground and sky; but I think the only thing spherical about the sky is its projection onto the ground, so it really just comes back to Earth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I find it helpful to think about in terms of a circle's &lt;a href=&quot;https://en.wikipedia.org/wiki/Fundamental_polygon#Examples&quot; rel=&quot;nofollow noreferrer&quot;&gt;fundamental polygon&lt;/a&gt;:&#xA;&lt;img src=&quot;https://i.stack.imgur.com/vxubU.png&quot; alt=&quot;Fundamental polygons&quot;&gt;&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am using rattle in R for predictive models and am trying to see whether there is a difference in different sampling methods. The split function at the start of rattle for splitting into training and testing (optional validation) I take it is split validation. Is there a way to do bootstrap validation and cross validation directly in rattle?&lt;/p&gt;&#xA;',),\n",
       " (\"&lt;p&gt;I'm writing my thesis at the moment, and for some time - due to a lack of a proper alternative - I've stuck with &quot;unstructured data&quot; for referring to natural, free flowing text, e.g. Wikipedia articles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This nomenclature has bothered me from the very beginning, since it opens a debate that I don't want to get into. Namely, that &quot;unstructured&quot; implies that natural language lacks structure, which it does not - the most obvious being syntax. It also gives a negative impression, since it is the opposite of &quot;structured&quot;, which is accepted as being positive. This is not the focus of my thesis, though the &quot;unstructured&quot; part itself plays an important role.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I completely agree with the writer of &lt;a href=&quot;https://gigaom.com/2013/03/17/can-we-please-stop-saying-unstructured-data/&quot;&gt;this article&lt;/a&gt;, but he proposes no alternative except for &quot;rich data&quot;, which doesn't cover my point. The point I'm trying to make that the text lacks a traditional database-like (e.g. tabular) structure of the data, with every piece of data having a clear data type and semantics that is easy to interpret using computer programs. Of course I'd like to condense this definition into a term, but so far I've been unsuccessful coming up with, or discovering an acceptable taxonomy in literature.&lt;/p&gt;&#xA;\",),\n",
       " ('&lt;p&gt;I am new to data science. I have a dataset of around 200,000 records, having 5 columns. There is a field called, &lt;code&gt;class&lt;/code&gt;. For each &lt;code&gt;class&lt;/code&gt;, there are one or many &lt;code&gt;divisions&lt;/code&gt;. I have to do this:&#xA;1. Filter the dataset, such that only those &lt;code&gt;classes&lt;/code&gt; with at least 5 divisions turn up.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;&lt;p&gt;For each division, I have to calculate &lt;code&gt;attendance&lt;/code&gt; from another column.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;There is a minimum &lt;code&gt;attendance&lt;/code&gt; value for each &lt;code&gt;class&lt;/code&gt;. I have to find the &lt;code&gt;percentage of divisions in each class with the minimum attendance&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I started with importing the data in python using Pandas and started writing loops for processing this. But I am sure this is not the right way to do. Can you please give some idea.Can I do this in Excel pivot table?&lt;/p&gt;&#xA;',),\n",
       " ('&lt;p&gt;What I mean is the following: Instead of processing all the training data at once and calculating a model, we process one data point at a time and update the model directly afterwards. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have seen the terms &quot;on-line (or online) learning&quot; and &quot;incremental learning&quot; for this. Is there a subtle difference? Is one term used more frequently? Or does it depend on the research community?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: The Bishop book (Pattern Recognition and Machine Learning) uses the terms on-line learning and sequential learning as synonyms but does not mention incremental learning. &lt;/p&gt;&#xA;',),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samTekst.rdd.map(tuple).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "samTekstrdd=samTekst.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(samTekstrdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/35769489/adding-the-resulting-tfidf-calculation-to-the-dataframe-of-the-original-document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:506)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:455)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:506)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-547247b2c2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m df1 = (samTekst\n\u001b[0;32m      2\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   )\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \"\"\"\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:506)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:455)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:506)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:518)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n"
     ]
    }
   ],
   "source": [
    "df1 = (samTekst\n",
    "  .rdd\n",
    "  .map(lambda x : (x, x.Body.split(\" \")))\n",
    "  .toDF()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer( #źle działa :( \n",
    "    inputCol='Body', \n",
    "    outputCol='Body_arr', \n",
    "    pattern='\\s+|[;,.\\\"?/&-)$0123456789:(]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer( \n",
    "    inputCol='Body', \n",
    "    outputCol='Body_arr', \n",
    "    pattern='\\\\W+|[;,.\\\"?/&-)$0123456789:(]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Body: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samTekst.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'always', 'been', 'interested', 'in', 'machine', 'learning', 'but', 'i', 'can', 't', 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', 'quot', 'hello', 'world', 'quot', 'example', 'how', 'can', 'i', 'avoid', 'hard', 'coding', 'behavior', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'i', 'wanted', 'to', 'quot', 'teach', 'quot', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles', 'i', 'couldn', 't', 'just', 'use', 'relative', 'motion', 'because', 'the', 'obstacles', 'move', 'around', 'but', 'i', 'don', 't', 'want', 'to', 'hard', 'code', 'say', 'distance', 'because', 'that', 'ruins', 'the', 'whole', 'point', 'of', 'machine', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'obviously', 'randomly', 'generating', 'code', 'would', 'be', 'impractical', 'so', 'how', 'could', 'i', 'do', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'a', 'researcher', 'and', 'instructor', 'i', 'm', 'looking', 'for', 'open', 'source', 'books', 'or', 'similar', 'materials', 'that', 'provide', 'a', 'relatively', 'thorough', 'overview', 'of', 'data', 'science', 'from', 'an', 'applied', 'perspective', 'to', 'be', 'clear', 'i', 'm', 'especially', 'interested', 'in', 'a', 'thorough', 'overview', 'that', 'provides', 'material', 'suitable', 'for', 'a', 'college', 'level', 'course', 'not', 'particular', 'pieces', 'or', 'papers', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'sure', 'data', 'science', 'as', 'will', 'be', 'discussed', 'in', 'this', 'forum', 'has', 'several', 'synonyms', 'or', 'at', 'least', 'related', 'fields', 'where', 'large', 'data', 'is', 'analyzed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'particular', 'question', 'is', 'in', 'regards', 'to', 'data', 'mining', 'i', 'took', 'a', 'graduate', 'class', 'in', 'data', 'mining', 'a', 'few', 'years', 'back', 'what', 'are', 'the', 'differences', 'between', 'data', 'science', 'and', 'data', 'mining', 'and', 'in', 'particular', 'what', 'more', 'would', 'i', 'need', 'to', 'look', 'at', 'to', 'become', 'proficient', 'in', 'data', 'mining', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'which', 'situations', 'would', 'one', 'system', 'be', 'preferred', 'over', 'the', 'other', 'what', 'are', 'the', 'relative', 'advantages', 'and', 'disadvantages', 'of', 'relational', 'databases', 'versus', 'non', 'relational', 'databases', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'use', 'lt', 'a', 'href', 'quot', 'http', 'www', 'csie', 'ntu', 'edu', 'tw', 'cjlin', 'libsvm', 'quot', 'gt', 'libsvm', 'lt', 'a', 'gt', 'to', 'train', 'data', 'and', 'predict', 'classification', 'on', 'lt', 'strong', 'gt', 'semantic', 'analysis', 'lt', 'strong', 'gt', 'problem', 'but', 'it', 'has', 'a', 'lt', 'strong', 'gt', 'performance', 'lt', 'strong', 'gt', 'issue', 'on', 'large', 'scale', 'data', 'because', 'semantic', 'analysis', 'concerns', 'lt', 'strong', 'gt', 'lt', 'em', 'gt', 'n', 'dimension', 'lt', 'em', 'gt', 'lt', 'strong', 'gt', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'last', 'year', 'lt', 'a', 'href', 'quot', 'http', 'www', 'csie', 'ntu', 'edu', 'tw', 'cjlin', 'liblinear', 'quot', 'gt', 'liblinear', 'lt', 'a', 'gt', 'was', 'release', 'and', 'it', 'can', 'solve', 'performance', 'bottleneck', 'xa', 'but', 'it', 'cost', 'too', 'much', 'lt', 'strong', 'gt', 'memory', 'lt', 'strong', 'gt', 'is', 'lt', 'strong', 'gt', 'mapreduce', 'lt', 'strong', 'gt', 'the', 'only', 'way', 'to', 'solve', 'semantic', 'analysis', 'problem', 'on', 'big', 'data', 'or', 'are', 'there', 'any', 'other', 'methods', 'that', 'can', 'improve', 'memory', 'bottleneck', 'on', 'lt', 'strong', 'gt', 'liblinear', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lots', 'of', 'people', 'use', 'the', 'term', 'lt', 'em', 'gt', 'big', 'data', 'lt', 'em', 'gt', 'in', 'a', 'rather', 'lt', 'em', 'gt', 'commercial', 'lt', 'em', 'gt', 'way', 'as', 'a', 'means', 'of', 'indicating', 'that', 'large', 'datasets', 'are', 'involved', 'in', 'the', 'computation', 'and', 'therefore', 'potential', 'solutions', 'must', 'have', 'good', 'performance', 'of', 'course', 'lt', 'em', 'gt', 'big', 'data', 'lt', 'em', 'gt', 'always', 'carry', 'associated', 'terms', 'like', 'scalability', 'and', 'efficiency', 'but', 'what', 'exactly', 'defines', 'a', 'problem', 'as', 'a', 'lt', 'em', 'gt', 'big', 'data', 'lt', 'em', 'gt', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'the', 'computation', 'have', 'to', 'be', 'related', 'to', 'some', 'set', 'of', 'specific', 'purposes', 'like', 'data', 'mining', 'information', 'retrieval', 'or', 'could', 'an', 'algorithm', 'for', 'general', 'graph', 'problems', 'be', 'labeled', 'lt', 'em', 'gt', 'big', 'data', 'lt', 'em', 'gt', 'if', 'the', 'dataset', 'was', 'lt', 'em', 'gt', 'big', 'enough', 'lt', 'em', 'gt', 'also', 'how', 'lt', 'em', 'gt', 'big', 'lt', 'em', 'gt', 'is', 'lt', 'em', 'gt', 'big', 'enough', 'lt', 'em', 'gt', 'if', 'this', 'is', 'possible', 'to', 'define', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 'created', 'this', 'social', 'network', 'application', 'for', 'elearning', 'purposes', 'it', 's', 'an', 'experimental', 'thing', 'we', 'are', 'researching', 'on', 'in', 'our', 'lab', 'it', 'has', 'been', 'used', 'by', 'some', 'case', 'studies', 'for', 'a', 'while', 'and', 'the', 'data', 'on', 'our', 'relational', 'dbms', 'sql', 'server', 'is', 'getting', 'big', 'it', 's', 'a', 'few', 'gigabytes', 'now', 'and', 'the', 'tables', 'are', 'highly', 'connected', 'to', 'each', 'other', 'the', 'performance', 'is', 'still', 'fine', 'but', 'when', 'should', 'we', 'consider', 'other', 'options', 'is', 'it', 'the', 'matter', 'of', 'performance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'my', 'data', 'set', 'contains', 'a', 'number', 'of', 'numeric', 'attributes', 'and', 'one', 'categorical', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'say', 'lt', 'code', 'gt', 'numericattr', 'numericattr', 'numericattrn', 'categoricalattr', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'lt', 'code', 'gt', 'categoricalattr', 'lt', 'code', 'gt', 'takes', 'one', 'of', 'three', 'possible', 'values', 'lt', 'code', 'gt', 'categoricalattrvalue', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'categoricalattrvalue', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'categoricalattrvalue', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'using', 'default', 'k', 'means', 'clustering', 'algorithm', 'implementation', 'for', 'octave', 'lt', 'a', 'href', 'quot', 'https', 'blog', 'west', 'uni', 'koblenz', 'de', 'a', 'working', 'k', 'means', 'code', 'for', 'octave', 'quot', 'gt', 'https', 'blog', 'west', 'uni', 'koblenz', 'de', 'a', 'working', 'k', 'means', 'code', 'for', 'octave', 'lt', 'a', 'gt', 'xa', 'it', 'works', 'with', 'numeric', 'data', 'only', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'my', 'question', 'is', 'it', 'correct', 'to', 'split', 'the', 'categorical', 'attribute', 'lt', 'code', 'gt', 'categoricalattr', 'lt', 'code', 'gt', 'into', 'three', 'numeric', 'binary', 'variables', 'like', 'lt', 'code', 'gt', 'iscategoricalattrvalue', 'iscategoricalattrvalue', 'iscategoricalattrvalue', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'bunch', 'of', 'customer', 'profiles', 'stored', 'in', 'a', 'lt', 'a', 'href', 'quot', 'questions', 'tagged', 'elasticsearch', 'quot', 'class', 'quot', 'post', 'tag', 'quot', 'title', 'quot', 'show', 'questions', 'tagged', 'amp', 'elasticsearch', 'amp', 'quot', 'rel', 'quot', 'tag', 'quot', 'gt', 'elasticsearch', 'lt', 'a', 'gt', 'cluster', 'these', 'profiles', 'are', 'now', 'used', 'for', 'creation', 'of', 'target', 'groups', 'for', 'our', 'email', 'subscriptions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'target', 'groups', 'are', 'now', 'formed', 'manually', 'using', 'elasticsearch', 'faceted', 'search', 'capabilities', 'like', 'get', 'all', 'male', 'customers', 'of', 'age', 'with', 'one', 'car', 'and', 'children', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'could', 'i', 'search', 'for', 'interesting', 'groups', 'lt', 'strong', 'gt', 'automatically', 'lt', 'strong', 'gt', 'using', 'data', 'science', 'machine', 'learning', 'clustering', 'or', 'something', 'else', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'questions', 'tagged', 'r', 'quot', 'class', 'quot', 'post', 'tag', 'quot', 'title', 'quot', 'show', 'questions', 'tagged', 'amp', 'r', 'amp', 'quot', 'rel', 'quot', 'tag', 'quot', 'gt', 'r', 'lt', 'a', 'gt', 'programming', 'language', 'seems', 'to', 'be', 'a', 'good', 'tool', 'for', 'this', 'task', 'but', 'i', 'can', 't', 'form', 'a', 'methodology', 'of', 'such', 'group', 'search', 'one', 'solution', 'is', 'to', 'somehow', 'find', 'the', 'largest', 'clusters', 'of', 'customers', 'and', 'use', 'them', 'as', 'target', 'groups', 'so', 'the', 'question', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'how', 'can', 'i', 'automatically', 'choose', 'largest', 'clusters', 'of', 'similar', 'customers', 'similar', 'by', 'parameters', 'that', 'i', 'don', 't', 'know', 'at', 'this', 'moment', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'my', 'program', 'will', 'connect', 'to', 'elasticsearch', 'offload', 'customer', 'data', 'to', 'csv', 'and', 'using', 'r', 'language', 'script', 'will', 'find', 'that', 'large', 'portion', 'of', 'customers', 'are', 'male', 'with', 'no', 'children', 'and', 'another', 'large', 'portion', 'of', 'customers', 'have', 'a', 'car', 'and', 'their', 'eye', 'color', 'is', 'brown', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'working', 'on', 'exploratory', 'data', 'analysis', 'and', 'developing', 'algorithms', 'i', 'find', 'that', 'most', 'of', 'my', 'time', 'is', 'spent', 'in', 'a', 'cycle', 'of', 'visualize', 'write', 'some', 'code', 'run', 'on', 'small', 'dataset', 'repeat', 'the', 'data', 'i', 'have', 'tends', 'to', 'be', 'computer', 'vision', 'sensor', 'fusion', 'type', 'stuff', 'and', 'algorithms', 'are', 'vision', 'heavy', 'for', 'example', 'object', 'detection', 'and', 'tracking', 'etc', 'and', 'the', 'off', 'the', 'shelf', 'algorithms', 'don', 't', 'work', 'in', 'this', 'context', 'i', 'find', 'that', 'this', 'takes', 'a', 'lot', 'of', 'iterations', 'for', 'example', 'to', 'dial', 'in', 'the', 'type', 'of', 'algorithm', 'or', 'tune', 'the', 'parameters', 'in', 'the', 'algorithm', 'or', 'to', 'get', 'a', 'visualization', 'right', 'and', 'also', 'the', 'run', 'times', 'even', 'on', 'a', 'small', 'dataset', 'are', 'quite', 'long', 'so', 'all', 'together', 'it', 'takes', 'a', 'while', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'the', 'algorithm', 'development', 'itself', 'be', 'sped', 'up', 'and', 'made', 'more', 'scalable', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'specific', 'challenges', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'the', 'number', 'of', 'iterations', 'be', 'reduced', 'esp', 'when', 'what', 'kind', 'of', 'algorithm', 'let', 'alone', 'the', 'specifics', 'of', 'it', 'does', 'not', 'seem', 'to', 'be', 'easily', 'foreseeable', 'without', 'trying', 'different', 'versions', 'and', 'examining', 'their', 'behavior', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'run', 'on', 'bigger', 'datasets', 'during', 'development', 'often', 'going', 'from', 'small', 'to', 'large', 'dataset', 'is', 'when', 'a', 'bunch', 'of', 'new', 'behavior', 'and', 'new', 'issues', 'is', 'seen', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'algorithm', 'parameters', 'be', 'tuned', 'faster', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'apply', 'machine', 'learning', 'type', 'tools', 'to', 'algorithm', 'development', 'itself', 'for', 'example', 'instead', 'of', 'writing', 'the', 'algorithm', 'by', 'hand', 'write', 'some', 'simple', 'building', 'blocks', 'and', 'combine', 'them', 'in', 'a', 'way', 'learned', 'from', 'the', 'problem', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'heard', 'about', 'many', 'tools', 'frameworks', 'for', 'helping', 'people', 'to', 'process', 'their', 'data', 'big', 'data', 'environment', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'is', 'called', 'hadoop', 'and', 'the', 'other', 'is', 'the', 'nosql', 'concept', 'what', 'is', 'the', 'difference', 'in', 'point', 'of', 'processing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'they', 'complementary', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'r', 'has', 'many', 'libraries', 'which', 'are', 'aimed', 'at', 'data', 'analysis', 'e', 'g', 'jags', 'bugs', 'arules', 'etc', 'and', 'is', 'mentioned', 'in', 'popular', 'textbooks', 'such', 'as', 'j', 'krusche', 'doing', 'bayesian', 'data', 'analysis', 'b', 'lantz', 'quot', 'machine', 'learning', 'with', 'r', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'seen', 'a', 'guideline', 'of', 'tb', 'for', 'a', 'dataset', 'to', 'be', 'considered', 'as', 'big', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'is', 'r', 'suitable', 'for', 'the', 'amount', 'of', 'data', 'typically', 'seen', 'in', 'big', 'data', 'problems', 'xa', 'are', 'there', 'strategies', 'to', 'be', 'employed', 'when', 'using', 'r', 'with', 'this', 'size', 'of', 'dataset', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'an', 'r', 'script', 'that', 'generates', 'a', 'report', 'based', 'on', 'the', 'current', 'contents', 'of', 'a', 'database', 'this', 'database', 'is', 'constantly', 'in', 'flux', 'with', 'records', 'being', 'added', 'deleted', 'many', 'times', 'each', 'day', 'how', 'can', 'i', 'ask', 'my', 'computer', 'to', 'run', 'this', 'every', 'night', 'at', 'am', 'so', 'that', 'i', 'have', 'an', 'up', 'to', 'date', 'report', 'waiting', 'for', 'me', 'in', 'the', 'morning', 'or', 'perhaps', 'i', 'want', 'it', 'to', 're', 'run', 'once', 'a', 'certain', 'number', 'of', 'new', 'records', 'have', 'been', 'added', 'to', 'the', 'database', 'how', 'might', 'i', 'go', 'about', 'automating', 'this', 'i', 'should', 'mention', 'i', 'm', 'on', 'windows', 'but', 'i', 'could', 'easily', 'put', 'this', 'script', 'on', 'my', 'linux', 'machine', 'if', 'that', 'would', 'simplify', 'the', 'process', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'from', 'my', 'limited', 'dabbling', 'with', 'data', 'science', 'using', 'r', 'i', 'realized', 'that', 'cleaning', 'bad', 'data', 'is', 'a', 'very', 'important', 'part', 'of', 'preparing', 'data', 'for', 'analysis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'any', 'best', 'practices', 'or', 'processes', 'for', 'cleaning', 'data', 'before', 'processing', 'it', 'if', 'so', 'are', 'there', 'any', 'automated', 'or', 'semi', 'automated', 'tools', 'which', 'implement', 'some', 'of', 'these', 'best', 'practices', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'reviewing', 'lt', 'a', 'href', 'quot', 'http', 'rads', 'stackoverflow', 'com', 'amzn', 'click', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'applied', 'predictive', 'modeling', 'lt', 'a', 'gt', 'quot', 'a', 'lt', 'a', 'href', 'quot', 'http', 'www', 'information', 'management', 'com', 'blogs', 'applied', 'predictive', 'modeling', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'reviewer', 'states', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'one', 'critique', 'i', 'have', 'of', 'statistical', 'learning', 'sl', 'pedagogy', 'is', 'the', 'xa', 'absence', 'of', 'computation', 'performance', 'considerations', 'in', 'the', 'evaluation', 'of', 'xa', 'different', 'modeling', 'techniques', 'with', 'its', 'emphases', 'on', 'bootstrapping', 'and', 'xa', 'cross', 'validation', 'to', 'tune', 'test', 'models', 'sl', 'is', 'quite', 'compute', 'intensive', 'xa', 'add', 'to', 'that', 'the', 're', 'sampling', 'that', 's', 'embedded', 'in', 'techniques', 'like', 'bagging', 'xa', 'and', 'boosting', 'and', 'you', 'have', 'the', 'specter', 'of', 'computation', 'hell', 'for', 'xa', 'supervised', 'learning', 'of', 'large', 'data', 'sets', 'lt', 'strong', 'gt', 'in', 'fact', 'r', 's', 'memory', 'xa', 'constraints', 'impose', 'pretty', 'severe', 'limits', 'on', 'the', 'size', 'of', 'models', 'that', 'can', 'xa', 'be', 'fit', 'by', 'top', 'performing', 'methods', 'like', 'random', 'forests', 'lt', 'strong', 'gt', 'though', 'sl', 'does', 'a', 'xa', 'good', 'job', 'calibrating', 'model', 'performance', 'against', 'small', 'data', 'sets', 'it', 'd', 'xa', 'sure', 'be', 'nice', 'to', 'understand', 'performance', 'versus', 'computational', 'cost', 'for', 'xa', 'larger', 'data', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'r', 's', 'memory', 'constraints', 'and', 'do', 'they', 'impose', 'severe', 'limits', 'on', 'the', 'size', 'of', 'models', 'that', 'can', 'be', 'fit', 'by', 'top', 'performing', 'methods', 'like', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'random_forest', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'random', 'forests', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'logic', 'often', 'states', 'that', 'by', 'overfitting', 'a', 'model', 'its', 'capacity', 'to', 'generalize', 'is', 'limited', 'though', 'this', 'might', 'only', 'mean', 'that', 'overfitting', 'stops', 'a', 'model', 'from', 'improving', 'after', 'a', 'certain', 'complexity', 'does', 'overfitting', 'cause', 'models', 'to', 'become', 'worse', 'regardless', 'of', 'the', 'complexity', 'of', 'data', 'and', 'if', 'so', 'why', 'is', 'this', 'the', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'related', 'lt', 'strong', 'gt', 'followup', 'to', 'the', 'question', 'above', 'quot', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'when', 'is', 'a', 'model', 'underfitted', 'quot', 'gt', 'when', 'is', 'a', 'model', 'underfitted', 'lt', 'a', 'gt', 'quot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'first', 'think', 'it', 's', 'worth', 'me', 'stating', 'what', 'i', 'mean', 'by', 'replication', 'amp', 'amp', 'reproducibility', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'replication', 'of', 'analysis', 'a', 'results', 'in', 'an', 'exact', 'copy', 'of', 'all', 'inputs', 'and', 'processes', 'that', 'are', 'supply', 'and', 'result', 'in', 'incidental', 'outputs', 'in', 'analysis', 'b', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'reproducibility', 'of', 'analysis', 'a', 'results', 'in', 'inputs', 'processes', 'and', 'outputs', 'that', 'are', 'semantically', 'incidental', 'to', 'analysis', 'a', 'without', 'access', 'to', 'the', 'exact', 'inputs', 'and', 'processes', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'putting', 'aside', 'how', 'easy', 'it', 'might', 'be', 'to', 'replicate', 'a', 'given', 'build', 'especially', 'an', 'ad', 'hoc', 'one', 'to', 'me', 'replication', 'always', 'possible', 'if', 'it', 's', 'planned', 'for', 'and', 'worth', 'doing', 'that', 'said', 'it', 'is', 'unclear', 'to', 'me', 'is', 'how', 'to', 'execute', 'a', 'data', 'science', 'workflow', 'that', 'allows', 'for', 'reproducibility', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'closet', 'comparison', 'i', 'm', 'able', 'to', 'think', 'of', 'is', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'documentation_generator', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'documentation', 'generators', 'lt', 'a', 'gt', 'that', 'generates', 'software', 'documentation', 'intended', 'for', 'programmers', 'though', 'the', 'main', 'difference', 'i', 'see', 'is', 'that', 'in', 'theory', 'if', 'two', 'sets', 'of', 'analysis', 'ran', 'the', 'quot', 'reproducibility', 'documentation', 'generators', 'quot', 'the', 'documentation', 'should', 'match', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'issue', 'is', 'that', 'while', 'i', 'get', 'the', 'concept', 'of', 'reproducibility', 'documentation', 'i', 'am', 'having', 'a', 'hard', 'time', 'imagining', 'what', 'it', 'would', 'look', 'like', 'in', 'usable', 'form', 'without', 'just', 'being', 'a', 'guide', 'to', 'replicating', 'the', 'analysis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lastly', 'whole', 'intent', 'of', 'this', 'is', 'to', 'understand', 'if', 'it', 's', 'possible', 'to', 'quot', 'bake', 'in', 'quot', 'reproducibility', 'documentation', 'as', 'you', 'build', 'out', 'a', 'stack', 'not', 'after', 'the', 'stack', 'is', 'built', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'is', 'it', 'possible', 'to', 'automate', 'generating', 'reproducibility', 'documentation', 'and', 'if', 'so', 'how', 'and', 'what', 'would', 'it', 'look', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'lt', 'em', 'gt', 'update', 'lt', 'strong', 'gt', 'please', 'note', 'that', 'this', 'is', 'the', 'second', 'draft', 'of', 'this', 'question', 'and', 'that', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'users', 'christopher', 'louden', 'quot', 'gt', 'christopher', 'louden', 'lt', 'a', 'gt', 'was', 'kind', 'enough', 'to', 'let', 'me', 'edit', 'the', 'question', 'after', 'i', 'realized', 'it', 'was', 'likely', 'the', 'first', 'draft', 'was', 'unclear', 'thanks', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'the', 'data', 'conditions', 'that', 'we', 'should', 'watch', 'out', 'for', 'where', 'p', 'values', 'may', 'not', 'be', 'the', 'best', 'way', 'of', 'deciding', 'statistical', 'significance', 'are', 'there', 'specific', 'problem', 'types', 'that', 'fall', 'into', 'this', 'category', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'if', 'small', 'p', 'values', 'are', 'plentiful', 'in', 'big', 'data', 'what', 'is', 'a', 'comparable', 'replacement', 'for', 'p', 'values', 'in', 'data', 'with', 'million', 'of', 'samples', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'em', 'gt', 'note', 'pulled', 'this', 'question', 'from', 'the', 'lt', 'a', 'href', 'quot', 'http', 'area', 'stackexchange', 'com', 'proposals', 'data', 'science', 'quot', 'gt', 'list', 'of', 'questions', 'in', 'area', 'lt', 'a', 'gt', 'but', 'believe', 'the', 'question', 'is', 'self', 'explanatory', 'that', 'said', 'believe', 'i', 'get', 'the', 'general', 'intent', 'of', 'the', 'question', 'and', 'as', 'a', 'result', 'likely', 'able', 'to', 'field', 'any', 'questions', 'on', 'the', 'question', 'that', 'might', 'pop', 'up', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'which', 'big', 'data', 'technology', 'stack', 'is', 'most', 'suitable', 'for', 'processing', 'tweets', 'extracting', 'expanding', 'urls', 'and', 'pushing', 'only', 'new', 'links', 'into', 'rd', 'party', 'system', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'strong', 'gt', 'background', 'lt', 'strong', 'gt', 'following', 'is', 'from', 'the', 'book', 'lt', 'a', 'href', 'quot', 'http', 'rads', 'stackoverflow', 'com', 'amzn', 'click', 'quot', 'gt', 'graph', 'databases', 'lt', 'a', 'gt', 'which', 'covers', 'a', 'performance', 'test', 'mentioned', 'in', 'the', 'book', 'lt', 'a', 'href', 'quot', 'http', 'rads', 'stackoverflow', 'com', 'amzn', 'click', 'quot', 'gt', 'neo', 'j', 'in', 'action', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'relationships', 'in', 'a', 'graph', 'naturally', 'form', 'paths', 'querying', 'or', 'xa', 'traversing', 'the', 'graph', 'involves', 'following', 'paths', 'because', 'of', 'the', 'xa', 'fundamentally', 'path', 'oriented', 'nature', 'of', 'the', 'datamodel', 'the', 'majority', 'of', 'xa', 'path', 'based', 'graph', 'database', 'operations', 'are', 'highly', 'aligned', 'with', 'the', 'way', 'xa', 'in', 'which', 'the', 'data', 'is', 'laid', 'out', 'making', 'them', 'extremely', 'efficient', 'in', 'xa', 'their', 'book', 'neo', 'j', 'in', 'action', 'partner', 'and', 'vukotic', 'perform', 'an', 'experiment', 'xa', 'using', 'a', 'relational', 'store', 'and', 'neo', 'j', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'comparison', 'shows', 'that', 'the', 'graph', 'database', 'is', 'substantially', 'quicker', 'xa', 'for', 'connected', 'data', 'than', 'a', 'relational', 'store', 'partner', 'and', 'vukotic', 's', 'xa', 'experiment', 'seeks', 'to', 'find', 'friends', 'of', 'friends', 'in', 'a', 'social', 'network', 'to', 'a', 'xa', 'maximum', 'depth', 'of', 'five', 'given', 'any', 'two', 'persons', 'chosen', 'at', 'random', 'is', 'xa', 'there', 'a', 'path', 'that', 'connects', 'them', 'which', 'is', 'at', 'most', 'five', 'relationships', 'xa', 'long', 'for', 'a', 'social', 'network', 'containing', 'people', 'each', 'with', 'xa', 'approximately', 'friends', 'the', 'results', 'strongly', 'suggest', 'that', 'graph', 'xa', 'databases', 'are', 'the', 'best', 'choice', 'for', 'connected', 'data', 'as', 'we', 'see', 'in', 'table', 'xa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'table', 'finding', 'extended', 'friends', 'in', 'a', 'relational', 'database', 'versus', 'efficient', 'finding', 'in', 'neo', 'j', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'depth', 'rdbms', 'execution', 'time', 's', 'neo', 'j', 'execution', 'time', 's', 'records', 'returned', 'xa', 'xa', 'xa', 'xa', 'unfinished', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'depth', 'two', 'friends', 'of', 'friends', 'both', 'the', 'relational', 'database', 'and', 'the', 'graph', 'database', 'perform', 'well', 'enough', 'for', 'us', 'to', 'consider', 'using', 'them', 'in', 'an', 'online', 'system', 'while', 'the', 'neo', 'j', 'query', 'runs', 'in', 'two', 'thirds', 'the', 'time', 'of', 'the', 'relational', 'one', 'an', 'end', 'user', 'would', 'barely', 'notice', 'the', 'the', 'difference', 'in', 'milliseconds', 'between', 'the', 'two', 'by', 'the', 'time', 'we', 'reach', 'depth', 'three', 'friend', 'of', 'friend', 'of', 'friend', 'however', 'it', 's', 'clear', 'that', 'the', 'relational', 'database', 'can', 'no', 'longer', 'deal', 'with', 'the', 'query', 'in', 'a', 'reasonable', 'timeframe', 'the', 'thirty', 'seconds', 'it', 'takes', 'to', 'complete', 'would', 'be', 'completely', 'unacceptable', 'for', 'an', 'online', 'system', 'in', 'contrast', 'neo', 'j', 's', 'response', 'time', 'remains', 'relatively', 'flat', 'just', 'a', 'fraction', 'of', 'a', 'second', 'to', 'perform', 'the', 'query', 'definitely', 'quick', 'enough', 'for', 'an', 'online', 'system', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'depth', 'four', 'the', 'relational', 'database', 'exhibits', 'crippling', 'latency', 'xa', 'making', 'it', 'practically', 'useless', 'for', 'an', 'online', 'system', 'neo', 'j', 's', 'timings', 'xa', 'have', 'deteriorated', 'a', 'little', 'too', 'but', 'the', 'latency', 'here', 'is', 'at', 'the', 'xa', 'periphery', 'of', 'being', 'acceptable', 'for', 'a', 'responsive', 'online', 'system', 'finally', 'xa', 'at', 'depth', 'five', 'the', 'relational', 'database', 'simply', 'takes', 'too', 'long', 'to', 'xa', 'complete', 'the', 'query', 'neo', 'j', 'in', 'contrast', 'returns', 'a', 'result', 'in', 'around', 'two', 'xa', 'seconds', 'at', 'depth', 'five', 'it', 'transpires', 'almost', 'the', 'entire', 'network', 'is', 'our', 'xa', 'friend', 'for', 'many', 'real', 'world', 'use', 'cases', 'we', 'd', 'likely', 'trim', 'the', 'results', 'xa', 'and', 'the', 'timings', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'questions', 'are', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'this', 'a', 'reasonable', 'test', 'to', 'emulate', 'what', 'one', 'might', 'except', 'to', 'find', 'in', 'a', 'social', 'network', 'lt', 'em', 'gt', 'meaning', 'do', 'real', 'social', 'networks', 'normally', 'have', 'nodes', 'with', 'approximately', 'friends', 'for', 'example', 'seems', 'like', 'the', 'quot', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'the_rich_get_richer_', 'statistics', 'quot', 'gt', 'rich', 'get', 'richer', 'lt', 'a', 'gt', 'quot', 'model', 'would', 'be', 'more', 'natural', 'for', 'social', 'networks', 'though', 'might', 'be', 'wrong', 'lt', 'em', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'regardless', 'of', 'the', 'naturalness', 'of', 'the', 'emulation', 'is', 'there', 'any', 'reason', 'to', 'believe', 'the', 'results', 'are', 'off', 'or', 'unreproducible', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'are', 'the', 'difference', 's', 'between', 'parallel', 'and', 'distributed', 'computing', 'when', 'it', 'comes', 'to', 'scalability', 'and', 'efficiency', 'it', 'is', 'very', 'common', 'to', 'see', 'solutions', 'dealing', 'with', 'computations', 'in', 'clusters', 'of', 'machines', 'and', 'sometimes', 'it', 'is', 'referred', 'to', 'as', 'a', 'parallel', 'processing', 'or', 'as', 'distributed', 'processing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'a', 'certain', 'way', 'the', 'computation', 'seems', 'to', 'be', 'always', 'parallel', 'since', 'there', 'are', 'things', 'running', 'concurrently', 'but', 'is', 'the', 'distributed', 'computation', 'simply', 'related', 'to', 'the', 'use', 'of', 'more', 'than', 'one', 'machine', 'or', 'are', 'there', 'any', 'further', 'specificities', 'that', 'distinguishes', 'these', 'two', 'kinds', 'of', 'processing', 'wouldn', 't', 'it', 'be', 'redundant', 'to', 'say', 'for', 'example', 'that', 'a', 'computation', 'is', 'lt', 'em', 'gt', 'parallel', 'and', 'distributed', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'given', 'website', 'access', 'data', 'in', 'the', 'form', 'lt', 'code', 'gt', 'session_id', 'ip', 'user_agent', 'lt', 'code', 'gt', 'and', 'optionally', 'timestamp', 'following', 'the', 'conditions', 'below', 'how', 'would', 'you', 'best', 'cluster', 'the', 'sessions', 'into', 'unique', 'visitors', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'session_id', 'lt', 'code', 'gt', 'is', 'an', 'id', 'given', 'to', 'every', 'new', 'visitor', 'it', 'does', 'not', 'expire', 'however', 'if', 'the', 'user', 'doesn', 't', 'accept', 'cookies', 'clears', 'cookies', 'changes', 'browser', 'changes', 'device', 'he', 'will', 'not', 'be', 'recognised', 'anymore', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'ip', 'lt', 'code', 'gt', 'can', 'be', 'shared', 'between', 'different', 'users', 'imagine', 'a', 'free', 'wi', 'fi', 'cafe', 'or', 'your', 'isp', 'reassigning', 'ips', 'and', 'they', 'will', 'often', 'have', 'at', 'least', 'home', 'and', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'user_agent', 'lt', 'code', 'gt', 'is', 'the', 'browser', 'os', 'version', 'allowing', 'to', 'distinguish', 'between', 'devices', 'for', 'example', 'a', 'user', 'is', 'likely', 'to', 'use', 'both', 'phone', 'and', 'laptop', 'but', 'is', 'unlikely', 'to', 'use', 'windows', 'apple', 'laptops', 'it', 'is', 'unlikely', 'that', 'the', 'same', 'session', 'id', 'has', 'multiple', 'useragents', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'data', 'might', 'look', 'as', 'the', 'fiddle', 'here', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'sqlfiddle', 'com', 'c', 'de', 'quot', 'gt', 'http', 'sqlfiddle', 'com', 'c', 'de', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'of', 'course', 'we', 'are', 'talking', 'about', 'assumptions', 'but', 'it', 's', 'about', 'getting', 'as', 'close', 'to', 'reality', 'as', 'possible', 'for', 'example', 'if', 'we', 'encounter', 'the', 'same', 'ip', 'and', 'useragent', 'in', 'a', 'limited', 'time', 'frame', 'with', 'a', 'different', 'session_id', 'it', 'would', 'be', 'a', 'fair', 'assumption', 'that', 'it', 's', 'the', 'same', 'user', 'with', 'some', 'edge', 'case', 'exceptions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'language', 'in', 'which', 'the', 'problem', 'is', 'solved', 'is', 'irellevant', 'it', 's', 'mostly', 'about', 'logic', 'and', 'not', 'implementation', 'pseudocode', 'is', 'fine', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'due', 'to', 'the', 'slow', 'nature', 'of', 'the', 'fiddle', 'you', 'can', 'alternatively', 'read', 'run', 'the', 'mysql', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'select', 'session_id', 'floor', 'rand', 'as', 'ip_num', 'floor', 'rand', 'as', 'user_agent_id', 'xa', 'from', 'xa', 'select', 'a', 'nr', 'b', 'nr', 'as', 'session_id', 'ceil', 'rand', 'as', 'nr', 'xa', 'from', 'xa', 'select', 'as', 'nr', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'xa', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'a', 'xa', 'join', 'xa', 'select', 'as', 'nr', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'xa', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'b', 'xa', 'order', 'by', 'xa', 'd', 'xa', 'inner', 'join', 'xa', 'select', 'as', 'nr', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'xa', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'union', 'all', 'select', 'e', 'xa', 'on', 'd', 'nr', 'amp', 'gt', 'e', 'nr', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'for', 'example', 'when', 'searching', 'something', 'in', 'google', 'results', 'return', 'nigh', 'instantly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'understand', 'that', 'google', 'sorts', 'and', 'indexes', 'pages', 'with', 'algorithms', 'etc', 'but', 'i', 'imagine', 'it', 'infeasible', 'for', 'the', 'results', 'of', 'every', 'single', 'possible', 'query', 'to', 'be', 'indexed', 'and', 'results', 'are', 'personalized', 'which', 'renders', 'this', 'even', 'more', 'infeasible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'moreover', 'wouldn', 't', 'the', 'hardware', 'latency', 'in', 'google', 's', 'hardware', 'be', 'huge', 'even', 'if', 'the', 'data', 'in', 'google', 'were', 'all', 'stored', 'in', 'tb', 's', 'ssds', 'i', 'imagine', 'the', 'hardware', 'latency', 'to', 'be', 'huge', 'given', 'the', 'sheer', 'amount', 'of', 'data', 'to', 'process', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'mapreduce', 'help', 'solve', 'this', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'okay', 'so', 'i', 'understand', 'that', 'popular', 'searches', 'can', 'be', 'cached', 'in', 'memory', 'but', 'what', 'about', 'unpopular', 'searches', 'even', 'for', 'the', 'most', 'obscure', 'search', 'i', 'have', 'conducted', 'i', 'don', 't', 'think', 'the', 'search', 'has', 'ever', 'been', 'reported', 'to', 'be', 'larger', 'than', 'seconds', 'how', 'is', 'this', 'possible', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'while', 'building', 'a', 'rank', 'say', 'for', 'a', 'search', 'engine', 'or', 'a', 'recommendation', 'system', 'is', 'it', 'valid', 'to', 'rely', 'on', 'click', 'frequency', 'to', 'determine', 'the', 'relevance', 'of', 'an', 'entry', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'best', 'nosql', 'backend', 'to', 'use', 'for', 'a', 'mobile', 'game', 'users', 'can', 'make', 'a', 'lot', 'of', 'servers', 'requests', 'it', 'needs', 'also', 'to', 'retrieve', 'users', 'historical', 'records', 'like', 'app', 'purchasing', 'and', 'analytics', 'of', 'usage', 'behavior', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'assume', 'that', 'we', 'have', 'a', 'set', 'of', 'elements', 'lt', 'em', 'gt', 'e', 'lt', 'em', 'gt', 'and', 'a', 'similarity', 'lt', 'strong', 'gt', 'not', 'distance', 'lt', 'strong', 'gt', 'function', 'lt', 'em', 'gt', 'sim', 'ei', 'ej', 'lt', 'em', 'gt', 'between', 'two', 'elements', 'lt', 'em', 'gt', 'ei', 'ej', 'e', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'could', 'we', 'efficiently', 'cluster', 'the', 'elements', 'of', 'lt', 'em', 'gt', 'e', 'lt', 'em', 'gt', 'using', 'lt', 'em', 'gt', 'sim', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'em', 'gt', 'k', 'lt', 'em', 'gt', 'means', 'for', 'example', 'requires', 'a', 'given', 'lt', 'em', 'gt', 'k', 'lt', 'em', 'gt', 'canopy', 'clustering', 'requires', 'two', 'threshold', 'values', 'what', 'if', 'we', 'don', 't', 'want', 'such', 'predefined', 'parameters', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'that', 'lt', 'em', 'gt', 'sim', 'lt', 'em', 'gt', 'is', 'not', 'neccessarily', 'a', 'metric', 'i', 'e', 'the', 'triangle', 'inequality', 'may', 'or', 'may', 'not', 'hold', 'moreover', 'it', 'doesn', 't', 'matter', 'if', 'the', 'clusters', 'are', 'disjoint', 'partitions', 'of', 'lt', 'em', 'gt', 'e', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'consider', 'a', 'stream', 'containing', 'lt', 'a', 'href', 'quot', 'http', 'en', 'm', 'wikipedia', 'org', 'wiki', 'tuple', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'tuples', 'lt', 'a', 'gt', 'lt', 'code', 'gt', 'user', 'new_score', 'lt', 'code', 'gt', 'representing', 'users', 'scores', 'in', 'an', 'online', 'game', 'the', 'stream', 'could', 'have', 'new', 'elements', 'per', 'second', 'the', 'game', 'has', 'k', 'to', 'k', 'unique', 'players', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'have', 'some', 'standing', 'queries', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'which', 'players', 'posted', 'more', 'than', 'x', 'scores', 'in', 'a', 'sliding', 'window', 'of', 'one', 'hour', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'which', 'players', 'gained', 'x', 'score', 'in', 'a', 'sliding', 'window', 'of', 'one', 'hour', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'which', 'open', 'source', 'tools', 'can', 'i', 'employ', 'to', 'jumpstart', 'this', 'project', 'i', 'am', 'considering', 'lt', 'a', 'href', 'quot', 'http', 'esper', 'codehaus', 'org', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'esper', 'lt', 'a', 'gt', 'at', 'the', 'moment', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'i', 'have', 'just', 'completed', 'reading', 'quot', 'mining', 'data', 'streams', 'quot', 'chapter', 'of', 'lt', 'a', 'href', 'quot', 'http', 'infolab', 'stanford', 'edu', 'ullman', 'mmds', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'mining', 'of', 'massive', 'datasets', 'lt', 'a', 'gt', 'and', 'i', 'am', 'quite', 'new', 'to', 'mining', 'data', 'streams', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'when', 'a', 'relational', 'database', 'like', 'mysql', 'has', 'better', 'performance', 'than', 'a', 'no', 'relational', 'like', 'mongodb', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'saw', 'a', 'question', 'on', 'quora', 'other', 'day', 'about', 'why', 'quora', 'still', 'uses', 'mysql', 'as', 'their', 'backend', 'and', 'that', 'their', 'performance', 'is', 'still', 'good', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'if', 'i', 'have', 'a', 'very', 'long', 'list', 'of', 'paper', 'names', 'how', 'could', 'i', 'get', 'abstract', 'of', 'these', 'papers', 'from', 'internet', 'or', 'any', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'paper', 'names', 'are', 'like', 'quot', 'assessment', 'of', 'utility', 'in', 'web', 'mining', 'for', 'the', 'domain', 'of', 'public', 'health', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'any', 'one', 'know', 'any', 'api', 'that', 'can', 'give', 'me', 'a', 'solution', 'i', 'tried', 'to', 'crawl', 'google', 'scholar', 'however', 'google', 'blocked', 'my', 'crawler', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'database', 'from', 'my', 'facebook', 'application', 'and', 'i', 'am', 'trying', 'to', 'use', 'machine', 'learning', 'to', 'estimate', 'users', 'age', 'based', 'on', 'what', 'facebook', 'sites', 'they', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'three', 'crucial', 'characteristics', 'of', 'my', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'the', 'age', 'distribution', 'in', 'my', 'training', 'set', 'k', 'of', 'users', 'in', 'sum', 'is', 'skewed', 'towards', 'younger', 'users', 'i', 'e', 'i', 'have', 'users', 'aged', 'and', 'users', 'aged', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'many', 'sites', 'have', 'no', 'more', 'than', 'likers', 'i', 'filtered', 'out', 'the', 'fb', 'sites', 'with', 'less', 'than', 'likers', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'there', 's', 'many', 'more', 'features', 'than', 'samples', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'my', 'questions', 'are', 'what', 'strategy', 'would', 'you', 'suggest', 'to', 'prepare', 'the', 'data', 'for', 'further', 'analysis', 'should', 'i', 'perform', 'some', 'sort', 'of', 'dimensionality', 'reduction', 'which', 'ml', 'method', 'would', 'be', 'most', 'appropriate', 'to', 'use', 'in', 'this', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'mainly', 'use', 'python', 'so', 'python', 'specific', 'hints', 'would', 'be', 'greatly', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'learn', 'about', 'nosql', 'and', 'when', 'is', 'better', 'to', 'use', 'sql', 'or', 'nosql', 'i', 'know', 'that', 'this', 'question', 'depends', 'on', 'the', 'case', 'but', 'i', 'm', 'asking', 'for', 'a', 'good', 'documentation', 'on', 'nosql', 'and', 'some', 'explanation', 'of', 'when', 'is', 'better', 'to', 'use', 'sql', 'or', 'nosql', 'use', 'cases', 'etc', 'also', 'your', 'opinions', 'on', 'nosql', 'databases', 'and', 'any', 'recommendations', 'for', 'learning', 'about', 'this', 'topic', 'are', 'welcome', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'latent_dirichlet_allocation', 'quot', 'gt', 'latent', 'dirichlet', 'allocation', 'lda', 'lt', 'a', 'gt', 'and', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'hierarchical_dirichlet_process', 'quot', 'gt', 'hierarchical', 'dirichlet', 'process', 'hdp', 'lt', 'a', 'gt', 'are', 'both', 'topic', 'modeling', 'processes', 'the', 'major', 'difference', 'is', 'lda', 'requires', 'the', 'specification', 'of', 'the', 'number', 'of', 'topics', 'and', 'hdp', 'doesn', 't', 'why', 'is', 'that', 'so', 'and', 'what', 'are', 'the', 'differences', 'pros', 'and', 'cons', 'of', 'both', 'topic', 'modelling', 'methods', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'stackoverflow', 'com', 'questions', 'what', 'is', 'the', 'difference', 'between', 'a', 'generative', 'and', 'discriminative', 'algorithm', 'quot', 'gt', 'this', 'question', 'lt', 'a', 'gt', 'asks', 'about', 'generative', 'vs', 'discriminative', 'algorithm', 'but', 'can', 'someone', 'give', 'an', 'example', 'of', 'the', 'difference', 'between', 'these', 'forms', 'when', 'applied', 'to', 'natural', 'language', 'processing', 'lt', 'strong', 'gt', 'how', 'are', 'generative', 'and', 'discriminative', 'models', 'used', 'in', 'nlp', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'from', 'wikipedia', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'dimensionality', 'reduction', 'or', 'dimension', 'reduction', 'is', 'the', 'process', 'of', 'xa', 'reducing', 'the', 'number', 'of', 'random', 'variables', 'under', 'consideration', 'and', 'xa', 'can', 'be', 'divided', 'into', 'feature', 'selection', 'and', 'feature', 'extraction', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'what', 'is', 'the', 'difference', 'between', 'feature', 'selection', 'and', 'feature', 'extraction', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'what', 'is', 'an', 'example', 'of', 'dimensionality', 'reduction', 'in', 'a', 'natural', 'language', 'processing', 'task', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'our', 'company', 'we', 'have', 'a', 'mongodb', 'database', 'containing', 'a', 'lot', 'of', 'unstructured', 'data', 'on', 'which', 'we', 'need', 'to', 'run', 'map', 'reduce', 'algorithms', 'to', 'generate', 'reports', 'and', 'other', 'analyses', 'we', 'have', 'two', 'approaches', 'to', 'select', 'from', 'for', 'implementing', 'the', 'required', 'analyses', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'one', 'approach', 'is', 'to', 'extract', 'the', 'data', 'from', 'mongodb', 'to', 'a', 'hadoop', 'cluster', 'and', 'do', 'the', 'analysis', 'completely', 'in', 'hadoop', 'platform', 'however', 'this', 'requires', 'considerable', 'investment', 'on', 'preparing', 'the', 'platform', 'software', 'and', 'hardware', 'and', 'educating', 'the', 'team', 'to', 'work', 'with', 'hadoop', 'and', 'write', 'map', 'reduce', 'tasks', 'for', 'it', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'another', 'approach', 'is', 'to', 'just', 'put', 'our', 'effort', 'on', 'designing', 'the', 'map', 'reduce', 'algorithms', 'and', 'run', 'the', 'algorithms', 'on', 'mongodb', 'map', 'reduce', 'functionalities', 'this', 'way', 'we', 'can', 'create', 'an', 'initial', 'prototype', 'of', 'final', 'system', 'that', 'can', 'generate', 'the', 'reports', 'i', 'know', 'that', 'the', 'mongodb', 's', 'map', 'reduce', 'functionalities', 'are', 'much', 'slower', 'compared', 'to', 'hadoop', 'but', 'currently', 'the', 'data', 'is', 'not', 'that', 'big', 'that', 'makes', 'this', 'a', 'bottleneck', 'yet', 'at', 'least', 'not', 'for', 'the', 'next', 'six', 'months', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'question', 'is', 'using', 'the', 'second', 'approach', 'and', 'writing', 'the', 'algorithms', 'for', 'mongodb', 'can', 'them', 'be', 'later', 'ported', 'to', 'hadoop', 'with', 'little', 'needed', 'modification', 'and', 'algorithm', 'redesign', 'mongodb', 'just', 'supports', 'javascript', 'but', 'programming', 'language', 'differences', 'are', 'easy', 'to', 'handle', 'however', 'is', 'there', 'any', 'fundamental', 'differences', 'in', 'the', 'map', 'reduce', 'model', 'of', 'mongodb', 'and', 'hadoop', 'that', 'may', 'force', 'us', 'to', 'redesign', 'algorithms', 'substantially', 'for', 'porting', 'to', 'hadoop', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'any', 'small', 'database', 'processing', 'can', 'be', 'easily', 'tackled', 'by', 'python', 'perl', 'scripts', 'that', 'uses', 'libraries', 'and', 'or', 'even', 'utilities', 'from', 'the', 'language', 'itself', 'however', 'when', 'it', 'comes', 'to', 'performance', 'people', 'tend', 'to', 'reach', 'out', 'for', 'c', 'c', 'low', 'level', 'languages', 'the', 'possibility', 'of', 'tailoring', 'the', 'code', 'to', 'the', 'needs', 'seems', 'to', 'be', 'what', 'makes', 'these', 'languages', 'so', 'appealing', 'for', 'bigdata', 'be', 'it', 'concerning', 'memory', 'management', 'parallelism', 'disk', 'access', 'or', 'even', 'low', 'level', 'optimizations', 'via', 'assembly', 'constructs', 'at', 'c', 'c', 'level', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'of', 'course', 'such', 'set', 'of', 'benefits', 'would', 'not', 'come', 'without', 'a', 'cost', 'writing', 'the', 'code', 'and', 'sometimes', 'even', 'lt', 'em', 'gt', 'reinventing', 'the', 'wheel', 'lt', 'em', 'gt', 'can', 'be', 'quite', 'expensive', 'tiresome', 'although', 'there', 'are', 'lots', 'of', 'libraries', 'available', 'people', 'are', 'inclined', 'to', 'write', 'the', 'code', 'by', 'themselves', 'whenever', 'they', 'need', 'to', 'lt', 'em', 'gt', 'grant', 'lt', 'em', 'gt', 'performance', 'what', 'lt', 'em', 'gt', 'disables', 'lt', 'em', 'gt', 'performance', 'assertions', 'from', 'using', 'libraries', 'while', 'processing', 'large', 'databases', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'consider', 'an', 'entreprise', 'that', 'continuously', 'crawls', 'webpages', 'and', 'parses', 'the', 'data', 'collected', 'for', 'each', 'sliding', 'window', 'different', 'data', 'mining', 'algorithms', 'are', 'run', 'upon', 'the', 'data', 'extracted', 'why', 'would', 'the', 'developers', 'ditch', 'off', 'using', 'available', 'libraries', 'frameworks', 'be', 'it', 'for', 'crawling', 'text', 'processing', 'and', 'data', 'mining', 'using', 'stuff', 'already', 'implemented', 'would', 'not', 'only', 'ease', 'the', 'burden', 'of', 'coding', 'the', 'whole', 'process', 'but', 'also', 'would', 'save', 'a', 'lot', 'of', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'in', 'a', 'single', 'shot', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'makes', 'writing', 'the', 'code', 'by', 'oneself', 'a', 'lt', 'em', 'gt', 'guarantee', 'lt', 'em', 'gt', 'of', 'performance', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'why', 'is', 'it', 'lt', 'em', 'gt', 'risky', 'lt', 'em', 'gt', 'to', 'rely', 'on', 'a', 'frameworks', 'libraries', 'when', 'you', 'must', 'lt', 'strong', 'gt', 'assure', 'lt', 'strong', 'gt', 'high', 'performance', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'we', 'all', 'know', 'there', 'are', 'some', 'data', 'indexing', 'techniques', 'using', 'by', 'well', 'known', 'indexing', 'apps', 'like', 'lucene', 'for', 'java', 'or', 'lucene', 'net', 'for', 'net', 'murmurhash', 'b', 'tree', 'etc', 'for', 'a', 'no', 'sql', 'object', 'oriented', 'database', 'which', 'i', 'try', 'to', 'write', 'play', 'a', 'little', 'around', 'with', 'c', 'which', 'technique', 'you', 'suggest', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'read', 'about', 'murmurhash', 'and', 'specially', 'v', 'comments', 'say', 'murmur', 'is', 'very', 'fast', 'also', 'lucene', 'net', 'has', 'good', 'comments', 'on', 'it', 'but', 'what', 'about', 'their', 'memory', 'footprints', 'in', 'general', 'is', 'there', 'any', 'efficient', 'solution', 'which', 'uses', 'less', 'footprint', 'and', 'of', 'course', 'if', 'faster', 'is', 'preferable', 'than', 'lucene', 'or', 'murmur', 'or', 'should', 'i', 'write', 'a', 'special', 'index', 'structure', 'to', 'get', 'the', 'best', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'try', 'to', 'write', 'my', 'own', 'then', 'is', 'there', 'any', 'accepted', 'scale', 'for', 'a', 'good', 'indexing', 'something', 'like', 'of', 'data', 'node', 'or', 'of', 'data', 'node', 'any', 'useful', 'hint', 'will', 'be', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'one', 'of', 'the', 'common', 'problems', 'in', 'data', 'science', 'is', 'gathering', 'data', 'from', 'various', 'sources', 'in', 'a', 'somehow', 'cleaned', 'semi', 'structured', 'format', 'and', 'combining', 'metrics', 'from', 'various', 'sources', 'for', 'making', 'a', 'higher', 'level', 'analysis', 'looking', 'at', 'the', 'other', 'people', 's', 'effort', 'especially', 'other', 'questions', 'on', 'this', 'site', 'it', 'appears', 'that', 'many', 'people', 'in', 'this', 'field', 'are', 'doing', 'somewhat', 'repetitive', 'work', 'for', 'example', 'analyzing', 'tweets', 'facebook', 'posts', 'wikipedia', 'articles', 'etc', 'is', 'a', 'part', 'of', 'a', 'lot', 'of', 'big', 'data', 'problems', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'of', 'these', 'data', 'sets', 'are', 'accessible', 'using', 'public', 'apis', 'provided', 'by', 'the', 'provider', 'site', 'but', 'usually', 'some', 'valuable', 'information', 'or', 'metrics', 'are', 'missing', 'from', 'these', 'apis', 'and', 'everyone', 'has', 'to', 'do', 'the', 'same', 'analyses', 'again', 'and', 'again', 'for', 'example', 'although', 'clustering', 'users', 'may', 'depend', 'on', 'different', 'use', 'cases', 'and', 'selection', 'of', 'features', 'but', 'having', 'a', 'base', 'clustering', 'of', 'twitter', 'facebook', 'users', 'can', 'be', 'useful', 'in', 'many', 'big', 'data', 'applications', 'which', 'is', 'neither', 'provided', 'by', 'the', 'api', 'nor', 'available', 'publicly', 'in', 'independent', 'data', 'sets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'any', 'index', 'or', 'publicly', 'available', 'data', 'set', 'hosting', 'site', 'containing', 'valuable', 'data', 'sets', 'that', 'can', 'be', 'reused', 'in', 'solving', 'other', 'big', 'data', 'problems', 'i', 'mean', 'something', 'like', 'github', 'or', 'a', 'group', 'of', 'sites', 'public', 'datasets', 'or', 'at', 'least', 'a', 'comprehensive', 'listing', 'for', 'the', 'data', 'science', 'if', 'not', 'what', 'are', 'the', 'reasons', 'for', 'not', 'having', 'such', 'a', 'platform', 'for', 'data', 'science', 'the', 'commercial', 'value', 'of', 'data', 'need', 'to', 'frequently', 'update', 'data', 'sets', 'can', 'we', 'not', 'have', 'an', 'open', 'source', 'model', 'for', 'sharing', 'data', 'sets', 'devised', 'for', 'data', 'scientists', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'see', 'a', 'lot', 'of', 'courses', 'in', 'data', 'science', 'emerging', 'in', 'the', 'last', 'years', 'even', 'big', 'universities', 'like', 'stanford', 'and', 'columbia', 'offers', 'ms', 'specifically', 'in', 'data', 'science', 'but', 'as', 'long', 'as', 'i', 'see', 'it', 'looks', 'like', 'data', 'science', 'is', 'just', 'a', 'mix', 'of', 'computer', 'science', 'and', 'statistics', 'techniques', 'xa', 'so', 'i', 'always', 'think', 'about', 'this', 'if', 'it', 'is', 'just', 'a', 'trend', 'and', 'if', 'in', 'years', 'from', 'now', 'someone', 'will', 'still', 'mention', 'data', 'science', 'as', 'an', 'entire', 'field', 'or', 'just', 'a', 'subject', 'topic', 'inside', 'cs', 'or', 'stats', 'xa', 'what', 'do', 'you', 'think', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'assume', 'a', 'set', 'of', 'loosely', 'structured', 'data', 'e', 'g', 'web', 'tables', 'linked', 'open', 'data', 'composed', 'of', 'many', 'data', 'sources', 'there', 'is', 'no', 'common', 'schema', 'followed', 'by', 'the', 'data', 'and', 'each', 'source', 'can', 'use', 'synonym', 'attributes', 'to', 'describe', 'the', 'values', 'e', 'g', 'quot', 'nationality', 'quot', 'vs', 'quot', 'bornin', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'goal', 'is', 'to', 'find', 'some', 'quot', 'important', 'quot', 'attributes', 'that', 'somehow', 'quot', 'define', 'quot', 'the', 'entities', 'that', 'they', 'describe', 'so', 'when', 'i', 'find', 'the', 'same', 'value', 'for', 'such', 'an', 'attribute', 'i', 'will', 'know', 'that', 'the', 'two', 'descriptions', 'are', 'most', 'likely', 'about', 'the', 'same', 'entity', 'e', 'g', 'the', 'same', 'person', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'the', 'attribute', 'quot', 'lastname', 'quot', 'is', 'more', 'discriminative', 'than', 'the', 'attribute', 'quot', 'nationality', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'how', 'could', 'i', 'statistically', 'find', 'such', 'attributes', 'that', 'are', 'more', 'important', 'than', 'others', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'naive', 'solution', 'would', 'be', 'to', 'take', 'the', 'average', 'idf', 'of', 'the', 'values', 'of', 'each', 'attribute', 'and', 'make', 'this', 'the', 'quot', 'importance', 'quot', 'factor', 'of', 'the', 'attribute', 'a', 'similar', 'approach', 'would', 'be', 'to', 'count', 'how', 'many', 'distinct', 'values', 'appear', 'for', 'each', 'attribute', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'seen', 'the', 'term', 'feature', 'or', 'attribute', 'selection', 'in', 'machine', 'learning', 'but', 'i', 'don', 't', 'want', 'to', 'discard', 'the', 'remaining', 'attributes', 'i', 'just', 'want', 'to', 'put', 'higher', 'weights', 'to', 'the', 'most', 'important', 'ones', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'modeling', 'and', 'scoring', 'program', 'that', 'makes', 'heavy', 'use', 'of', 'the', 'lt', 'code', 'gt', 'dataframe', 'isin', 'lt', 'code', 'gt', 'function', 'of', 'pandas', 'searching', 'through', 'lists', 'of', 'facebook', 'quot', 'like', 'quot', 'records', 'of', 'individual', 'users', 'for', 'each', 'of', 'a', 'few', 'thousand', 'specific', 'pages', 'this', 'is', 'the', 'most', 'time', 'consuming', 'part', 'of', 'the', 'program', 'more', 'so', 'than', 'the', 'modeling', 'or', 'scoring', 'pieces', 'simply', 'because', 'it', 'only', 'runs', 'on', 'one', 'core', 'while', 'the', 'rest', 'runs', 'on', 'a', 'few', 'dozen', 'simultaneously', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'though', 'i', 'know', 'i', 'could', 'manually', 'break', 'up', 'the', 'dataframe', 'into', 'chunks', 'and', 'run', 'the', 'operation', 'in', 'parallel', 'is', 'there', 'any', 'straightforward', 'way', 'to', 'do', 'that', 'automatically', 'in', 'other', 'words', 'is', 'there', 'any', 'kind', 'of', 'package', 'out', 'there', 'that', 'will', 'recognize', 'i', 'm', 'running', 'an', 'easily', 'delegated', 'operation', 'and', 'automatically', 'distribute', 'it', 'perhaps', 'that', 's', 'asking', 'for', 'too', 'much', 'but', 'i', 've', 'been', 'surprised', 'enough', 'in', 'the', 'past', 'by', 'what', 's', 'already', 'available', 'in', 'python', 'so', 'i', 'figure', 'it', 's', 'worth', 'asking', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'other', 'suggestions', 'about', 'how', 'this', 'might', 'be', 'accomplished', 'even', 'if', 'not', 'by', 'some', 'magic', 'unicorn', 'package', 'would', 'also', 'be', 'appreciated', 'mainly', 'just', 'trying', 'to', 'find', 'a', 'way', 'to', 'shave', 'off', 'minutes', 'per', 'run', 'without', 'spending', 'an', 'equal', 'amount', 'of', 'time', 'coding', 'the', 'solution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'data', 'coming', 'from', 'a', 'source', 'system', 'that', 'is', 'pipe', 'delimited', 'pipe', 'was', 'selected', 'over', 'comma', 'since', 'it', 'was', 'believed', 'no', 'pipes', 'appeared', 'in', 'field', 'while', 'it', 'was', 'known', 'that', 'commas', 'do', 'occur', 'after', 'ingesting', 'this', 'data', 'into', 'hive', 'however', 'it', 'has', 'been', 'discovered', 'that', 'rarely', 'a', 'field', 'does', 'in', 'fact', 'contain', 'a', 'pipe', 'character', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'due', 'to', 'a', 'constraint', 'we', 'are', 'unable', 'to', 'regenerate', 'from', 'source', 'to', 'escape', 'the', 'delimiter', 'or', 'change', 'delimiters', 'in', 'the', 'usual', 'way', 'however', 'we', 'have', 'the', 'metadata', 'used', 'to', 'create', 'the', 'hive', 'table', 'could', 'we', 'use', 'knowledge', 'of', 'the', 'fields', 'around', 'the', 'problem', 'field', 'to', 'reprocess', 'the', 'file', 'on', 'our', 'side', 'to', 'escape', 'it', 'or', 'to', 'change', 'the', 'file', 'delimiter', 'prior', 'to', 'reloading', 'the', 'data', 'into', 'hive', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'seeking', 'for', 'a', 'library', 'tool', 'to', 'visualize', 'how', 'social', 'network', 'changes', 'when', 'new', 'nodes', 'edges', 'are', 'added', 'to', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'of', 'the', 'existing', 'solutions', 'is', 'lt', 'a', 'href', 'quot', 'http', 'www', 'stanford', 'edu', 'group', 'sonia', 'quot', 'gt', 'sonia', 'social', 'network', 'image', 'animator', 'lt', 'a', 'gt', 'it', 'let', 's', 'you', 'make', 'movies', 'like', 'lt', 'a', 'href', 'quot', 'https', 'www', 'youtube', 'com', 'watch', 'v', 'ygsnced', 'mdc', 'quot', 'gt', 'this', 'one', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'sonia', 's', 'documentation', 'says', 'that', 'it', 's', 'broken', 'at', 'the', 'moment', 'and', 'besides', 'this', 'i', 'would', 'prefer', 'javascript', 'based', 'solution', 'instead', 'so', 'my', 'question', 'is', 'are', 'you', 'familiar', 'with', 'any', 'tools', 'or', 'are', 'you', 'able', 'to', 'point', 'me', 'to', 'some', 'libraries', 'which', 'would', 'make', 'this', 'task', 'as', 'easy', 'as', 'possible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'right', 'after', 'posting', 'this', 'question', 'i', 'll', 'dig', 'into', 'lt', 'a', 'href', 'quot', 'http', 'sigmajs', 'org', 'quot', 'gt', 'sigma', 'js', 'lt', 'a', 'gt', 'so', 'please', 'consider', 'this', 'library', 'covered', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'general', 'my', 'input', 'data', 'would', 'be', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'time_elapsed', 'node', 'node', 'xa', 'a', 'b', 'xa', 'a', 'c', 'xa', 'b', 'c', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'here', 'we', 'have', 'three', 'points', 'in', 'time', 'three', 'nodes', 'a', 'b', 'c', 'and', 'three', 'edges', 'which', 'represent', 'a', 'triadic', 'closure', 'between', 'the', 'three', 'considered', 'nodes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'moreover', 'every', 'node', 'will', 'have', 'two', 'attributes', 'age', 'and', 'gender', 'so', 'i', 'would', 'like', 'to', 'be', 'able', 'to', 'change', 'the', 'shape', 'colour', 'of', 'the', 'nodes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'after', 'adding', 'a', 'new', 'node', 'it', 'would', 'be', 'perfect', 'to', 'have', 'some', 'forceatlas', 'or', 'similar', 'algorithm', 'to', 'adjust', 'the', 'layout', 'of', 'the', 'graph', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'details', 'of', 'the', 'google', 'prediction', 'api', 'are', 'on', 'this', 'lt', 'a', 'href', 'quot', 'https', 'developers', 'google', 'com', 'prediction', 'quot', 'gt', 'page', 'lt', 'a', 'gt', 'but', 'i', 'am', 'not', 'able', 'to', 'find', 'any', 'details', 'about', 'the', 'prediction', 'algorithms', 'running', 'behind', 'the', 'api', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'far', 'i', 'have', 'gathered', 'that', 'they', 'let', 'you', 'provide', 'your', 'preprocessing', 'steps', 'in', 'pmml', 'format', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'learning', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'support_vector_machine', 'quot', 'gt', 'support', 'vector', 'machines', 'lt', 'a', 'gt', 'and', 'i', 'm', 'unable', 'to', 'understand', 'how', 'a', 'class', 'label', 'is', 'chosen', 'for', 'a', 'data', 'point', 'in', 'a', 'binary', 'classifier', 'is', 'it', 'chosen', 'by', 'consensus', 'with', 'respect', 'to', 'the', 'classification', 'in', 'each', 'dimension', 'of', 'the', 'separating', 'hyperplane', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'using', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'general_algebraic_modeling_system', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'general', 'algebraic', 'modeling', 'system', 'lt', 'a', 'gt', 'gams', 'and', 'more', 'specifically', 'cplex', 'within', 'gams', 'to', 'solve', 'a', 'very', 'large', 'mixed', 'integer', 'programming', 'problem', 'this', 'allows', 'me', 'to', 'parallelize', 'the', 'process', 'over', 'cores', 'although', 'i', 'have', 'more', 'cplex', 'utilizes', 'a', 'maximum', 'of', 'cores', 'and', 'it', 'finds', 'an', 'optimal', 'solution', 'in', 'a', 'relatively', 'short', 'amount', 'of', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'an', 'open', 'source', 'mixed', 'integer', 'programming', 'tool', 'that', 'i', 'could', 'use', 'as', 'an', 'alternative', 'to', 'gams', 'and', 'cplex', 'it', 'must', 'be', 'comparable', 'in', 'speed', 'or', 'faster', 'for', 'me', 'to', 'consider', 'it', 'i', 'have', 'a', 'preference', 'for', 'r', 'based', 'solutions', 'but', 'i', 'm', 'open', 'to', 'suggestions', 'of', 'all', 'kinds', 'and', 'other', 'users', 'may', 'be', 'interested', 'in', 'different', 'solutions', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'can', 'someone', 'explain', 'me', 'how', 'to', 'classify', 'a', 'data', 'like', 'mnist', 'with', 'mlbp', 'neural', 'network', 'if', 'i', 'make', 'more', 'than', 'one', 'output', 'e', 'g', 'i', 'mean', 'if', 'i', 'just', 'use', 'one', 'output', 'i', 'can', 'easily', 'classify', 'the', 'data', 'but', 'if', 'i', 'use', 'more', 'than', 'one', 'which', 'output', 'should', 'i', 'choose', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'most', 'popular', 'use', 'case', 'seem', 'to', 'be', 'recommender', 'systems', 'of', 'different', 'kinds', 'such', 'as', 'recommending', 'shopping', 'items', 'users', 'in', 'social', 'networks', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'what', 'are', 'other', 'typical', 'data', 'science', 'applications', 'which', 'may', 'be', 'used', 'in', 'a', 'different', 'verticals', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'customer', 'churn', 'prediction', 'with', 'machine', 'learning', 'evaluating', 'customer', 'lifetime', 'value', 'sales', 'forecasting', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'so', 'we', 'have', 'potential', 'for', 'a', 'machine', 'learning', 'application', 'that', 'fits', 'fairly', 'neatly', 'into', 'the', 'traditional', 'problem', 'domain', 'solved', 'by', 'classifiers', 'i', 'e', 'we', 'have', 'a', 'set', 'of', 'attributes', 'describing', 'an', 'item', 'and', 'a', 'quot', 'bucket', 'quot', 'that', 'they', 'end', 'up', 'in', 'however', 'rather', 'than', 'create', 'models', 'of', 'probabilities', 'like', 'in', 'naive', 'bayes', 'or', 'similar', 'classifiers', 'we', 'want', 'our', 'output', 'to', 'be', 'a', 'set', 'of', 'roughly', 'human', 'readable', 'rules', 'that', 'can', 'be', 'reviewed', 'and', 'modified', 'by', 'an', 'end', 'user', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'association', 'rule', 'learning', 'looks', 'like', 'the', 'family', 'of', 'algorithms', 'that', 'solves', 'this', 'type', 'of', 'problem', 'but', 'these', 'algorithms', 'seem', 'to', 'focus', 'on', 'identifying', 'common', 'combinations', 'of', 'features', 'and', 'don', 't', 'include', 'the', 'concept', 'of', 'a', 'final', 'bucket', 'that', 'those', 'features', 'might', 'point', 'to', 'for', 'example', 'our', 'data', 'set', 'looks', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'item', 'a', 'door', 'small', 'steel', 'amp', 'gt', 'sedan', 'xa', 'item', 'b', 'door', 'big', 'steel', 'amp', 'gt', 'truck', 'xa', 'item', 'c', 'door', 'small', 'steel', 'amp', 'gt', 'coupe', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'just', 'want', 'the', 'rules', 'that', 'say', 'quot', 'if', 'it', 's', 'big', 'and', 'a', 'door', 'it', 's', 'a', 'truck', 'quot', 'not', 'the', 'rules', 'that', 'say', 'quot', 'if', 'it', 's', 'a', 'door', 'it', 's', 'also', 'small', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'workaround', 'i', 'can', 'think', 'of', 'is', 'to', 'simply', 'use', 'association', 'rule', 'learning', 'algorithms', 'and', 'ignore', 'the', 'rules', 'that', 'don', 't', 'involve', 'an', 'end', 'bucket', 'but', 'that', 'seems', 'a', 'bit', 'hacky', 'have', 'i', 'missed', 'some', 'family', 'of', 'algorithms', 'out', 'there', 'or', 'perhaps', 'i', 'm', 'approaching', 'the', 'problem', 'incorrectly', 'to', 'begin', 'with', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lda', 'has', 'two', 'hyperparameters', 'tuning', 'them', 'changes', 'the', 'induced', 'topics', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'does', 'the', 'alpha', 'and', 'beta', 'hyperparameters', 'contribute', 'to', 'lda', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'does', 'the', 'topic', 'change', 'if', 'one', 'or', 'the', 'other', 'hyperparameters', 'increase', 'or', 'decrease', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'are', 'they', 'hyperparamters', 'and', 'not', 'just', 'parameters', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'working', 'on', 'what', 'could', 'often', 'be', 'called', 'quot', 'medium', 'data', 'quot', 'projects', 'i', 've', 'been', 'able', 'to', 'parallelize', 'my', 'code', 'mostly', 'for', 'modeling', 'and', 'prediction', 'in', 'python', 'on', 'a', 'single', 'system', 'across', 'anywhere', 'from', 'to', 'cores', 'now', 'i', 'm', 'looking', 'at', 'scaling', 'up', 'to', 'clusters', 'on', 'ec', 'probably', 'with', 'starcluster', 'ipython', 'but', 'open', 'to', 'other', 'suggestions', 'as', 'well', 'and', 'have', 'been', 'puzzled', 'by', 'how', 'to', 'reconcile', 'distributing', 'work', 'across', 'cores', 'on', 'an', 'instance', 'vs', 'instances', 'on', 'a', 'cluster', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'even', 'practical', 'to', 'parallelize', 'across', 'instances', 'as', 'well', 'as', 'across', 'cores', 'on', 'each', 'instance', 'if', 'so', 'can', 'anyone', 'give', 'a', 'quick', 'rundown', 'of', 'the', 'pros', 'cons', 'of', 'running', 'many', 'instances', 'with', 'few', 'cores', 'each', 'vs', 'a', 'few', 'instances', 'with', 'many', 'cores', 'is', 'there', 'a', 'rule', 'of', 'thumb', 'for', 'choosing', 'the', 'right', 'ratio', 'of', 'instances', 'to', 'cores', 'per', 'instance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'bandwidth', 'and', 'ram', 'are', 'non', 'trivial', 'concerns', 'in', 'my', 'projects', 'but', 'it', 's', 'easy', 'to', 'spot', 'when', 'those', 'are', 'the', 'bottlenecks', 'and', 'readjust', 'it', 's', 'much', 'harder', 'i', 'd', 'imagine', 'to', 'benchmark', 'the', 'right', 'mix', 'of', 'cores', 'to', 'instances', 'without', 'repeated', 'testing', 'and', 'my', 'projects', 'vary', 'too', 'much', 'for', 'any', 'single', 'test', 'to', 'apply', 'to', 'all', 'circumstances', 'thanks', 'in', 'advance', 'and', 'if', 'i', 've', 'just', 'failed', 'to', 'google', 'this', 'one', 'properly', 'feel', 'free', 'to', 'point', 'me', 'to', 'the', 'right', 'answer', 'somewhere', 'else', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'a', 'recommendation', 'system', 'keeps', 'a', 'log', 'of', 'what', 'recommendations', 'have', 'been', 'made', 'to', 'a', 'particular', 'user', 'and', 'whether', 'that', 'user', 'accepts', 'the', 'recommendation', 'it', 's', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'user_id', 'item_id', 'result', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'means', 'the', 'user', 'accepted', 'the', 'recommendation', 'while', 'means', 'the', 'user', 'did', 'not', 'respond', 'to', 'the', 'recommendation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'question', 'lt', 'strong', 'gt', 'if', 'i', 'am', 'going', 'to', 'make', 'recommendations', 'to', 'a', 'bunch', 'of', 'users', 'based', 'on', 'the', 'kind', 'of', 'log', 'described', 'above', 'and', 'i', 'want', 'to', 'maximize', 'map', 'scores', 'how', 'should', 'i', 'deal', 'with', 'the', 'implicit', 'data', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'idea', 'is', 'to', 'treat', 'and', 'as', 'ratings', 'and', 'predict', 'the', 'rating', 'using', 'factorization', 'machines', 'type', 'algorithms', 'but', 'this', 'does', 'not', 'seem', 'right', 'given', 'the', 'asymmetry', 'of', 'the', 'implicit', 'data', 'does', 'not', 'mean', 'the', 'user', 'does', 'not', 'like', 'the', 'recommendation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'xa', 'let', 'us', 'think', 'about', 'it', 'in', 'the', 'context', 'of', 'a', 'matrix', 'factorization', 'approach', 'if', 'we', 'treat', 'and', 'as', 'ratings', 'there', 'will', 'be', 'some', 'problem', 'for', 'example', 'user', 'likes', 'movie', 'a', 'which', 'scores', 'high', 'in', 'one', 'factor', 'e', 'g', 'having', 'glorious', 'background', 'music', 'in', 'the', 'latent', 'factor', 'space', 'the', 'system', 'recommends', 'movie', 'b', 'which', 'also', 'scores', 'high', 'in', 'quot', 'glorious', 'background', 'music', 'quot', 'but', 'for', 'some', 'reason', 'user', 'is', 'too', 'busy', 'to', 'look', 'into', 'the', 'recommendation', 'and', 'we', 'have', 'a', 'rating', 'movie', 'b', 'if', 'we', 'just', 'treat', 'or', 'equally', 'then', 'the', 'system', 'might', 'be', 'discouraged', 'to', 'recommend', 'movie', 'with', 'glorious', 'bgm', 'to', 'user', 'while', 'user', 'still', 'loves', 'movie', 'with', 'glorious', 'bgm', 'i', 'think', 'this', 'situation', 'is', 'to', 'be', 'avoided', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'new', 'to', 'this', 'community', 'and', 'hopefully', 'my', 'question', 'will', 'well', 'fit', 'in', 'here', 'xa', 'as', 'part', 'of', 'my', 'undergraduate', 'data', 'analytics', 'course', 'i', 'have', 'choose', 'to', 'do', 'the', 'project', 'on', 'human', 'activity', 'recognition', 'using', 'smartphone', 'data', 'sets', 'as', 'far', 'as', 'i', 'm', 'concern', 'this', 'topic', 'relates', 'to', 'machine', 'learning', 'and', 'support', 'vector', 'machines', 'i', 'm', 'not', 'well', 'familiar', 'with', 'this', 'technologies', 'yet', 'so', 'i', 'will', 'need', 'some', 'help', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'decided', 'to', 'follow', 'this', 'project', 'idea', 'lt', 'a', 'href', 'quot', 'http', 'www', 'inf', 'ed', 'ac', 'uk', 'teaching', 'courses', 'dme', 'datasets', 'html', 'quot', 'gt', 'http', 'www', 'inf', 'ed', 'ac', 'uk', 'teaching', 'courses', 'dme', 'datasets', 'html', 'lt', 'a', 'gt', 'first', 'project', 'on', 'the', 'top', 'xa', 'the', 'project', 'goal', 'is', 'determine', 'what', 'activity', 'a', 'person', 'is', 'engaging', 'in', 'e', 'g', 'walking', 'walking_upstairs', 'walking_downstairs', 'sitting', 'standing', 'laying', 'from', 'data', 'recorded', 'by', 'a', 'smartphone', 'samsung', 'galaxy', 's', 'ii', 'on', 'the', 'subject', 's', 'waist', 'using', 'its', 'embedded', 'accelerometer', 'and', 'gyroscope', 'the', 'data', 'includes', 'axial', 'linear', 'acceleration', 'and', 'axial', 'angular', 'velocity', 'at', 'a', 'constant', 'rate', 'of', 'hz', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'all', 'the', 'data', 'set', 'is', 'given', 'in', 'one', 'folder', 'with', 'some', 'description', 'and', 'feature', 'labels', 'the', 'data', 'is', 'divided', 'for', 'test', 'and', 'train', 'files', 'in', 'which', 'data', 'is', 'represented', 'in', 'this', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'that', 's', 'only', 'a', 'very', 'small', 'sample', 'of', 'what', 'the', 'file', 'contain', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'don', 't', 'really', 'know', 'what', 'this', 'data', 'represents', 'and', 'how', 'can', 'be', 'interpreted', 'also', 'for', 'analyzing', 'classification', 'and', 'clustering', 'of', 'the', 'data', 'what', 'tools', 'will', 'i', 'need', 'to', 'use', 'xa', 'is', 'there', 'any', 'way', 'i', 'can', 'put', 'this', 'data', 'into', 'excel', 'with', 'labels', 'included', 'and', 'for', 'example', 'use', 'r', 'or', 'python', 'to', 'extract', 'sample', 'data', 'and', 'work', 'on', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'hints', 'tips', 'would', 'be', 'much', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'building', 'a', 'workflow', 'for', 'creating', 'machine', 'learning', 'models', 'in', 'my', 'case', 'using', 'python', 's', 'lt', 'code', 'gt', 'pandas', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'sklearn', 'lt', 'code', 'gt', 'packages', 'from', 'data', 'pulled', 'from', 'a', 'very', 'large', 'database', 'here', 'vertica', 'by', 'way', 'of', 'sql', 'and', 'lt', 'code', 'gt', 'pyodbc', 'lt', 'code', 'gt', 'and', 'a', 'critical', 'step', 'in', 'that', 'process', 'involves', 'imputing', 'missing', 'values', 'of', 'the', 'predictors', 'this', 'is', 'straightforward', 'within', 'a', 'single', 'analytics', 'or', 'stats', 'platform', 'be', 'it', 'python', 'r', 'stata', 'etc', 'but', 'i', 'm', 'curious', 'where', 'best', 'to', 'locate', 'this', 'step', 'in', 'a', 'multi', 'platform', 'workflow', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 's', 'simple', 'enough', 'to', 'do', 'this', 'in', 'python', 'either', 'with', 'the', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'modules', 'preprocessing', 'html', 'imputation', 'of', 'missing', 'values', 'quot', 'gt', 'lt', 'code', 'gt', 'sklearn', 'preprocessing', 'imputer', 'lt', 'code', 'gt', 'lt', 'a', 'gt', 'class', 'using', 'the', 'lt', 'a', 'href', 'quot', 'http', 'pandas', 'pydata', 'org', 'pandas', 'docs', 'version', 'generated', 'pandas', 'dataframe', 'fillna', 'html', 'quot', 'gt', 'lt', 'code', 'gt', 'pandas', 'dataframe', 'fillna', 'lt', 'code', 'gt', 'lt', 'a', 'gt', 'method', 'or', 'by', 'hand', 'depending', 'upon', 'the', 'complexity', 'of', 'the', 'imputation', 'method', 'used', 'but', 'since', 'i', 'm', 'going', 'to', 'be', 'using', 'this', 'for', 'dozens', 'or', 'hundreds', 'of', 'columns', 'across', 'hundreds', 'of', 'millions', 'of', 'records', 'i', 'wonder', 'if', 'there', 's', 'a', 'more', 'efficient', 'way', 'to', 'do', 'this', 'directly', 'through', 'sql', 'ahead', 'of', 'time', 'aside', 'from', 'the', 'potential', 'efficiencies', 'of', 'doing', 'this', 'in', 'a', 'distributed', 'platform', 'like', 'vertica', 'this', 'would', 'have', 'the', 'added', 'benefit', 'of', 'allowing', 'us', 'to', 'create', 'an', 'automated', 'pipeline', 'for', 'building', 'quot', 'complete', 'quot', 'versions', 'of', 'tables', 'so', 'we', 'don', 't', 'need', 'to', 'fill', 'in', 'a', 'new', 'set', 'of', 'missing', 'values', 'from', 'scratch', 'every', 'time', 'we', 'want', 'to', 'run', 'a', 'model', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'haven', 't', 'been', 'able', 'to', 'find', 'much', 'guidance', 'about', 'this', 'but', 'i', 'imagine', 'that', 'we', 'could', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'create', 'a', 'table', 'of', 'substitute', 'values', 'e', 'g', 'mean', 'median', 'mode', 'either', 'overall', 'or', 'by', 'group', 'for', 'each', 'incomplete', 'column', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'join', 'the', 'substitute', 'value', 'table', 'with', 'the', 'original', 'table', 'to', 'assign', 'a', 'substitute', 'value', 'for', 'each', 'row', 'and', 'incomplete', 'column', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'use', 'a', 'series', 'of', 'case', 'statements', 'to', 'take', 'the', 'original', 'value', 'if', 'available', 'and', 'the', 'substitute', 'value', 'otherwise', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'a', 'reasonable', 'thing', 'to', 'do', 'in', 'vertica', 'sql', 'or', 'is', 'there', 'a', 'good', 'reason', 'not', 'to', 'bother', 'and', 'just', 'handle', 'it', 'in', 'python', 'instead', 'and', 'if', 'the', 'latter', 'is', 'there', 'a', 'strong', 'case', 'for', 'doing', 'this', 'in', 'pandas', 'rather', 'than', 'sklearn', 'or', 'vice', 'versa', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'so', 'i', 'have', 'a', 'dataset', 'with', 'variables', 'and', 'rows', 'dataset', 'is', 'successfully', 'xa', 'saved', 'in', 'dataframe', 'but', 'when', 'i', 'try', 'to', 'find', 'cov', 'it', 'result', 'an', 'error', 'xa', 'here', 'is', 'the', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'import', 'pandas', 'as', 'pd', 'xa', 'cov_data', 'pd', 'dataframe', 'dataset', 'cov', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'the', 'error', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'file', 'quot', 'home', 'syahdeini', 'desktop', 'fp', 'pca_', 'py', 'quot', 'line', 'in', 'find_eagen', 'xa', 'cov_data', 'pd', 'dataframe', 'data_mat', 'cov', 'xa', 'file', 'quot', 'usr', 'lib', 'python', 'dist', 'packages', 'pandas', 'core', 'frame', 'py', 'quot', 'line', 'in', 'cov', 'xa', 'basecov', 'np', 'cov', 'mat', 't', 'xa', 'file', 'quot', 'usr', 'lib', 'python', 'dist', 'packages', 'numpy', 'lib', 'function_base', 'py', 'quot', 'line', 'in', 'cov', 'xa', 'return', 'dot', 'x', 'x', 't', 'conj', 'fact', 'squeeze', 'xa', 'valueerror', 'array', 'is', 'too', 'big', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'having', 'a', 'lot', 'of', 'text', 'documents', 'in', 'natural', 'language', 'unstructured', 'what', 'are', 'the', 'possible', 'ways', 'of', 'annotating', 'them', 'with', 'some', 'semantic', 'meta', 'data', 'for', 'example', 'consider', 'a', 'short', 'document', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'i', 'saw', 'the', 'company', 's', 'manager', 'last', 'day', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'be', 'able', 'to', 'extract', 'information', 'from', 'it', 'it', 'must', 'be', 'annotated', 'with', 'additional', 'data', 'to', 'be', 'less', 'ambiguous', 'the', 'process', 'of', 'finding', 'such', 'meta', 'data', 'is', 'not', 'in', 'question', 'so', 'assume', 'it', 'is', 'done', 'manually', 'the', 'question', 'is', 'how', 'to', 'store', 'these', 'data', 'in', 'a', 'way', 'that', 'further', 'analysis', 'on', 'it', 'can', 'be', 'done', 'more', 'conveniently', 'efficiently', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'possible', 'approach', 'is', 'to', 'use', 'xml', 'tags', 'see', 'below', 'but', 'it', 'seems', 'too', 'verbose', 'and', 'maybe', 'there', 'are', 'better', 'approaches', 'guidelines', 'for', 'storing', 'such', 'meta', 'data', 'on', 'text', 'documents', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'amp', 'lt', 'person', 'name', 'quot', 'john', 'quot', 'amp', 'gt', 'i', 'amp', 'lt', 'person', 'amp', 'gt', 'saw', 'the', 'amp', 'lt', 'organization', 'name', 'quot', 'acme', 'quot', 'amp', 'gt', 'company', 'amp', 'lt', 'organization', 'amp', 'gt', 's', 'xa', 'manager', 'amp', 'lt', 'time', 'value', 'quot', 'quot', 'amp', 'gt', 'last', 'day', 'amp', 'lt', 'time', 'amp', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'output', 'of', 'my', 'word', 'alignment', 'file', 'looks', 'as', 'such', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'i', 'wish', 'to', 'say', 'with', 'regard', 'to', 'the', 'initiative', 'of', 'the', 'portuguese', 'presidency', 'that', 'we', 'support', 'the', 'spirit', 'and', 'the', 'political', 'intention', 'behind', 'it', 'in', 'bezug', 'auf', 'die', 'initiative', 'der', 'portugiesischen', 'pr', 'sidentschaft', 'm', 'chte', 'ich', 'zum', 'ausdruck', 'bringen', 'da', 'wir', 'den', 'geist', 'und', 'die', 'politische', 'absicht', 'die', 'dahinter', 'stehen', 'unterst', 'tzen', 'xa', 'it', 'may', 'not', 'be', 'an', 'ideal', 'initiative', 'in', 'terms', 'of', 'its', 'structure', 'but', 'we', 'accept', 'mr', 'president', 'in', 'office', 'that', 'it', 'is', 'rooted', 'in', 'idealism', 'and', 'for', 'that', 'reason', 'we', 'are', 'inclined', 'to', 'support', 'it', 'von', 'der', 'struktur', 'her', 'ist', 'es', 'vielleicht', 'keine', 'ideale', 'initiative', 'aber', 'herr', 'amtierender', 'ratspr', 'sident', 'wir', 'akzeptieren', 'da', 'sie', 'auf', 'idealismus', 'fu', 't', 'und', 'sind', 'deshalb', 'geneigt', 'sie', 'mitzutragen', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'i', 'produce', 'the', 'phrase', 'tables', 'that', 'are', 'used', 'by', 'moses', 'from', 'this', 'output', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'can', 'someone', 'kindly', 'tell', 'me', 'about', 'the', 'trade', 'offs', 'involved', 'when', 'choosing', 'between', 'storm', 'and', 'mapreduce', 'in', 'hadoop', 'cluster', 'for', 'data', 'processing', 'of', 'course', 'aside', 'from', 'the', 'obvious', 'one', 'that', 'hadoop', 'processing', 'via', 'mapreduce', 'in', 'a', 'hadoop', 'cluster', 'is', 'a', 'batch', 'processing', 'system', 'and', 'storm', 'is', 'a', 'real', 'time', 'processing', 'system', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'worked', 'a', 'bit', 'with', 'hadoop', 'eco', 'system', 'but', 'i', 'haven', 't', 'worked', 'with', 'storm', 'after', 'looking', 'through', 'a', 'lot', 'of', 'presentations', 'and', 'articles', 'i', 'still', 'haven', 't', 'been', 'able', 'to', 'find', 'a', 'satisfactory', 'and', 'comprehensive', 'answer', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'the', 'term', 'tradeoff', 'here', 'is', 'not', 'meant', 'to', 'compare', 'to', 'similar', 'things', 'it', 'is', 'meant', 'to', 'represent', 'the', 'consequences', 'of', 'getting', 'results', 'real', 'time', 'that', 'are', 'absent', 'from', 'a', 'batch', 'processing', 'system', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'going', 'through', 'the', 'presentation', 'and', 'material', 'of', 'summingbird', 'by', 'twitter', 'one', 'of', 'the', 'reasons', 'that', 'is', 'mentioned', 'for', 'using', 'storm', 'and', 'hadoop', 'clusters', 'together', 'in', 'summingbird', 'is', 'that', 'processing', 'through', 'storm', 'results', 'in', 'cascading', 'of', 'error', 'in', 'order', 'to', 'avoid', 'this', 'cascading', 'of', 'error', 'and', 'accumulation', 'of', 'it', 'hadoop', 'cluster', 'is', 'used', 'to', 'batch', 'process', 'the', 'data', 'and', 'discard', 'the', 'storm', 'results', 'after', 'the', 'same', 'data', 'is', 'processed', 'by', 'hadoop', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'is', 'the', 'reasons', 'for', 'generation', 'of', 'this', 'accumulation', 'of', 'error', 'and', 'why', 'is', 'it', 'not', 'present', 'in', 'hadoop', 'since', 'i', 'have', 'not', 'worked', 'with', 'storm', 'i', 'do', 'not', 'know', 'the', 'reasons', 'for', 'it', 'is', 'it', 'because', 'storm', 'uses', 'some', 'approximate', 'algorithm', 'to', 'process', 'the', 'data', 'in', 'order', 'to', 'process', 'them', 'in', 'real', 'time', 'or', 'is', 'the', 'cause', 'something', 'else', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'test', 'the', 'accuracy', 'of', 'a', 'methodology', 'i', 'ran', 'it', 'times', 'and', 'i', 'got', 'a', 'different', 'classification', 'for', 'each', 'run', 'i', 'also', 'have', 'the', 'ground', 'truth', 'i', 'e', 'the', 'real', 'classification', 'to', 'test', 'against', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'each', 'classification', 'i', 'computed', 'a', 'confusion', 'matrix', 'now', 'i', 'want', 'to', 'aggregate', 'these', 'results', 'in', 'order', 'to', 'get', 'the', 'overall', 'confusion', 'matrix', 'how', 'can', 'i', 'achieve', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'may', 'i', 'sum', 'all', 'confusion', 'matrices', 'in', 'order', 'to', 'obtain', 'the', 'overall', 'one', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'yann', 'lecun', 'lt', 'a', 'href', 'quot', 'http', 'www', 'reddit', 'com', 'r', 'machinelearning', 'comments', 'lnbt', 'ama_yann_lecun', 'chisdw', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'mentioned', 'lt', 'a', 'gt', 'a', 'number', 'of', 'phd', 'programs', 'in', 'data', 'science', 'will', 'be', 'popping', 'up', 'in', 'the', 'next', 'few', 'years', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'datascience', 'nyu', 'edu', 'academics', 'programs', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'nyu', 'lt', 'a', 'gt', 'already', 'have', 'one', 'where', 'prof', 'lecun', 'is', 'at', 'right', 'now', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'statistics', 'or', 'cs', 'phd', 'in', 'machine', 'learning', 'is', 'probably', 'more', 'rigorous', 'than', 'a', 'data', 'science', 'one', 'is', 'data', 'science', 'phd', 'for', 'the', 'less', 'mathy', 'people', 'like', 'myself', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'these', 'cash', 'cow', 'programs', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'is', 'a', 'huge', 'industry', 'demand', 'for', 'big', 'data', 'but', 'what', 'is', 'the', 'academic', 'value', 'of', 'these', 'programs', 'as', 'you', 'probably', 'can', 't', 'be', 'a', 'professor', 'or', 'publish', 'any', 'paper', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'data', 'visualization', 'is', 'an', 'important', 'sub', 'field', 'in', 'data', 'science', 'and', 'python', 'programmers', 'would', 'need', 'to', 'have', 'available', 'toolkits', 'for', 'them', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'is', 'there', 'a', 'python', 'api', 'to', 'tableau', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'are', 'there', 'any', 'python', 'based', 'data', 'visualization', 'toolkits', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'seem', 'to', 'be', 'at', 'least', 'ways', 'to', 'connect', 'to', 'hbase', 'from', 'external', 'application', 'with', 'language', 'other', 'then', 'java', 'i', 'e', 'python', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'hbase', 'thrift', 'api', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'hbase', 'stargate', 'rest', 'api', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anyone', 'know', 'which', 'one', 'should', 'be', 'used', 'in', 'which', 'circumstances', 'xa', 'i', 'e', 'what', 'are', 'their', 'main', 'differences', 'and', 'pros', 'cons', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'an', 'aspiring', 'data', 'scientist', 'here', 'i', 'don', 't', 'know', 'anything', 'about', 'hadoop', 'but', 'as', 'i', 'have', 'been', 'reading', 'about', 'data', 'science', 'and', 'big', 'data', 'i', 'see', 'a', 'lot', 'of', 'talk', 'about', 'hadoop', 'is', 'it', 'absolutely', 'necessary', 'to', 'learn', 'hadoop', 'to', 'be', 'a', 'data', 'scientist', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'the', 'main', 'benefits', 'from', 'storing', 'data', 'in', 'hdf', 'and', 'what', 'are', 'the', 'main', 'data', 'science', 'tasks', 'where', 'hdf', 'is', 'really', 'suitable', 'and', 'useful', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'variety', 'of', 'nfl', 'datasets', 'that', 'i', 'think', 'might', 'make', 'a', 'good', 'side', 'project', 'but', 'i', 'haven', 't', 'done', 'anything', 'with', 'them', 'just', 'yet', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'coming', 'to', 'this', 'site', 'made', 'me', 'think', 'of', 'machine', 'learning', 'algorithms', 'and', 'i', 'wondering', 'how', 'good', 'they', 'might', 'be', 'at', 'either', 'predicting', 'the', 'outcome', 'of', 'football', 'games', 'or', 'even', 'the', 'next', 'play', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 'seems', 'to', 'me', 'that', 'there', 'would', 'be', 'some', 'trends', 'that', 'could', 'be', 'identified', 'on', 'rd', 'down', 'and', 'a', 'team', 'with', 'a', 'strong', 'running', 'back', 'lt', 'em', 'gt', 'theoretically', 'should', 'lt', 'em', 'gt', 'have', 'a', 'tendency', 'to', 'run', 'the', 'ball', 'in', 'that', 'situation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'scoring', 'might', 'be', 'more', 'difficult', 'to', 'predict', 'but', 'the', 'winning', 'team', 'might', 'be', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'whether', 'these', 'are', 'good', 'questions', 'to', 'throw', 'at', 'a', 'machine', 'learning', 'algorithm', 'it', 'could', 'be', 'that', 'a', 'thousand', 'people', 'have', 'tried', 'it', 'before', 'but', 'the', 'nature', 'of', 'sports', 'makes', 'it', 'an', 'unreliable', 'topic', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'being', 'new', 'to', 'machine', 'learning', 'in', 'general', 'i', 'd', 'like', 'to', 'start', 'playing', 'around', 'and', 'see', 'what', 'the', 'possibilities', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'curious', 'as', 'to', 'what', 'applications', 'you', 'might', 'recommend', 'that', 'would', 'offer', 'the', 'fastest', 'time', 'from', 'installation', 'to', 'producing', 'a', 'meaningful', 'result', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'any', 'recommendations', 'for', 'good', 'getting', 'started', 'materials', 'on', 'the', 'subject', 'of', 'machine', 'learning', 'in', 'general', 'would', 'be', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'developing', 'a', 'system', 'that', 'is', 'intended', 'to', 'capture', 'the', 'quot', 'context', 'quot', 'of', 'user', 'activity', 'within', 'an', 'application', 'it', 'is', 'a', 'framework', 'that', 'web', 'applications', 'can', 'use', 'to', 'tag', 'user', 'activity', 'based', 'on', 'requests', 'made', 'to', 'the', 'system', 'it', 'is', 'hoped', 'that', 'this', 'data', 'can', 'then', 'power', 'ml', 'features', 'such', 'as', 'context', 'aware', 'information', 'retrieval', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'having', 'trouble', 'deciding', 'on', 'what', 'features', 'to', 'select', 'in', 'addition', 'to', 'these', 'user', 'tags', 'the', 'url', 'being', 'requested', 'approximate', 'time', 'spent', 'with', 'any', 'given', 'resource', 'estimating', 'the', 'current', 'quot', 'activity', 'quot', 'within', 'the', 'system', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'interested', 'to', 'know', 'if', 'there', 'are', 'good', 'examples', 'of', 'this', 'kind', 'of', 'technology', 'or', 'any', 'prior', 'research', 'on', 'the', 'subject', 'a', 'cursory', 'search', 'of', 'the', 'acm', 'dl', 'revealed', 'some', 'related', 'papers', 'but', 'nothing', 'really', 'spot', 'on', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'yann', 'lecun', 'mentioned', 'in', 'his', 'lt', 'a', 'href', 'quot', 'http', 'www', 'reddit', 'com', 'r', 'machinelearning', 'comments', 'lnbt', 'ama_yann_lecun', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'ama', 'lt', 'a', 'gt', 'that', 'he', 'considers', 'having', 'a', 'phd', 'very', 'important', 'in', 'order', 'to', 'get', 'a', 'job', 'at', 'a', 'top', 'company', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'a', 'masters', 'in', 'statistics', 'and', 'my', 'undergrad', 'was', 'in', 'economics', 'and', 'applied', 'math', 'but', 'i', 'am', 'now', 'looking', 'into', 'ml', 'phd', 'programs', 'most', 'programs', 'say', 'there', 'are', 'no', 'absolutely', 'necessary', 'cs', 'courses', 'however', 'i', 'tend', 'to', 'think', 'most', 'accepted', 'students', 'have', 'at', 'least', 'a', 'very', 'strong', 'cs', 'background', 'i', 'am', 'currently', 'working', 'as', 'a', 'data', 'scientist', 'statistician', 'but', 'my', 'company', 'will', 'pay', 'for', 'courses', 'should', 'i', 'take', 'some', 'intro', 'software', 'engineering', 'courses', 'at', 'my', 'local', 'university', 'to', 'make', 'myself', 'a', 'stronger', 'candidate', 'what', 'other', 'advice', 'you', 'have', 'for', 'someone', 'applying', 'to', 'phd', 'programs', 'from', 'outside', 'the', 'cs', 'field', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'i', 'have', 'taken', 'a', 'few', 'moocs', 'machine', 'learning', 'recommender', 'systems', 'nlp', 'and', 'code', 'r', 'python', 'on', 'a', 'daily', 'basis', 'i', 'have', 'a', 'lot', 'of', 'coding', 'experience', 'with', 'statistical', 'languages', 'and', 'implement', 'ml', 'algorithms', 'daily', 'i', 'am', 'more', 'concerned', 'with', 'things', 'that', 'i', 'can', 'put', 'on', 'applications', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'is', 'plenty', 'of', 'hype', 'surrounding', 'hadoop', 'and', 'its', 'eco', 'system', 'however', 'in', 'practice', 'where', 'many', 'data', 'sets', 'are', 'in', 'the', 'terabyte', 'range', 'is', 'it', 'not', 'more', 'reasonable', 'to', 'use', 'lt', 'a', 'href', 'quot', 'http', 'aws', 'amazon', 'com', 'redshift', 'quot', 'gt', 'amazon', 'redshift', 'lt', 'a', 'gt', 'for', 'querying', 'large', 'data', 'sets', 'rather', 'than', 'spending', 'time', 'and', 'effort', 'building', 'a', 'hadoop', 'cluster', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'how', 'does', 'amazon', 'redshift', 'compare', 'with', 'hadoop', 'with', 'respect', 'to', 'setup', 'complexity', 'cost', 'and', 'performance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'read', 'lot', 'of', 'blogs', 'article', 'on', 'how', 'different', 'type', 'of', 'industries', 'are', 'using', 'big', 'data', 'analytic', 'but', 'most', 'of', 'these', 'article', 'fails', 'to', 'mention', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'kinda', 'data', 'these', 'companies', 'used', 'what', 'was', 'the', 'size', 'of', 'the', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'kinda', 'of', 'tools', 'technologies', 'they', 'used', 'to', 'process', 'the', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'was', 'the', 'problem', 'they', 'were', 'facing', 'and', 'how', 'the', 'insight', 'they', 'got', 'the', 'data', 'helped', 'them', 'to', 'resolve', 'the', 'issue', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'they', 'selected', 'the', 'tool', 'technology', 'to', 'suit', 'their', 'need', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'kinda', 'pattern', 'they', 'identified', 'from', 'the', 'data', 'amp', 'amp', 'what', 'kind', 'of', 'patterns', 'they', 'were', 'looking', 'from', 'the', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'wonder', 'if', 'someone', 'can', 'provide', 'me', 'answer', 'to', 'all', 'these', 'questions', 'or', 'a', 'link', 'which', 'at', 'least', 'answer', 'some', 'of', 'the', 'the', 'questions', 'i', 'am', 'looking', 'for', 'real', 'world', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 'would', 'be', 'great', 'if', 'someone', 'share', 'how', 'finance', 'industry', 'is', 'making', 'use', 'of', 'big', 'data', 'analytic', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'improving', 'an', 'existing', 'supervised', 'classifier', 'for', 'classifying', 'protein', 'sequences', 'as', 'belonging', 'to', 'a', 'specific', 'class', 'neuropeptide', 'hormone', 'precursors', 'or', 'not', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'about', 'known', 'quot', 'positives', 'quot', 'against', 'a', 'background', 'of', 'about', 'million', 'protein', 'sequences', 'quot', 'unknown', 'poorly', 'annotated', 'background', 'quot', 'or', 'about', 'reviewed', 'relevant', 'proteins', 'annotated', 'with', 'a', 'variety', 'of', 'properties', 'but', 'very', 'few', 'annotated', 'in', 'an', 'explicitly', 'quot', 'negative', 'quot', 'way', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'previous', 'implementation', 'looked', 'at', 'this', 'as', 'a', 'binary', 'classification', 'problem', 'xa', 'positive', 'set', 'proteins', 'marked', 'as', 'neuropeptides', 'xa', 'negative', 'set', 'random', 'sampling', 'of', 'samples', 'total', 'from', 'among', 'the', 'remaining', 'proteins', 'of', 'a', 'roughly', 'similar', 'length', 'wise', 'distribution', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'that', 'worked', 'but', 'i', 'want', 'to', 'greatly', 'improve', 'the', 'machines', 'discriminatory', 'abilities', 'currently', 'it', 's', 'at', 'about', 'in', 'terms', 'of', 'accuracy', 'auc', 'f', 'measured', 'by', 'cv', 'on', 'multiple', 'randomly', 'sampled', 'negative', 'sets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'thoughts', 'were', 'to', 'xa', 'make', 'this', 'a', 'multiclass', 'problem', 'choosing', 'different', 'classes', 'of', 'protein', 'that', 'will', 'definetly', 'be', 'negatives', 'by', 'their', 'properties', 'functional', 'class', 'along', 'with', 'maybe', 'another', 'randomly', 'sampled', 'set', 'xa', 'priority', 'here', 'would', 'be', 'negative', 'sets', 'that', 'are', 'similar', 'in', 'their', 'characteristics', 'features', 'to', 'the', 'positive', 'set', 'while', 'still', 'having', 'defining', 'characteristics', 'xa', 'one', 'class', 'learning', 'would', 'be', 'nice', 'but', 'as', 'i', 'understand', 'it', 'it', 's', 'meant', 'just', 'for', 'anomaly', 'detection', 'and', 'has', 'poorer', 'performance', 'than', 'discriminatory', 'approaches', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'heard', 'of', 'p', 'u', 'learning', 'which', 'sounds', 'neat', 'but', 'i', 'm', 'a', 'programming', 'n', 'b', 'and', 'i', 'don', 't', 'know', 'of', 'any', 'existing', 'implementations', 'for', 'it', 'in', 'python', 'sci', 'kit', 'learn', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'does', 'approach', 'make', 'sense', 'in', 'a', 'theoretical', 'pov', 'is', 'there', 'a', 'best', 'way', 'to', 'make', 'multiple', 'negative', 'sets', 'i', 'could', 'also', 'simply', 'use', 'a', 'massive', 'k', 'pick', 'of', 'the', 'quot', 'negative', 'quot', 'proteins', 'but', 'they', 're', 'all', 'very', 'very', 'different', 'from', 'each', 'other', 'so', 'i', 'don', 't', 'know', 'how', 'well', 'the', 'classifier', 'would', 'handle', 'them', 'as', 'one', 'big', 'unbalanced', 'mix', 'xa', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'the', 'books', 'about', 'the', 'science', 'and', 'mathematics', 'behind', 'data', 'science', 'it', 'feels', 'like', 'so', 'many', 'quot', 'data', 'science', 'quot', 'books', 'are', 'programming', 'tutorials', 'and', 'don', 't', 'touch', 'things', 'like', 'data', 'generating', 'processes', 'and', 'statistical', 'inference', 'i', 'can', 'already', 'code', 'what', 'i', 'am', 'weak', 'on', 'is', 'the', 'math', 'stats', 'theory', 'behind', 'what', 'i', 'am', 'doing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'am', 'ready', 'to', 'burn', 'on', 'books', 'so', 'around', 'books', 'sigh', 'what', 'could', 'i', 'buy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'examples', 'agresti', 's', 'lt', 'a', 'href', 'quot', 'http', 'rads', 'stackoverflow', 'com', 'amzn', 'click', 'quot', 'gt', 'categorical', 'data', 'analysis', 'lt', 'a', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'rads', 'stackoverflow', 'com', 'amzn', 'click', 'quot', 'gt', 'linear', 'mixed', 'models', 'for', 'longitudinal', 'data', 'lt', 'a', 'gt', 'etc', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'built', 'an', 'artificial', 'neural', 'network', 'in', 'python', 'using', 'the', 'scipy', 'optimize', 'minimize', 'conjugate', 'gradient', 'optimization', 'function', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'implemented', 'gradient', 'checking', 'double', 'checked', 'everything', 'etc', 'and', 'i', 'm', 'pretty', 'certain', 'it', 's', 'working', 'correctly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'run', 'it', 'a', 'few', 'times', 'and', 'it', 'reaches', 'optimization', 'terminated', 'successfully', 'however', 'when', 'i', 'increase', 'the', 'number', 'of', 'hidden', 'layers', 'the', 'cost', 'of', 'the', 'hypothesis', 'increases', 'everything', 'else', 'is', 'kept', 'the', 'same', 'after', 'it', 'has', 'successfully', 'terminated', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'intuitively', 'it', 'feels', 'as', 'if', 'the', 'cost', 'should', 'decrease', 'when', 'the', 'number', 'of', 'hidden', 'layers', 'is', 'increased', 'as', 'it', 'is', 'able', 'to', 'generate', 'a', 'more', 'complex', 'hypothesis', 'which', 'can', 'fit', 'the', 'data', 'better', 'however', 'this', 'appears', 'not', 'to', 'be', 'the', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'be', 'interested', 'to', 'understand', 'what', 's', 'going', 'on', 'here', 'or', 'if', 'i', 've', 'implemented', 'neural', 'net', 'incorrectly', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'setup', 'is', 'simple', 'binary', 'classification', 'using', 'a', 'simple', 'decision', 'tree', 'each', 'node', 'of', 'the', 'tree', 'has', 'a', 'single', 'threshold', 'applied', 'on', 'a', 'single', 'feature', 'in', 'general', 'building', 'a', 'roc', 'curve', 'requires', 'moving', 'a', 'decision', 'threshold', 'over', 'different', 'values', 'and', 'computing', 'the', 'effect', 'of', 'that', 'change', 'on', 'the', 'true', 'positive', 'rate', 'and', 'the', 'false', 'positives', 'rate', 'of', 'predictions', 'what', 's', 'that', 'decision', 'threshold', 'in', 'the', 'case', 'of', 'a', 'simple', 'fixed', 'decision', 'tree', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'extract', 'news', 'about', 'a', 'company', 'from', 'online', 'news', 'by', 'using', 'rodbc', 'package', 'in', 'r', 'and', 'i', 'want', 'to', 'use', 'the', 'extracted', 'data', 'for', 'sentiment', 'analysis', 'i', 'want', 'to', 'accomplish', 'this', 'in', 'such', 'a', 'way', 'that', 'the', 'positive', 'news', 'is', 'assigned', 'a', 'value', 'of', 'the', 'negative', 'news', 'is', 'assigned', 'a', 'value', 'of', 'and', 'the', 'neutral', 'news', 'is', 'assigned', 'a', 'value', 'of', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'just', 'starting', 'to', 'develop', 'a', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'machine_learning', 'quot', 'gt', 'machine', 'learning', 'lt', 'a', 'gt', 'application', 'for', 'academic', 'purposes', 'i', 'm', 'currently', 'using', 'lt', 'strong', 'gt', 'r', 'lt', 'strong', 'gt', 'and', 'training', 'myself', 'in', 'it', 'however', 'in', 'a', 'lot', 'of', 'places', 'i', 'saw', 'people', 'using', 'lt', 'strong', 'gt', 'python', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'people', 'using', 'in', 'academia', 'and', 'industry', 'and', 'what', 'is', 'the', 'recommendation', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'now', 'seen', 'two', 'data', 'science', 'certification', 'programs', 'the', 'lt', 'a', 'href', 'quot', 'https', 'www', 'coursera', 'org', 'specialization', 'jhudatascience', 'utm_medium', 'listingpage', 'quot', 'gt', 'john', 'hopkins', 'one', 'available', 'at', 'coursera', 'lt', 'a', 'gt', 'and', 'the', 'lt', 'a', 'href', 'quot', 'http', 'cloudera', 'com', 'content', 'cloudera', 'en', 'training', 'certification', 'ccp', 'ds', 'html', 'quot', 'gt', 'cloudera', 'one', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'sure', 'there', 'are', 'others', 'out', 'there', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'john', 'hopkins', 'set', 'of', 'classes', 'is', 'focused', 'on', 'r', 'as', 'a', 'toolset', 'but', 'covers', 'a', 'range', 'of', 'topics', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'r', 'programming', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'cleaning', 'and', 'obtaining', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'data', 'analysis', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'reproducible', 'research', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'statistical', 'inference', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'regression', 'models', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'machine', 'learning', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'developing', 'data', 'products', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'and', 'what', 'looks', 'to', 'be', 'a', 'project', 'based', 'completion', 'task', 'similar', 'to', 'cloudera', 's', 'data', 'science', 'challenge', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'cloudera', 'program', 'looks', 'thin', 'on', 'the', 'surface', 'but', 'looks', 'to', 'answer', 'the', 'two', 'important', 'questions', 'quot', 'do', 'you', 'know', 'the', 'tools', 'quot', 'quot', 'can', 'you', 'apply', 'the', 'tools', 'in', 'the', 'real', 'world', 'quot', 'their', 'program', 'consists', 'of', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'introduction', 'to', 'data', 'science', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'data', 'science', 'essentials', 'exam', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'data', 'science', 'challenge', 'a', 'real', 'world', 'data', 'science', 'project', 'scenario', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'not', 'looking', 'for', 'a', 'recommendation', 'on', 'a', 'program', 'or', 'a', 'quality', 'comparison', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'curious', 'about', 'other', 'certifications', 'out', 'there', 'the', 'topics', 'they', 'cover', 'and', 'how', 'seriously', 'ds', 'certifications', 'are', 'viewed', 'at', 'this', 'point', 'by', 'the', 'community', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'these', 'are', 'all', 'great', 'answers', 'i', 'm', 'choosing', 'the', 'correct', 'answer', 'by', 'votes', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'could', 'you', 'give', 'some', 'examples', 'of', 'typical', 'tasks', 'that', 'a', 'data', 'scientist', 'does', 'in', 'his', 'daily', 'job', 'and', 'the', 'must', 'know', 'minimum', 'for', 'each', 'of', 'the', 'levels', 'like', 'junior', 'senior', 'etc', 'if', 'there', 'are', 'any', 'if', 'possible', 'something', 'like', 'a', 'lt', 'a', 'href', 'quot', 'http', 'www', 'starling', 'software', 'com', 'employment', 'programmer', 'competency', 'matrix', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'programmer', 'competency', 'matrix', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'some', 'cases', 'lt', 'a', 'href', 'quot', 'http', 'www', 'ncbi', 'nlm', 'nih', 'gov', 'pubmed', 'quot', 'gt', 'it', 'may', 'be', 'impossible', 'lt', 'a', 'gt', 'to', 'draw', 'euler', 'diagrams', 'with', 'overlapping', 'circles', 'to', 'represent', 'all', 'the', 'overlapping', 'subsets', 'in', 'the', 'correct', 'proportions', 'this', 'type', 'of', 'data', 'then', 'requires', 'using', 'polygons', 'or', 'other', 'figures', 'to', 'represent', 'each', 'set', 'when', 'dealing', 'with', 'data', 'that', 'describes', 'overlapping', 'subsets', 'how', 'can', 'i', 'figure', 'out', 'whether', 'a', 'simple', 'euler', 'diagram', 'is', 'possible', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'i', 'am', 'very', 'interested', 'in', 'programming', 'and', 'statistics', 'data', 'science', 'seems', 'like', 'a', 'great', 'career', 'path', 'to', 'me', 'i', 'like', 'both', 'fields', 'and', 'would', 'like', 'to', 'combine', 'them', 'unfortunately', 'i', 'have', 'studied', 'political', 'science', 'with', 'a', 'non', 'statistical', 'sounding', 'master', 'i', 'focused', 'on', 'statistics', 'in', 'this', 'master', 'visiting', 'optional', 'courses', 'and', 'writing', 'a', 'statistical', 'thesis', 'on', 'a', 'rather', 'large', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'since', 'almost', 'all', 'job', 'adds', 'are', 'requiring', 'a', 'degree', 'in', 'informatics', 'physics', 'or', 'some', 'other', 'techy', 'field', 'i', 'am', 'wondering', 'if', 'there', 'is', 'a', 'chance', 'to', 'become', 'a', 'data', 'scientist', 'or', 'if', 'i', 'should', 'drop', 'that', 'idea', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'lacking', 'knowledge', 'in', 'machine', 'learning', 'sql', 'and', 'hadoop', 'while', 'having', 'a', 'rather', 'strong', 'informatics', 'and', 'statistics', 'background', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'can', 'somebody', 'tell', 'me', 'how', 'feasible', 'my', 'goal', 'of', 'becoming', 'a', 'data', 'scientist', 'is', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'attack', 'this', 'problem', 'frequently', 'with', 'inefficiency', 'because', 'it', 's', 'always', 'pretty', 'low', 'on', 'the', 'priority', 'list', 'and', 'my', 'clients', 'are', 'resistant', 'to', 'change', 'until', 'things', 'break', 'i', 'would', 'like', 'some', 'input', 'on', 'how', 'to', 'speed', 'things', 'up', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'multiple', 'datasets', 'of', 'information', 'in', 'a', 'sql', 'database', 'the', 'database', 'is', 'vendor', 'designed', 'so', 'i', 'have', 'little', 'control', 'over', 'the', 'structure', 'it', 's', 'a', 'sql', 'representation', 'of', 'a', 'class', 'based', 'structure', 'it', 'looks', 'a', 'little', 'bit', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'main', 'class', 'table', 'xa', 'sub', 'class', 'table', 'xa', 'sub', 'class', 'table', 'xa', 'sub', 'sub', 'class', 'table', 'xa', 'xa', 'sub', 'class', 'table', 'n', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'each', 'table', 'contains', 'fields', 'for', 'each', 'attribute', 'of', 'the', 'class', 'a', 'join', 'exists', 'which', 'contains', 'all', 'of', 'the', 'fields', 'for', 'each', 'of', 'the', 'sub', 'classes', 'which', 'contains', 'all', 'of', 'the', 'fields', 'in', 'the', 'class', 'table', 'and', 'all', 'of', 'the', 'fields', 'in', 'each', 'parent', 'class', 'table', 'joined', 'by', 'a', 'unique', 'identifier', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'hundreds', 'of', 'classes', 'which', 'means', 'thousands', 'of', 'views', 'and', 'tens', 'of', 'thousands', 'of', 'columns', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'beyond', 'that', 'there', 'are', 'multiple', 'datasets', 'indicated', 'by', 'a', 'field', 'value', 'in', 'the', 'main', 'class', 'table', 'there', 'is', 'the', 'production', 'dataset', 'visible', 'to', 'all', 'end', 'users', 'and', 'there', 'are', 'several', 'other', 'datasets', 'comprised', 'of', 'the', 'most', 'current', 'version', 'of', 'the', 'same', 'data', 'from', 'various', 'integration', 'sources', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'daily', 'we', 'run', 'jobs', 'that', 'compare', 'the', 'production', 'dataset', 'to', 'the', 'live', 'datasets', 'and', 'based', 'on', 'a', 'set', 'of', 'rules', 'we', 'merge', 'the', 'data', 'purge', 'the', 'live', 'datasets', 'then', 'start', 'all', 'over', 'again', 'the', 'rules', 'are', 'in', 'place', 'because', 'we', 'might', 'trust', 'one', 'source', 'of', 'data', 'more', 'than', 'another', 'for', 'a', 'particular', 'value', 'of', 'a', 'particular', 'class', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'jobs', 'are', 'essentially', 'a', 'series', 'of', 'sql', 'statements', 'that', 'go', 'row', 'by', 'row', 'through', 'each', 'dataset', 'and', 'field', 'by', 'field', 'within', 'each', 'row', 'the', 'common', 'changes', 'are', 'limited', 'to', 'a', 'handful', 'of', 'fields', 'in', 'each', 'row', 'but', 'since', 'anything', 'can', 'change', 'we', 'compare', 'each', 'value', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 's', 'of', 'millions', 'of', 'rows', 'of', 'data', 'and', 'in', 'some', 'environments', 'the', 'merge', 'jobs', 'can', 'take', 'longer', 'than', 'hours', 'we', 'resolve', 'that', 'problem', 'generally', 'by', 'throwing', 'more', 'hardware', 'at', 'it', 'but', 'this', 'isn', 't', 'a', 'hadoop', 'environment', 'currently', 'so', 'there', 's', 'a', 'pretty', 'finite', 'limit', 'to', 'what', 'can', 'be', 'done', 'in', 'that', 'regard', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'would', 'you', 'go', 'about', 'scaling', 'a', 'solution', 'to', 'this', 'problem', 'such', 'that', 'there', 'were', 'no', 'limitations', 'and', 'how', 'would', 'you', 'go', 'about', 'accomplishing', 'the', 'most', 'efficient', 'data', 'merge', 'currently', 'it', 'is', 'field', 'by', 'field', 'comparisons', 'painfully', 'slow', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'logic', 'often', 'states', 'that', 'by', 'underfitting', 'a', 'model', 'it', 's', 'capacity', 'to', 'generalize', 'is', 'increased', 'that', 'said', 'clearly', 'at', 'some', 'point', 'underfitting', 'a', 'model', 'cause', 'models', 'to', 'become', 'worse', 'regardless', 'of', 'the', 'complexity', 'of', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'do', 'you', 'know', 'when', 'your', 'model', 'has', 'struck', 'the', 'right', 'balance', 'and', 'is', 'not', 'underfitting', 'the', 'data', 'it', 'seeks', 'to', 'model', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'note', 'lt', 'strong', 'gt', 'this', 'is', 'a', 'followup', 'to', 'my', 'question', 'quot', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'why', 'is', 'overfitting', 'bad', 'quot', 'gt', 'why', 'is', 'overfitting', 'bad', 'lt', 'a', 'gt', 'quot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'kind', 'of', 'error', 'measures', 'do', 'rmse', 'and', 'ndcg', 'give', 'while', 'evaluating', 'a', 'recommender', 'system', 'and', 'how', 'do', 'i', 'know', 'when', 'to', 'use', 'one', 'over', 'the', 'other', 'if', 'you', 'could', 'give', 'an', 'example', 'of', 'when', 'to', 'use', 'each', 'that', 'would', 'be', 'great', 'as', 'well', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'd', 'like', 'to', 'explore', 'data', 'science', 'the', 'term', 'seems', 'a', 'little', 'vague', 'to', 'me', 'but', 'i', 'expect', 'it', 'to', 'require', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'machine', 'learning', 'rather', 'than', 'traditional', 'statistics', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'large', 'enough', 'dataset', 'that', 'you', 'have', 'to', 'run', 'analyses', 'on', 'clusters', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'some', 'good', 'datasets', 'and', 'problems', 'accessible', 'to', 'a', 'statistician', 'with', 'some', 'programming', 'background', 'that', 'i', 'can', 'use', 'to', 'explore', 'the', 'field', 'of', 'data', 'science', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'keep', 'this', 'as', 'narrow', 'as', 'possible', 'i', 'd', 'ideally', 'like', 'links', 'to', 'open', 'well', 'used', 'datasets', 'and', 'example', 'problems', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'curious', 'about', 'natural', 'language', 'querying', 'stanford', 'has', 'what', 'looks', 'to', 'be', 'a', 'strong', 'set', 'of', 'lt', 'a', 'href', 'quot', 'http', 'nlp', 'stanford', 'edu', 'software', 'index', 'shtml', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'software', 'for', 'processing', 'natural', 'language', 'lt', 'a', 'gt', 'i', 've', 'also', 'seen', 'the', 'lt', 'a', 'href', 'quot', 'http', 'opennlp', 'apache', 'org', 'documentation', 'manual', 'opennlp', 'html', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'apache', 'opennlp', 'library', 'lt', 'a', 'gt', 'and', 'the', 'lt', 'a', 'href', 'quot', 'http', 'gate', 'ac', 'uk', 'science', 'html', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'general', 'architecture', 'for', 'text', 'engineering', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'an', 'incredible', 'amount', 'of', 'uses', 'for', 'natural', 'language', 'processing', 'and', 'that', 'makes', 'the', 'documentation', 'of', 'these', 'projects', 'difficult', 'to', 'quickly', 'absorb', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'you', 'simplify', 'things', 'for', 'me', 'a', 'bit', 'and', 'at', 'a', 'high', 'level', 'outline', 'the', 'tasks', 'necessary', 'for', 'performing', 'a', 'basic', 'translation', 'of', 'simple', 'questions', 'into', 'sql', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'first', 'rectangle', 'on', 'my', 'flow', 'chart', 'is', 'a', 'bit', 'of', 'a', 'mystery', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'wjpx', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'i', 'might', 'want', 'to', 'know', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'how', 'many', 'books', 'were', 'sold', 'last', 'month', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'i', 'd', 'want', 'that', 'translated', 'into', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'select', 'count', 'xa', 'from', 'sales', 'xa', 'where', 'xa', 'item_type', 'book', 'and', 'xa', 'sales_date', 'amp', 'gt', 'and', 'xa', 'sales_date', 'amp', 'lt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'majority', 'of', 'people', 'use', 's', 'however', 'google', 'drive', 'seems', 'a', 'promising', 'alternative', 'solution', 'for', 'storing', 'large', 'amounts', 'of', 'data', 'are', 'there', 'specific', 'reasons', 'why', 'one', 'is', 'better', 'than', 'the', 'other', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'developing', 'a', 'distributed', 'algorithm', 'and', 'to', 'improve', 'efficiency', 'it', 'relies', 'both', 'on', 'the', 'number', 'of', 'disks', 'one', 'per', 'machine', 'and', 'on', 'an', 'efficient', 'load', 'balance', 'strategy', 'with', 'more', 'disks', 'we', 're', 'able', 'to', 'reduce', 'the', 'time', 'spent', 'with', 'i', 'o', 'and', 'with', 'an', 'efficient', 'load', 'balance', 'policy', 'we', 'can', 'distribute', 'tasks', 'without', 'much', 'data', 'replication', 'overhead', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'many', 'studies', 'on', 'the', 'literature', 'that', 'deal', 'with', 'the', 'same', 'problem', 'and', 'each', 'of', 'them', 'runs', 'different', 'experiments', 'to', 'evaluate', 'their', 'proposal', 'some', 'experiments', 'are', 'specific', 'of', 'the', 'strategy', 'presented', 'and', 'some', 'others', 'like', 'weak', 'scaling', 'scalability', 'and', 'strong', 'scaling', 'speedup', 'are', 'common', 'to', 'all', 'of', 'the', 'works', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'problem', 'is', 'the', 'experiments', 'are', 'usually', 'executed', 'over', 'entirely', 'different', 'infrastructures', 'disks', 'processors', 'machines', 'network', 'and', 'depending', 'on', 'what', 'is', 'being', 'evaluated', 'it', 'may', 'raise', 'lt', 'em', 'gt', 'false', 'unfair', 'lt', 'em', 'gt', 'comparisons', 'for', 'example', 'i', 'may', 'get', 'of', 'speedup', 'in', 'my', 'application', 'running', 'on', 'machines', 'with', 'infiniband', 'connection', 'whereas', 'i', 'could', 'get', 'the', 'same', 'or', 'even', 'worse', 'results', 'if', 'my', 'connection', 'was', 'ethernet', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'how', 'can', 'one', 'honestly', 'compare', 'different', 'experiments', 'to', 'point', 'out', 'efficiency', 'gains', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'came', 'across', 'the', 'following', 'problem', 'that', 'i', 'recon', 'is', 'rather', 'typical', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'some', 'large', 'data', 'say', 'a', 'few', 'million', 'rows', 'i', 'run', 'some', 'non', 'trivial', 'analysis', 'on', 'it', 'e', 'g', 'an', 'sql', 'query', 'consisting', 'of', 'several', 'sub', 'queries', 'i', 'get', 'some', 'result', 'stating', 'for', 'example', 'that', 'property', 'x', 'is', 'increasing', 'over', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'there', 'are', 'two', 'possible', 'things', 'that', 'could', 'lead', 'to', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'x', 'is', 'indeed', 'increasing', 'over', 'time', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'have', 'a', 'bug', 'in', 'my', 'analysis', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'i', 'test', 'that', 'the', 'first', 'happened', 'rather', 'than', 'the', 'second', 'a', 'step', 'wise', 'debugger', 'even', 'if', 'one', 'exists', 'won', 't', 'help', 'since', 'intermediate', 'results', 'can', 'still', 'consist', 'of', 'millions', 'of', 'lines', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'thing', 'i', 'could', 'think', 'of', 'was', 'to', 'somehow', 'generate', 'a', 'small', 'synthetic', 'data', 'set', 'with', 'the', 'property', 'that', 'i', 'want', 'to', 'test', 'and', 'run', 'the', 'analysis', 'on', 'it', 'as', 'a', 'unit', 'test', 'are', 'there', 'tools', 'to', 'do', 'this', 'particularly', 'but', 'not', 'limited', 'to', 'sql', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'binary', 'classification', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'approximately', 'samples', 'in', 'training', 'set', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'attributes', 'including', 'binary', 'numeric', 'and', 'categorical', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'algorithm', 'is', 'the', 'best', 'choice', 'for', 'this', 'type', 'of', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'by', 'default', 'i', 'm', 'going', 'to', 'start', 'with', 'svm', 'preliminary', 'having', 'nominal', 'attributes', 'values', 'converted', 'to', 'binary', 'features', 'as', 'it', 'is', 'considered', 'the', 'best', 'for', 'relatively', 'clean', 'and', 'not', 'noisy', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'once', 'heard', 'that', 'filtering', 'spam', 'by', 'using', 'blacklists', 'is', 'not', 'a', 'good', 'approach', 'since', 'some', 'user', 'searching', 'for', 'entries', 'in', 'your', 'dataset', 'may', 'be', 'looking', 'for', 'particular', 'information', 'from', 'the', 'sources', 'blocked', 'also', 'it', 'd', 'become', 'a', 'burden', 'to', 'continuously', 'validate', 'the', 'lt', 'em', 'gt', 'current', 'state', 'lt', 'em', 'gt', 'of', 'each', 'spammer', 'blocked', 'checking', 'if', 'the', 'site', 'domain', 'still', 'disseminate', 'spam', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'considering', 'that', 'any', 'approach', 'must', 'be', 'efficient', 'and', 'scalable', 'so', 'as', 'to', 'support', 'filtering', 'on', 'very', 'large', 'datasets', 'what', 'are', 'the', 'strategies', 'available', 'to', 'get', 'rid', 'of', 'spam', 'in', 'a', 'non', 'biased', 'manner', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'if', 'possible', 'any', 'example', 'of', 'strategy', 'even', 'if', 'just', 'the', 'intuition', 'behind', 'it', 'would', 'be', 'very', 'welcome', 'along', 'with', 'the', 'answer', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'in', 'the', 'very', 'early', 'stages', 'of', 'preparing', 'a', 'new', 'research', 'project', 'still', 'at', 'the', 'funding', 'application', 'stage', 'and', 'expect', 'that', 'data', 'analysis', 'and', 'especially', 'visualisation', 'tools', 'will', 'play', 'a', 'role', 'in', 'this', 'project', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'view', 'of', 'this', 'i', 'face', 'the', 'following', 'dilemma', 'should', 'i', 'learn', 'python', 'to', 'be', 'able', 'to', 'use', 'its', 'extensive', 'scientific', 'libraries', 'pandas', 'numpy', 'scipy', 'or', 'should', 'i', 'just', 'dive', 'into', 'similar', 'packages', 'of', 'a', 'language', 'i', 'm', 'already', 'acquainted', 'with', 'racket', 'or', 'to', 'a', 'lesser', 'extent', 'scala', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'ideally', 'i', 'would', 'learn', 'python', 'in', 'parallel', 'with', 'using', 'statistical', 'libraries', 'in', 'racket', 'but', 'i', 'm', 'not', 'sure', 'i', 'll', 'have', 'time', 'for', 'both', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'not', 'looking', 'for', 'an', 'answer', 'to', 'this', 'dilemma', 'but', 'rather', 'for', 'feedback', 'on', 'my', 'different', 'considerations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'current', 'position', 'is', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'in', 'favour', 'of', 'python', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'extensively', 'used', 'libraries', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'widely', 'used', 'may', 'be', 'decisive', 'in', 'case', 'of', 'collaboration', 'with', 'others', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'lot', 'of', 'online', 'material', 'to', 'start', 'learning', 'it', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'conferences', 'that', 'are', 'specifically', 'dedicated', 'to', 'scientific', 'computing', 'with', 'python', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'learning', 'python', 'won', 't', 'be', 'a', 'waste', 'of', 'time', 'anyway', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'in', 'favour', 'of', 'a', 'language', 'i', 'already', 'know', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'it', 's', 'a', 'way', 'to', 'deepen', 'my', 'knowledge', 'of', 'one', 'language', 'rather', 'than', 'getting', 'superficial', 'knowledge', 'of', 'one', 'more', 'language', 'under', 'the', 'motto', 'you', 'should', 'at', 'least', 'know', 'one', 'language', 'really', 'well', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'it', 'is', 'feasible', 'both', 'racket', 'and', 'scala', 'have', 'good', 'mathematics', 'and', 'statistics', 'libraries', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'can', 'start', 'right', 'away', 'with', 'learning', 'what', 'i', 'need', 'to', 'know', 'rather', 'than', 'first', 'having', 'to', 'learn', 'the', 'basics', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'two', 'concrete', 'questions', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'am', 'i', 'forgetting', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'big', 'of', 'a', 'nuisance', 'could', 'the', 'python', 'vs', 'issue', 'be', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'ih', 'o', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'do', 'logistic', 'regression', 'using', 'sas', 'enterprise', 'miner', 'xa', 'my', 'independent', 'variables', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'cpr', 'inc', 'categorical', 'to', 'xa', 'od', 'inc', 'categorical', 'to', 'xa', 'insurance', 'binary', 'or', 'xa', 'income', 'loss', 'binary', 'or', 'xa', 'living', 'arrangement', 'categorical', 'to', 'xa', 'employment', 'status', 'categorical', 'to', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'dependent', 'variable', 'is', 'default', 'binary', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'following', 'is', 'the', 'output', 'from', 'running', 'regression', 'model', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'analysis', 'of', 'maximum', 'likelihood', 'estimates', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'standard', 'wald', 'xa', 'parameter', 'df', 'estimate', 'error', 'chi', 'square', 'pr', 'amp', 'gt', 'chisq', 'exp', 'est', 'xa', 'xa', 'intercept', 'amp', 'lt', 'xa', 'cpr___inc', 'amp', 'lt', 'xa', 'cpr___inc', 'amp', 'lt', 'xa', 'cpr___inc', 'xa', 'cpr___inc', 'xa', 'cpr___inc', 'xa', 'cpr___inc', 'xa', 'emp_status', 'xa', 'emp_status', 'amp', 'lt', 'xa', 'emp_status', 'xa', 'emp_status', 'xa', 'emp_status', 'xa', 'emp_status', 'amp', 'lt', 'xa', 'emp_status', 'xa', 'inc_loss', 'amp', 'lt', 'xa', 'insurance', 'xa', 'liv_arran', 'xa', 'liv_arran', 'xa', 'liv_arran', 'xa', 'liv_arran', 'xa', 'liv_arran', 'xa', 'liv_arran', 'xa', 'od___inc', 'xa', 'od___inc', 'xa', 'od___inc', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'used', 'this', 'model', 'to', 'score', 'a', 'new', 'set', 'of', 'data', 'an', 'example', 'row', 'of', 'my', 'new', 'data', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'cpr', 'xa', 'od', 'xa', 'living', 'arrangement', 'xa', 'employment', 'status', 'xa', 'insurance', 'xa', 'income', 'loss', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'this', 'sample', 'row', 'the', 'model', 'predicted', 'output', 'probability', 'of', 'default', 'as', 'xa', 'to', 'check', 'this', 'manually', 'i', 'added', 'the', 'estimates', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'intercept', 'emp', 'status', 'liv', 'arran', 'insurance', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'odds', 'ratio', 'exponential', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'hence', 'probability', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'unable', 'to', 'understand', 'why', 'there', 'is', 'such', 'a', 'mismatch', 'between', 'the', 'model', 's', 'predicted', 'probability', 'and', 'theoretical', 'probability', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'help', 'would', 'be', 'much', 'appreciated', 'xa', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'if', 'i', 'have', 'a', 'retail', 'store', 'and', 'have', 'a', 'way', 'to', 'measure', 'how', 'many', 'people', 'enter', 'my', 'store', 'every', 'minute', 'and', 'timestamp', 'that', 'data', 'how', 'can', 'i', 'predict', 'future', 'foot', 'traffic', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'looked', 'into', 'machine', 'learning', 'algorithms', 'but', 'i', 'm', 'not', 'sure', 'which', 'one', 'to', 'use', 'in', 'my', 'test', 'data', 'a', 'year', 'over', 'year', 'trend', 'is', 'more', 'accurate', 'compared', 'to', 'other', 'things', 'i', 've', 'tried', 'like', 'knn', 'with', 'what', 'i', 'think', 'are', 'sensible', 'parameters', 'and', 'distance', 'function', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 'almost', 'seems', 'like', 'this', 'could', 'be', 'similar', 'to', 'financial', 'modeling', 'where', 'you', 'deal', 'with', 'time', 'series', 'data', 'any', 'ideas', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'working', 'on', 'implementing', 'stochastic', 'gradient', 'descent', 'lt', 'code', 'gt', 'sgd', 'lt', 'code', 'gt', 'for', 'neural', 'nets', 'using', 'back', 'propagation', 'and', 'while', 'i', 'understand', 'its', 'purpose', 'i', 'have', 'some', 'questions', 'about', 'how', 'to', 'choose', 'values', 'for', 'the', 'learning', 'rate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'the', 'learning', 'rate', 'related', 'to', 'the', 'shape', 'of', 'the', 'error', 'gradient', 'as', 'it', 'dictates', 'the', 'rate', 'of', 'descent', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'if', 'so', 'how', 'do', 'you', 'use', 'this', 'information', 'to', 'inform', 'your', 'decision', 'about', 'a', 'value', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'if', 'it', 's', 'not', 'what', 'sort', 'of', 'values', 'should', 'i', 'choose', 'and', 'how', 'should', 'i', 'choose', 'them', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'it', 'seems', 'like', 'you', 'would', 'want', 'small', 'values', 'to', 'avoid', 'overshooting', 'but', 'how', 'do', 'you', 'choose', 'one', 'such', 'that', 'you', 'don', 't', 'get', 'stuck', 'in', 'local', 'minima', 'or', 'take', 'to', 'long', 'to', 'descend', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'does', 'it', 'make', 'sense', 'to', 'have', 'a', 'constant', 'learning', 'rate', 'or', 'should', 'i', 'use', 'some', 'metric', 'to', 'alter', 'its', 'value', 'as', 'i', 'get', 'nearer', 'a', 'minimum', 'in', 'the', 'gradient', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'short', 'how', 'do', 'i', 'choose', 'the', 'learning', 'rate', 'for', 'sgd', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'it', 'seems', 'as', 'though', 'most', 'languages', 'have', 'some', 'number', 'of', 'scientific', 'computing', 'libraries', 'available', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'python', 'has', 'lt', 'code', 'gt', 'scipy', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'code', 'gt', 'rust', 'lt', 'code', 'gt', 'has', 'lt', 'code', 'gt', 'scirust', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'has', 'several', 'including', 'lt', 'code', 'gt', 'viennacl', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'armadillo', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'code', 'gt', 'java', 'lt', 'code', 'gt', 'has', 'lt', 'code', 'gt', 'java', 'numerics', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'colt', 'lt', 'code', 'gt', 'as', 'well', 'as', 'several', 'other', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'not', 'to', 'mention', 'languages', 'like', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'julia', 'lt', 'code', 'gt', 'designed', 'explicitly', 'for', 'scientific', 'computing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'with', 'so', 'many', 'options', 'how', 'do', 'you', 'choose', 'the', 'best', 'language', 'for', 'a', 'task', 'additionally', 'which', 'languages', 'will', 'be', 'the', 'most', 'performant', 'lt', 'code', 'gt', 'python', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'seem', 'to', 'have', 'the', 'most', 'traction', 'in', 'the', 'space', 'but', 'logically', 'a', 'compiled', 'language', 'seems', 'like', 'it', 'would', 'be', 'a', 'better', 'choice', 'and', 'will', 'anything', 'ever', 'outperform', 'lt', 'code', 'gt', 'fortran', 'lt', 'code', 'gt', 'additionally', 'compiled', 'languages', 'tend', 'to', 'have', 'gpu', 'acceleration', 'while', 'interpreted', 'languages', 'like', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'python', 'lt', 'code', 'gt', 'don', 't', 'what', 'should', 'i', 'take', 'into', 'account', 'when', 'choosing', 'a', 'language', 'and', 'which', 'languages', 'provide', 'the', 'best', 'balance', 'of', 'utility', 'and', 'performance', 'also', 'are', 'there', 'any', 'languages', 'with', 'significant', 'scientific', 'computing', 'resources', 'that', 'i', 've', 'missed', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'h', 'gt', 'motivation', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'work', 'with', 'datasets', 'that', 'contain', 'personally', 'identifiable', 'information', 'pii', 'and', 'sometimes', 'need', 'to', 'share', 'part', 'of', 'a', 'dataset', 'with', 'third', 'parties', 'in', 'a', 'way', 'that', 'doesn', 't', 'expose', 'pii', 'and', 'subject', 'my', 'employer', 'to', 'liability', 'our', 'usual', 'approach', 'here', 'is', 'to', 'withhold', 'data', 'entirely', 'or', 'in', 'some', 'cases', 'to', 'reduce', 'its', 'resolution', 'e', 'g', 'replacing', 'an', 'exact', 'street', 'address', 'with', 'the', 'corresponding', 'county', 'or', 'census', 'tract', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'means', 'that', 'certain', 'types', 'of', 'analysis', 'and', 'processing', 'must', 'be', 'done', 'in', 'house', 'even', 'when', 'a', 'third', 'party', 'has', 'resources', 'and', 'expertise', 'more', 'suited', 'to', 'the', 'task', 'since', 'the', 'source', 'data', 'is', 'not', 'disclosed', 'the', 'way', 'we', 'go', 'about', 'this', 'analysis', 'and', 'processing', 'lacks', 'transparency', 'as', 'a', 'result', 'any', 'third', 'party', 's', 'ability', 'to', 'perform', 'qa', 'qc', 'adjust', 'parameters', 'or', 'make', 'refinements', 'may', 'be', 'very', 'limited', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'anonymizing', 'confidential', 'data', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'task', 'involves', 'identifying', 'individuals', 'by', 'their', 'names', 'in', 'user', 'submitted', 'data', 'while', 'taking', 'into', 'account', 'errors', 'and', 'inconsistencies', 'a', 'private', 'individual', 'might', 'be', 'recorded', 'in', 'one', 'place', 'as', 'quot', 'dave', 'quot', 'and', 'in', 'another', 'as', 'quot', 'david', 'quot', 'commercial', 'entities', 'can', 'have', 'many', 'different', 'abbreviations', 'and', 'there', 'are', 'always', 'some', 'typos', 'i', 've', 'developed', 'scripts', 'based', 'on', 'a', 'number', 'of', 'criteria', 'that', 'determine', 'when', 'two', 'records', 'with', 'non', 'identical', 'names', 'represent', 'the', 'same', 'individual', 'and', 'assign', 'them', 'a', 'common', 'id', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'this', 'point', 'we', 'can', 'make', 'the', 'dataset', 'anonymous', 'by', 'withholding', 'the', 'names', 'and', 'replacing', 'them', 'with', 'this', 'personal', 'id', 'number', 'but', 'this', 'means', 'the', 'recipient', 'has', 'almost', 'no', 'information', 'about', 'e', 'g', 'the', 'strength', 'of', 'the', 'match', 'we', 'would', 'prefer', 'to', 'be', 'able', 'to', 'pass', 'along', 'as', 'much', 'information', 'as', 'possible', 'without', 'divulging', 'identity', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'what', 'doesn', 't', 'work', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'it', 'would', 'be', 'great', 'to', 'be', 'able', 'to', 'encrypt', 'strings', 'while', 'preserving', 'edit', 'distance', 'this', 'way', 'third', 'parties', 'could', 'do', 'some', 'of', 'their', 'own', 'qa', 'qc', 'or', 'choose', 'to', 'do', 'further', 'processing', 'on', 'their', 'own', 'without', 'ever', 'accessing', 'or', 'being', 'able', 'to', 'potentially', 'reverse', 'engineer', 'pii', 'perhaps', 'we', 'match', 'strings', 'in', 'house', 'with', 'edit', 'distance', 'amp', 'lt', 'and', 'the', 'recipient', 'wants', 'to', 'look', 'at', 'the', 'implications', 'of', 'tightening', 'that', 'tolerance', 'to', 'edit', 'distance', 'amp', 'lt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'the', 'only', 'method', 'i', 'am', 'familiar', 'with', 'that', 'does', 'this', 'is', 'lt', 'a', 'href', 'quot', 'http', 'www', 'techrepublic', 'com', 'blog', 'it', 'security', 'cryptographys', 'running', 'gag', 'rot', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'rot', 'lt', 'a', 'gt', 'more', 'generally', 'any', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'caesar_cipher', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'shift', 'cipher', 'lt', 'a', 'gt', 'which', 'hardly', 'even', 'counts', 'as', 'encryption', 'it', 's', 'like', 'writing', 'the', 'names', 'upside', 'down', 'and', 'saying', 'quot', 'promise', 'you', 'won', 't', 'flip', 'the', 'paper', 'over', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'lt', 'strong', 'gt', 'bad', 'lt', 'strong', 'gt', 'solution', 'would', 'be', 'to', 'abbreviate', 'everything', 'quot', 'ellen', 'roberts', 'quot', 'becomes', 'quot', 'er', 'quot', 'and', 'so', 'forth', 'this', 'is', 'a', 'poor', 'solution', 'because', 'in', 'some', 'cases', 'the', 'initials', 'in', 'association', 'with', 'public', 'data', 'will', 'reveal', 'a', 'person', 's', 'identity', 'and', 'in', 'other', 'cases', 'it', 's', 'too', 'ambiguous', 'quot', 'benjamin', 'othello', 'ames', 'quot', 'and', 'quot', 'bank', 'of', 'america', 'quot', 'will', 'have', 'the', 'same', 'initials', 'but', 'their', 'names', 'are', 'otherwise', 'dissimilar', 'so', 'it', 'doesn', 't', 'do', 'either', 'of', 'the', 'things', 'we', 'want', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'an', 'inelegant', 'alternative', 'is', 'to', 'introduce', 'additional', 'fields', 'to', 'track', 'certain', 'attributes', 'of', 'the', 'name', 'e', 'g', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'row', 'id', 'name', 'wordchars', 'origin', 'xa', 'xa', 'quot', 'amelia', 'bedelia', 'quot', 'eng', 'xa', 'xa', 'quot', 'christoph', 'bauer', 'quot', 'ger', 'xa', 'xa', 'quot', 'c', 'j', 'bauer', 'quot', 'ger', 'xa', 'xa', 'quot', 'franz', 'heller', 'quot', 'ger', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'call', 'this', 'quot', 'inelegant', 'quot', 'because', 'it', 'requires', 'anticipating', 'which', 'qualities', 'might', 'be', 'interesting', 'and', 'it', 's', 'relatively', 'coarse', 'if', 'the', 'names', 'are', 'removed', 'there', 's', 'not', 'much', 'you', 'can', 'reasonably', 'conclude', 'about', 'the', 'strength', 'of', 'the', 'match', 'between', 'rows', 'amp', 'amp', 'or', 'about', 'the', 'distance', 'between', 'rows', 'amp', 'amp', 'i', 'e', 'how', 'close', 'they', 'are', 'to', 'matching', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'conclusion', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'goal', 'is', 'to', 'transform', 'strings', 'in', 'such', 'a', 'way', 'that', 'as', 'many', 'useful', 'qualities', 'of', 'the', 'original', 'string', 'are', 'preserved', 'as', 'possible', 'while', 'obscuring', 'the', 'original', 'string', 'decryption', 'should', 'be', 'impossible', 'or', 'so', 'impractical', 'as', 'to', 'be', 'effectively', 'impossible', 'no', 'matter', 'the', 'size', 'of', 'the', 'data', 'set', 'in', 'particular', 'a', 'method', 'that', 'preserves', 'the', 'edit', 'distance', 'between', 'arbitrary', 'strings', 'would', 'be', 'very', 'useful', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'found', 'a', 'couple', 'papers', 'that', 'might', 'be', 'relevant', 'but', 'they', 're', 'a', 'bit', 'over', 'my', 'head', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'merl', 'com', 'publications', 'docs', 'tr', 'pdf', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'privacy', 'preserving', 'string', 'comparisons', 'based', 'on', 'levenshtein', 'distance', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'www', 'uni', 'due', 'de', 'hq', 'documents', 'bachteler_', '_an_empirical_comparison_of_approaches_to_approximate_string_matching_in_private_record_linkage', 'pdf', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'an', 'empirical', 'comparison', 'of', 'approaches', 'to', 'approximate', 'string', 'xa', 'matching', 'in', 'private', 'record', 'linkage', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'know', 'what', 'is', 'the', 'best', 'way', 'to', 'classify', 'a', 'data', 'set', 'composed', 'of', 'mixed', 'types', 'of', 'attributes', 'for', 'example', 'textual', 'and', 'numerical', 'i', 'know', 'i', 'can', 'convert', 'textual', 'to', 'boolean', 'but', 'the', 'vocabulary', 'is', 'diverse', 'and', 'data', 'become', 'too', 'sparse', 'i', 'also', 'tried', 'to', 'classify', 'the', 'types', 'of', 'attributes', 'separately', 'and', 'combine', 'the', 'results', 'through', 'meta', 'learning', 'techniques', 'but', 'it', 'did', 'not', 'work', 'well', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'does', 'anyone', 'know', 'some', 'good', 'tutorials', 'on', 'online', 'machine', 'learning', 'technics', 'xa', 'i', 'e', 'how', 'it', 'can', 'be', 'used', 'in', 'real', 'time', 'environments', 'what', 'are', 'key', 'differences', 'compared', 'to', 'normal', 'machine', 'learning', 'methods', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'upd', 'thank', 'you', 'everyone', 'for', 'answers', 'by', 'quot', 'online', 'quot', 'i', 'mean', 'methods', 'which', 'can', 'be', 'trained', 'in', 'a', 'real', 'time', 'mode', 'based', 'on', 'a', 'new', 'inputs', 'one', 'by', 'one', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'an', 'extension', 'to', 'our', 'great', 'list', 'of', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'publicly', 'available', 'datasets', 'quot', 'gt', 'publicly', 'available', 'datasets', 'lt', 'a', 'gt', 'i', 'd', 'like', 'to', 'know', 'if', 'there', 'is', 'any', 'list', 'of', 'publicly', 'available', 'social', 'network', 'datasets', 'crawling', 'apis', 'it', 'would', 'be', 'very', 'nice', 'if', 'alongside', 'with', 'a', 'link', 'to', 'the', 'dataset', 'api', 'characteristics', 'of', 'the', 'data', 'available', 'were', 'added', 'such', 'information', 'should', 'be', 'and', 'is', 'not', 'limited', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'the', 'name', 'of', 'the', 'social', 'network', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'kind', 'of', 'user', 'information', 'it', 'provides', 'posts', 'profile', 'friendship', 'network', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'whether', 'it', 'allows', 'for', 'crawling', 'its', 'contents', 'via', 'an', 'api', 'and', 'rate', 'min', 'k', 'month', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'whether', 'it', 'simply', 'provides', 'a', 'snapshot', 'of', 'the', 'whole', 'dataset', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'suggestions', 'and', 'further', 'characteristics', 'to', 'be', 'added', 'are', 'very', 'welcome', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'planning', 'to', 'run', 'experiments', 'with', 'large', 'datasets', 'on', 'distributed', 'system', 'in', 'order', 'to', 'evaluate', 'efficiency', 'gains', 'in', 'comparison', 'with', 'previous', 'proposals', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'limited', 'number', 'of', 'machines', 'nearly', 'ten', 'machines', 'having', 'gb', 'of', 'free', 'space', 'on', 'hard', 'disk', 'on', 'each', 'on', 'the', 'contrary', 'i', 'wished', 'to', 'perform', 'experiments', 'on', 'more', 'than', 'available', 'nodes', 'in', 'order', 'to', 'measure', 'scalability', 'lt', 'strong', 'gt', 'more', 'precisely', 'lt', 'strong', 'gt', 'since', 'i', 'don', 't', 'have', 'any', 'i', 'thought', 'about', 'using', 'a', 'commodity', 'cluster', 'however', 'i', 'm', 'not', 'sure', 'about', 'the', 'policies', 'of', 'usage', 'and', 'i', 'need', 'to', 'reliably', 'measure', 'execution', 'times', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'commodity', 'services', 'which', 'will', 'grant', 'me', 'that', 'only', 'my', 'application', 'would', 'be', 'running', 'at', 'a', 'given', 'time', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'recently', 'saw', 'a', 'cool', 'feature', 'that', 'lt', 'a', 'href', 'quot', 'https', 'support', 'google', 'com', 'docs', 'answer', 'hl', 'en', 'quot', 'gt', 'was', 'once', 'available', 'lt', 'a', 'gt', 'in', 'google', 'sheets', 'you', 'start', 'by', 'writing', 'a', 'few', 'related', 'keywords', 'in', 'consecutive', 'cells', 'say', 'quot', 'blue', 'quot', 'quot', 'green', 'quot', 'quot', 'yellow', 'quot', 'and', 'it', 'automatically', 'generates', 'similar', 'keywords', 'in', 'this', 'case', 'other', 'colors', 'see', 'more', 'examples', 'in', 'lt', 'a', 'href', 'quot', 'http', 'youtu', 'be', 'dlslnhfrqmw', 'quot', 'gt', 'this', 'youtube', 'video', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'reproduce', 'this', 'in', 'my', 'own', 'program', 'i', 'm', 'thinking', 'of', 'using', 'freebase', 'and', 'it', 'would', 'work', 'like', 'this', 'intuitively', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'retrieve', 'the', 'list', 'of', 'given', 'words', 'in', 'freebase', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'find', 'their', 'quot', 'common', 'denominator', 's', 'quot', 'and', 'construct', 'a', 'distance', 'metric', 'based', 'on', 'this', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'rank', 'other', 'concepts', 'based', 'on', 'their', 'quot', 'distance', 'quot', 'to', 'the', 'original', 'keywords', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'display', 'the', 'next', 'closest', 'concepts', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'i', 'm', 'not', 'familiar', 'with', 'this', 'area', 'my', 'questions', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'a', 'better', 'way', 'to', 'do', 'this', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'tools', 'are', 'available', 'for', 'each', 'step', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'is', 'anyone', 'using', 'lt', 'code', 'gt', 'julia', 'lt', 'code', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'julialang', 'org', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'http', 'julialang', 'org', 'lt', 'a', 'gt', 'for', 'professional', 'jobs', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'using', 'it', 'instead', 'of', 'r', 'matlab', 'or', 'mathematica', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'a', 'good', 'language', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'you', 'have', 'to', 'predict', 'next', 'years', 'do', 'you', 'think', 'it', 'grow', 'up', 'enough', 'to', 'became', 'such', 'a', 'standard', 'in', 'data', 'science', 'like', 'r', 'or', 'similar', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'understand', 'how', 'all', 'the', 'quot', 'big', 'data', 'quot', 'components', 'play', 'together', 'in', 'a', 'real', 'world', 'use', 'case', 'e', 'g', 'hadoop', 'monogodb', 'nosql', 'storm', 'kafka', 'i', 'know', 'that', 'this', 'is', 'quite', 'a', 'wide', 'range', 'of', 'tools', 'used', 'for', 'different', 'types', 'but', 'i', 'd', 'like', 'to', 'get', 'to', 'know', 'more', 'about', 'their', 'interaction', 'in', 'applications', 'e', 'g', 'thinking', 'machine', 'learning', 'for', 'an', 'app', 'webapp', 'online', 'shop', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'vistors', 'session', 'transaction', 'data', 'etc', 'and', 'store', 'that', 'but', 'if', 'i', 'want', 'to', 'make', 'recommendations', 'on', 'the', 'fly', 'i', 'can', 't', 'run', 'slow', 'map', 'reduce', 'jobs', 'for', 'that', 'on', 'some', 'big', 'database', 'of', 'logs', 'i', 'have', 'where', 'can', 'i', 'learn', 'more', 'about', 'the', 'infrastructure', 'aspects', 'i', 'think', 'i', 'can', 'use', 'most', 'of', 'the', 'tools', 'on', 'their', 'own', 'but', 'plugging', 'them', 'into', 'each', 'other', 'seems', 'to', 'be', 'an', 'art', 'of', 'its', 'own', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'any', 'public', 'examples', 'use', 'cases', 'etc', 'available', 'i', 'understand', 'that', 'the', 'individual', 'pipelines', 'strongly', 'depend', 'on', 'the', 'use', 'case', 'and', 'the', 'user', 'but', 'just', 'examples', 'will', 'probably', 'be', 'very', 'useful', 'to', 'me', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'huge', 'dataset', 'from', 'a', 'relational', 'database', 'which', 'i', 'need', 'to', 'create', 'a', 'classification', 'model', 'for', 'normally', 'for', 'this', 'situation', 'i', 'would', 'use', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'inductive_logic_programming', 'quot', 'gt', 'inductive', 'logic', 'programming', 'lt', 'a', 'gt', 'ilp', 'but', 'due', 'to', 'special', 'circumstances', 'i', 'can', 't', 'do', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'other', 'way', 'to', 'tackle', 'this', 'would', 'be', 'just', 'to', 'try', 'to', 'aggregate', 'the', 'values', 'when', 'i', 'have', 'a', 'foreign', 'relation', 'however', 'i', 'have', 'thousands', 'of', 'important', 'and', 'distinct', 'rows', 'for', 'some', 'nominal', 'attributes', 'e', 'g', 'a', 'patient', 'with', 'a', 'relation', 'to', 'several', 'distinct', 'drug', 'prescriptions', 'so', 'i', 'just', 'can', 't', 'do', 'that', 'without', 'creating', 'a', 'new', 'attribute', 'for', 'each', 'distinct', 'row', 'of', 'that', 'nominal', 'attribute', 'and', 'furthermore', 'most', 'of', 'the', 'new', 'columns', 'would', 'have', 'null', 'values', 'if', 'i', 'do', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'any', 'non', 'ilp', 'algorithm', 'that', 'allows', 'me', 'to', 'data', 'mine', 'relational', 'databases', 'without', 'resorting', 'to', 'techniques', 'like', 'pivoting', 'which', 'would', 'create', 'thousands', 'of', 'new', 'columns', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'think', 'that', 'bootstrap', 'can', 'be', 'useful', 'in', 'my', 'work', 'where', 'we', 'have', 'a', 'lot', 'a', 'variables', 'that', 'we', 'don', 't', 'know', 'the', 'distribution', 'of', 'it', 'so', 'simulations', 'could', 'help', 'xa', 'what', 'are', 'good', 'sources', 'to', 'learn', 'about', 'bootstrap', 'other', 'useful', 'simulation', 'methods', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'with', 'hadoop', 'and', 'yarn', 'hadoop', 'is', 'supposedly', 'no', 'longer', 'tied', 'only', 'map', 'reduce', 'solutions', 'with', 'that', 'advancement', 'what', 'are', 'the', 'use', 'cases', 'for', 'apache', 'spark', 'vs', 'hadoop', 'considering', 'both', 'sit', 'atop', 'of', 'hdfs', 'i', 've', 'read', 'through', 'the', 'introduction', 'documentation', 'for', 'spark', 'but', 'i', 'm', 'curious', 'if', 'anyone', 'has', 'encountered', 'a', 'problem', 'that', 'was', 'more', 'efficient', 'and', 'easier', 'to', 'solve', 'with', 'spark', 'compared', 'to', 'hadoop', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'large', 'number', 'of', 'samples', 'which', 'represent', 'manchester', 'encoded', 'bit', 'streams', 'as', 'audio', 'signals', 'the', 'frequency', 'at', 'which', 'they', 'are', 'encoded', 'is', 'the', 'primary', 'frequency', 'component', 'when', 'it', 'is', 'high', 'and', 'there', 'is', 'a', 'consistent', 'amount', 'of', 'white', 'noise', 'in', 'the', 'background', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'manually', 'decoded', 'these', 'streams', 'but', 'i', 'was', 'wondering', 'if', 'i', 'could', 'use', 'some', 'sort', 'of', 'machine', 'learning', 'technique', 'to', 'learn', 'the', 'encoding', 'schemes', 'this', 'would', 'save', 'a', 'great', 'deal', 'of', 'time', 'manually', 'recognizing', 'these', 'schemes', 'the', 'difficulty', 'is', 'that', 'different', 'signals', 'are', 'encoded', 'differently', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'possible', 'to', 'build', 'a', 'model', 'which', 'can', 'learn', 'to', 'decode', 'more', 'than', 'one', 'encoding', 'scheme', 'how', 'robust', 'would', 'such', 'a', 'model', 'be', 'and', 'what', 'sort', 'of', 'techniques', 'would', 'i', 'want', 'to', 'employ', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'independent_component_analysis', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'independent', 'component', 'analysis', 'lt', 'a', 'gt', 'ica', 'seems', 'like', 'could', 'be', 'useful', 'for', 'isolating', 'the', 'frequency', 'i', 'care', 'about', 'but', 'how', 'would', 'i', 'learn', 'the', 'encoding', 'scheme', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'developing', 'a', 'distributed', 'application', 'and', 'as', 'it', 's', 'been', 'designed', 'there', 'll', 'be', 'a', 'great', 'load', 'of', 'communication', 'during', 'the', 'processing', 'since', 'the', 'communication', 'is', 'already', 'as', 'much', 'lt', 'em', 'gt', 'spread', 'lt', 'em', 'gt', 'along', 'the', 'entire', 'process', 'as', 'possible', 'i', 'm', 'wondering', 'if', 'there', 'any', 'standard', 'solutions', 'to', 'improve', 'the', 'performance', 'of', 'the', 'message', 'passing', 'layer', 'of', 'my', 'application', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'changes', 'improvements', 'could', 'i', 'apply', 'to', 'my', 'code', 'to', 'reduce', 'the', 'time', 'spent', 'sending', 'messages', 'for', 'what', 'it', 's', 'worth', 'i', 'm', 'communicating', 'up', 'to', 'gb', 'between', 'computing', 'nodes', 'and', 'the', 'framework', 'i', 'm', 'using', 'is', 'implemented', 'with', 'openmpi', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'understand', 'that', 'compression', 'methods', 'may', 'be', 'split', 'into', 'two', 'main', 'sets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'global', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'local', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'first', 'set', 'works', 'regardless', 'of', 'the', 'data', 'being', 'processed', 'i', 'e', 'they', 'do', 'not', 'rely', 'on', 'any', 'characteristic', 'of', 'the', 'data', 'and', 'thus', 'need', 'not', 'to', 'perform', 'any', 'preprocessing', 'on', 'any', 'part', 'of', 'the', 'dataset', 'before', 'the', 'compression', 'itself', 'on', 'the', 'other', 'hand', 'local', 'methods', 'analyze', 'the', 'data', 'extracting', 'information', 'that', 'usually', 'improves', 'the', 'compression', 'rate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'while', 'reading', 'about', 'some', 'of', 'these', 'methods', 'i', 'noticed', 'that', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'universal_code_', 'data_compression', 'universal_and_non', 'universal_codes', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'the', 'unary', 'method', 'is', 'not', 'universal', 'lt', 'a', 'gt', 'which', 'surprised', 'me', 'since', 'i', 'thought', 'quot', 'globality', 'quot', 'and', 'quot', 'universality', 'quot', 'referred', 'to', 'the', 'same', 'thing', 'the', 'unary', 'method', 'does', 'not', 'rely', 'on', 'characteristics', 'of', 'the', 'data', 'to', 'yield', 'its', 'encoding', 'i', 'e', 'it', 'is', 'a', 'global', 'method', 'and', 'therefore', 'it', 'should', 'be', 'global', 'universal', 'shouldn', 't', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'my', 'primary', 'questions', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'is', 'the', 'difference', 'between', 'universal', 'and', 'global', 'methods', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'aren', 't', 'these', 'classifications', 'synonyms', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'highly', 'biased', 'binary', 'dataset', 'i', 'have', 'x', 'more', 'examples', 'of', 'the', 'negative', 'class', 'than', 'the', 'positive', 'class', 'i', 'would', 'like', 'to', 'train', 'a', 'tree', 'ensemble', 'like', 'extra', 'random', 'trees', 'or', 'a', 'random', 'forest', 'on', 'this', 'data', 'but', 'it', 's', 'difficult', 'to', 'create', 'training', 'datasets', 'that', 'contain', 'enough', 'examples', 'of', 'the', 'positive', 'class', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'would', 'be', 'the', 'implications', 'of', 'doing', 'a', 'stratified', 'sampling', 'approach', 'to', 'normalize', 'the', 'number', 'of', 'positive', 'and', 'negative', 'examples', 'in', 'other', 'words', 'is', 'it', 'a', 'bad', 'idea', 'to', 'for', 'instance', 'artificially', 'inflate', 'by', 'resampling', 'the', 'number', 'of', 'positive', 'class', 'examples', 'in', 'the', 'training', 'set', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'which', 'freely', 'available', 'datasets', 'can', 'i', 'use', 'to', 'train', 'a', 'text', 'classifier', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'are', 'trying', 'to', 'enhance', 'our', 'users', 'engagement', 'by', 'recommending', 'the', 'most', 'related', 'content', 'for', 'him', 'so', 'we', 'thought', 'if', 'we', 'classified', 'our', 'content', 'based', 'on', 'a', 'predefined', 'bag', 'of', 'words', 'we', 'can', 'recommend', 'to', 'him', 'engaging', 'content', 'by', 'getting', 'his', 'feedback', 'on', 'random', 'number', 'of', 'posts', 'already', 'classified', 'before', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'can', 'use', 'this', 'info', 'to', 'recommend', 'for', 'him', 'pulses', 'labeled', 'with', 'those', 'classes', 'but', 'we', 'found', 'if', 'we', 'used', 'a', 'predefined', 'bag', 'of', 'words', 'not', 'related', 'to', 'our', 'content', 'the', 'feature', 'vector', 'will', 'be', 'full', 'of', 'zeros', 'also', 'categories', 'may', 'be', 'not', 'relevant', 'to', 'our', 'content', 'so', 'for', 'those', 'reasons', 'we', 'tried', 'another', 'solution', 'that', 'will', 'be', 'clustering', 'our', 'content', 'not', 'classifying', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'k', 'means_clustering', 'quot', 'gt', 'k', 'means', 'lt', 'a', 'gt', 'is', 'a', 'well', 'known', 'algorithm', 'for', 'clustering', 'but', 'there', 'is', 'also', 'an', 'online', 'variation', 'of', 'such', 'algorithm', 'online', 'k', 'means', 'what', 'are', 'the', 'pros', 'and', 'cons', 'of', 'these', 'approaches', 'and', 'when', 'should', 'each', 'be', 'preferred', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 's', 'this', 'side', 'project', 'i', 'm', 'working', 'on', 'where', 'i', 'need', 'to', 'structure', 'a', 'solution', 'to', 'the', 'following', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'two', 'groups', 'of', 'people', 'clients', 'group', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'intends', 'to', 'buy', 'and', 'group', 'lt', 'code', 'gt', 'b', 'lt', 'code', 'gt', 'intends', 'to', 'sell', 'a', 'determined', 'product', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'the', 'product', 'has', 'a', 'series', 'of', 'attributes', 'lt', 'code', 'gt', 'x_i', 'lt', 'code', 'gt', 'and', 'my', 'objective', 'is', 'to', 'facilitate', 'the', 'transaction', 'between', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'b', 'lt', 'code', 'gt', 'by', 'matching', 'their', 'preferences', 'the', 'main', 'idea', 'is', 'to', 'point', 'out', 'to', 'each', 'member', 'of', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'a', 'corresponding', 'in', 'lt', 'code', 'gt', 'b', 'lt', 'code', 'gt', 'whose', 'product', 'better', 'suits', 'his', 'needs', 'and', 'vice', 'versa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'complicating', 'aspects', 'of', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'the', 'list', 'of', 'attributes', 'is', 'not', 'finite', 'the', 'buyer', 'might', 'be', 'interested', 'in', 'a', 'very', 'particular', 'characteristic', 'or', 'some', 'kind', 'of', 'design', 'which', 'is', 'rare', 'among', 'the', 'population', 'and', 'i', 'can', 't', 'predict', 'can', 't', 'previously', 'list', 'all', 'the', 'attributes', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'attributes', 'might', 'be', 'continuous', 'binary', 'or', 'non', 'quantifiable', 'ex', 'price', 'functionality', 'design', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'suggestion', 'on', 'how', 'to', 'approach', 'this', 'problem', 'and', 'solve', 'it', 'in', 'an', 'automated', 'way', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'also', 'appreciate', 'some', 'references', 'to', 'other', 'similar', 'problems', 'if', 'possible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'great', 'suggestions', 'many', 'similarities', 'in', 'to', 'the', 'way', 'i', 'm', 'thinking', 'of', 'approaching', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'main', 'issue', 'on', 'mapping', 'the', 'attributes', 'is', 'that', 'the', 'level', 'of', 'detail', 'to', 'which', 'the', 'product', 'should', 'be', 'described', 'depends', 'on', 'each', 'buyers', 'let', 's', 'take', 'an', 'example', 'of', 'a', 'car', 'the', 'product', 'car', 'has', 'lots', 'and', 'lots', 'of', 'attributes', 'that', 'range', 'from', 'its', 'performance', 'mechanical', 'structure', 'price', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'suppose', 'i', 'just', 'want', 'a', 'cheap', 'car', 'or', 'an', 'electric', 'car', 'ok', 'that', 's', 'easy', 'to', 'map', 'because', 'they', 'represent', 'main', 'features', 'of', 'this', 'product', 'but', 'let', 's', 'say', 'for', 'instance', 'that', 'i', 'want', 'a', 'car', 'with', 'dual', 'clutch', 'transmission', 'or', 'xenon', 'headlights', 'well', 'there', 'might', 'be', 'many', 'cars', 'on', 'the', 'data', 'base', 'with', 'this', 'attributes', 'but', 'i', 'wouldn', 't', 'ask', 'the', 'seller', 'to', 'fill', 'in', 'this', 'level', 'of', 'detail', 'to', 'their', 'product', 'prior', 'to', 'the', 'information', 'that', 'there', 'is', 'someone', 'looking', 'them', 'such', 'a', 'procedure', 'would', 'require', 'every', 'seller', 'fill', 'a', 'complex', 'very', 'detailed', 'form', 'just', 'try', 'to', 'sell', 'his', 'car', 'on', 'the', 'platform', 'just', 'wouldn', 't', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'still', 'my', 'challenge', 'is', 'to', 'try', 'to', 'be', 'as', 'detailed', 'as', 'necessary', 'in', 'the', 'search', 'to', 'make', 'a', 'good', 'match', 'so', 'the', 'way', 'i', 'm', 'thinking', 'is', 'mapping', 'main', 'aspects', 'of', 'the', 'product', 'those', 'that', 'are', 'probably', 'relevant', 'to', 'everyone', 'to', 'narrow', 'down', 'de', 'group', 'of', 'potential', 'sellers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'next', 'step', 'would', 'be', 'a', 'refined', 'search', 'in', 'order', 'to', 'avoid', 'creating', 'a', 'too', 'detailed', 'form', 'i', 'could', 'ask', 'buyers', 'and', 'sellers', 'to', 'write', 'a', 'free', 'text', 'of', 'their', 'specification', 'and', 'then', 'use', 'some', 'word', 'matching', 'algorithm', 'to', 'find', 'possible', 'matches', 'although', 'i', 'understand', 'that', 'this', 'is', 'not', 'a', 'proper', 'solution', 'to', 'the', 'problem', 'because', 'the', 'seller', 'cannot', 'guess', 'what', 'the', 'buyer', 'needs', 'but', 'might', 'get', 'me', 'close', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'weighting', 'criteria', 'suggested', 'is', 'great', 'it', 'allows', 'me', 'to', 'quantify', 'the', 'level', 'to', 'which', 'the', 'seller', 'matches', 'the', 'buyer', 's', 'needs', 'the', 'scaling', 'part', 'might', 'be', 'a', 'problem', 'though', 'because', 'the', 'importance', 'of', 'each', 'attribute', 'varies', 'from', 'client', 'to', 'client', 'i', 'm', 'thinking', 'of', 'using', 'some', 'kind', 'of', 'pattern', 'recognition', 'or', 'just', 'asking', 'de', 'buyer', 'to', 'input', 'the', 'level', 'of', 'importance', 'of', 'each', 'attribute', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'tried', 'to', 'detect', 'outliers', 'in', 'the', 'energy', 'gas', 'consumption', 'of', 'some', 'dutch', 'buildings', 'building', 'a', 'neural', 'network', 'model', 'i', 'have', 'very', 'bad', 'results', 'but', 'i', 'can', 't', 'find', 'the', 'reason', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'not', 'an', 'expert', 'so', 'i', 'would', 'like', 'to', 'ask', 'you', 'what', 'i', 'can', 'improve', 'and', 'what', 'i', 'm', 'doing', 'wrong', 'this', 'is', 'the', 'complete', 'description', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'denadai', 'gas', 'consumption', 'outliers', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'https', 'github', 'com', 'denadai', 'gas', 'consumption', 'outliers', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'neural', 'network', 'is', 'a', 'feedfoward', 'network', 'with', 'back', 'propagation', 'as', 'described', 'lt', 'a', 'href', 'quot', 'http', 'nbviewer', 'ipython', 'org', 'github', 'denadai', 'gas', 'consumption', 'outliers', 'blob', 'master', 'regression_nn', 'ipynb', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'i', 'splitted', 'the', 'dataset', 'in', 'a', 'quot', 'small', 'quot', 'dataset', 'of', 'rows', 'features', 'and', 'i', 'tried', 'to', 'add', 'more', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'trained', 'the', 'networks', 'but', 'the', 'results', 'have', 'rmse', 'so', 'it', 'can', 't', 'predict', 'so', 'well', 'the', 'gas', 'consumptions', 'consecutively', 'i', 'can', 't', 'run', 'a', 'good', 'outlier', 'detection', 'mechanism', 'i', 'see', 'that', 'in', 'some', 'papers', 'that', 'even', 'if', 'they', 'predict', 'daily', 'or', 'hourly', 'consumption', 'in', 'the', 'electric', 'power', 'they', 'have', 'errors', 'like', 'mse', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'can', 'i', 'improve', 'what', 'am', 'i', 'doing', 'wrong', 'can', 'you', 'have', 'a', 'look', 'of', 'my', 'description', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'a', 'project', 'and', 'need', 'resources', 'to', 'get', 'me', 'up', 'to', 'speed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'dataset', 'is', 'around', 'observations', 'on', 'or', 'so', 'variables', 'about', 'half', 'the', 'variables', 'are', 'categorical', 'with', 'some', 'having', 'many', 'different', 'possible', 'values', 'i', 'e', 'if', 'you', 'split', 'the', 'categorical', 'variables', 'into', 'dummy', 'variables', 'you', 'would', 'have', 'a', 'lot', 'more', 'than', 'variables', 'but', 'still', 'probably', 'on', 'the', 'order', 'of', 'a', 'couple', 'of', 'hundred', 'max', 'n', 'gt', 'p', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'response', 'we', 'want', 'to', 'predict', 'is', 'ordinal', 'with', 'levels', 'predictors', 'are', 'a', 'mix', 'of', 'continuous', 'and', 'categorical', 'about', 'half', 'of', 'each', 'these', 'are', 'my', 'thoughts', 'plans', 'so', 'far', 'xa', 'treat', 'the', 'response', 'as', 'continuous', 'and', 'run', 'vanilla', 'linear', 'regression', 'xa', 'run', 'nominal', 'and', 'ordinal', 'logistic', 'and', 'probit', 'regression', 'xa', 'use', 'mars', 'and', 'or', 'another', 'flavor', 'of', 'non', 'linear', 'regression', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'familiar', 'with', 'linear', 'regression', 'mars', 'is', 'well', 'enough', 'described', 'by', 'hastie', 'and', 'tibshirani', 'but', 'i', 'm', 'at', 'a', 'loss', 'when', 'it', 'comes', 'to', 'ordinal', 'logit', 'probit', 'especially', 'with', 'so', 'many', 'variables', 'and', 'a', 'big', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'r', 'package', 'lt', 'a', 'href', 'quot', 'http', 'cran', 'r', 'project', 'org', 'web', 'packages', 'glmnetcr', 'index', 'html', 'quot', 'gt', 'glmnetcr', 'lt', 'a', 'gt', 'seems', 'to', 'be', 'my', 'best', 'bet', 'so', 'far', 'but', 'the', 'documentation', 'hardly', 'suffices', 'to', 'get', 'me', 'where', 'i', 'need', 'to', 'be', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'can', 'i', 'go', 'to', 'learn', 'more', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'usual', 'definition', 'of', 'regression', 'as', 'far', 'as', 'i', 'am', 'aware', 'is', 'lt', 'em', 'gt', 'predicting', 'a', 'continuous', 'output', 'variable', 'from', 'a', 'given', 'set', 'of', 'input', 'variables', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'logistic', 'regression', 'is', 'a', 'binary', 'classification', 'algorithm', 'so', 'it', 'produces', 'a', 'categorical', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'really', 'a', 'regression', 'algorithm', 'if', 'so', 'why', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'network', 'structure', 'what', 'is', 'the', 'difference', 'between', 'lt', 'strong', 'gt', 'k', 'cliques', 'lt', 'strong', 'gt', 'and', 'lt', 'strong', 'gt', 'p', 'cliques', 'lt', 'strong', 'gt', 'can', 'anyone', 'give', 'a', 'brief', 'explaination', 'with', 'examples', 'thanks', 'in', 'advanced', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'xa', 'lt', 'br', 'gt', 'edit', 'xa', 'i', 'found', 'an', 'online', 'lt', 'a', 'href', 'quot', 'http', 'open', 'umich', 'edu', 'sites', 'default', 'files', 'si', 'f', 'week', 'lab', 'ppt', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'ppt', 'lt', 'a', 'gt', 'while', 'i', 'am', 'googling', 'please', 'take', 'a', 'look', 'on', 'lt', 'strong', 'gt', 'p', 'lt', 'strong', 'gt', 'and', 'lt', 'strong', 'gt', 'p', 'lt', 'strong', 'gt', 'can', 'you', 'comment', 'on', 'them', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 's', 'this', 'side', 'project', 'i', 'm', 'working', 'on', 'where', 'i', 'need', 'to', 'structure', 'a', 'solution', 'to', 'the', 'following', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'two', 'groups', 'of', 'people', 'clients', 'group', 'quot', 'a', 'quot', 'intends', 'to', 'buy', 'and', 'group', 'quot', 'b', 'quot', 'intends', 'to', 'sell', 'a', 'determined', 'product', 'quot', 'x', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'product', 'has', 'a', 'series', 'of', 'attributes', 'x_i', 'and', 'my', 'objective', 'is', 'to', 'facilitate', 'the', 'transaction', 'between', 'quot', 'a', 'quot', 'e', 'quot', 'b', 'quot', 'by', 'matching', 'their', 'preferences', 'the', 'main', 'idea', 'is', 'to', 'point', 'out', 'to', 'each', 'member', 'of', 'quot', 'a', 'quot', 'a', 'corresponding', 'in', 'quot', 'b', 'quot', 'who', 's', 'product', 'better', 'suits', 'his', 'needs', 'and', 'vice', 'versa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'complicating', 'aspects', 'of', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'the', 'list', 'of', 'attributes', 'is', 'not', 'finite', 'the', 'buyer', 'might', 'be', 'interested', 'in', 'a', 'very', 'particular', 'characteristic', 'or', 'some', 'kind', 'of', 'design', 'which', 'is', 'rare', 'among', 'the', 'population', 'and', 'i', 'can', 't', 'predict', 'can', 't', 'previously', 'list', 'all', 'the', 'attributes', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'attributes', 'might', 'be', 'continuous', 'binary', 'or', 'non', 'quantifiable', 'ex', 'price', 'functionality', 'design', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'suggestion', 'on', 'how', 'to', 'approach', 'this', 'problem', 'and', 'solve', 'it', 'in', 'an', 'automated', 'way', 'lt', 'br', 'gt', 'xa', 'the', 'idea', 'is', 'to', 'really', 'think', 'out', 'of', 'the', 'box', 'here', 'so', 'feel', 'free', 'to', 'quot', 'go', 'wild', 'quot', 'on', 'your', 'suggestions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'also', 'appreciate', 'some', 'references', 'to', 'other', 'similar', 'problems', 'if', 'possible', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'thought', 'that', 'generalized', 'linear', 'model', 'glm', 'would', 'be', 'considered', 'a', 'statistical', 'model', 'but', 'a', 'friend', 'told', 'me', 'that', 'some', 'papers', 'classify', 'it', 'as', 'a', 'machine', 'learning', 'technique', 'which', 'one', 'is', 'true', 'or', 'more', 'precise', 'any', 'explanation', 'would', 'be', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'to', 'use', 'google', 's', 'word', 'vec', 'implementation', 'to', 'build', 'a', 'named', 'entity', 'recognition', 'system', 'i', 've', 'heard', 'that', 'recursive', 'neural', 'nets', 'with', 'back', 'propagation', 'through', 'structure', 'are', 'well', 'suited', 'for', 'named', 'entity', 'recognition', 'tasks', 'but', 'i', 've', 'been', 'unable', 'to', 'find', 'a', 'decent', 'implementation', 'or', 'a', 'decent', 'tutorial', 'for', 'that', 'type', 'of', 'model', 'because', 'i', 'm', 'working', 'with', 'an', 'atypical', 'corpus', 'standard', 'ner', 'tools', 'in', 'nltk', 'and', 'similar', 'have', 'performed', 'very', 'poorly', 'and', 'it', 'looks', 'like', 'i', 'll', 'have', 'to', 'train', 'my', 'own', 'system', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'short', 'what', 'resources', 'are', 'available', 'for', 'this', 'kind', 'of', 'problem', 'is', 'there', 'a', 'standard', 'recursive', 'neural', 'net', 'implementation', 'available', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'my', 'data', 'set', 'is', 'formatted', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'user', 'id', 'threat_score', 'xa', 'aaa', 'xa', 'bbb', 'xa', 'ccc', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'list', 'contains', 'the', 'top', 'users', 'with', 'the', 'highest', 'threat', 'scores', 'i', 'generate', 'such', 'a', 'list', 'monthly', 'and', 'store', 'each', 'month', 's', 'list', 'in', 'its', 'own', 'file', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'three', 'things', 'i', 'would', 'like', 'to', 'get', 'from', 'this', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'users', 'who', 'are', 'consistently', 'showing', 'up', 'in', 'this', 'list', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'users', 'who', 'are', 'consistently', 'showing', 'up', 'in', 'this', 'list', 'with', 'high', 'threat', 'scores', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'users', 'whose', 'threat', 'scores', 'are', 'increasing', 'very', 'quickly', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'thinking', 'a', 'visual', 'summary', 'would', 'be', 'nice', 'each', 'month', 'somehow', 'decide', 'which', 'users', 'i', 'want', 'to', 'plot', 'on', 'a', 'graph', 'of', 'historic', 'threat', 'scores', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'any', 'known', 'visualization', 'techniques', 'that', 'deal', 'with', 'similar', 'requirements', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'should', 'i', 'be', 'transforming', 'my', 'current', 'data', 'to', 'achieve', 'what', 'i', 'am', 'looking', 'for', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'find', 'a', 'formula', 'method', 'or', 'model', 'to', 'use', 'to', 'analyze', 'the', 'likelihood', 'that', 'a', 'specific', 'event', 'influenced', 'some', 'longitudinal', 'data', 'i', 'am', 'having', 'difficultly', 'figuring', 'out', 'what', 'to', 'search', 'for', 'on', 'google', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'an', 'example', 'scenario', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'image', 'you', 'own', 'a', 'business', 'that', 'has', 'an', 'average', 'of', 'walk', 'in', 'customers', 'every', 'day', 'one', 'day', 'you', 'decide', 'you', 'want', 'to', 'increase', 'the', 'number', 'of', 'walk', 'in', 'customers', 'arriving', 'at', 'your', 'store', 'each', 'day', 'so', 'you', 'pull', 'a', 'crazy', 'stunt', 'outside', 'your', 'store', 'to', 'get', 'attention', 'over', 'the', 'next', 'week', 'you', 'see', 'on', 'average', 'customers', 'a', 'day', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'over', 'the', 'next', 'few', 'months', 'you', 'again', 'decide', 'that', 'you', 'want', 'to', 'get', 'some', 'more', 'business', 'and', 'perhaps', 'sustain', 'it', 'a', 'bit', 'longer', 'so', 'you', 'try', 'some', 'other', 'random', 'things', 'to', 'get', 'more', 'customers', 'in', 'your', 'store', 'unfortunately', 'you', 'are', 'not', 'the', 'best', 'marketer', 'and', 'some', 'of', 'your', 'tactics', 'have', 'little', 'or', 'no', 'effect', 'and', 'others', 'even', 'have', 'a', 'negative', 'impact', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'methodology', 'could', 'i', 'use', 'to', 'determine', 'the', 'probability', 'that', 'any', 'one', 'individual', 'event', 'positively', 'or', 'negatively', 'impacted', 'the', 'number', 'of', 'walk', 'in', 'customers', 'i', 'am', 'fully', 'aware', 'that', 'correlation', 'does', 'not', 'necessarily', 'equal', 'causation', 'but', 'what', 'methods', 'could', 'i', 'use', 'to', 'determine', 'the', 'likely', 'increase', 'or', 'decrease', 'in', 'your', 'business', 's', 'daily', 'walk', 'in', 'client', 's', 'following', 'a', 'specific', 'event', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'not', 'interested', 'in', 'analyzing', 'whether', 'or', 'not', 'there', 'is', 'a', 'correlation', 'between', 'your', 'attempts', 'to', 'increase', 'the', 'number', 'of', 'walk', 'in', 'customers', 'but', 'rather', 'whether', 'or', 'not', 'any', 'one', 'single', 'event', 'independent', 'of', 'all', 'others', 'was', 'impactful', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'realize', 'that', 'this', 'example', 'is', 'rather', 'contrived', 'and', 'simplistic', 'so', 'i', 'will', 'also', 'give', 'you', 'a', 'brief', 'description', 'of', 'the', 'actual', 'data', 'that', 'i', 'am', 'using', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'attempting', 'to', 'determine', 'the', 'impact', 'that', 'a', 'particular', 'marketing', 'agency', 'has', 'on', 'their', 'client', 's', 'website', 'when', 'they', 'publish', 'new', 'content', 'perform', 'social', 'media', 'campaigns', 'etc', 'for', 'any', 'one', 'specific', 'agency', 'they', 'may', 'have', 'anywhere', 'from', 'to', 'clients', 'each', 'client', 'has', 'websites', 'ranging', 'in', 'size', 'from', 'pages', 'to', 'well', 'over', 'million', 'over', 'the', 'course', 'of', 'the', 'past', 'year', 'each', 'agency', 'has', 'annotated', 'all', 'of', 'their', 'work', 'for', 'each', 'client', 'including', 'the', 'type', 'of', 'work', 'that', 'was', 'done', 'the', 'number', 'of', 'webpages', 'on', 'a', 'website', 'that', 'were', 'influenced', 'the', 'number', 'of', 'hours', 'spent', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'using', 'the', 'above', 'data', 'which', 'i', 'have', 'assembled', 'into', 'a', 'data', 'warehouse', 'placed', 'into', 'a', 'bunch', 'of', 'star', 'snowflake', 'schemas', 'i', 'need', 'to', 'determine', 'how', 'likely', 'it', 'was', 'that', 'any', 'one', 'piece', 'of', 'work', 'any', 'one', 'event', 'in', 'time', 'had', 'an', 'impact', 'on', 'the', 'traffic', 'hitting', 'any', 'all', 'pages', 'influenced', 'by', 'a', 'specific', 'piece', 'of', 'work', 'i', 'have', 'created', 'models', 'for', 'different', 'types', 'of', 'content', 'that', 'are', 'found', 'on', 'a', 'website', 'that', 'describes', 'the', 'typical', 'traffic', 'pattern', 'a', 'page', 'with', 'said', 'content', 'type', 'might', 'experience', 'from', 'launch', 'date', 'until', 'present', 'normalized', 'relative', 'to', 'the', 'appropriate', 'model', 'i', 'need', 'to', 'determine', 'the', 'highest', 'and', 'lowest', 'number', 'of', 'increased', 'or', 'decreased', 'visitors', 'a', 'specific', 'page', 'received', 'as', 'the', 'result', 'of', 'a', 'specific', 'piece', 'of', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'while', 'i', 'have', 'experience', 'with', 'basic', 'data', 'analysis', 'linear', 'and', 'multiple', 'regression', 'correlation', 'etc', 'i', 'am', 'at', 'a', 'loss', 'for', 'how', 'to', 'approach', 'solving', 'this', 'problem', 'whereas', 'in', 'the', 'past', 'i', 'have', 'typically', 'analyzed', 'data', 'with', 'multiple', 'measurements', 'for', 'a', 'given', 'axis', 'for', 'example', 'temperature', 'vs', 'thirst', 'vs', 'animal', 'and', 'determined', 'the', 'impact', 'on', 'thirst', 'that', 'increased', 'temperate', 'has', 'across', 'animals', 'i', 'feel', 'that', 'above', 'i', 'am', 'attempting', 'to', 'analyze', 'the', 'impact', 'of', 'a', 'single', 'event', 'at', 'some', 'point', 'in', 'time', 'for', 'a', 'non', 'linear', 'but', 'predictable', 'or', 'at', 'least', 'model', 'able', 'longitudinal', 'dataset', 'i', 'am', 'stumped', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'help', 'tips', 'pointers', 'recommendations', 'or', 'directions', 'would', 'be', 'extremely', 'helpful', 'and', 'i', 'would', 'be', 'eternally', 'grateful', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'working', 'with', 'a', 'dataset', 'with', 'a', 'wide', 'range', 'of', 'document', 'lengths', 'anywhere', 'from', 'a', 'single', 'word', 'to', 'a', 'full', 'page', 'of', 'text', 'in', 'addition', 'the', 'grammatical', 'structure', 'and', 'use', 'of', 'punctuation', 'varies', 'wildly', 'from', 'document', 'to', 'document', 'the', 'goal', 'is', 'to', 'classify', 'those', 'documents', 'into', 'one', 'of', 'about', 'categories', 'i', 'm', 'currently', 'using', 'ridge', 'regression', 'and', 'logistic', 'regression', 'for', 'the', 'task', 'and', 'cv', 'for', 'the', 'alpha', 'values', 'of', 'ridge', 'the', 'feature', 'vectors', 'are', 'tf', 'idf', 'ngrams', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'recently', 'i', 've', 'noticed', 'that', 'longer', 'documents', 'are', 'much', 'less', 'likely', 'to', 'be', 'categorized', 'why', 'might', 'this', 'be', 'the', 'case', 'and', 'how', 'can', 'one', 'quot', 'normalize', 'quot', 'for', 'this', 'kind', 'of', 'variation', 'as', 'a', 'more', 'general', 'question', 'how', 'does', 'one', 'typically', 'deal', 'with', 'diverse', 'data', 'sets', 'should', 'documents', 'be', 'grouped', 'based', 'off', 'of', 'metrics', 'like', 'document', 'length', 'use', 'of', 'punctuation', 'grammatical', 'rigor', 'etc', 'and', 'then', 'fed', 'through', 'different', 'classifiers', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'wonder', 'which', 'type', 'of', 'model', 'cross', 'validation', 'to', 'choose', 'for', 'classification', 'problem', 'k', 'fold', 'or', 'random', 'sub', 'sampling', 'bootstrap', 'sampling', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'best', 'guess', 'is', 'to', 'use', 'of', 'the', 'data', 'set', 'which', 'is', 'items', 'for', 'training', 'and', 'for', 'validation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'this', 'case', 'k', 'fold', 'gives', 'only', 'three', 'iterations', 'folds', 'which', 'is', 'not', 'enough', 'to', 'see', 'stable', 'average', 'error', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'on', 'the', 'other', 'hand', 'i', 'don', 't', 'like', 'random', 'sub', 'sampling', 'feature', 'that', 'some', 'items', 'won', 't', 'be', 'ever', 'selected', 'for', 'training', 'validation', 'and', 'some', 'will', 'be', 'used', 'more', 'than', 'once', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'classification', 'algorithms', 'used', 'random', 'forest', 'amp', 'amp', 'logistic', 'regression', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'multiclass', 'logistic', 'regression', 'model', 'with', 'a', 'large', 'number', 'of', 'features', 'numfeatures', 'gt', 'using', 'a', 'maximum', 'likelihood', 'estimation', 'based', 'on', 'the', 'cost', 'function', 'and', 'gradient', 'the', 'fmincg', 'algorithm', 'solves', 'the', 'problem', 'quickly', 'however', 'i', 'm', 'also', 'experimenting', 'with', 'a', 'different', 'cost', 'function', 'and', 'do', 'not', 'have', 'a', 'gradient', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'good', 'way', 'to', 'speed', 'up', 'the', 'calculation', 'process', 'e', 'g', 'is', 'there', 'a', 'different', 'algorithm', 'or', 'fmincg', 'setting', 'that', 'i', 'can', 'use', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'please', 'could', 'someone', 'recommend', 'a', 'paper', 'or', 'blog', 'post', 'that', 'describes', 'the', 'online', 'k', 'means', 'algorithm', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'load', 'of', 'documents', 'which', 'have', 'a', 'load', 'of', 'key', 'value', 'pairs', 'in', 'them', 'the', 'key', 'might', 'not', 'be', 'unique', 'so', 'there', 'might', 'be', 'multiple', 'keys', 'of', 'the', 'same', 'type', 'with', 'different', 'values', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'compare', 'the', 'similarity', 'of', 'the', 'keys', 'between', 'documents', 'more', 'specifically', 'the', 'string', 'similarity', 'of', 'these', 'values', 'i', 'am', 'thinking', 'of', 'using', 'something', 'like', 'the', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'smith', 'e', 'waterman_algorithm', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'smith', 'waterman', 'algorithm', 'lt', 'a', 'gt', 'to', 'compare', 'the', 'similarity', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 've', 'drawn', 'a', 'picture', 'of', 'how', 'i', 'm', 'thinking', 'about', 'representing', 'the', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'vc', 'em', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'values', 'in', 'the', 'cells', 'are', 'the', 'result', 'of', 'the', 'smith', 'waterman', 'algorithm', 'or', 'some', 'other', 'string', 'similarity', 'metric', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'image', 'that', 'this', 'matrix', 'represents', 'a', 'key', 'type', 'of', 'quot', 'things', 'quot', 'i', 'then', 'need', 'to', 'add', 'the', 'quot', 'things', 'quot', 'similarity', 'score', 'into', 'a', 'vector', 'of', 'or', 'thats', 'ok', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'i', 'can', 't', 'figure', 'out', 'is', 'how', 'i', 'determine', 'if', 'the', 'matrix', 'is', 'similar', 'or', 'not', 'similar', 'ideally', 'i', 'want', 'to', 'convert', 'the', 'matrix', 'to', 'an', 'number', 'between', 'and', 'and', 'then', 'i', 'll', 'just', 'set', 'a', 'threshold', 'to', 'score', 'it', 'as', 'either', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'ideas', 'how', 'i', 'can', 'create', 'a', 'score', 'of', 'the', 'matrix', 'does', 'anyone', 'know', 'any', 'algorithms', 'that', 'do', 'this', 'type', 'of', 'thing', 'obviously', 'things', 'like', 'how', 'smith', 'waterman', 'works', 'is', 'kind', 'of', 'applicable', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'is', 'a', 'general', 'recommendation', 'that', 'algorithms', 'in', 'ensemble', 'learning', 'combinations', 'should', 'be', 'different', 'in', 'nature', 'is', 'there', 'a', 'classification', 'table', 'a', 'scale', 'or', 'some', 'rules', 'that', 'allow', 'to', 'evaluate', 'how', 'far', 'away', 'are', 'the', 'algorithms', 'from', 'each', 'other', 'what', 'are', 'the', 'best', 'combinations', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'dataset', 'with', 'following', 'specifications', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'training', 'dataset', 'with', 'samples', 'with', 'positives', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'test', 'dataset', 'with', 'samples', 'with', 'positives', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'there', 'are', 'features', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'perform', 'a', 'binary', 'classification', 'say', 'the', 'issue', 'i', 'am', 'facing', 'is', 'that', 'the', 'data', 'is', 'very', 'biased', 'or', 'rather', 'sparse', 'after', 'normalization', 'and', 'scaling', 'the', 'data', 'along', 'with', 'some', 'feature', 'engineering', 'and', 'using', 'a', 'couple', 'of', 'different', 'algorithms', 'these', 'are', 'the', 'best', 'results', 'i', 'could', 'achieve', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'mean', 'square', 'error', 'xa', 'confusion', 'matrix', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'e', 'only', 'correct', 'positive', 'hits', 'this', 'is', 'using', 'logistic', 'regression', 'here', 'are', 'the', 'various', 'things', 'i', 'tried', 'with', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'different', 'algorithms', 'like', 'randomforest', 'decisiontree', 'svm', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'changing', 'parameters', 'value', 'to', 'call', 'the', 'function', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'some', 'intuition', 'based', 'feature', 'engineering', 'to', 'include', 'compounded', 'features', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'my', 'questions', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'can', 'i', 'do', 'to', 'improve', 'the', 'number', 'of', 'positive', 'hits', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'can', 'one', 'determine', 'if', 'there', 'is', 'an', 'overfit', 'in', 'such', 'a', 'case', 'i', 'have', 'tried', 'plotting', 'etc', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'at', 'what', 'point', 'could', 'one', 'conclude', 'if', 'maybe', 'this', 'is', 'the', 'best', 'possible', 'fit', 'i', 'could', 'have', 'which', 'seems', 'sad', 'considering', 'only', 'hits', 'out', 'of', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'a', 'way', 'i', 'could', 'make', 'the', 'positive', 'sample', 'instances', 'weigh', 'more', 'so', 'the', 'pattern', 'recognition', 'improves', 'leading', 'to', 'more', 'hits', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'which', 'graphical', 'plots', 'could', 'help', 'detect', 'outliers', 'or', 'some', 'intuition', 'about', 'which', 'pattern', 'would', 'fit', 'the', 'best', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'using', 'the', 'scikit', 'learn', 'library', 'with', 'python', 'and', 'all', 'implementations', 'are', 'library', 'functions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'are', 'the', 'results', 'with', 'a', 'few', 'other', 'algorithms', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'random', 'forest', 'classifier', 'n_estimators', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'decision', 'trees', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'more', 'often', 'than', 'not', 'data', 'i', 'am', 'working', 'with', 'is', 'not', 'clean', 'even', 'if', 'it', 'is', 'reasonably', 'clean', 'still', 'there', 'are', 'portions', 'that', 'need', 'to', 'be', 'fixed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'a', 'fraction', 'of', 'data', 'needs', 'it', 'i', 'write', 'a', 'script', 'and', 'incorporate', 'it', 'in', 'data', 'processing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'what', 'to', 'do', 'if', 'only', 'a', 'few', 'entries', 'needs', 'to', 'be', 'fixed', 'e', 'g', 'misspelled', 'city', 'or', 'zip', 'code', 'let', 's', 'focus', 'on', 'quot', 'small', 'data', 'quot', 'such', 'as', 'that', 'in', 'csv', 'files', 'or', 'a', 'relational', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'practical', 'problems', 'i', 'encountered', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'writing', 'a', 'general', 'script', 'trying', 'solve', 'all', 'similar', 'errors', 'may', 'give', 'unintended', 'consequences', 'e', 'g', 'matching', 'cities', 'that', 'are', 'different', 'but', 'happen', 'to', 'have', 'similar', 'names', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'copying', 'and', 'modifying', 'data', 'may', 'make', 'a', 'mess', 'as', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'generating', 'it', 'again', 'will', 'destroy', 'all', 'fixes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'when', 'there', 'are', 'more', 'errors', 'of', 'different', 'kinds', 'too', 'many', 'copies', 'of', 'the', 'same', 'file', 'result', 'and', 'it', 'is', 'hard', 'to', 'keep', 'track', 'of', 'them', 'all', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'writing', 'a', 'script', 'to', 'modify', 'particular', 'entries', 'seems', 'the', 'best', 'but', 'there', 'is', 'overhead', 'in', 'comparison', 'to', 'opening', 'csv', 'and', 'fixing', 'it', 'but', 'still', 'lt', 'em', 'gt', 'seems', 'lt', 'em', 'gt', 'to', 'be', 'the', 'best', 'and', 'either', 'we', 'need', 'to', 'create', 'more', 'copies', 'of', 'data', 'as', 'in', 'the', 'previous', 'point', 'or', 'run', 'the', 'script', 'every', 'time', 'we', 'load', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'the', 'best', 'practices', 'in', 'such', 'a', 'case', 'as', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'the', 'question', 'is', 'on', 'the', 'workflow', 'not', 'whether', 'to', 'use', 'it', 'or', 'not', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'my', 'particular', 'case', 'i', 'don', 't', 'want', 'the', 'end', 'user', 'to', 'see', 'misspelled', 'cities', 'and', 'even', 'worse', 'see', 'two', 'points', 'of', 'data', 'for', 'the', 'same', 'city', 'but', 'with', 'different', 'spelling', 'the', 'data', 'is', 'small', 'different', 'cities', 'so', 'manual', 'corrections', 'do', 'make', 'sense', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'data', 'science', 'and', 'machine', 'learning', 'include', 'a', 'lot', 'of', 'different', 'topics', 'and', 'it', 's', 'hard', 'to', 'stay', 'up', 'to', 'date', 'about', 'all', 'the', 'news', 'about', 'papers', 'researches', 'or', 'new', 'tutorials', 'and', 'tools', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'sources', 'do', 'you', 'use', 'to', 'get', 'all', 'the', 'information', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'use', 'mostly', 'reddit', 'as', 'my', 'first', 'source', 'and', 'the', 'subreddits', 'machine', 'learning', 'and', 'r', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'www', 'reddit', 'com', 'r', 'machinelearning', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'www', 'reddit', 'com', 'r', 'machinelearning', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'reddit', 'com', 'r', 'rstats', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'www', 'reddit', 'com', 'r', 'rstats', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'also', 'datatau', 'com', 'and', 'of', 'course', 'the', 'great', 'kdnuggets', 'page', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'very', 'first', 'thing', 'you', 'do', 'when', 'you', 'get', 'your', 'hands', 'on', 'a', 'new', 'data', 'set', 'assuming', 'it', 'is', 'cleaned', 'and', 'well', 'structured', 'please', 'share', 'sample', 'code', 'snippets', 'as', 'i', 'am', 'sure', 'this', 'would', 'be', 'extremely', 'helpful', 'for', 'both', 'beginners', 'and', 'experienced', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'often', 'am', 'building', 'a', 'model', 'classification', 'or', 'regression', 'where', 'i', 'have', 'some', 'predictor', 'variables', 'that', 'are', 'sequences', 'and', 'i', 'have', 'been', 'trying', 'to', 'find', 'technique', 'recommendations', 'for', 'summarizing', 'them', 'in', 'the', 'best', 'way', 'possible', 'for', 'inclusion', 'as', 'predictors', 'in', 'the', 'model', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'a', 'concrete', 'example', 'say', 'a', 'model', 'is', 'being', 'built', 'to', 'predict', 'if', 'a', 'customer', 'will', 'leave', 'the', 'company', 'in', 'the', 'next', 'days', 'anytime', 'between', 't', 'and', 't', 'thus', 'a', 'binary', 'outcome', 'one', 'of', 'the', 'predictors', 'available', 'is', 'the', 'level', 'of', 'the', 'customers', 'financial', 'balance', 'for', 'periods', 't_', 'to', 't', 'maybe', 'this', 'represents', 'monthly', 'observations', 'for', 'the', 'prior', 'months', 'i', 'e', 'measurements', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'ways', 'to', 'construct', 'features', 'from', 'this', 'series', 'i', 'use', 'descriptives', 'of', 'each', 'customers', 'series', 'such', 'as', 'the', 'mean', 'high', 'low', 'std', 'dev', 'fit', 'a', 'ols', 'regression', 'to', 'get', 'the', 'trend', 'are', 'their', 'other', 'methods', 'of', 'calculating', 'features', 'other', 'measures', 'of', 'change', 'or', 'volatility', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'add', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'mentioned', 'in', 'a', 'response', 'below', 'i', 'also', 'considered', 'but', 'forgot', 'to', 'add', 'here', 'using', 'dynamic', 'time', 'warping', 'dtw', 'and', 'then', 'hierarchical', 'clustering', 'on', 'the', 'resulting', 'distance', 'matrix', 'creating', 'some', 'number', 'of', 'clusters', 'and', 'then', 'using', 'the', 'cluster', 'membership', 'as', 'a', 'feature', 'scoring', 'test', 'data', 'would', 'likely', 'have', 'to', 'follow', 'a', 'process', 'where', 'the', 'dtw', 'was', 'done', 'on', 'new', 'cases', 'and', 'the', 'cluster', 'centroids', 'matching', 'the', 'new', 'data', 'series', 'to', 'their', 'closest', 'centroids', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'cs', 'master', 'student', 'in', 'data', 'mining', 'my', 'supervisor', 'once', 'told', 'me', 'that', 'before', 'i', 'run', 'any', 'classifier', 'or', 'do', 'anything', 'with', 'a', 'dataset', 'i', 'must', 'fully', 'understand', 'the', 'data', 'and', 'make', 'sure', 'that', 'the', 'data', 'is', 'clean', 'and', 'correct', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'what', 'are', 'the', 'best', 'practices', 'to', 'understand', 'a', 'dataset', 'high', 'dimensional', 'with', 'numerical', 'and', 'nominal', 'attributes', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'practices', 'to', 'make', 'sure', 'the', 'dataset', 'is', 'clean', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'practices', 'to', 'make', 'sure', 'the', 'dataset', 'doesn', 't', 'have', 'wrong', 'values', 'or', 'so', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'hobby', 'project', 'which', 'i', 'am', 'contemplating', 'committing', 'to', 'as', 'a', 'way', 'of', 'increasing', 'my', 'so', 'far', 'limited', 'experience', 'of', 'machine', 'learning', 'i', 'have', 'taken', 'and', 'completed', 'the', 'coursera', 'mooc', 'on', 'the', 'topic', 'my', 'question', 'is', 'with', 'regards', 'to', 'the', 'feasibility', 'of', 'the', 'project', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'task', 'is', 'the', 'following', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'neighboring', 'cats', 'are', 'from', 'time', 'to', 'time', 'visiting', 'my', 'garden', 'which', 'i', 'dislike', 'since', 'they', 'tend', 'to', 'defecate', 'on', 'my', 'lawn', 'i', 'would', 'like', 'to', 'have', 'a', 'warning', 'system', 'that', 'alerts', 'me', 'when', 'there', 's', 'a', 'cat', 'present', 'so', 'that', 'i', 'may', 'go', 'chase', 'it', 'off', 'using', 'my', 'super', 'soaker', 'for', 'simplicity', 's', 'sake', 'say', 'that', 'i', 'only', 'care', 'about', 'a', 'cat', 'with', 'black', 'and', 'white', 'coloring', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'setup', 'a', 'raspberry', 'pi', 'with', 'camera', 'module', 'that', 'can', 'capture', 'video', 'and', 'or', 'pictures', 'of', 'a', 'part', 'of', 'the', 'garden', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'sample', 'image', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'cnqus', 'jpg', 'quot', 'alt', 'quot', 'sample', 'garden', 'image', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'first', 'idea', 'was', 'to', 'train', 'a', 'classifier', 'to', 'identify', 'cat', 'or', 'cat', 'like', 'objects', 'but', 'after', 'realizing', 'that', 'i', 'will', 'be', 'unable', 'to', 'obtain', 'a', 'large', 'enough', 'number', 'of', 'positive', 'samples', 'i', 'have', 'abandoned', 'that', 'in', 'favor', 'of', 'anomaly', 'detection', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'estimate', 'that', 'if', 'i', 'captured', 'a', 'photo', 'every', 'second', 'of', 'the', 'day', 'i', 'would', 'end', 'up', 'with', 'maybe', 'five', 'photos', 'containing', 'cats', 'out', 'of', 'about', 'with', 'sunlight', 'per', 'day', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'feasible', 'using', 'anomaly', 'detection', 'if', 'so', 'what', 'features', 'would', 'you', 'suggest', 'my', 'ideas', 'so', 'far', 'would', 'be', 'to', 'simply', 'count', 'the', 'number', 'of', 'pixels', 'with', 'that', 'has', 'certain', 'colors', 'do', 'some', 'kind', 'of', 'blob', 'detection', 'image', 'segmenting', 'which', 'i', 'do', 'not', 'know', 'how', 'do', 'to', 'and', 'would', 'thus', 'like', 'to', 'avoid', 'and', 'perform', 'the', 'same', 'color', 'analysis', 'on', 'them', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'written', 'a', 'simple', 'recommender', 'which', 'generates', 'recommendations', 'for', 'users', 'based', 'on', 'what', 'they', 'have', 'clicked', 'the', 'recommender', 'generates', 'a', 'data', 'file', 'with', 'the', 'following', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'userid', 'userid', 'simmilarity', 'between', 'and', 'closer', 'to', 'the', 'more', 'similar', 'the', 'users', 'xa', 'a', 'b', 'xa', 'a', 'c', 'xa', 'a', 'd', 'xa', 'a', 'e', 'xa', 'e', 'b', 'xa', 'e', 'c', 'xa', 'e', 'd', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'looked', 'at', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'mbostock', 'd', 'wiki', 'gallery', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'some', 'graphs', 'lt', 'a', 'gt', 'but', 'i', 'm', 'not', 'sure', 'which', 'one', 'to', 'use', 'or', 'if', 'are', 'there', 'other', 'ones', 'that', 'will', 'better', 'display', 'the', 'user', 'similarities', 'from', 'the', 'dataset', 'above', 'any', 'suggestions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'aiming', 'this', 'visualization', 'at', 'business', 'users', 'who', 'are', 'not', 'at', 'all', 'technical', 'i', 'would', 'just', 'like', 'to', 'show', 'them', 'an', 'easy', 'to', 'understand', 'visual', 'that', 'details', 'how', 'similar', 'some', 'users', 'are', 'and', 'so', 'convince', 'the', 'business', 'that', 'for', 'these', 'users', 'the', 'recommendation', 'system', 'is', 'useful', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'steve', 'kallestad', 'do', 'you', 'mean', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'zyqr', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'i', 'increase', 'the', 'number', 'of', 'trees', 'in', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'scikit', 'learn', 'lt', 'a', 'gt', 's', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'modules', 'generated', 'sklearn', 'ensemble', 'gradientboostingregressor', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'lt', 'code', 'gt', 'gradientboostingregressor', 'lt', 'code', 'gt', 'lt', 'a', 'gt', 'i', 'get', 'more', 'negative', 'predictions', 'even', 'though', 'there', 'are', 'no', 'negative', 'values', 'in', 'my', 'training', 'or', 'testing', 'set', 'i', 'have', 'about', 'features', 'most', 'of', 'which', 'are', 'binary', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'of', 'the', 'parameters', 'that', 'i', 'was', 'tuning', 'were', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'the', 'number', 'of', 'trees', 'iterations', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'learning', 'depth', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'and', 'learning', 'rate', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'percentage', 'of', 'negative', 'values', 'seemed', 'to', 'max', 'at', 'the', 'learning', 'depth', 'of', 'stumps', 'seemed', 'to', 'have', 'the', 'largest', 'of', 'negative', 'values', 'this', 'percentage', 'also', 'seemed', 'to', 'increase', 'with', 'more', 'trees', 'and', 'a', 'smaller', 'learning', 'rate', 'the', 'dataset', 'is', 'from', 'one', 'of', 'the', 'kaggle', 'playground', 'competitions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'code', 'is', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'from', 'sklearn', 'ensemble', 'import', 'gradientboostingregressor', 'xa', 'xa', 'x_train', 'x_test', 'y_train', 'y_test', 'train_test_split', 'x', 'y', 'xa', 'xa', 'reg', 'gradientboostingregressor', 'n_estimators', 'max_depth', 'loss', 'ls', 'learning_rate', 'xa', 'xa', 'reg', 'fit', 'x_train', 'y_train', 'xa', 'xa', 'ypred', 'reg', 'predict', 'x_test', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'many', 'times', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'named', 'entity_recognition', 'quot', 'gt', 'named', 'entity', 'recognition', 'lt', 'a', 'gt', 'ner', 'doesn', 't', 'tag', 'consecutive', 'nnps', 'as', 'one', 'ne', 'i', 'think', 'editing', 'the', 'ner', 'to', 'use', 'regexptagger', 'also', 'can', 'improve', 'the', 'ner', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'consider', 'the', 'following', 'input', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'quot', 'barack', 'obama', 'is', 'a', 'great', 'person', 'quot', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'the', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'tree', 's', 'tree', 'person', 'barack', 'nnp', 'tree', 'organization', 'obama', 'nnp', 'xa', 'is', 'vbz', 'a', 'dt', 'great', 'jj', 'person', 'nn', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'as', 'for', 'the', 'input', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'former', 'vice', 'president', 'dick', 'cheney', 'told', 'conservative', 'radio', 'host', 'laura', 'ingraham', 'that', 'he', 'quot', 'was', 'honored', 'quot', 'to', 'be', 'compared', 'to', 'darth', 'vader', 'while', 'in', 'office', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'output', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'tree', 's', 'former', 'jj', 'vice', 'nnp', 'president', 'nnp', 'xa', 'tree', 'ne', 'dick', 'nnp', 'cheney', 'nnp', 'told', 'vbd', 'conservative', 'jj', 'xa', 'radio', 'nn', 'host', 'nn', 'tree', 'ne', 'laura', 'nnp', 'ingraham', 'nnp', 'xa', 'that', 'in', 'he', 'prp', 'was', 'vbd', 'honored', 'vbn', 'xa', 'quot', 'quot', 'quot', 'quot', 'to', 'to', 'be', 'vb', 'compared', 'vbn', 'to', 'to', 'xa', 'tree', 'ne', 'darth', 'nnp', 'vader', 'nnp', 'while', 'in', 'in', 'in', 'xa', 'office', 'nn', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'lt', 'code', 'gt', 'vice', 'nnp', 'president', 'nnp', 'dick', 'nnp', 'cheney', 'nnp', 'lt', 'code', 'gt', 'is', 'correctly', 'extracted', 'so', 'i', 'think', 'if', 'lt', 'code', 'gt', 'nltk', 'ne_chunk', 'lt', 'code', 'gt', 'is', 'used', 'first', 'and', 'then', 'if', 'two', 'consecutive', 'trees', 'are', 'nnp', 'there', 'are', 'higher', 'chances', 'that', 'both', 'refer', 'to', 'one', 'entity', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'been', 'playing', 'with', 'nltk', 'toolkit', 'and', 'i', 'came', 'across', 'this', 'problem', 'a', 'lot', 'but', 'couldn', 't', 'find', 'a', 'satisfying', 'answer', 'any', 'suggestion', 'will', 'be', 'really', 'appreciated', 'i', 'm', 'looking', 'for', 'flaws', 'in', 'my', 'approach', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'rank', 'some', 'percentages', 'i', 'have', 'numerators', 'and', 'denominators', 'for', 'each', 'ratio', 'to', 'give', 'a', 'concrete', 'example', 'consider', 'ratio', 'as', 'lt', 'code', 'gt', 'total', 'graduates', 'total', 'students', 'lt', 'code', 'gt', 'in', 'a', 'school', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'the', 'issue', 'is', 'that', 'lt', 'code', 'gt', 'total', 'students', 'lt', 'code', 'gt', 'vary', 'over', 'a', 'long', 'range', 'smaller', 'schools', 'seem', 'to', 'have', 'higher', 'percentage', 'of', 'students', 'graduating', 'but', 'i', 'want', 'to', 'standardize', 'it', 'and', 'not', 'let', 'the', 'size', 'of', 'the', 'school', 'affect', 'the', 'ranking', 'is', 'there', 'a', 'way', 'to', 'do', 'it', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'new', 'to', 'the', 'world', 'of', 'text', 'mining', 'and', 'have', 'been', 'reading', 'up', 'on', 'annotators', 'at', 'places', 'like', 'the', 'lt', 'a', 'href', 'quot', 'http', 'uima', 'apache', 'org', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'uima', 'website', 'lt', 'a', 'gt', 'i', 'm', 'encountering', 'many', 'new', 'terms', 'like', 'named', 'entity', 'recognition', 'tokenizer', 'lemmatizer', 'gazetteer', 'etc', 'coming', 'from', 'a', 'layman', 'background', 'this', 'is', 'all', 'very', 'confusing', 'so', 'can', 'anyone', 'tell', 'me', 'or', 'link', 'to', 'resources', 'that', 'can', 'explain', 'what', 'the', 'main', 'categories', 'of', 'annotators', 'are', 'and', 'what', 'they', 'do', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'are', 'plenty', 'of', 'sources', 'which', 'provide', 'the', 'historical', 'stock', 'data', 'but', 'they', 'only', 'provide', 'the', 'ohlc', 'fields', 'along', 'with', 'volume', 'and', 'adjusted', 'close', 'also', 'a', 'couple', 'of', 'sources', 'i', 'found', 'provide', 'market', 'cap', 'data', 'sets', 'but', 'they', 're', 'restricted', 'to', 'us', 'stocks', 'yahoo', 'finance', 'provides', 'this', 'data', 'online', 'but', 'there', 's', 'no', 'option', 'to', 'download', 'it', 'or', 'none', 'i', 'am', 'aware', 'of', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'where', 'can', 'i', 'download', 'this', 'data', 'for', 'stocks', 'belonging', 'to', 'various', 'top', 'stock', 'exchanges', 'across', 'countries', 'by', 'using', 'their', 'ticker', 'name', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'some', 'way', 'to', 'download', 'it', 'via', 'yahoo', 'finance', 'or', 'google', 'finance', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'need', 'data', 'for', 'the', 'last', 'decade', 'or', 'so', 'and', 'hence', 'need', 'some', 'script', 'or', 'api', 'which', 'would', 'do', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'while', 'doing', 'a', 'google', 'image', 'search', 'the', 'page', 'displays', 'some', 'figured', 'out', 'categories', 'for', 'the', 'images', 'of', 'the', 'topic', 'being', 'searched', 'for', 'i', 'm', 'interested', 'in', 'learning', 'how', 'this', 'works', 'and', 'how', 'it', 'chooses', 'and', 'creates', 'categories', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'unfortunately', 'i', 'couldn', 't', 'find', 'much', 'about', 'it', 'at', 'all', 'is', 'anyone', 'able', 'to', 'shed', 'some', 'light', 'on', 'algorithms', 'they', 'may', 'be', 'using', 'to', 'do', 'this', 'and', 'what', 'basis', 'these', 'categories', 'are', 'created', 'from', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'i', 'search', 'for', 'quot', 'animals', 'quot', 'i', 'get', 'the', 'categories', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'quot', 'cute', 'quot', 'quot', 'baby', 'quot', 'quot', 'wild', 'quot', 'quot', 'farm', 'quot', 'quot', 'zoo', 'quot', 'quot', 'clipart', 'quot', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'go', 'into', 'quot', 'wild', 'quot', 'i', 'then', 'have', 'subcategories', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'quot', 'forest', 'quot', 'quot', 'baby', 'quot', 'quot', 'africa', 'quot', 'quot', 'clipart', 'quot', 'quot', 'rainforest', 'quot', 'quot', 'domestic', 'quot', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'new', 'to', 'machine', 'learning', 'but', 'i', 'have', 'an', 'interesting', 'problem', 'i', 'have', 'a', 'large', 'sample', 'of', 'people', 'and', 'visited', 'sites', 'some', 'people', 'have', 'indicated', 'gender', 'age', 'and', 'other', 'parameters', 'now', 'i', 'want', 'to', 'restore', 'these', 'parameters', 'to', 'each', 'user', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'way', 'do', 'i', 'look', 'for', 'which', 'algorithm', 'is', 'suitable', 'to', 'solve', 'this', 'problem', 'i', 'm', 'familiar', 'with', 'neural', 'networks', 'supervised', 'learning', 'but', 'it', 'seems', 'they', 'don', 't', 'fit', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'are', 'several', 'classic', 'datasets', 'for', 'machine', 'learning', 'classification', 'regression', 'tasks', 'the', 'most', 'popular', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'auto_examples', 'datasets', 'plot_iris_dataset', 'html', 'quot', 'gt', 'iris', 'flower', 'data', 'set', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'kaggle', 'com', 'c', 'titanic', 'gettingstarted', 'quot', 'gt', 'titanic', 'data', 'set', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'stat', 'ethz', 'ch', 'r', 'manual', 'r', 'devel', 'library', 'datasets', 'html', 'mtcars', 'html', 'quot', 'gt', 'motor', 'trend', 'cars', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'etc', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'does', 'anyone', 'know', 'similar', 'datasets', 'for', 'networks', 'analysis', 'graph', 'theory', 'more', 'concrete', 'i', 'm', 'looking', 'for', 'lt', 'strong', 'gt', 'gold', 'standard', 'lt', 'strong', 'gt', 'datasets', 'for', 'comparing', 'evaluating', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'centrality', 'measures', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'network', 'clustering', 'algorithms', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'don', 't', 'need', 'a', 'huge', 'list', 'of', 'publicly', 'available', 'networks', 'graphs', 'but', 'a', 'couple', 'of', 'actually', 'must', 'know', 'datasets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 's', 'quite', 'difficult', 'to', 'provide', 'exact', 'features', 'for', 'quot', 'gold', 'standard', 'dataset', 'quot', 'but', 'here', 'are', 'some', 'thoughts', 'i', 'think', 'real', 'classic', 'dataset', 'should', 'satisfy', 'these', 'criteria', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'multiple', 'references', 'in', 'articles', 'and', 'textbooks', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'inclusion', 'in', 'well', 'known', 'network', 'analysis', 'software', 'packages', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'sufficient', 'time', 'of', 'existence', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'usage', 'in', 'a', 'number', 'of', 'courses', 'on', 'graph', 'analysis', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'concerning', 'my', 'field', 'of', 'interest', 'i', 'also', 'need', 'labeled', 'classes', 'for', 'vertices', 'and', 'or', 'precomputed', 'or', 'predefined', 'quot', 'authority', 'scores', 'quot', 'i', 'e', 'centrality', 'estimates', 'after', 'asking', 'this', 'question', 'i', 'continued', 'searching', 'and', 'here', 'are', 'some', 'suitable', 'examples', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'networkdata', 'ics', 'uci', 'edu', 'data', 'php', 'id', 'quot', 'gt', 'zachary', 's', 'karate', 'club', 'lt', 'a', 'gt', 'introduced', 'in', 'cited', 'more', 'than', 'k', 'times', 'according', 'to', 'google', 'scholar', 'vertexes', 'have', 'attribute', 'faction', 'which', 'can', 'be', 'used', 'for', 'clustering', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'orgnet', 'com', 'erdos', 'html', 'quot', 'gt', 'erdos', 'collaboration', 'network', 'lt', 'a', 'gt', 'unfortunately', 'i', 'haven', 't', 'find', 'this', 'network', 'in', 'form', 'of', 'data', 'file', 'but', 'it', 's', 'rather', 'famous', 'and', 'if', 'someone', 'will', 'enrich', 'network', 'with', 'mathematicians', 'specialisations', 'data', 'it', 'also', 'could', 'be', 'used', 'for', 'testing', 'clustering', 'algorithms', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'working', 'on', 'a', 'political', 'campaign', 'where', 'dozens', 'of', 'volunteers', 'will', 'be', 'conducting', 'door', 'knocking', 'promotions', 'over', 'the', 'next', 'few', 'weeks', 'given', 'a', 'list', 'with', 'names', 'addresses', 'and', 'long', 'lat', 'coordinates', 'what', 'algorithms', 'can', 'be', 'used', 'to', 'create', 'an', 'optimized', 'walk', 'list', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'building', 'a', 'recommender', 'system', 'and', 'using', 'svd', 'as', 'one', 'of', 'the', 'preprocessing', 'techniques', 'xa', 'however', 'i', 'want', 'to', 'normalize', 'all', 'my', 'preprocessed', 'data', 'between', 'and', 'because', 'all', 'of', 'my', 'similarity', 'measures', 'cosine', 'pearson', 'euclidean', 'depend', 'on', 'that', 'assumption', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'after', 'i', 'take', 'the', 'svd', 'a', 'usv', 't', 'is', 'there', 'a', 'standard', 'way', 'to', 'normalize', 'the', 'matrix', 'a', 'between', 'and', 'thanks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'i', 'want', 'all', 'of', 'my', 'similarity', 'measurements', 'to', 'give', 'results', 'between', 'and', 'and', 'my', 'normalized', 'euclidean', 'distance', 'in', 'particular', 'fails', 'if', 'the', 'input', 'matrix', 'does', 'not', 'have', 'values', 'between', 'and', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'brand', 'new', 'to', 'the', 'field', 'of', 'data', 'science', 'want', 'to', 'break', 'into', 'it', 'and', 'there', 'are', 'so', 'many', 'tools', 'out', 'there', 'these', 'vms', 'have', 'alot', 'of', 'software', 'on', 'them', 'but', 'i', 'haven', 't', 'been', 'able', 'to', 'find', 'any', 'side', 'by', 'side', 'comparison', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 's', 'a', 'start', 'from', 'my', 'research', 'but', 'if', 'someone', 'could', 'tell', 'me', 'that', 'one', 'is', 'objectively', 'more', 'rich', 'featured', 'with', 'a', 'larger', 'community', 'of', 'support', 'and', 'useful', 'to', 'get', 'started', 'then', 'that', 'would', 'help', 'greatly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'datasciencetoolkit', 'org', 'gt', 'vm', 'is', 'on', 'vagrant', 'cloud', 'gb', 'and', 'seems', 'to', 'be', 'more', 'quot', 'hip', 'quot', 'with', 'r', 'ipython', 'notebook', 'and', 'other', 'useful', 'command', 'line', 'tools', 'html', 'gt', 'txt', 'json', 'gt', 'xml', 'etc', 'there', 'is', 'a', 'book', 'being', 'released', 'in', 'august', 'with', 'detail', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'datasciencetoolbox', 'org', 'gt', 'vm', 'is', 'a', 'vagrant', 'box', 'gb', 'downloadable', 'from', 'their', 'website', 'there', 'seems', 'to', 'be', 'more', 'features', 'here', 'and', 'more', 'literature', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'just', 'learned', 'about', 'regularisation', 'as', 'an', 'approach', 'to', 'control', 'over', 'fitting', 'and', 'i', 'would', 'like', 'to', 'incorporate', 'the', 'idea', 'into', 'a', 'simple', 'implementation', 'of', 'backpropagation', 'and', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'multilayer_perceptron', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'multilayer', 'perceptron', 'lt', 'a', 'gt', 'mlp', 'that', 'i', 'put', 'together', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'currently', 'to', 'avoid', 'over', 'fitting', 'i', 'cross', 'validate', 'and', 'keep', 'the', 'network', 'with', 'best', 'score', 'so', 'far', 'on', 'the', 'validation', 'set', 'this', 'works', 'ok', 'but', 'adding', 'regularisation', 'would', 'benefit', 'me', 'in', 'that', 'correct', 'choice', 'of', 'the', 'regularisation', 'algorithm', 'and', 'parameter', 'would', 'make', 'my', 'network', 'converge', 'on', 'a', 'non', 'overfit', 'model', 'more', 'systematically', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'formula', 'i', 'have', 'for', 'the', 'update', 'term', 'from', 'coursera', 'ml', 'course', 'is', 'stated', 'as', 'a', 'batch', 'update', 'e', 'g', 'for', 'each', 'weight', 'after', 'summing', 'all', 'the', 'applicable', 'deltas', 'for', 'the', 'whole', 'training', 'set', 'from', 'error', 'propagation', 'an', 'adjustment', 'of', 'lt', 'code', 'gt', 'lambda', 'current_weight', 'lt', 'code', 'gt', 'is', 'added', 'as', 'well', 'before', 'the', 'combined', 'delta', 'is', 'subtracted', 'at', 'the', 'end', 'of', 'the', 'batch', 'where', 'lt', 'code', 'gt', 'lambda', 'lt', 'code', 'gt', 'is', 'the', 'regularisation', 'parameter', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'implementation', 'of', 'backpropagation', 'uses', 'per', 'item', 'weight', 'updates', 'i', 'am', 'concerned', 'that', 'i', 'cannot', 'just', 'copy', 'the', 'batch', 'approach', 'although', 'it', 'looks', 'ok', 'intuitively', 'to', 'me', 'lt', 'em', 'gt', 'does', 'a', 'smaller', 'regularisation', 'term', 'per', 'item', 'work', 'just', 'as', 'well', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'lt', 'code', 'gt', 'lambda', 'current_weight', 'n', 'lt', 'code', 'gt', 'where', 'n', 'is', 'size', 'of', 'training', 'set', 'at', 'first', 'glance', 'this', 'looks', 'reasonable', 'i', 'could', 'not', 'find', 'anything', 'on', 'the', 'subject', 'though', 'and', 'i', 'wonder', 'if', 'that', 'is', 'because', 'regularisation', 'does', 'not', 'work', 'as', 'well', 'with', 'a', 'per', 'item', 'update', 'or', 'even', 'goes', 'under', 'a', 'different', 'name', 'or', 'altered', 'formula', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'understand', 'hadoop', 'mapreduce', 'and', 'its', 'features', 'but', 'i', 'am', 'confused', 'about', 'r', 'mapreduce', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'difference', 'i', 'have', 'read', 'is', 'that', 'r', 'utilizes', 'maximum', 'ram', 'so', 'do', 'perform', 'parallel', 'processing', 'integrated', 'r', 'with', 'hadoop', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'my', 'doubt', 'is', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'r', 'can', 'do', 'all', 'stats', 'math', 'and', 'data', 'science', 'related', 'stuff', 'but', 'why', 'r', 'mapreduce', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'any', 'new', 'task', 'i', 'can', 'achieve', 'by', 'using', 'r', 'mapreduce', 'instead', 'of', 'hadoop', 'mapreduce', 'if', 'yes', 'please', 'specify', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'we', 'can', 'achieve', 'the', 'task', 'by', 'using', 'r', 'with', 'hadoop', 'directly', 'but', 'what', 'is', 'the', 'importance', 'of', 'mapreduce', 'in', 'r', 'and', 'how', 'it', 'is', 'different', 'from', 'normal', 'mapreduce', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'matrix', 'that', 'is', 'populated', 'with', 'discrete', 'elements', 'and', 'i', 'need', 'to', 'cluster', 'them', 'using', 'r', 'into', 'intact', 'groups', 'so', 'for', 'example', 'take', 'this', 'matrix', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'a', 'b', 'b', 'c', 'a', 'xa', 'a', 'a', 'b', 'a', 'a', 'xa', 'a', 'b', 'b', 'c', 'c', 'xa', 'a', 'a', 'a', 'a', 'a', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'would', 'be', 'two', 'separate', 'clusters', 'for', 'a', 'two', 'separate', 'clusters', 'for', 'c', 'and', 'one', 'cluster', 'for', 'b', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'output', 'i', 'm', 'looking', 'for', 'would', 'ideally', 'assign', 'a', 'unique', 'id', 'to', 'each', 'cluster', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'right', 'now', 'i', 'wrote', 'a', 'code', 'that', 'does', 'this', 'recursively', 'by', 'just', 'iteratively', 'checking', 'nearest', 'neighbor', 'but', 'it', 'quickly', 'overflows', 'when', 'the', 'matrix', 'gets', 'large', 'i', 'e', 'x', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'built', 'in', 'function', 'in', 'r', 'that', 'can', 'do', 'this', 'i', 'looked', 'into', 'raster', 'and', 'image', 'processing', 'but', 'no', 'luck', 'i', 'm', 'convinced', 'it', 'must', 'be', 'out', 'there', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'using', 'several', 'different', 'classifiers', 'on', 'various', 'entities', 'extracted', 'from', 'text', 'and', 'using', 'precision', 'recall', 'as', 'a', 'summary', 'of', 'how', 'well', 'each', 'separate', 'classifier', 'performs', 'across', 'a', 'given', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'wondering', 'if', 'there', 's', 'a', 'meaningful', 'way', 'of', 'comparing', 'the', 'performance', 'of', 'these', 'classifiers', 'in', 'a', 'similar', 'way', 'but', 'which', 'also', 'takes', 'into', 'account', 'the', 'total', 'numbers', 'of', 'each', 'entity', 'in', 'the', 'test', 'data', 'that', 's', 'being', 'classified', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'currently', 'i', 'm', 'using', 'precision', 'recall', 'as', 'a', 'measure', 'of', 'performance', 'so', 'might', 'have', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'precision', 'recall', 'xa', 'person', 'classifier', 'xa', 'company', 'classifier', 'xa', 'cheese', 'classifier', 'xa', 'egg', 'classifier', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'the', 'dataset', 'i', 'm', 'running', 'these', 'on', 'might', 'contain', 'k', 'people', 'k', 'companies', 'cheeses', 'and', 'egg', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'is', 'there', 'a', 'summary', 'statistic', 'i', 'can', 'add', 'to', 'the', 'above', 'table', 'which', 'also', 'takes', 'into', 'account', 'the', 'total', 'number', 'of', 'each', 'item', 'or', 'is', 'there', 'some', 'way', 'of', 'measuring', 'the', 'fact', 'that', 'e', 'g', 'prec', 'rec', 'on', 'the', 'egg', 'classifier', 'might', 'not', 'be', 'meaningful', 'with', 'only', 'data', 'item', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'let', 's', 'say', 'we', 'had', 'hundreds', 'of', 'such', 'classifiers', 'i', 'guess', 'i', 'm', 'looking', 'for', 'a', 'good', 'way', 'to', 'answer', 'questions', 'like', 'quot', 'which', 'classifiers', 'are', 'underperforming', 'which', 'classifiers', 'lack', 'sufficient', 'test', 'data', 'to', 'tell', 'whether', 'they', 're', 'underperforming', 'quot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'a', 'fraud', 'detection', 'system', 'in', 'this', 'field', 'new', 'frauds', 'appear', 'regularly', 'so', 'that', 'new', 'features', 'have', 'to', 'be', 'added', 'to', 'the', 'model', 'on', 'ongoing', 'basis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'wonder', 'what', 'is', 'the', 'best', 'way', 'to', 'handle', 'it', 'from', 'the', 'development', 'process', 'perspective', 'just', 'adding', 'a', 'new', 'feature', 'into', 'the', 'feature', 'vector', 'and', 're', 'training', 'the', 'classifier', 'seems', 'to', 'be', 'a', 'naive', 'approach', 'because', 'too', 'much', 'time', 'will', 'be', 'spent', 'for', 're', 'learning', 'of', 'the', 'old', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'thinking', 'along', 'the', 'way', 'of', 'training', 'a', 'classifier', 'for', 'each', 'feature', 'or', 'a', 'couple', 'of', 'related', 'features', 'and', 'then', 'combining', 'the', 'results', 'of', 'those', 'classifiers', 'with', 'an', 'overall', 'classifier', 'are', 'there', 'any', 'drawbacks', 'of', 'this', 'approach', 'how', 'can', 'i', 'choose', 'an', 'algorithm', 'for', 'the', 'overall', 'classifier', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'new', 'to', 'machine', 'learning', 'i', 'have', 'a', 'task', 'at', 'hand', 'of', 'predicting', 'click', 'probability', 'given', 'user', 'information', 'like', 'city', 'state', 'os', 'version', 'os', 'family', 'device', 'browser', 'family', 'browser', 'version', 'etc', 'i', 'have', 'been', 'advised', 'to', 'try', 'logit', 'since', 'logit', 'seems', 'to', 'be', 'what', 'ms', 'and', 'google', 'are', 'using', 'i', 'have', 'some', 'questions', 'regarding', 'logistic', 'regression', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'click', 'and', 'non', 'click', 'is', 'a', 'very', 'very', 'unbalanced', 'class', 'and', 'the', 'simple', 'glm', 'predictions', 'do', 'not', 'look', 'good', 'how', 'can', 'i', 'make', 'the', 'data', 'work', 'better', 'with', 'the', 'glm', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'all', 'the', 'variables', 'i', 'have', 'are', 'categorical', 'and', 'things', 'like', 'device', 'and', 'city', 'can', 'be', 'numerous', 'also', 'the', 'frequency', 'of', 'occurrence', 'of', 'some', 'devices', 'or', 'some', 'cities', 'can', 'be', 'very', 'very', 'low', 'how', 'can', 'i', 'deal', 'with', 'this', 'distribution', 'of', 'categorical', 'variables', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'of', 'the', 'variables', 'that', 'we', 'get', 'is', 'device', 'id', 'this', 'is', 'a', 'very', 'unique', 'feature', 'that', 'can', 'be', 'translated', 'to', 'a', 'user', 's', 'identity', 'how', 'can', 'i', 'make', 'use', 'of', 'it', 'in', 'logit', 'or', 'should', 'it', 'be', 'used', 'in', 'a', 'completely', 'different', 'model', 'based', 'on', 'user', 'identity', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'working', 'on', 'a', 'project', 'that', 'would', 'benefit', 'from', 'personalized', 'predictions', 'given', 'an', 'input', 'document', 'a', 'set', 'of', 'output', 'documents', 'and', 'a', 'history', 'of', 'user', 'behavior', 'i', 'd', 'like', 'to', 'predict', 'which', 'of', 'the', 'output', 'documents', 'are', 'clicked', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'short', 'i', 'm', 'wondering', 'what', 'the', 'typical', 'approach', 'to', 'this', 'kind', 'of', 'personalization', 'problem', 'is', 'are', 'models', 'trained', 'per', 'user', 'or', 'does', 'a', 'single', 'global', 'model', 'take', 'in', 'summary', 'statistics', 'of', 'past', 'user', 'behavior', 'to', 'help', 'inform', 'that', 'decision', 'per', 'user', 'models', 'won', 't', 'be', 'accurate', 'until', 'the', 'user', 'has', 'been', 'active', 'for', 'a', 'while', 'while', 'most', 'global', 'models', 'have', 'to', 'take', 'in', 'a', 'fixed', 'length', 'feature', 'vector', 'meaning', 'we', 'more', 'or', 'less', 'have', 'to', 'compress', 'a', 'stream', 'of', 'past', 'events', 'into', 'a', 'smaller', 'number', 'of', 'summary', 'statistics', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'searching', 'for', 'labeled', 'datasets', 'to', 'use', 'to', 'train', 'a', 'model', 'to', 'extract', 'named', 'entities', 'from', 'informal', 'text', 'think', 'something', 'similar', 'to', 'tweets', 'because', 'capitalization', 'and', 'grammar', 'are', 'often', 'lacking', 'in', 'the', 'documents', 'in', 'my', 'dataset', 'i', 'm', 'looking', 'for', 'out', 'of', 'domain', 'data', 'that', 's', 'a', 'bit', 'more', 'quot', 'informal', 'quot', 'than', 'the', 'news', 'articles', 'and', 'journal', 'entries', 'that', 'many', 'of', 'today', 's', 'state', 'of', 'the', 'art', 'named', 'entity', 'recognition', 'systems', 'are', 'trained', 'on', 'any', 'recommendations', 'so', 'far', 'i', 've', 'only', 'been', 'able', 'to', 'locate', 'k', 'tokens', 'from', 'twitter', 'published', 'here', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'aritter', 'twitter_nlp', 'blob', 'master', 'data', 'annotated', 'ner', 'txt', 'quot', 'gt', 'https', 'github', 'com', 'aritter', 'twitter_nlp', 'blob', 'master', 'data', 'annotated', 'ner', 'txt', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'for', 'a', 'recommendation', 'system', 'i', 'm', 'using', 'cosine', 'similarity', 'to', 'compute', 'similarities', 'between', 'items', 'however', 'for', 'items', 'with', 'small', 'amounts', 'of', 'data', 'i', 'd', 'like', 'to', 'bin', 'them', 'under', 'a', 'general', 'quot', 'average', 'quot', 'category', 'in', 'the', 'general', 'not', 'mathematical', 'sense', 'to', 'accomplish', 'this', 'i', 'm', 'currently', 'trying', 'to', 'create', 'a', 'synthetic', 'observation', 'to', 'represent', 'that', 'middle', 'of', 'the', 'road', 'point', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'for', 'example', 'if', 'these', 'were', 'my', 'observations', 'rows', 'are', 'observations', 'cols', 'are', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'strategy', 'where', 'i', 'd', 'simply', 'take', 'the', 'actual', 'average', 'of', 'all', 'features', 'across', 'observations', 'would', 'generate', 'a', 'synthetic', 'datapoint', 'such', 'as', 'follows', 'which', 'i', 'd', 'then', 'append', 'to', 'the', 'matrix', 'before', 'doing', 'the', 'similarity', 'calculation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'while', 'this', 'might', 'work', 'well', 'with', 'certain', 'similarity', 'metrics', 'e', 'g', 'l', 'distance', 'i', 'm', 'sure', 'there', 'are', 'much', 'better', 'ways', 'for', 'cosine', 'similarity', 'though', 'at', 'the', 'moment', 'i', 'm', 'having', 'trouble', 'reasoning', 'my', 'way', 'through', 'angles', 'between', 'lines', 'in', 'high', 'dimensional', 'space', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'ideas', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'build', 'a', 'nonparametric', 'density', 'function', 'for', 'a', 'fairly', 'large', 'dataset', 'that', 'can', 'be', 'evaluated', 'efficently', 'and', 'can', 'be', 'updated', 'efficiently', 'when', 'new', 'points', 'are', 'added', 'there', 'will', 'only', 'ever', 'be', 'a', 'maximum', 'of', 'independent', 'variables', 'but', 'we', 'can', 'start', 'off', 'with', 'lets', 'use', 'a', 'gaussian', 'kernel', 'let', 'the', 'result', 'be', 'a', 'probability', 'density', 'function', 'i', 'e', 'its', 'volume', 'will', 'be', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'each', 'evaluation', 'we', 'can', 'omit', 'all', 'points', 'for', 'which', 'the', 'evaluation', 'point', 'is', 'outside', 'a', 'certain', 'ellipsoid', 'corresponding', 'to', 'the', 'minimum', 'gaussian', 'value', 'we', 'care', 'about', 'we', 'can', 'change', 'this', 'threshold', 'for', 'accuracy', 'or', 'performance', 'and', 'the', 'maximum', 'number', 'of', 'points', 'inside', 'the', 'threshold', 'will', 'depend', 'on', 'the', 'chosen', 'covariance', 'matrix', 'of', 'the', 'kernel', 'then', 'we', 'can', 'evaluate', 'the', 'distribution', 'approximately', 'using', 'the', 'subset', 'of', 'points', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'we', 'use', 'a', 'fixed', 'kernel', 'then', 'we', 'can', 'use', 'the', 'eigenvalues', 'and', 'eigenvectors', 'we', 'get', 'from', 'the', 'covariance', 'matrix', 'to', 'transform', 'each', 'point', 'so', 'that', 'the', 'threshold', 'ellipsoid', 'is', 'a', 'fixed', 'circle', 'we', 'can', 'then', 'shove', 'all', 'the', 'transformed', 'points', 'into', 'a', 'spatial', 'index', 'and', 'efficiently', 'find', 'all', 'points', 'within', 'the', 'required', 'radius', 'of', 'the', 'evaluation', 'point', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'we', 'would', 'the', 'kernel', 'to', 'be', 'variable', 'for', 'two', 'reasons', 'to', 'fit', 'the', 'data', 'better', 'and', 'because', 'adding', 'or', 'modifying', 'points', 'would', 'require', 'the', 'fixed', 'kernel', 'to', 'be', 'updated', 'which', 'would', 'mean', 'that', 'the', 'entire', 'data', 'set', 'would', 'need', 'to', 'be', 'reindexed', 'with', 'a', 'variable', 'kernel', 'we', 'could', 'make', 'new', 'updated', 'points', 'only', 'affect', 'the', 'closest', 'points', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'specifically', 'is', 'there', 'a', 'spatial', 'index', 'that', 'can', 'efficiently', 'find', 'ellipses', 'surrounding', 'a', 'given', 'point', 'from', 'a', 'set', 'of', 'around', 'million', 'ellipses', 'of', 'different', 'shapes', 'and', 'sizes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'more', 'generally', 'though', 'does', 'my', 'approach', 'look', 'sound', 'i', 'am', 'open', 'to', 'answers', 'like', 'quot', 'give', 'up', 'and', 'precalculate', 'a', 'grid', 'of', 'results', 'quot', 'answers', 'much', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'find', 'which', 'classification', 'methods', 'that', 'do', 'not', 'use', 'a', 'training', 'phase', 'are', 'available', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'scenario', 'is', 'gene', 'expression', 'based', 'classification', 'in', 'which', 'you', 'have', 'a', 'matrix', 'of', 'gene', 'expression', 'of', 'm', 'genes', 'features', 'and', 'n', 'samples', 'observations', 'xa', 'a', 'signature', 'for', 'each', 'class', 'is', 'also', 'provided', 'that', 'is', 'a', 'list', 'of', 'the', 'features', 'to', 'consider', 'to', 'define', 'to', 'which', 'class', 'belongs', 'a', 'sample', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'an', 'application', 'non', 'training', 'is', 'the', 'lt', 'a', 'href', 'quot', 'http', 'www', 'plosone', 'org', 'article', 'info', 'adoi', 'f', 'fjournal', 'pone', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'nearest', 'template', 'prediction', 'lt', 'a', 'gt', 'method', 'in', 'this', 'case', 'it', 'is', 'computed', 'the', 'cosine', 'distance', 'between', 'each', 'sample', 'and', 'each', 'signature', 'on', 'the', 'common', 'set', 'of', 'features', 'then', 'each', 'sample', 'is', 'assigned', 'to', 'the', 'nearest', 'class', 'the', 'sample', 'class', 'comparison', 'resulting', 'in', 'a', 'smaller', 'distance', 'no', 'already', 'classified', 'samples', 'are', 'needed', 'in', 'this', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'different', 'application', 'training', 'is', 'the', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'k', 'nearest_neighbors_algorithm', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'knn', 'lt', 'a', 'gt', 'method', 'in', 'which', 'we', 'have', 'a', 'set', 'of', 'already', 'labeled', 'samples', 'then', 'each', 'new', 'sample', 'is', 'labeled', 'depending', 'on', 'how', 'are', 'labeled', 'the', 'k', 'nearest', 'samples', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'any', 'other', 'non', 'training', 'methods', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'use', 'ann', 'to', 'automate', 'trading', 'currencies', 'preferably', 'usd', 'eur', 'or', 'usd', 'gbp', 'i', 'know', 'this', 'is', 'hard', 'and', 'may', 'not', 'be', 'straightforward', 'i', 'have', 'already', 'read', 'some', 'papers', 'and', 'done', 'some', 'experiments', 'but', 'without', 'much', 'luck', 'i', 'would', 'like', 'to', 'get', 'advice', 'from', 'experts', 'to', 'make', 'this', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'what', 'i', 'did', 'so', 'far', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'got', 'tick', 'by', 'tick', 'data', 'for', 'the', 'month', 'of', 'july', 'it', 'has', 'bid', 'ask', 'bid', 'volume', 'ask', 'volume', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'extracted', 'all', 'ticks', 'for', 'the', 'time', 'frame', 'pm', 'to', 'pm', 'for', 'all', 'days', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'from', 'this', 'data', 'created', 'a', 'data', 'set', 'where', 'each', 'entry', 'consists', 'of', 'n', 'bid', 'values', 'in', 'sequence', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'used', 'that', 'data', 'to', 'train', 'an', 'ann', 'with', 'n', 'inputs', 'and', 'the', 'output', 'is', 'the', 'forecasted', 'nth', 'bid', 'value', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'the', 'ann', 'had', 'n', 'inputs', 'neurons', 'n', 'hidden', 'and', 'output', 'neuron', 'input', 'layer', 'had', 'linear', 'tf', 'hidden', 'had', 'log', 'tf', 'and', 'output', 'had', 'linear', 'tf', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'trained', 'the', 'network', 'with', 'back', 'propagation', 'with', 'n', 'first', 'and', 'then', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'both', 'n', 'the', 'mse', 'did', 'not', 'drop', 'below', 'and', 'stayed', 'at', 'that', 'value', 'during', 'full', 'training', 'assuming', 'that', 'this', 'could', 'be', 'due', 'to', 'the', 'time', 'series', 'being', 'totally', 'random', 'i', 'used', 'the', 'r', 'package', 'to', 'find', 'partial', 'autocorrelation', 'on', 'the', 'data', 'set', 'pacf', 'this', 'gave', 'non', 'zero', 'values', 'for', 'and', 'lags', 'only', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'what', 'does', 'this', 'mean', 'exactly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'then', 'i', 'used', 'hurst', 'exponent', 'to', 'evaluate', 'the', 'randomness', 'in', 'r', 'hurst', 'values', 'showed', 'values', 'above', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'it', 'is', 'supposed', 'to', 'be', 'nearly', 'random', 'should', 'it', 'have', 'a', 'value', 'closer', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'repeated', 'the', 'training', 'of', 'the', 'ann', 'with', 'n', 'the', 'ann', 'was', 'trained', 'and', 'was', 'able', 'to', 'obtain', 'a', 'pretty', 'low', 'value', 'for', 'mse', 'however', 'the', 'calculated', 'output', 'from', 'this', 'ann', 'does', 'not', 'differ', 'much', 'from', 'the', 'n', 'th', 'bid', 'value', 'it', 'looks', 'like', 'ann', 'just', 'takes', 'the', 'last', 'bid', 'as', 'the', 'next', 'bid', 'i', 'tried', 'different', 'network', 'structures', 'all', 'multilayer', 'perceptions', 'different', 'training', 'parameters', 'etc', 'but', 'results', 'are', 'same', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'how', 'can', 'i', 'improve', 'the', 'accuracy', 'are', 'there', 'any', 'other', 'training', 'methods', 'than', 'backpropagation', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'suppose', 'i', 'want', 'to', 'use', 'cart', 'as', 'classification', 'tree', 'i', 'want', 'a', 'categorical', 'response', 'i', 'have', 'the', 'training', 'set', 'and', 'i', 'split', 'it', 'using', 'observation', 'labels', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'to', 'build', 'the', 'decision', 'tree', 'classification', 'tree', 'how', 'are', 'selected', 'the', 'features', 'to', 'decide', 'which', 'label', 'apply', 'to', 'testing', 'observations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'supposing', 'we', 'are', 'working', 'on', 'gene', 'expression', 'matrix', 'in', 'which', 'each', 'element', 'is', 'a', 'real', 'number', 'is', 'that', 'done', 'using', 'features', 'that', 'are', 'more', 'distant', 'between', 'classes', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'use', 'non', 'atomic', 'data', 'as', 'a', 'feature', 'for', 'a', 'prediction', 'xa', 'suppose', 'i', 'have', 'a', 'table', 'with', 'these', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'column', 'categorical', 'house', 'xa', 'column', 'numerical', 'xa', 'column', 'a', 'vector', 'xa', 'column', 'a', 'tree', 'boolean', 'categorical', 'xa', 'column', 'a', 'list', 'boolean', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'predict', 'classify', 'for', 'instance', 'column', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'making', 'something', 'to', 'automatically', 'respond', 'to', 'questions', 'any', 'type', 'of', 'question', 'like', 'quot', 'where', 'was', 'foo', 'born', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'first', 'make', 'a', 'query', 'to', 'a', 'search', 'engine', 'then', 'i', 'get', 'some', 'text', 'data', 'as', 'a', 'result', 'then', 'i', 'do', 'all', 'the', 'parsing', 'stuff', 'tagging', 'stemming', 'parsing', 'splitting', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'first', 'approach', 'was', 'to', 'make', 'a', 'table', 'each', 'row', 'with', 'a', 'line', 'of', 'text', 'and', 'a', 'lot', 'of', 'features', 'like', 'quot', 'first', 'word', 'quot', 'quot', 'tag', 'of', 'first', 'word', 'quot', 'quot', 'chunks', 'quot', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'with', 'this', 'approach', 'i', 'am', 'missing', 'the', 'relationships', 'between', 'the', 'sentences', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'know', 'if', 'there', 'is', 'an', 'algorithm', 'that', 'looks', 'inside', 'the', 'tree', 'structures', 'or', 'vectors', 'and', 'makes', 'the', 'relations', 'and', 'extract', 'whatever', 'is', 'relevant', 'for', 'predicting', 'classifying', 'i', 'd', 'prefer', 'to', 'know', 'about', 'a', 'library', 'that', 'does', 'that', 'than', 'an', 'algorithm', 'that', 'i', 'have', 'to', 'implement', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'new', 'to', 'rhadoop', 'and', 'also', 'to', 'rmr', 'xa', 'i', 'had', 'an', 'requirement', 'to', 'write', 'a', 'mapreduce', 'job', 'in', 'r', 'mapreduce', 'i', 'have', 'tried', 'writing', 'but', 'while', 'executing', 'this', 'it', 'gives', 'an', 'error', 'xa', 'tring', 'to', 'read', 'the', 'file', 'from', 'hdfs', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'error', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'error', 'in', 'mr', 'map', 'map', 'reduce', 'reduce', 'combine', 'combine', 'vectorized', 'reduce', 'xa', 'hadoop', 'streaming', 'failed', 'with', 'error', 'code', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'code', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'sys', 'setenv', 'hadoop_home', 'quot', 'opt', 'cloudera', 'parcels', 'cdh', 'cdh', 'p', 'lib', 'hadoop', 'quot', 'xa', 'sys', 'setenv', 'hadoop_cmd', 'quot', 'opt', 'cloudera', 'parcels', 'cdh', 'cdh', 'p', 'bin', 'hadoop', 'quot', 'xa', 'xa', 'sys', 'setenv', 'hadoop_streaming', 'quot', 'opt', 'cloudera', 'parcels', 'cdh', 'cdh', 'p', 'lib', 'hadoop', 'mapreduce', 'contrib', 'streaming', 'hadoop', 'streaming', 'mr', 'cdh', 'jar', 'quot', 'xa', 'library', 'rmr', 'xa', 'library', 'rhdfs', 'xa', 'hdfs', 'init', 'xa', 'day_file', 'hdfs', 'file', 'quot', 'hdfs', 'bikes_lr', 'day', 'csv', 'quot', 'quot', 'r', 'quot', 'xa', 'day_read', 'hdfs', 'read', 'day_file', 'xa', 'c', 'rawtochar', 'day_read', 'xa', 'xa', 'xtx', 'xa', 'values', 'from', 'dfs', 'xa', 'mapreduce', 'xa', 'input', 'quot', 'hdfs', 'bikes_lr', 'day', 'csv', 'quot', 'xa', 'map', 'xa', 'function', 'xi', 'xa', 'yi', 'c', 'xi', 'xa', 'xi', 'xi', 'xa', 'keyval', 'list', 't', 'xi', 'xi', 'xa', 'xa', 'reduce', 'function', 'k', 'v', 'xa', 'xa', 'vals', 'as', 'numeric', 'v', 'xa', 'keyval', 'k', 'sum', 'vals', 'xa', 'xa', 'combine', 'true', 'xa', 'xa', 'xty', 'xa', 'values', 'from', 'dfs', 'xa', 'mapreduce', 'xa', 'input', 'quot', 'hdfs', 'bikes_lr', 'day', 'csv', 'quot', 'xa', 'map', 'xa', 'function', 'xi', 'xa', 'yi', 'c', 'xi', 'xa', 'xi', 'xi', 'xa', 'keyval', 'list', 't', 'xi', 'yi', 'xa', 'xa', 'reduce', 'true', 'xa', 'combine', 'true', 'xa', 'solve', 'xtx', 'xty', 'xa', 'xa', 'xa', 'xa', 'input', 'xa', 'xa', 'xa', 'instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'please', 'suggest', 'me', 'any', 'mistakes', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'download', 'and', 'installed', 'cdh', 'package', 'succesfully', 'on', 'a', 'single', 'linux', 'node', 'in', 'pseudo', 'distributed', 'mode', 'on', 'my', 'centos', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'starting', 'hadoop', 'and', 'verifying', 'it', 'is', 'working', 'properly', 'as', 'in', 'lt', 'a', 'href', 'quot', 'http', 'www', 'cloudera', 'com', 'content', 'cloudera', 'content', 'cloudera', 'docs', 'cdh', 'latest', 'cdh', 'quick', 'start', 'cdh', 'qs_mrv', '_pseudo', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'this', 'link', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'succesfully', 'finished', 'the', 'following', 'steps', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'format', 'the', 'namenode', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'start', 'hdfs', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'create', 'the', 'tmp', 'directory', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'create', 'the', 'mapreduce', 'system', 'directories', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'verify', 'the', 'hdfs', 'file', 'structure', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'start', 'mapreduce', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'while', 'following', 'command', 'in', 'step', 'i', 'get', 'the', 'following', 'error', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'step', 'create', 'user', 'directories', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'lt', 'em', 'gt', 'sudo', 'u', 'hdfs', 'hadoop', 'fs', 'mkdir', 'p', 'user', 'hadoopuser', 'lt', 'em', 'gt', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'lt', 'em', 'gt', 'mkdir', 'user', 'hadoopuser', 'no', 'such', 'file', 'or', 'directory', 'lt', 'em', 'gt', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'hadoopuser', 'is', 'my', 'linux', 'login', 'username', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'create', 'the', 'directory', 'manually', 'as', 'user', 'hadoopuser', 'in', 'the', 'filesystem', 'it', 'is', 'not', 'accepting', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'success', 'the', 'step', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'provide', 'the', 'sloution', 'to', 'procced', 'the', 'remaining', 'installation', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'note', 'that', 'i', 'am', 'doing', 'everything', 'in', 'r', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'problem', 'goes', 'as', 'follow', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'basically', 'i', 'have', 'a', 'list', 'of', 'resumes', 'cvs', 'some', 'candidates', 'will', 'have', 'work', 'experience', 'before', 'and', 'some', 'don', 't', 'the', 'goal', 'here', 'is', 'to', 'based', 'on', 'the', 'text', 'on', 'their', 'cvs', 'i', 'want', 'to', 'classify', 'them', 'into', 'different', 'job', 'sectors', 'i', 'am', 'particular', 'in', 'those', 'cases', 'in', 'which', 'the', 'candidates', 'do', 'not', 'have', 'any', 'experience', 'is', 'a', 'student', 'and', 'i', 'want', 'to', 'make', 'a', 'prediction', 'to', 'classify', 'which', 'job', 'sectors', 'this', 'candidate', 'will', 'most', 'likely', 'belongs', 'to', 'after', 'graduation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'i', 'know', 'machine', 'learning', 'algorithms', 'however', 'i', 'have', 'never', 'done', 'nlp', 'before', 'i', 'came', 'across', 'latent', 'dirichlet', 'allocation', 'on', 'the', 'internet', 'however', 'i', 'am', 'not', 'sure', 'if', 'this', 'is', 'the', 'best', 'approach', 'to', 'tackle', 'my', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'original', 'idea', 'lt', 'em', 'gt', 'make', 'this', 'a', 'supervised', 'learning', 'problem', 'lt', 'em', 'gt', 'xa', 'suppose', 'we', 'already', 'have', 'large', 'amount', 'of', 'labelled', 'data', 'meaning', 'that', 'we', 'have', 'correctly', 'labelled', 'the', 'job', 'sectors', 'for', 'a', 'list', 'of', 'candidates', 'we', 'train', 'the', 'model', 'up', 'using', 'ml', 'algorithms', 'i', 'e', 'nearest', 'neighbor', 'and', 'feed', 'in', 'those', 'lt', 'em', 'gt', 'unlabelled', 'data', 'lt', 'em', 'gt', 'which', 'are', 'candidates', 'that', 'have', 'no', 'work', 'experience', 'are', 'students', 'and', 'try', 'to', 'predict', 'which', 'job', 'sector', 'they', 'will', 'belong', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'update', 'lt', 'strong', 'gt', 'xa', 'question', 'would', 'it', 'be', 'a', 'good', 'idea', 'to', 'create', 'an', 'text', 'file', 'by', 'extracting', 'everything', 'in', 'a', 'resume', 'and', 'print', 'these', 'data', 'out', 'in', 'the', 'text', 'file', 'so', 'that', 'each', 'resume', 'is', 'associated', 'with', 'a', 'text', 'file', 'which', 'contains', 'unstructured', 'strings', 'and', 'then', 'we', 'applied', 'text', 'mining', 'techniques', 'to', 'the', 'text', 'files', 'and', 'make', 'the', 'data', 'become', 'structured', 'or', 'even', 'to', 'create', 'a', 'frequency', 'matrix', 'of', 'terms', 'used', 'out', 'of', 'the', 'text', 'files', 'for', 'example', 'the', 'text', 'file', 'may', 'look', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'i', 'deployed', 'ml', 'algorithm', 'in', 'this', 'project', 'and', 'skills', 'java', 'python', 'c', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'is', 'what', 'i', 'meant', 'by', 'unstructured', 'i', 'e', 'collapsing', 'everything', 'into', 'a', 'single', 'line', 'string', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'approach', 'wrong', 'please', 'correct', 'me', 'if', 'you', 'think', 'my', 'approach', 'is', 'wrong', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'the', 'tricky', 'part', 'is', 'how', 'to', 'lt', 'strong', 'gt', 'identify', 'and', 'extract', 'the', 'keywords', 'lt', 'strong', 'gt', 'using', 'the', 'lt', 'code', 'gt', 'tm', 'lt', 'code', 'gt', 'package', 'in', 'r', 'what', 'algorithm', 'is', 'the', 'lt', 'code', 'gt', 'tm', 'lt', 'code', 'gt', 'package', 'based', 'on', 'should', 'i', 'use', 'nlp', 'algorithms', 'if', 'yes', 'what', 'algorithms', 'should', 'i', 'look', 'at', 'please', 'point', 'me', 'to', 'some', 'good', 'resources', 'to', 'look', 'at', 'as', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'ideas', 'would', 'be', 'great', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'r', 'base', 'function', 'lt', 'code', 'gt', 'glm', 'lt', 'code', 'gt', 'uses', 'fishers', 'scoring', 'for', 'mle', 'while', 'the', 'lt', 'code', 'gt', 'glmnet', 'lt', 'code', 'gt', 'appears', 'to', 'use', 'the', 'coordinate', 'descent', 'method', 'to', 'solve', 'the', 'same', 'equation', 'coordinate', 'descent', 'is', 'more', 'time', 'efficient', 'than', 'fisher', 'scoring', 'as', 'fisher', 'scoring', 'calculates', 'the', 'second', 'order', 'derivative', 'matrix', 'in', 'addition', 'to', 'some', 'other', 'matrix', 'operations', 'which', 'makes', 'expensive', 'to', 'perform', 'while', 'coordinate', 'descent', 'can', 'do', 'the', 'same', 'task', 'in', 'o', 'np', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'would', 'r', 'base', 'function', 'use', 'fisher', 'scoring', 'does', 'this', 'method', 'have', 'an', 'advantage', 'over', 'other', 'optimization', 'methods', 'how', 'does', 'coordinate', 'descent', 'and', 'fisher', 'scoring', 'compare', 'i', 'am', 'relatively', 'new', 'to', 'do', 'this', 'field', 'so', 'any', 'help', 'or', 'resource', 'will', 'be', 'helpful', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'building', 'a', 'model', 'that', 'predicts', 'user', 'churn', 'for', 'a', 'website', 'where', 'i', 'have', 'data', 'on', 'all', 'users', 'both', 'past', 'and', 'present', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'can', 'build', 'a', 'model', 'that', 'only', 'uses', 'those', 'users', 'that', 'have', 'left', 'but', 'then', 'i', 'm', 'leaving', 'of', 'the', 'total', 'user', 'population', 'unused', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'good', 'way', 'to', 'incorporate', 'data', 'from', 'these', 'users', 'into', 'a', 'model', 'from', 'a', 'conceptual', 'standpoint', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'linearly', 'increasing', 'time', 'series', 'dataset', 'of', 'a', 'sensor', 'with', 'value', 'ranges', 'between', 'and', 'i', 've', 'implemented', 'a', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'simple_linear_regression', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'simple', 'linear', 'regression', 'lt', 'a', 'gt', 'algorithm', 'to', 'fit', 'a', 'regression', 'line', 'on', 'such', 'data', 'and', 'i', 'm', 'predicting', 'the', 'date', 'when', 'the', 'series', 'would', 'reach', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'all', 'works', 'fine', 'when', 'the', 'series', 'move', 'upwards', 'but', 'there', 'are', 'cases', 'in', 'which', 'the', 'sensor', 'reaches', 'around', 'or', 'and', 'it', 'is', 'reset', 'in', 'such', 'cases', 'the', 'values', 'would', 'start', 'over', 'again', 'at', 'say', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'is', 'where', 'i', 'start', 'facing', 'issues', 'with', 'the', 'regression', 'line', 'as', 'it', 'starts', 'moving', 'downwards', 'and', 'it', 'starts', 'predicting', 'old', 'date', 'i', 'think', 'i', 'should', 'be', 'considering', 'only', 'the', 'subset', 'of', 'data', 'from', 'where', 'it', 'was', 'previously', 'reset', 'however', 'i', 'm', 'trying', 'to', 'understand', 'if', 'there', 'are', 'any', 'algorithms', 'available', 'that', 'consider', 'this', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'new', 'to', 'data', 'science', 'would', 'appreciate', 'any', 'pointers', 'to', 'move', 'further', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'nfmcclure', 's', 'suggestions', 'applied', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'before', 'applying', 'the', 'suggestions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'zsyyq', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'below', 'is', 'the', 'snapshot', 'of', 'what', 'i', 've', 'got', 'after', 'splitting', 'the', 'dataset', 'where', 'the', 'reset', 'occurs', 'and', 'the', 'slope', 'of', 'two', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'oeqcw', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'finding', 'the', 'mean', 'of', 'the', 'two', 'slopes', 'and', 'drawing', 'the', 'line', 'from', 'the', 'mean', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'i', 'qv', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'ok', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'use', 'the', 'sklearn_pandas', 'module', 'to', 'extend', 'the', 'work', 'i', 'do', 'in', 'pandas', 'and', 'dip', 'a', 'toe', 'into', 'machine', 'learning', 'but', 'i', 'm', 'struggling', 'with', 'an', 'error', 'i', 'don', 't', 'really', 'understand', 'how', 'to', 'fix', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'was', 'working', 'through', 'the', 'following', 'dataset', 'on', 'lt', 'a', 'href', 'quot', 'https', 'www', 'kaggle', 'com', 'c', 'data', 'science', 'london', 'scikit', 'learn', 'data', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'kaggle', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 's', 'essentially', 'an', 'unheadered', 'table', 'rows', 'features', 'with', 'floating', 'point', 'values', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'import', 'pandas', 'as', 'pdfrom', 'sklearn', 'import', 'neighbors', 'xa', 'from', 'sklearn_pandas', 'import', 'dataframemapper', 'cross_val_score', 'xa', 'path_train', 'quot', 'kaggle', 'scikitlearn', 'train', 'csv', 'quot', 'xa', 'path_labels', 'quot', 'kaggle', 'scikitlearn', 'trainlabels', 'csv', 'quot', 'xa', 'path_test', 'quot', 'kaggle', 'scikitlearn', 'test', 'csv', 'quot', 'xa', 'xa', 'train', 'pd', 'read_csv', 'path_train', 'header', 'none', 'xa', 'labels', 'pd', 'read_csv', 'path_labels', 'header', 'none', 'xa', 'test', 'pd', 'read_csv', 'path_test', 'header', 'none', 'xa', 'mapper_train', 'dataframemapper', 'list', 'train', 'columns', 'neighbors', 'kneighborsclassifier', 'n_neighbors', 'xa', 'mapper_train', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'dataframemapper', 'features', 'kneighborsclassifier', 'algorithm', 'auto', 'leaf_size', 'metric', 'minkowski', 'xa', 'n_neighbors', 'p', 'weights', 'uniform', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'far', 'so', 'good', 'but', 'then', 'i', 'try', 'the', 'fit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'mapper_train', 'fit_transform', 'train', 'labels', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'typeerror', 'traceback', 'most', 'recent', 'call', 'last', 'xa', 'amp', 'lt', 'ipython', 'input', 'e', 'd', 'db', 'b', 'amp', 'gt', 'in', 'amp', 'lt', 'module', 'amp', 'gt', 'xa', 'amp', 'gt', 'mapper_train', 'fit_transform', 'train', 'labels', 'xa', 'xa', 'anaconda', 'lib', 'python', 'site', 'packages', 'sklearn', 'base', 'pyc', 'in', 'fit_transform', 'self', 'x', 'y', 'fit_params', 'xa', 'else', 'xa', 'fit', 'method', 'of', 'arity', 'supervised', 'transformation', 'xa', 'amp', 'gt', 'return', 'self', 'fit', 'x', 'y', 'fit_params', 'transform', 'x', 'xa', 'xa', 'xa', 'xa', 'anaconda', 'lib', 'python', 'site', 'packages', 'sklearn_pandas', '__init__', 'pyc', 'in', 'fit', 'self', 'x', 'y', 'xa', 'for', 'columns', 'transformer', 'in', 'self', 'features', 'xa', 'if', 'transformer', 'is', 'not', 'none', 'xa', 'amp', 'gt', 'transformer', 'fit', 'self', '_get_col_subset', 'x', 'columns', 'xa', 'return', 'self', 'xa', 'xa', 'xa', 'typeerror', 'fit', 'takes', 'exactly', 'arguments', 'given', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'am', 'i', 'doing', 'wrong', 'while', 'the', 'data', 'in', 'this', 'case', 'is', 'all', 'the', 'same', 'i', 'm', 'planning', 'to', 'work', 'up', 'a', 'workflow', 'for', 'mixtures', 'categorical', 'nominal', 'and', 'floating', 'point', 'features', 'and', 'sklearn_pandas', 'seemed', 'to', 'be', 'a', 'logical', 'fit', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'when', 'i', 'say', 'quot', 'document', 'quot', 'i', 'have', 'in', 'mind', 'web', 'pages', 'like', 'wikipedia', 'articles', 'and', 'news', 'stories', 'i', 'prefer', 'answers', 'giving', 'either', 'vanilla', 'lexical', 'distance', 'metrics', 'or', 'state', 'of', 'the', 'art', 'semantic', 'distance', 'metrics', 'with', 'stronger', 'preference', 'for', 'the', 'latter', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'made', 'a', 'similar', 'question', 'asking', 'about', 'distance', 'between', 'quot', 'documents', 'quot', 'wikipedia', 'articles', 'news', 'stories', 'etc', 'i', 'made', 'this', 'a', 'separate', 'question', 'because', 'search', 'queries', 'are', 'considerably', 'smaller', 'than', 'documents', 'and', 'are', 'considerably', 'noisier', 'i', 'hence', 'don', 't', 'know', 'and', 'doubt', 'if', 'the', 'same', 'distance', 'metrics', 'would', 'be', 'used', 'here', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'either', 'vanilla', 'lexical', 'distance', 'metrics', 'or', 'state', 'of', 'the', 'art', 'semantic', 'distance', 'metrics', 'are', 'preferred', 'with', 'stronger', 'preference', 'for', 'the', 'latter', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'datasets', 'one', 'with', 'positive', 'instances', 'of', 'what', 'i', 'would', 'like', 'to', 'detect', 'and', 'one', 'with', 'unlabeled', 'instances', 'what', 'methods', 'can', 'i', 'use', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'an', 'example', 'suppose', 'we', 'want', 'to', 'understand', 'detect', 'spam', 'email', 'based', 'on', 'a', 'few', 'structured', 'email', 'characteristics', 'we', 'have', 'one', 'dataset', 'of', 'spam', 'emails', 'and', 'one', 'dataset', 'of', 'emails', 'for', 'which', 'we', 'don', 't', 'know', 'whether', 'they', 'are', 'spam', 'or', 'not', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'we', 'tackle', 'this', 'problem', 'without', 'labeling', 'manually', 'any', 'of', 'the', 'unlabeled', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'can', 'we', 'do', 'if', 'we', 'have', 'additional', 'information', 'about', 'the', 'proportion', 'of', 'spam', 'in', 'the', 'unlabeled', 'data', 'i', 'e', 'what', 'if', 'we', 'estimate', 'that', 'between', 'of', 'the', 'unlabeled', 'emails', 'are', 'spam', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'very', 'new', 'to', 'machine', 'learning', 'and', 'in', 'my', 'first', 'project', 'have', 'stumbled', 'across', 'a', 'lot', 'of', 'issues', 'which', 'i', 'really', 'want', 'to', 'get', 'through', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'using', 'logistic', 'regression', 'with', 'r', 's', 'lt', 'code', 'gt', 'glmnet', 'lt', 'code', 'gt', 'package', 'and', 'alpha', 'for', 'ridge', 'regression', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'using', 'ridge', 'regression', 'actually', 'since', 'lasso', 'deleted', 'all', 'my', 'variables', 'and', 'gave', 'very', 'low', 'area', 'under', 'curve', 'but', 'with', 'ridge', 'there', 'isn', 't', 'much', 'of', 'a', 'difference', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'dependent', 'variable', 'output', 'is', 'probability', 'of', 'click', 'based', 'on', 'if', 'there', 'is', 'a', 'click', 'or', 'not', 'in', 'historical', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'independent', 'variables', 'are', 'state', 'city', 'device', 'user', 'age', 'user', 'gender', 'ip', 'carrier', 'keyword', 'mobile', 'manufacturer', 'ad', 'template', 'browser', 'version', 'browser', 'family', 'os', 'version', 'and', 'os', 'family', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'of', 'these', 'for', 'prediction', 'i', 'm', 'using', 'state', 'device', 'user', 'age', 'user', 'gender', 'ip', 'carrier', 'browser', 'version', 'browser', 'family', 'os', 'version', 'and', 'os', 'family', 'i', 'am', 'not', 'using', 'keyword', 'or', 'template', 'since', 'we', 'want', 'to', 'reject', 'a', 'user', 'request', 'before', 'deep', 'diving', 'in', 'our', 'system', 'and', 'selecting', 'a', 'keyword', 'or', 'template', 'i', 'am', 'not', 'using', 'city', 'because', 'they', 'are', 'too', 'many', 'or', 'mobile', 'manufacturer', 'because', 'they', 'are', 'too', 'few', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'is', 'that', 'okay', 'or', 'should', 'i', 'be', 'using', 'the', 'rejected', 'variables', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'start', 'i', 'create', 'a', 'sparse', 'matrix', 'from', 'my', 'variables', 'which', 'are', 'mapped', 'against', 'the', 'column', 'of', 'clicks', 'that', 'have', 'yes', 'or', 'no', 'values', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'after', 'training', 'the', 'model', 'i', 'save', 'the', 'coefficients', 'and', 'intercept', 'these', 'are', 'used', 'for', 'new', 'incoming', 'requests', 'using', 'the', 'formula', 'for', 'logistic', 'regression', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'azbrq', 'png', 'quot', 'alt', 'quot', 'e', 'sum', 'a', 'k', 'ith', 'x', 'ith', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'is', 'intercept', 'lt', 'code', 'gt', 'k', 'lt', 'code', 'gt', 'is', 'the', 'lt', 'code', 'gt', 'i', 'lt', 'code', 'gt', 'th', 'coefficient', 'and', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'is', 'the', 'lt', 'code', 'gt', 'i', 'lt', 'code', 'gt', 'th', 'variable', 'value', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'is', 'my', 'approach', 'correct', 'so', 'far', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'simple', 'glm', 'in', 'r', 'that', 'is', 'where', 'there', 'is', 'no', 'regularized', 'regression', 'right', 'gave', 'me', 'auc', 'with', 'regularization', 'i', 'get', 'but', 'there', 'is', 'no', 'distinct', 'threshold', 'where', 'we', 'could', 'say', 'that', 'above', 'xx', 'its', 'mostly', 'ones', 'and', 'below', 'it', 'most', 'zeros', 'are', 'covered', 'actually', 'the', 'max', 'probability', 'that', 'a', 'click', 'didn', 't', 'happen', 'is', 'almost', 'always', 'greater', 'than', 'the', 'max', 'probability', 'that', 'a', 'click', 'happened', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'so', 'basically', 'what', 'should', 'i', 'do', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'read', 'how', 'stochastic', 'gradient', 'descent', 'is', 'an', 'effective', 'technique', 'in', 'logit', 'so', 'how', 'do', 'i', 'implement', 'stochastic', 'gradient', 'descent', 'in', 'r', 'if', 'it', 's', 'not', 'straightforward', 'is', 'there', 'a', 'way', 'to', 'implement', 'this', 'system', 'in', 'python', 'is', 'sgd', 'implemented', 'after', 'generating', 'a', 'regularized', 'logistic', 'regression', 'model', 'or', 'is', 'it', 'a', 'different', 'process', 'altogether', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'there', 'is', 'an', 'algorithm', 'called', 'follow', 'the', 'regularized', 'leader', 'ftrl', 'that', 'is', 'used', 'in', 'click', 'through', 'rate', 'prediction', 'is', 'there', 'a', 'sample', 'code', 'and', 'use', 'of', 'ftrl', 'that', 'i', 'could', 'go', 'through', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'using', 'neural', 'networks', 'to', 'solve', 'different', 'machine', 'learning', 'problems', 'i', 'm', 'using', 'python', 'and', 'lt', 'a', 'href', 'quot', 'http', 'pybrain', 'org', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'pybrain', 'lt', 'a', 'gt', 'but', 'this', 'library', 'is', 'almost', 'discontinued', 'are', 'there', 'other', 'good', 'alternatives', 'in', 'python', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'run', 'some', 'analysis', 'with', 'some', 'big', 'datasets', 'eg', 'k', 'rows', 'vs', 'columns', 'with', 'r', 'e', 'g', 'using', 'neural', 'networks', 'and', 'recommendation', 'systems', 'xa', 'but', 'it', 's', 'taking', 'too', 'long', 'to', 'process', 'the', 'data', 'with', 'huge', 'matrices', 'e', 'g', 'k', 'rows', 'vs', 'k', 'columns', 'xa', 'what', 'are', 'some', 'free', 'cheap', 'ways', 'to', 'improve', 'r', 'performance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'accepting', 'packages', 'or', 'web', 'services', 'suggestions', 'other', 'options', 'are', 'welcome', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'set', 'of', 'datapoints', 'from', 'the', 'unit', 'interval', 'i', 'e', 'dimensional', 'dataset', 'with', 'numerical', 'values', 'i', 'receive', 'some', 'additional', 'datapoints', 'online', 'and', 'moreover', 'the', 'value', 'of', 'some', 'datapoints', 'might', 'change', 'dynamically', 'i', 'm', 'looking', 'for', 'an', 'ideal', 'clustering', 'algorithm', 'which', 'can', 'handle', 'these', 'issues', 'efficiently', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'know', 'lt', 'a', 'href', 'quot', 'https', 'www', 'cs', 'princeton', 'edu', 'courses', 'archive', 'fall', 'cos', 'duda', 'c', 'sk_means', 'htm', 'quot', 'gt', 'sequential', 'k', 'means', 'clustering', 'lt', 'a', 'gt', 'copes', 'with', 'the', 'addition', 'of', 'new', 'instances', 'and', 'i', 'suppose', 'with', 'minor', 'modification', 'it', 'can', 'work', 'with', 'dynamic', 'instance', 'values', 'i', 'e', 'first', 'taking', 'the', 'modified', 'instance', 'from', 'the', 'respective', 'cluster', 'then', 'updating', 'the', 'mean', 'of', 'the', 'cluster', 'and', 'finally', 'giving', 'the', 'modified', 'instance', 'as', 'an', 'input', 'to', 'the', 'algorithm', 'just', 'as', 'the', 'addition', 'of', 'an', 'unseen', 'instance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'concern', 'with', 'using', 'the', 'k', 'means', 'algorithm', 'is', 'the', 'requirement', 'of', 'supplying', 'the', 'number', 'of', 'clusters', 'as', 'an', 'input', 'i', 'know', 'that', 'they', 'beat', 'other', 'clustering', 'algorithms', 'gas', 'msts', 'hierarchical', 'methods', 'etc', 'in', 'time', 'amp', 'amp', 'space', 'complexity', 'honestly', 'i', 'm', 'not', 'sure', 'but', 'maybe', 'i', 'can', 'get', 'away', 'with', 'using', 'one', 'of', 'the', 'aforementioned', 'algorithms', 'even', 'that', 'my', 'datasets', 'are', 'relatively', 'large', 'the', 'existence', 'of', 'a', 'single', 'dimension', 'makes', 'me', 'wonder', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'more', 'specifically', 'a', 'typical', 'test', 'case', 'of', 'mine', 'would', 'contain', 'about', 'k', 'k', 'dimensional', 'datapoints', 'i', 'would', 'like', 'to', 'complete', 'the', 'clustering', 'preferably', 'under', 'a', 'second', 'the', 'dynamic', 'changes', 'in', 'the', 'value', 'points', 'are', 'assumed', 'to', 'be', 'smooth', 'i', 'e', 'relatively', 'small', 'thus', 'being', 'able', 'to', 'use', 'existing', 'solutions', 'i', 'e', 'being', 'able', 'to', 'continue', 'clustering', 'on', 'the', 'existing', 'one', 'when', 'a', 'value', 'is', 'changed', 'or', 'new', 'one', 'is', 'added', 'is', 'highly', 'preferred', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'all', 'in', 'all', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'can', 'you', 'think', 'of', 'an', 'algorithm', 'which', 'will', 'provide', 'a', 'sweet', 'spot', 'between', 'computational', 'efficiency', 'and', 'the', 'accuracy', 'of', 'clusters', 'wrt', 'the', 'problem', 'defined', 'above', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'some', 'nice', 'heuristics', 'for', 'the', 'k', 'means', 'algorithm', 'to', 'automatically', 'compute', 'the', 'value', 'of', 'k', 'beforehand', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'generated', 'a', 'dataset', 'of', 'pairwise', 'distances', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'id_', 'id_', 'dist_', 'xa', 'id_', 'id_', 'dist_', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'cluster', 'this', 'data', 'so', 'as', 'to', 'identify', 'the', 'pattern', 'i', 'have', 'been', 'looking', 'at', 'spectral', 'clustering', 'and', 'dbscan', 'but', 'i', 'haven', 't', 'been', 'able', 'to', 'come', 'to', 'a', 'conclusion', 'and', 'have', 'been', 'ambiguous', 'on', 'how', 'to', 'make', 'use', 'of', 'the', 'existing', 'implementations', 'of', 'these', 'algorithms', 'i', 'have', 'been', 'looking', 'at', 'python', 'and', 'java', 'implementations', 'so', 'far', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'could', 'anyone', 'point', 'me', 'to', 'a', 'tutorial', 'or', 'demo', 'on', 'how', 'to', 'make', 'use', 'of', 'these', 'clustering', 'algorithms', 'to', 'handle', 'the', 'situation', 'in', 'hand', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'my', 'university', 'we', 'have', 'an', 'hpc', 'computing', 'cluster', 'i', 'use', 'the', 'cluster', 'to', 'train', 'classifiers', 'and', 'so', 'on', 'so', 'usually', 'to', 'send', 'a', 'job', 'to', 'the', 'cluster', 'e', 'g', 'python', 'scikit', 'learn', 'script', 'i', 'need', 'to', 'write', 'a', 'bash', 'script', 'that', 'contains', 'among', 'others', 'a', 'command', 'like', 'lt', 'code', 'gt', 'qsub', 'script', 'py', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'find', 'this', 'process', 'very', 'very', 'frustrating', 'usually', 'what', 'happens', 'is', 'that', 'i', 'write', 'the', 'python', 'script', 'on', 'my', 'laptop', 'and', 'then', 'i', 'login', 'to', 'the', 'server', 'and', 'update', 'the', 'svn', 'repository', 'so', 'i', 'get', 'the', 'same', 'python', 'script', 'there', 'then', 'i', 'write', 'that', 'bash', 'script', 'or', 'edit', 'it', 'so', 'i', 'can', 'run', 'the', 'bash', 'script', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'you', 'see', 'this', 'is', 'really', 'frustrating', 'since', 'for', 'every', 'little', 'update', 'for', 'the', 'python', 'script', 'i', 'need', 'to', 'do', 'many', 'steps', 'to', 'have', 'it', 'executed', 'at', 'the', 'computing', 'cluster', 'of', 'course', 'the', 'task', 'gets', 'even', 'more', 'complicated', 'when', 'i', 'have', 'to', 'put', 'the', 'data', 'on', 'the', 'server', 'and', 'use', 'the', 'datasets', 'path', 'on', 'the', 'server', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'sure', 'many', 'people', 'here', 'are', 'using', 'computing', 'clusters', 'for', 'their', 'data', 'science', 'tasks', 'i', 'just', 'want', 'to', 'know', 'how', 'you', 'guys', 'manage', 'sending', 'the', 'jobs', 'to', 'the', 'clusters', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'commercial', 'text', 'summarization', 'tools', 'apis', 'libraries', 'which', 'are', 'able', 'to', 'perform', 'any', 'of', 'the', 'following', 'tasks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'extractive', 'multi', 'document', 'summarization', 'generic', 'or', 'query', 'based', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'extractive', 'single', 'document', 'summarization', 'generic', 'or', 'query', 'based', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'generative', 'single', 'document', 'summarization', 'generic', 'or', 'query', 'based', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'generative', 'multi', 'document', 'summarization', 'generic', 'or', 'query', 'based', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'this', 'question', 'is', 'in', 'response', 'to', 'a', 'comment', 'i', 'saw', 'on', 'another', 'question', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'comment', 'was', 'regarding', 'the', 'machine', 'learning', 'course', 'syllabus', 'on', 'coursera', 'and', 'along', 'the', 'lines', 'of', 'quot', 'svms', 'are', 'not', 'used', 'so', 'much', 'nowadays', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'only', 'just', 'finished', 'the', 'relevant', 'lectures', 'myself', 'and', 'my', 'understanding', 'of', 'svms', 'is', 'that', 'they', 'are', 'a', 'robust', 'and', 'efficient', 'learning', 'algorithm', 'for', 'classification', 'and', 'that', 'when', 'using', 'a', 'kernel', 'they', 'have', 'a', 'quot', 'niche', 'quot', 'covering', 'number', 'of', 'features', 'perhaps', 'to', 'and', 'number', 'of', 'training', 'samples', 'perhaps', 'to', 'the', 'limit', 'on', 'training', 'samples', 'is', 'because', 'the', 'core', 'algorithm', 'revolves', 'around', 'optimising', 'results', 'generated', 'from', 'a', 'square', 'matrix', 'with', 'dimensions', 'based', 'on', 'number', 'of', 'training', 'samples', 'not', 'number', 'of', 'original', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'does', 'the', 'comment', 'i', 'saw', 'refer', 'some', 'real', 'change', 'since', 'the', 'course', 'was', 'made', 'and', 'if', 'so', 'what', 'is', 'that', 'change', 'a', 'new', 'algorithm', 'that', 'covers', 'svm', 's', 'quot', 'sweet', 'spot', 'quot', 'just', 'as', 'well', 'better', 'cpus', 'meaning', 'svm', 's', 'computational', 'advantages', 'are', 'not', 'worth', 'as', 'much', 'or', 'is', 'it', 'perhaps', 'opinion', 'or', 'personal', 'experience', 'of', 'the', 'commenter', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'a', 'search', 'for', 'e', 'g', 'quot', 'are', 'support', 'vector', 'machines', 'out', 'of', 'fashion', 'quot', 'and', 'found', 'nothing', 'to', 'imply', 'they', 'were', 'being', 'dropped', 'in', 'favour', 'of', 'anything', 'else', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'wikipedia', 'has', 'this', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'support_vector_machine', 'issues', 'quot', 'gt', 'http', 'en', 'wikipedia', 'org', 'wiki', 'support_vector_machine', 'issues', 'lt', 'a', 'gt', 'the', 'main', 'sticking', 'point', 'appears', 'to', 'be', 'difficulty', 'of', 'interpreting', 'the', 'model', 'which', 'makes', 'svm', 'fine', 'for', 'a', 'black', 'box', 'predicting', 'engine', 'but', 'not', 'so', 'good', 'for', 'generating', 'insights', 'i', 'don', 't', 'see', 'that', 'as', 'a', 'major', 'issue', 'just', 'another', 'minor', 'thing', 'to', 'take', 'into', 'account', 'when', 'picking', 'the', 'right', 'tool', 'for', 'the', 'job', 'along', 'with', 'nature', 'of', 'the', 'training', 'data', 'and', 'learning', 'task', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'installed', 'cloudera', 'cdh', 'quick', 'start', 'vm', 'on', 'vm', 'player', 'when', 'i', 'login', 'through', 'hue', 'in', 'the', 'first', 'page', 'i', 'am', 'the', 'following', 'error', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'potential', 'misconfiguration', 'detected', 'fix', 'and', 'restart', 'hue', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'vnq', 'p', 'png', 'quot', 'alt', 'quot', 'potential', 'misconfiguration', 'detected', 'fix', 'and', 'restart', 'hue', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'solve', 'this', 'issue', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'know', 'that', 'there', 'is', 'no', 'a', 'clear', 'answer', 'for', 'this', 'question', 'but', 'let', 's', 'suppose', 'that', 'i', 'have', 'a', 'huge', 'neural', 'network', 'with', 'a', 'lot', 'of', 'data', 'and', 'i', 'want', 'to', 'add', 'a', 'new', 'feature', 'in', 'input', 'the', 'quot', 'best', 'quot', 'way', 'would', 'be', 'to', 'test', 'the', 'network', 'with', 'the', 'new', 'feature', 'and', 'see', 'the', 'results', 'but', 'is', 'there', 'a', 'method', 'to', 'test', 'if', 'the', 'feature', 'is', 'unlikely', 'helpful', 'like', 'correlation', 'measures', 'lt', 'a', 'href', 'quot', 'http', 'www', 'nd', 'edu', 'mclark', 'learn', 'correlationcomparison', 'pdf', 'quot', 'gt', 'http', 'www', 'nd', 'edu', 'mclark', 'learn', 'correlationcomparison', 'pdf', 'lt', 'a', 'gt', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'using', 'an', 'experimental', 'design', 'to', 'test', 'the', 'robustness', 'of', 'different', 'classification', 'methods', 'and', 'now', 'i', 'm', 'searching', 'for', 'the', 'correct', 'definition', 'of', 'such', 'design', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'creating', 'different', 'subsets', 'of', 'the', 'full', 'dataset', 'by', 'cutting', 'away', 'some', 'samples', 'each', 'subset', 'is', 'created', 'independently', 'with', 'respect', 'to', 'the', 'others', 'then', 'i', 'run', 'each', 'classification', 'method', 'on', 'every', 'subset', 'finally', 'i', 'estimate', 'the', 'accuracy', 'of', 'each', 'method', 'as', 'how', 'many', 'classifications', 'on', 'subsets', 'are', 'in', 'agreement', 'with', 'the', 'classification', 'on', 'the', 'full', 'dataset', 'for', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'classification', 'full', 'xa', 'xa', 'classification', 'subset', 'xa', 'classification', 'subset', 'xa', 'xa', 'xa', 'accuracy', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'correct', 'name', 'to', 'this', 'methodology', 'i', 'thought', 'it', 'can', 'fall', 'under', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'bootstrapping_', 'statistics', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'bootstrapping', 'lt', 'a', 'gt', 'but', 'i', 'm', 'not', 'sure', 'about', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'understand', 'a', 'neuroscience', 'article', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'friston', 'karl', 'j', 'et', 'al', 'quot', 'action', 'and', 'behavior', 'a', 'free', 'energy', 'formulation', 'quot', 'lt', 'em', 'gt', 'biological', 'cybernetics', 'lt', 'em', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'link', 'springer', 'com', 'article', 'fs', 'z', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'doi', 's', 'z', 'lt', 'a', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'this', 'article', 'friston', 'gives', 'three', 'equations', 'that', 'are', 'as', 'i', 'understand', 'him', 'equivalent', 'or', 'inter', 'convertertable', 'and', 'refer', 'to', 'both', 'physical', 'and', 'shannon', 'entropy', 'they', 'appear', 'on', 'page', 'of', 'the', 'article', 'as', 'equation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'the', 'resulting', 'expression', 'for', 'free', 'energy', 'can', 'be', 'expressed', 'in', 'three', 'ways', 'with', 'the', 'use', 'of', 'the', 'bayes', 'rules', 'and', 'simple', 'rearrangements', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'energy', 'minus', 'entropy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'divergence', 'plus', 'surprise', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'complexity', 'minus', 'accuracy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'mathematically', 'these', 'correspond', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'eqgg', 'png', 'quot', 'alt', 'quot', 'begin', 'aligned', 'f', 'amp', 'amp', 'langle', 'ln', 'p', 'tilde', 's', 'psi', 'vert', 'm', 'rangle', '_q', 'langle', 'ln', 'q', 'psi', 'vert', 'mu', 'rangle', '_q', 'amp', 'amp', 'd', 'q', 'psi', 'vert', 'mu', 'parallel', 'p', 'psi', 'vert', 'tilde', 's', 'm', 'ln', 'p', 'tilde', 's', 'vert', 'm', 'amp', 'amp', 'qquad', 'qquad', 'amp', 'amp', 'amp', 'amp', 'd', 'q', 'psi', 'vert', 'mu', 'parallel', 'p', 'psi', 'vert', 'm', 'langle', 'ln', 'p', 'tilde', 's', 'vert', 'psi', 'm', 'rangle', '_q', 'end', 'aligned', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'things', 'i', 'am', 'struggling', 'with', 'at', 'this', 'point', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'the', 'meaning', 'of', 'the', 'in', 'the', 'nd', 'and', 'rd', 'versions', 'of', 'the', 'equations', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'and', 'the', 'negative', 'logs', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'help', 'in', 'understanding', 'how', 'these', 'equations', 'are', 'actually', 'what', 'fristen', 'claims', 'them', 'to', 'be', 'would', 'be', 'greatly', 'appreciated', 'for', 'example', 'in', 'the', 'st', 'equation', 'in', 'what', 'sense', 'is', 'the', 'first', 'term', 'energy', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'far', 'as', 'i', 'know', 'the', 'development', 'of', 'algorithms', 'to', 'solve', 'the', 'frequent', 'pattern', 'mining', 'fpm', 'problem', 'the', 'road', 'of', 'improvements', 'have', 'some', 'main', 'checkpoints', 'firstly', 'the', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'apriori_algorithm', 'quot', 'gt', 'apriori', 'lt', 'a', 'gt', 'algorithm', 'was', 'proposed', 'in', 'by', 'lt', 'a', 'href', 'quot', 'http', 'dl', 'acm', 'org', 'citation', 'cfm', 'id', 'quot', 'gt', 'agrawal', 'et', 'al', 'lt', 'a', 'gt', 'along', 'with', 'the', 'formalization', 'of', 'the', 'problem', 'the', 'algorithm', 'was', 'able', 'to', 'lt', 'em', 'gt', 'strip', 'off', 'lt', 'em', 'gt', 'some', 'sets', 'from', 'the', 'lt', 'code', 'gt', 'n', 'lt', 'code', 'gt', 'sets', 'powerset', 'by', 'using', 'a', 'lattice', 'to', 'maintain', 'the', 'data', 'a', 'drawback', 'of', 'the', 'approach', 'was', 'the', 'need', 'to', 're', 'read', 'the', 'database', 'to', 'compute', 'the', 'frequency', 'of', 'each', 'set', 'expanded', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'later', 'on', 'year', 'lt', 'a', 'href', 'quot', 'http', 'www', 'computer', 'org', 'csdl', 'trans', 'tk', 'k', 'abs', 'html', 'quot', 'gt', 'zaki', 'et', 'al', 'lt', 'a', 'gt', 'proposed', 'the', 'algorithm', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikibooks', 'org', 'wiki', 'data_mining_algorithms_in_r', 'frequent_pattern_mining', 'the_eclat_algorithm', 'quot', 'gt', 'eclat', 'lt', 'a', 'gt', 'which', 'lt', 'em', 'gt', 'inserted', 'lt', 'em', 'gt', 'the', 'resulting', 'frequency', 'of', 'each', 'set', 'inside', 'the', 'lattice', 'this', 'was', 'done', 'by', 'adding', 'at', 'each', 'node', 'of', 'the', 'lattice', 'the', 'set', 'of', 'transaction', 'ids', 'that', 'had', 'the', 'items', 'from', 'root', 'to', 'the', 'referred', 'node', 'the', 'main', 'contribution', 'is', 'that', 'one', 'does', 'not', 'have', 'to', 're', 'read', 'the', 'entire', 'dataset', 'to', 'know', 'the', 'frequency', 'of', 'each', 'set', 'but', 'the', 'memory', 'required', 'to', 'keep', 'such', 'data', 'structure', 'built', 'may', 'exceed', 'the', 'size', 'of', 'the', 'dataset', 'itself', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'lt', 'a', 'href', 'quot', 'http', 'dl', 'acm', 'org', 'citation', 'cfm', 'doid', 'quot', 'gt', 'han', 'et', 'al', 'lt', 'a', 'gt', 'proposed', 'an', 'algorithm', 'named', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikibooks', 'org', 'wiki', 'data_mining_algorithms_in_r', 'frequent_pattern_mining', 'the_fp', 'growth_algorithm', 'quot', 'gt', 'fpgrowth', 'lt', 'a', 'gt', 'along', 'with', 'a', 'prefix', 'tree', 'data', 'structure', 'named', 'fptree', 'the', 'algorithm', 'was', 'able', 'to', 'provide', 'significant', 'data', 'compression', 'while', 'also', 'granting', 'that', 'only', 'frequent', 'itemsets', 'would', 'be', 'yielded', 'without', 'candidate', 'itemset', 'generation', 'this', 'was', 'done', 'mainly', 'by', 'sorting', 'the', 'items', 'of', 'each', 'transaction', 'in', 'decreasing', 'order', 'so', 'that', 'the', 'most', 'frequent', 'items', 'are', 'the', 'ones', 'with', 'the', 'least', 'repetitions', 'in', 'the', 'tree', 'data', 'structure', 'since', 'the', 'frequency', 'only', 'descends', 'while', 'traversing', 'the', 'tree', 'in', 'depth', 'the', 'algorithm', 'is', 'able', 'to', 'lt', 'em', 'gt', 'strip', 'off', 'lt', 'em', 'gt', 'non', 'frequent', 'itemsets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strike', 'gt', 'as', 'far', 'as', 'i', 'know', 'this', 'may', 'be', 'considered', 'a', 'state', 'of', 'the', 'art', 'algorithm', 'but', 'i', 'd', 'like', 'to', 'know', 'about', 'other', 'proposed', 'solutions', 'what', 'other', 'algorithms', 'for', 'fpm', 'are', 'considered', 'quot', 'state', 'of', 'the', 'art', 'quot', 'what', 'is', 'the', 'lt', 'em', 'gt', 'intuition', 'lt', 'em', 'gt', 'lt', 'em', 'gt', 'main', 'contribution', 'lt', 'em', 'gt', 'of', 'such', 'algorithms', 'lt', 'strike', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'the', 'fpgrowth', 'algorithm', 'still', 'considered', 'quot', 'state', 'of', 'the', 'art', 'quot', 'in', 'frequent', 'pattern', 'mining', 'if', 'not', 'what', 'algorithm', 's', 'may', 'extract', 'frequent', 'itemsets', 'from', 'large', 'datasets', 'more', 'efficiently', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'when', 'i', 'started', 'with', 'artificial', 'neural', 'networks', 'nn', 'i', 'thought', 'i', 'd', 'have', 'to', 'fight', 'overfitting', 'as', 'the', 'main', 'problem', 'but', 'in', 'practice', 'i', 'can', 't', 'even', 'get', 'my', 'nn', 'to', 'pass', 'the', 'error', 'rate', 'barrier', 'i', 'can', 't', 'even', 'beat', 'my', 'score', 'on', 'random', 'forest', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'seeking', 'some', 'very', 'general', 'or', 'not', 'so', 'general', 'advice', 'on', 'what', 'should', 'one', 'do', 'to', 'make', 'a', 'nn', 'start', 'capturing', 'trends', 'in', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'implementing', 'nn', 'i', 'use', 'theano', 'stacked', 'auto', 'encoder', 'with', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'lisa', 'lab', 'deeplearningtutorials', 'blob', 'master', 'code', 'sda', 'py', 'quot', 'gt', 'the', 'code', 'from', 'tutorial', 'lt', 'a', 'gt', 'that', 'works', 'great', 'less', 'than', 'error', 'rate', 'for', 'classifying', 'the', 'mnist', 'dataset', 'it', 'is', 'a', 'multilayer', 'perceptron', 'with', 'softmax', 'layer', 'on', 'top', 'with', 'each', 'hidden', 'later', 'being', 'pre', 'trained', 'as', 'autoencoder', 'fully', 'described', 'at', 'lt', 'a', 'href', 'quot', 'http', 'deeplearning', 'net', 'tutorial', 'deeplearning', 'pdf', 'quot', 'gt', 'tutorial', 'lt', 'a', 'gt', 'chapter', 'there', 'are', 'input', 'features', 'and', 'output', 'classes', 'the', 'nn', 'has', 'sigmoid', 'neurons', 'and', 'all', 'data', 'are', 'normalized', 'to', 'i', 'tried', 'lots', 'of', 'different', 'configurations', 'number', 'of', 'hidden', 'layers', 'and', 'neurons', 'in', 'them', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'etc', 'different', 'learning', 'and', 'pre', 'train', 'rates', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'the', 'best', 'thing', 'i', 'can', 'get', 'is', 'a', 'error', 'rate', 'on', 'the', 'validation', 'set', 'and', 'a', 'error', 'rate', 'on', 'the', 'test', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'on', 'the', 'other', 'hand', 'when', 'i', 'try', 'to', 'use', 'random', 'forest', 'from', 'scikit', 'learn', 'i', 'easily', 'get', 'a', 'error', 'rate', 'on', 'the', 'validation', 'set', 'and', 'on', 'the', 'test', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'it', 'be', 'that', 'my', 'deep', 'nn', 'with', 'pre', 'training', 'behaves', 'so', 'badly', 'what', 'should', 'i', 'try', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'dataset', 'which', 'contains', 'samples', 'of', 'classes', 'i', 'have', 'been', 'using', 'svm', 'with', 'an', 'rbf', 'kernel', 'to', 'train', 'and', 'predict', 'new', 'data', 'the', 'problem', 'though', 'is', 'the', 'dataset', 'is', 'skewed', 'towards', 'different', 'classes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'class', 'each', 'class', 'each', 'class', 'each', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'see', 'that', 'the', 'model', 'tends', 'to', 'very', 'rarely', 'predict', 'the', 'classes', 'which', 'occur', 'less', 'frequent', 'in', 'the', 'training', 'set', 'even', 'though', 'the', 'test', 'set', 'has', 'the', 'same', 'class', 'distribution', 'as', 'the', 'training', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'aware', 'that', 'there', 'are', 'technique', 'such', 'as', 'undersampling', 'where', 'the', 'majority', 'class', 'is', 'scaled', 'down', 'to', 'the', 'minor', 'class', 'however', 'is', 'this', 'applicable', 'here', 'where', 'there', 'are', 'so', 'many', 'different', 'classes', 'are', 'there', 'other', 'methods', 'to', 'help', 'handle', 'this', 'case', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'an', 'msc', 'student', 'at', 'the', 'university', 'of', 'edinburgh', 'specialized', 'in', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'i', 'had', 'some', 'practical', 'courses', 'focused', 'on', 'data', 'mining', 'and', 'others', 'dealing', 'with', 'machine', 'learning', 'bayesian', 'statistics', 'and', 'graphical', 'models', 'my', 'background', 'is', 'a', 'bsc', 'in', 'computer', 'science', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'did', 'some', 'software', 'engineering', 'and', 'i', 'learnt', 'the', 'basic', 'concepts', 'such', 'as', 'design', 'patterns', 'but', 'i', 'have', 'never', 'been', 'involved', 'in', 'a', 'large', 'software', 'development', 'project', 'however', 'i', 'had', 'a', 'data', 'mining', 'project', 'in', 'my', 'msc', 'my', 'question', 'is', 'if', 'i', 'want', 'to', 'go', 'for', 'a', 'career', 'as', 'data', 'scientist', 'should', 'i', 'apply', 'for', 'a', 'graduate', 'data', 'scientist', 'position', 'first', 'or', 'should', 'i', 'get', 'a', 'position', 'as', 'graduate', 'software', 'engineer', 'first', 'maybe', 'something', 'related', 'to', 'data', 'science', 'such', 'as', 'big', 'data', 'infrastructure', 'or', 'machine', 'learning', 'software', 'development', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'concern', 'is', 'that', 'i', 'might', 'need', 'good', 'software', 'engineering', 'skills', 'for', 'data', 'science', 'and', 'i', 'am', 'not', 'sure', 'if', 'these', 'can', 'be', 'obtained', 'by', 'working', 'as', 'a', 'graduate', 'data', 'scientist', 'directly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'moreover', 'at', 'the', 'moment', 'i', 'like', 'data', 'mining', 'but', 'what', 'if', 'i', 'want', 'to', 'change', 'my', 'career', 'to', 'software', 'engineering', 'in', 'the', 'future', 'it', 'might', 'be', 'difficult', 'if', 'i', 'specialised', 'so', 'much', 'in', 'data', 'science', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'not', 'been', 'employed', 'yet', 'so', 'my', 'knowledge', 'is', 'still', 'limited', 'any', 'clarification', 'or', 'advice', 'are', 'welcome', 'as', 'i', 'am', 'about', 'to', 'finish', 'my', 'msc', 'and', 'i', 'want', 'to', 'start', 'applying', 'for', 'graduate', 'positions', 'in', 'early', 'october', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'it', 'looks', 'like', 'the', 'cosine', 'similarity', 'of', 'two', 'features', 'is', 'just', 'their', 'dot', 'product', 'scaled', 'by', 'the', 'product', 'of', 'their', 'magnitudes', 'when', 'does', 'cosine', 'similarity', 'make', 'a', 'better', 'distance', 'metric', 'than', 'the', 'dot', 'product', 'i', 'e', 'do', 'the', 'dot', 'product', 'and', 'cosine', 'similarity', 'have', 'different', 'strengths', 'or', 'weaknesses', 'in', 'different', 'situations', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'use', 'arma', 'arima', 'with', 'the', 'lt', 'a', 'href', 'quot', 'http', 'statsmodels', 'sourceforge', 'net', 'devel', 'tsa', 'html', 'descriptive', 'statistics', 'and', 'tests', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'statsmodel', 'python', 'package', 'lt', 'a', 'gt', 'in', 'order', 'to', 'predict', 'the', 'gas', 'consumption', 'i', 'tried', 'with', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'denadai', 'gas', 'consumption', 'outliers', 'blob', 'master', 'exportweb', 'csv', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'a', 'dataset', 'lt', 'a', 'gt', 'of', 'this', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'imgur', 'com', 'zuvblup', 'png', 'quot', 'alt', 'quot', 'with', 'this', 'format', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'using', 'only', 'the', 'gas', 'column', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'from', 'pandas', 'tseries', 'offsets', 'import', 'xa', 'xa', 'arma_mod', 'sm', 'tsa', 'arma', 'januaryfeb', 'gas', 'm', 'fit', 'xa', 'predict_sunspots', 'arma_mod', 'predict', 'dynamic', 'true', 'xa', 'ax', 'januaryfeb', 'ix', 'gas', 'm', 'plot', 'figsize', 'xa', 'ax', 'predict_sunspots', 'plot', 'ax', 'ax', 'style', 'r', 'label', 'dynamic', 'prediction', 'xa', 'ax', 'legend', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'imgur', 'com', 'ocponu', 'png', 'quot', 'alt', 'quot', 'result', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'is', 'the', 'prediction', 'so', 'bad', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'asked', 'a', 'data', 'science', 'question', 'regarding', 'how', 'to', 'decide', 'on', 'the', 'best', 'variation', 'of', 'a', 'split', 'test', 'on', 'the', 'statistics', 'section', 'of', 'stackexchange', 'i', 'hope', 'i', 'will', 'have', 'better', 'luck', 'here', 'the', 'question', 'is', 'basically', 'quot', 'why', 'is', 'mean', 'revenue', 'per', 'user', 'the', 'best', 'metric', 'to', 'make', 'your', 'decision', 'on', 'in', 'a', 'split', 'test', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'original', 'question', 'is', 'here', 'lt', 'a', 'href', 'quot', 'https', 'stats', 'stackexchange', 'com', 'questions', 'better', 'estimator', 'of', 'expected', 'sum', 'than', 'mean', 'quot', 'gt', 'https', 'stats', 'stackexchange', 'com', 'questions', 'better', 'estimator', 'of', 'expected', 'sum', 'than', 'mean', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'since', 'it', 'was', 'not', 'well', 'received', 'understood', 'i', 'simplified', 'the', 'problem', 'to', 'a', 'discrete', 'set', 'of', 'purchases', 'and', 'phrased', 'it', 'as', 'a', 'classical', 'probability', 'problem', 'that', 'question', 'is', 'here', 'lt', 'a', 'href', 'quot', 'https', 'stats', 'stackexchange', 'com', 'questions', 'drawing', 'numbered', 'balls', 'from', 'an', 'urn', 'quot', 'gt', 'https', 'stats', 'stackexchange', 'com', 'questions', 'drawing', 'numbered', 'balls', 'from', 'an', 'urn', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'mean', 'may', 'be', 'the', 'best', 'metric', 'for', 'such', 'a', 'decision', 'but', 'i', 'am', 'not', 'convinced', 'we', 'often', 'have', 'a', 'lot', 'of', 'prior', 'information', 'so', 'a', 'bayesian', 'method', 'would', 'likely', 'improve', 'our', 'estimates', 'i', 'realize', 'that', 'this', 'is', 'a', 'difficult', 'question', 'but', 'data', 'scientists', 'are', 'doing', 'such', 'split', 'tests', 'everyday', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'learning', 'about', 'matrix', 'factorization', 'for', 'recommender', 'systems', 'and', 'i', 'am', 'seeing', 'the', 'term', 'lt', 'code', 'gt', 'latent', 'features', 'lt', 'code', 'gt', 'occurring', 'too', 'frequently', 'but', 'i', 'am', 'unable', 'to', 'understand', 'what', 'it', 'means', 'i', 'know', 'what', 'a', 'feature', 'is', 'but', 'i', 'don', 't', 'understand', 'the', 'idea', 'of', 'latent', 'features', 'could', 'please', 'explain', 'it', 'or', 'at', 'least', 'point', 'me', 'to', 'a', 'paper', 'place', 'where', 'i', 'can', 'read', 'about', 'it', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'using', 'opencv', 'letter_recog', 'cpp', 'example', 'to', 'experiment', 'on', 'random', 'trees', 'and', 'other', 'classifiers', 'this', 'example', 'has', 'implementations', 'of', 'six', 'classifiers', 'random', 'trees', 'boosting', 'mlp', 'knn', 'naive', 'bayes', 'and', 'svm', 'uci', 'letter', 'recognition', 'dataset', 'with', 'instances', 'and', 'features', 'is', 'used', 'which', 'i', 'split', 'in', 'half', 'for', 'training', 'and', 'testing', 'i', 'have', 'experience', 'with', 'svm', 'so', 'i', 'quickly', 'set', 'its', 'recognition', 'error', 'to', 'after', 'some', 'experimentation', 'what', 'i', 'got', 'was', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'uci', 'letter', 'recognition', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'rtrees', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'boost', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'mlp', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'knn', 'k', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'bayes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'svm', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'parameters', 'used', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'rtrees', 'max_num_of_trees_in_the_forrest', 'max_depth', 'xa', 'min_sample_count', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'boost', 'boost_type', 'real', 'weak_count', 'weight_trim_rate', 'xa', 'max_depth', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'mlp', 'method', 'backprop', 'param', 'max_iter', 'default', 'values', 'too', 'xa', 'slow', 'to', 'experiment', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'knn', 'k', 'k', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'bayes', 'none', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'svm', 'rbf', 'kernel', 'c', 'gamma', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'after', 'that', 'i', 'used', 'same', 'parameters', 'and', 'tested', 'on', 'digits', 'and', 'mnist', 'datasets', 'by', 'extracting', 'gradient', 'features', 'first', 'vector', 'size', 'elements', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'digits', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'rtrees', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'boost', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'mlp', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'knn', 'k', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'bayes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'svm', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'mnist', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'rtrees', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'boost', 'out', 'of', 'memory', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'mlp', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'knn', 'k', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'bayes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'svm', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'new', 'to', 'all', 'classifiers', 'except', 'svm', 'and', 'knn', 'for', 'these', 'two', 'i', 'can', 'say', 'the', 'results', 'seem', 'fine', 'what', 'about', 'others', 'i', 'expected', 'more', 'from', 'random', 'trees', 'on', 'mnist', 'knn', 'gives', 'better', 'accuracy', 'any', 'ideas', 'how', 'to', 'get', 'it', 'higher', 'boost', 'and', 'bayes', 'give', 'very', 'low', 'accuracy', 'in', 'the', 'end', 'i', 'd', 'like', 'to', 'use', 'these', 'classifiers', 'to', 'make', 'a', 'multiple', 'classifier', 'system', 'any', 'advice', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'working', 'on', 'a', 'data', 'science', 'project', 'using', 'python', 'xa', 'the', 'project', 'has', 'several', 'stages', 'xa', 'each', 'stage', 'comprises', 'of', 'taking', 'a', 'data', 'set', 'using', 'python', 'scripts', 'auxiliary', 'data', 'configuration', 'and', 'parameters', 'and', 'creating', 'another', 'data', 'set', 'xa', 'i', 'store', 'the', 'code', 'in', 'git', 'so', 'that', 'part', 'is', 'covered', 'xa', 'i', 'would', 'like', 'to', 'hear', 'about', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'tools', 'for', 'data', 'version', 'control', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'tools', 'enabling', 'to', 'reproduce', 'stages', 'and', 'experiments', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'protocol', 'and', 'suggested', 'directory', 'structure', 'for', 'such', 'a', 'project', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'automated', 'build', 'run', 'tools', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'given', 'a', 'time', 'series', 'data', 'vector', 'ordered', 'by', 'months', 'and', 'years', 'which', 'contains', 'only', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 's', 'and', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 's', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 's', 'represent', 'a', 'person', 'changes', 'his', 'job', 'at', 'a', 'particular', 'a', 'month', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'questions', 'lt', 'strong', 'gt', 'what', 'model', 'can', 'i', 'use', 'to', 'determine', 'model', 'how', 'frequently', 'this', 'person', 'change', 'his', 'job', 'in', 'addition', 'this', 'model', 'should', 'be', 'able', 'to', 'predict', 'the', 'probability', 'of', 'this', 'person', 'changing', 'his', 'in', 'the', 'next', 'months', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'poisson', 'process', 'i', 'have', 'studied', 'poisson', 'process', 'before', 'however', 'i', 'have', 'no', 'idea', 'when', 'and', 'how', 'to', 'apply', 'it', 'any', 'assumptions', 'that', 'data', 'need', 'to', 'meet', 'before', 'applying', 'the', 'poisson', 'process', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'would', 'love', 'to', 'gather', 'more', 'information', 'on', 'how', 'to', 'model', 'something', 'like', 'this', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'right', 'approach', 'and', 'clustering', 'algorithm', 'for', 'geolocation', 'clustering', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'using', 'the', 'following', 'code', 'to', 'cluster', 'geolocation', 'coordinates', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'import', 'numpy', 'as', 'np', 'xa', 'import', 'matplotlib', 'pyplot', 'as', 'plt', 'xa', 'from', 'scipy', 'cluster', 'vq', 'import', 'kmeans', 'whiten', 'xa', 'xa', 'coordinates', 'np', 'array', 'xa', 'lat', 'long', 'xa', 'lat', 'long', 'xa', 'xa', 'lat', 'long', 'xa', 'xa', 'x', 'y', 'kmeans', 'whiten', 'coordinates', 'iter', 'xa', 'plt', 'scatter', 'coordinates', 'coordinates', 'c', 'y', 'xa', 'plt', 'show', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'right', 'to', 'use', 'k', 'means', 'for', 'geolocation', 'clustering', 'as', 'it', 'uses', 'euclidean', 'distance', 'and', 'not', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'haversine_formula', 'quot', 'rel', 'quot', 'noreferrer', 'quot', 'gt', 'haversine', 'formula', 'lt', 'a', 'gt', 'as', 'a', 'distance', 'function', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 't', 'sne', 'as', 'in', 'works', 'by', 'progressively', 'reducing', 'the', 'kullback', 'leibler', 'kl', 'divergence', 'until', 'a', 'certain', 'condition', 'is', 'met', 'xa', 'the', 'creators', 'of', 't', 'sne', 'suggests', 'to', 'use', 'kl', 'divergence', 'as', 'a', 'performance', 'criterion', 'for', 'the', 'visualizations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'you', 'can', 'compare', 'the', 'kullback', 'leibler', 'divergences', 'that', 't', 'sne', 'reports', 'it', 'is', 'perfectly', 'fine', 'to', 'run', 't', 'sne', 'ten', 'times', 'and', 'select', 'the', 'solution', 'with', 'the', 'lowest', 'kl', 'divergence', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'two', 'implementations', 'of', 't', 'sne', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'strong', 'gt', 'python', 'lt', 'strong', 'gt', 'sklearn', 'manifold', 'tsne', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'strong', 'gt', 'r', 'lt', 'strong', 'gt', 'tsne', 'from', 'library', 'tsne', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'both', 'these', 'implementations', 'when', 'verbosity', 'is', 'set', 'print', 'the', 'error', 'kullback', 'leibler', 'divergence', 'for', 'each', 'iteration', 'however', 'they', 'don', 't', 'allow', 'the', 'user', 'to', 'get', 'this', 'information', 'which', 'looks', 'a', 'bit', 'strange', 'to', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'the', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'import', 'numpy', 'as', 'np', 'xa', 'from', 'sklearn', 'manifold', 'import', 'tsne', 'xa', 'x', 'np', 'array', 'xa', 'model', 'tsne', 'n_components', 'verbose', 'n_iter', 'xa', 't', 'model', 'fit_transform', 'x', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'produces', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 't', 'sne', 'computing', 'pairwise', 'distances', 'xa', 't', 'sne', 'computed', 'conditional', 'probabilities', 'for', 'sample', 'xa', 't', 'sne', 'mean', 'sigma', 'xa', 't', 'sne', 'iteration', 'error', 'gradient', 'norm', 'xa', 't', 'sne', 'iteration', 'error', 'gradient', 'norm', 'xa', 't', 'sne', 'iteration', 'error', 'gradient', 'norm', 'xa', 'xa', 't', 'sne', 'error', 'after', 'iterations', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'as', 'far', 'as', 'i', 'understand', 'lt', 'strong', 'gt', 'lt', 'strong', 'gt', 'should', 'be', 'the', 'kl', 'divergence', 'however', 'i', 'cannot', 'get', 'this', 'information', 'neither', 'from', 'lt', 'strong', 'gt', 'model', 'lt', 'strong', 'gt', 'nor', 'from', 'lt', 'strong', 'gt', 't', 'lt', 'strong', 'gt', 'which', 'is', 'a', 'simple', 'numpy', 'ndarray', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'solve', 'this', 'problem', 'i', 'could', 'i', 'calculate', 'kl', 'divergence', 'by', 'my', 'self', 'ii', 'do', 'something', 'nasty', 'in', 'python', 'for', 'capturing', 'and', 'parsing', 'tsne', 'function', 's', 'output', 'however', 'i', 'would', 'be', 'quite', 'stupid', 'to', 're', 'calculate', 'kl', 'divergence', 'when', 'tsne', 'has', 'already', 'computed', 'it', 'ii', 'would', 'be', 'a', 'bit', 'unusual', 'in', 'terms', 'of', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'do', 'you', 'have', 'any', 'other', 'suggestion', 'is', 'there', 'a', 'standard', 'way', 'to', 'get', 'this', 'information', 'using', 'this', 'library', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'mentioned', 'i', 'tried', 'lt', 'em', 'gt', 'r', 'lt', 'em', 'gt', 's', 'tsne', 'library', 'but', 'i', 'd', 'prefer', 'the', 'answers', 'to', 'focus', 'on', 'the', 'lt', 'em', 'gt', 'python', 'lt', 'em', 'gt', 'sklearn', 'implementation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'references', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'nbviewer', 'ipython', 'org', 'urls', 'gist', 'githubusercontent', 'com', 'alexanderfabisch', 'a', 'c', 'de', 'eff', 'a', 'a', 'e', 'raw', 'd', 'bc', 'ed', 'f', 'bfd', 'ff', 'f', 'faa', 'd', 'b', 'aa', 'd', 'a', 't', 'sne', 'ipynb', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'http', 'nbviewer', 'ipython', 'org', 'urls', 'gist', 'githubusercontent', 'com', 'alexanderfabisch', 'a', 'c', 'de', 'eff', 'a', 'a', 'e', 'raw', 'd', 'bc', 'ed', 'f', 'bfd', 'ff', 'f', 'faa', 'd', 'b', 'aa', 'd', 'a', 't', 'sne', 'ipynb', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'homepage', 'tudelft', 'nl', 'j', 't', 'sne', 'html', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'http', 'homepage', 'tudelft', 'nl', 'j', 't', 'sne', 'html', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'stackoverflow', 'com', 'questions', 'how', 'to', 'capture', 'stdout', 'output', 'from', 'a', 'python', 'function', 'call', 'quot', 'gt', 'https', 'stackoverflow', 'com', 'questions', 'how', 'to', 'capture', 'stdout', 'output', 'from', 'a', 'python', 'function', 'call', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'new', 'to', 'the', 'data', 'science', 'forum', 'and', 'first', 'poster', 'here', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'may', 'be', 'kind', 'of', 'a', 'specific', 'question', 'hopefully', 'not', 'too', 'much', 'so', 'but', 'one', 'i', 'd', 'imagine', 'others', 'might', 'be', 'interested', 'in', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'a', 'way', 'to', 'basically', 'query', 'github', 'with', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'give', 'me', 'a', 'collection', 'of', 'all', 'of', 'the', 'public', 'repositories', 'that', 'have', 'more', 'than', 'stars', 'at', 'xa', 'least', 'two', 'forks', 'and', 'more', 'than', 'three', 'committers', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'result', 'could', 'take', 'any', 'viable', 'form', 'a', 'json', 'data', 'dump', 'a', 'url', 'to', 'the', 'web', 'page', 'etc', 'it', 'more', 'than', 'likely', 'will', 'consist', 'of', 'information', 'from', 'repos', 'or', 'something', 'large', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'sort', 'of', 'thing', 'possible', 'using', 'the', 'api', 'or', 'some', 'other', 'pre', 'built', 'way', 'or', 'am', 'i', 'going', 'to', 'have', 'to', 'build', 'out', 'my', 'own', 'custom', 'solution', 'where', 'i', 'try', 'to', 'scrape', 'every', 'page', 'if', 'so', 'how', 'feasible', 'is', 'this', 'and', 'how', 'might', 'i', 'approach', 'it', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'read', 'in', 'this', 'post', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'is', 'the', 'r', 'language', 'suitable', 'for', 'big', 'data', 'quot', 'gt', 'is', 'the', 'r', 'language', 'suitable', 'for', 'big', 'data', 'lt', 'a', 'gt', 'that', 'big', 'data', 'constitutes', 'lt', 'code', 'gt', 'tb', 'lt', 'code', 'gt', 'and', 'while', 'it', 'does', 'a', 'good', 'job', 'of', 'providing', 'information', 'about', 'the', 'feasibility', 'of', 'working', 'with', 'this', 'type', 'of', 'data', 'in', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'it', 'provides', 'very', 'little', 'information', 'about', 'lt', 'code', 'gt', 'python', 'lt', 'code', 'gt', 'i', 'was', 'wondering', 'if', 'lt', 'code', 'gt', 'python', 'lt', 'code', 'gt', 'can', 'work', 'with', 'this', 'much', 'data', 'as', 'well', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'plot', 'the', 'bytes', 'from', 'a', 'disk', 'image', 'in', 'order', 'to', 'understand', 'a', 'pattern', 'in', 'them', 'this', 'is', 'mainly', 'an', 'academic', 'task', 'since', 'i', 'm', 'almost', 'sure', 'this', 'pattern', 'was', 'created', 'by', 'a', 'disk', 'testing', 'program', 'but', 'i', 'd', 'like', 'to', 'reverse', 'engineer', 'it', 'anyway', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'already', 'know', 'that', 'the', 'pattern', 'is', 'aligned', 'with', 'a', 'periodicity', 'of', 'characters', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'can', 'envision', 'two', 'ways', 'of', 'visualizing', 'this', 'information', 'either', 'a', 'x', 'plane', 'viewed', 'through', 'time', 'dimensions', 'where', 'each', 'pixel', 's', 'color', 'is', 'the', 'ascii', 'code', 'for', 'the', 'character', 'or', 'a', 'pixel', 'line', 'for', 'each', 'period', 'dimensions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'is', 'a', 'snapshot', 'of', 'the', 'pattern', 'you', 'can', 'see', 'more', 'than', 'one', 'seen', 'through', 'lt', 'code', 'gt', 'xxd', 'lt', 'code', 'gt', 'x', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'zofsk', 'gif', 'quot', 'alt', 'quot', 'pattern', 'to', 'analyze', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'either', 'way', 'i', 'am', 'trying', 'to', 'find', 'a', 'way', 'of', 'visualizing', 'this', 'information', 'this', 'probably', 'isn', 't', 'hard', 'for', 'anyone', 'into', 'signal', 'analysis', 'but', 'i', 'can', 't', 'seem', 'to', 'find', 'a', 'way', 'using', 'open', 'source', 'software', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'like', 'to', 'avoid', 'matlab', 'or', 'mathematica', 'and', 'i', 'd', 'prefer', 'an', 'answer', 'in', 'r', 'since', 'i', 'have', 'been', 'learning', 'it', 'recently', 'but', 'nonetheless', 'any', 'language', 'is', 'welcome', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'update', 'given', 'emre', 's', 'answer', 'below', 'this', 'is', 'what', 'the', 'pattern', 'looks', 'like', 'given', 'the', 'first', 'mb', 'of', 'the', 'pattern', 'aligned', 'at', 'instead', 'of', 'this', 'alignment', 'looks', 'better', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'tdia', 'png', 'quot', 'alt', 'quot', 'graphical', 'pattern', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'further', 'ideas', 'are', 'welcome', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'timeseries', 'with', 'hourly', 'gas', 'consumption', 'i', 'want', 'to', 'use', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'autoregressive', 'e', 'moving', 'average_model', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'arma', 'lt', 'a', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'autoregressive_integrated_moving_average', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'arima', 'lt', 'a', 'gt', 'to', 'forecast', 'the', 'consumption', 'on', 'the', 'next', 'hour', 'basing', 'on', 'the', 'previous', 'why', 'should', 'i', 'analyze', 'find', 'the', 'seasonality', 'with', 'lt', 'a', 'href', 'quot', 'https', 'www', 'otexts', 'org', 'fpp', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'seasonal', 'and', 'trend', 'decomposition', 'using', 'loess', 'lt', 'a', 'gt', 'stl', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'hyyh', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'find', 'stock', 'data', 'to', 'practice', 'with', 'is', 'there', 'a', 'good', 'resource', 'for', 'this', 'i', 'found', 'this', 'lt', 'a', 'href', 'quot', 'ftp', 'emi', 'nasdaq', 'com', 'itch', 'quot', 'gt', 'ftp', 'emi', 'nasdaq', 'com', 'itch', 'lt', 'a', 'gt', 'but', 'it', 'only', 'has', 'the', 'current', 'year', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'already', 'have', 'a', 'way', 'of', 'parsing', 'the', 'protocol', 'but', 'would', 'like', 'to', 'have', 'some', 'more', 'data', 'to', 'compare', 'with', 'it', 'doesn', 't', 'have', 'to', 'be', 'in', 'the', 'same', 'format', 'as', 'long', 'as', 'it', 'has', 'price', 'trades', 'and', 'date', 'statistics', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'relatively', 'new', 'user', 'to', 'hadoop', 'using', 'version', 'i', 'installed', 'hadoop', 'on', 'my', 'first', 'node', 'without', 'a', 'hitch', 'but', 'i', 'can', 't', 'seem', 'to', 'get', 'the', 'resource', 'manager', 'to', 'start', 'on', 'my', 'second', 'node', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'cleared', 'up', 'some', 'quot', 'shared', 'library', 'quot', 'problems', 'by', 'adding', 'this', 'to', 'yarn', 'env', 'sh', 'and', 'hadoop', 'env', 'sh', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'export', 'hadoop_home', 'quot', 'usr', 'local', 'hadoop', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'export', 'hadoop_opts', 'quot', 'djava', 'library', 'path', 'hadoop_home', 'lib', 'quot', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'also', 'added', 'this', 'to', 'hadoop', 'env', 'sh', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'export', 'hadoop_common_lib_native_dir', 'hadoop_prefix', 'lib', 'native', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'based', 'on', 'the', 'advice', 'of', 'this', 'post', 'at', 'horton', 'works', 'lt', 'a', 'href', 'quot', 'http', 'hortonworks', 'com', 'community', 'forums', 'topic', 'hdfs', 'tmp', 'dir', 'issue', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'hortonworks', 'com', 'community', 'forums', 'topic', 'hdfs', 'tmp', 'dir', 'issue', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'that', 'cleared', 'up', 'all', 'of', 'my', 'error', 'messages', 'when', 'i', 'run', 'sbin', 'start', 'yarn', 'sh', 'i', 'get', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'starting', 'yarn', 'daemons', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'starting', 'resourcemanager', 'xa', 'logging', 'to', 'usr', 'local', 'hadoop', 'logs', 'yarn', 'hduser', 'resourcemanager', 'hdnode', 'out', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'localhost', 'starting', 'nodemanager', 'xa', 'logging', 'to', 'usr', 'local', 'hadoop', 'logs', 'yarn', 'hduser', 'nodemanager', 'hdnode', 'out', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'problem', 'is', 'jps', 'says', 'that', 'the', 'resource', 'manager', 'isn', 't', 'running', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 's', 'going', 'on', 'here', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'define', 'a', 'metric', 'between', 'job', 'titles', 'in', 'it', 'field', 'for', 'this', 'i', 'need', 'some', 'metric', 'between', 'words', 'of', 'job', 'titles', 'that', 'are', 'not', 'appearing', 'together', 'in', 'the', 'same', 'job', 'title', 'e', 'g', 'metric', 'between', 'the', 'words', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'senior', 'primary', 'lead', 'head', 'vp', 'director', 'stuff', 'principal', 'chief', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'the', 'words', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'analyst', 'expert', 'modeler', 'researcher', 'scientist', 'developer', 'engineer', 'architect', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'i', 'get', 'all', 'such', 'possible', 'words', 'with', 'their', 'distance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'how', 'can', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'nosql', 'quot', 'gt', 'nosql', 'lt', 'a', 'gt', 'databases', 'like', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'mongodb', 'quot', 'gt', 'mongodb', 'lt', 'a', 'gt', 'be', 'used', 'for', 'data', 'analysis', 'what', 'are', 'the', 'features', 'in', 'them', 'that', 'can', 'make', 'data', 'analysis', 'faster', 'and', 'powerful', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'an', 'application', 'which', 'requires', 'creating', 'a', 'very', 'large', 'database', 'of', 'n', 'grams', 'that', 'exist', 'in', 'a', 'large', 'text', 'corpus', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'need', 'three', 'efficient', 'operation', 'types', 'lookup', 'and', 'insertion', 'indexed', 'by', 'the', 'n', 'gram', 'itself', 'and', 'querying', 'for', 'all', 'n', 'grams', 'that', 'contain', 'a', 'sub', 'n', 'gram', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'sounds', 'to', 'me', 'like', 'the', 'database', 'should', 'be', 'a', 'gigantic', 'document', 'tree', 'and', 'document', 'databases', 'e', 'g', 'mongo', 'should', 'be', 'able', 'to', 'do', 'the', 'job', 'well', 'but', 'i', 've', 'never', 'used', 'those', 'at', 'scale', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'knowing', 'the', 'stack', 'exchange', 'question', 'format', 'i', 'd', 'like', 'to', 'clarify', 'that', 'i', 'm', 'not', 'asking', 'for', 'suggestions', 'on', 'specific', 'technologies', 'but', 'rather', 'a', 'type', 'of', 'database', 'that', 'i', 'should', 'be', 'looking', 'for', 'to', 'implement', 'something', 'like', 'this', 'at', 'scale', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'basically', 'both', 'are', 'software', 'systems', 'that', 'are', 'based', 'on', 'data', 'and', 'algorithms', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'starting', 'to', 'look', 'into', 'area', 'under', 'curve', 'auc', 'and', 'am', 'a', 'little', 'confused', 'about', 'its', 'usefulness', 'when', 'first', 'explained', 'to', 'me', 'auc', 'seemed', 'to', 'be', 'a', 'great', 'measure', 'of', 'performance', 'but', 'in', 'my', 'research', 'i', 've', 'found', 'that', 'some', 'claim', 'its', 'advantage', 'is', 'mostly', 'marginal', 'in', 'that', 'it', 'is', 'best', 'for', 'catching', 'lucky', 'models', 'with', 'high', 'standard', 'accuracy', 'measurements', 'and', 'low', 'auc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'should', 'i', 'avoid', 'relying', 'on', 'auc', 'for', 'validating', 'models', 'or', 'would', 'a', 'combination', 'be', 'best', 'thanks', 'for', 'all', 'your', 'help', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'become', 'a', 'lt', 'strong', 'gt', 'data', 'scientist', 'lt', 'strong', 'gt', 'i', 'studied', 'applied', 'lt', 'strong', 'gt', 'statistics', 'lt', 'strong', 'gt', 'actuarial', 'science', 'so', 'i', 'have', 'a', 'great', 'statistical', 'background', 'regression', 'stochastic', 'process', 'time', 'series', 'just', 'for', 'mention', 'a', 'few', 'but', 'now', 'i', 'am', 'going', 'to', 'do', 'a', 'master', 'degree', 'in', 'lt', 'strong', 'gt', 'computer', 'science', 'lt', 'strong', 'gt', 'focus', 'in', 'intelligent', 'systems', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'my', 'study', 'plan', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'machine', 'learning', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'advanced', 'machine', 'learning', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'data', 'mining', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'fuzzy', 'logic', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'recommendation', 'systems', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'distributed', 'data', 'systems', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'cloud', 'computing', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'knowledge', 'discovery', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'business', 'intelligence', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'information', 'retrieval', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'text', 'mining', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'the', 'end', 'with', 'all', 'my', 'statistical', 'and', 'computer', 'science', 'knowledge', 'can', 'i', 'call', 'myself', 'a', 'data', 'scientist', 'or', 'am', 'i', 'wrong', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'the', 'answers', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'my', 'machine', 'learning', 'task', 'is', 'of', 'separating', 'benign', 'internet', 'traffic', 'from', 'malicious', 'traffic', 'in', 'the', 'real', 'world', 'scenario', 'most', 'say', 'or', 'more', 'of', 'internet', 'traffic', 'is', 'benign', 'thus', 'i', 'felt', 'that', 'i', 'should', 'choose', 'a', 'similar', 'data', 'setup', 'for', 'training', 'my', 'models', 'as', 'well', 'but', 'i', 'came', 'across', 'a', 'research', 'paper', 'or', 'two', 'in', 'my', 'area', 'of', 'work', 'which', 'have', 'used', 'a', 'quot', 'class', 'balancing', 'quot', 'data', 'approach', 'to', 'training', 'the', 'models', 'implying', 'an', 'equal', 'number', 'of', 'instances', 'of', 'benign', 'and', 'malicious', 'traffic', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'general', 'if', 'i', 'am', 'building', 'machine', 'learning', 'models', 'should', 'i', 'go', 'for', 'a', 'dataset', 'which', 'is', 'representative', 'of', 'the', 'real', 'world', 'problem', 'or', 'is', 'a', 'balanced', 'dataset', 'better', 'suited', 'for', 'building', 'the', 'models', 'since', 'certain', 'classifiers', 'do', 'not', 'behave', 'well', 'with', 'class', 'imbalance', 'or', 'due', 'to', 'other', 'reasons', 'not', 'known', 'to', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'someone', 'shed', 'more', 'light', 'on', 'the', 'lt', 'em', 'gt', 'pros', 'lt', 'em', 'gt', 'and', 'lt', 'em', 'gt', 'cons', 'lt', 'em', 'gt', 'of', 'both', 'the', 'choices', 'and', 'how', 'to', 'decide', 'which', 'one', 'to', 'go', 'choose', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'best', 'tool', 'to', 'use', 'to', 'visualize', 'draw', 'the', 'vertices', 'and', 'edges', 'a', 'graph', 'with', 'vertices', 'there', 'are', 'about', 'edges', 'in', 'the', 'graph', 'and', 'i', 'can', 'compute', 'the', 'location', 'of', 'individual', 'vertices', 'and', 'edges', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'thinking', 'about', 'writing', 'a', 'program', 'to', 'generate', 'a', 'svg', 'any', 'other', 'suggestions', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'facing', 'a', 'project', 'that', 'i', 'could', 'solve', 'with', 'a', 'relational', 'database', 'in', 'a', 'relatively', 'painful', 'way', 'having', 'heard', 'so', 'much', 'about', 'nosql', 'i', 'm', 'wondering', 'if', 'there', 'is', 'not', 'a', 'more', 'appropriate', 'way', 'of', 'tackling', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'suppose', 'we', 'are', 'tracking', 'a', 'group', 'of', 'animals', 'in', 'a', 'forest', 'n', 'and', 'would', 'like', 'to', 'keep', 'a', 'record', 'of', 'a', 'set', 'of', 'observations', 'this', 'is', 'a', 'fictional', 'scenario', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'would', 'like', 'to', 'store', 'the', 'following', 'information', 'in', 'a', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'unique', 'identifier', 'for', 'each', 'animal', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'description', 'of', 'the', 'animal', 'with', 'structured', 'fields', 'species', 'genus', 'family', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'free', 'text', 'field', 'with', 'additional', 'information', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'each', 'time', 'point', 'at', 'which', 'it', 'was', 'detected', 'close', 'to', 'a', 'reference', 'point', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'picture', 'of', 'the', 'animal', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'an', 'indication', 'whether', 'two', 'given', 'animals', 'are', 'siblings', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'there', 'might', 'be', 'additional', 'features', 'appearing', 'later', 'as', 'more', 'data', 'comes', 'in', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'would', 'like', 'to', 'be', 'able', 'to', 'execute', 'the', 'following', 'types', 'of', 'queries', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'return', 'all', 'the', 'animals', 'spotted', 'between', 'in', 'a', 'given', 'time', 'interval', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'return', 'all', 'the', 'animals', 'of', 'a', 'given', 'species', 'or', 'family', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'perform', 'a', 'text', 'search', 'on', 'the', 'free', 'text', 'field', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'particular', 'database', 'system', 'would', 'you', 'recommend', 'is', 'there', 'any', 'tutorial', 'examples', 'that', 'i', 'could', 'use', 'as', 'a', 'starting', 'point', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'apologies', 'if', 'this', 'is', 'very', 'broad', 'question', 'what', 'i', 'would', 'like', 'to', 'know', 'is', 'how', 'effective', 'is', 'a', 'b', 'testing', 'or', 'other', 'methods', 'of', 'effectively', 'measuring', 'the', 'effects', 'of', 'a', 'design', 'decision', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'we', 'can', 'analyse', 'user', 'interactions', 'or', 'click', 'results', 'purchase', 'browse', 'decisions', 'and', 'then', 'modify', 'tailor', 'the', 'results', 'presented', 'to', 'the', 'user', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'could', 'then', 'test', 'the', 'effectiveness', 'of', 'this', 'design', 'change', 'by', 'subjecting', 'of', 'users', 'to', 'the', 'alternative', 'model', 'randomly', 'but', 'then', 'how', 'objective', 'is', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'do', 'we', 'avoid', 'influencing', 'the', 'user', 'by', 'the', 'model', 'change', 'for', 'instance', 'we', 'could', 'decided', 'that', 'search', 'queries', 'for', 'david', 'beckham', 'are', 'probably', 'about', 'football', 'so', 'search', 'results', 'become', 'biased', 'towards', 'this', 'but', 'we', 'could', 'equally', 'say', 'that', 'his', 'lifestyle', 'is', 'just', 'as', 'relevant', 'but', 'this', 'never', 'makes', 'it', 'into', 'the', 'top', 'results', 'that', 'are', 'returned', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'curious', 'how', 'this', 'is', 'dealt', 'with', 'and', 'how', 'to', 'measure', 'this', 'effectively', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'thoughts', 'are', 'that', 'you', 'could', 'be', 'in', 'danger', 'of', 'pushing', 'a', 'model', 'that', 'you', 'think', 'is', 'correct', 'and', 'the', 'user', 'obliges', 'and', 'this', 'becomes', 'a', 'self', 'fulfilling', 'prophecy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'read', 'an', 'article', 'on', 'this', 'lt', 'a', 'href', 'quot', 'http', 'techcrunch', 'com', 'ethics', 'in', 'a', 'data', 'driven', 'world', 'quot', 'gt', 'http', 'techcrunch', 'com', 'ethics', 'in', 'a', 'data', 'driven', 'world', 'lt', 'a', 'gt', 'and', 'also', 'the', 'book', 'lt', 'a', 'href', 'quot', 'http', 'shop', 'oreilly', 'com', 'product', 'do', 'quot', 'gt', 'http', 'shop', 'oreilly', 'com', 'product', 'do', 'lt', 'a', 'gt', 'which', 'discussed', 'this', 'so', 'it', 'piqued', 'my', 'interest', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'was', 'a', 'recent', 'furore', 'with', 'lt', 'a', 'href', 'quot', 'http', 'online', 'wsj', 'com', 'articles', 'furor', 'erupts', 'over', 'facebook', 'experiment', 'on', 'users', 'quot', 'gt', 'facebook', 'experimenting', 'on', 'their', 'users', 'to', 'see', 'if', 'they', 'could', 'alter', 'user', 's', 'emotions', 'lt', 'a', 'gt', 'and', 'now', 'lt', 'a', 'href', 'quot', 'http', 'www', 'bbc', 'co', 'uk', 'news', 'technology', 'quot', 'gt', 'okcupid', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'whilst', 'i', 'am', 'not', 'a', 'professional', 'data', 'scientist', 'i', 'read', 'about', 'lt', 'a', 'href', 'quot', 'http', 'columbiadatascience', 'com', 'data', 'science', 'ethics', 'quot', 'gt', 'data', 'science', 'ethics', 'lt', 'a', 'gt', 'from', 'lt', 'a', 'href', 'quot', 'http', 'shop', 'oreilly', 'com', 'product', 'do', 'quot', 'gt', 'cathy', 'o', 'neill', 's', 'book', 'doing', 'data', 'science', 'lt', 'a', 'gt', 'and', 'would', 'like', 'to', 'know', 'if', 'this', 'is', 'something', 'that', 'professionals', 'are', 'taught', 'at', 'academic', 'level', 'i', 'would', 'expect', 'so', 'or', 'something', 'that', 'is', 'ignored', 'or', 'is', 'lightly', 'applied', 'in', 'the', 'professional', 'world', 'particularly', 'for', 'those', 'who', 'ended', 'up', 'doing', 'data', 'science', 'lt', 'em', 'gt', 'accidentally', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'whilst', 'the', 'linked', 'article', 'touched', 'on', 'data', 'integrity', 'the', 'book', 'also', 'discussed', 'the', 'moral', 'ethics', 'behind', 'understanding', 'the', 'impact', 'of', 'the', 'data', 'models', 'that', 'are', 'created', 'and', 'the', 'impact', 'of', 'those', 'models', 'which', 'can', 'have', 'adverse', 'effects', 'when', 'used', 'inappropriately', 'sometimes', 'unwittingly', 'or', 'when', 'the', 'models', 'are', 'inaccurate', 'again', 'producing', 'adverse', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'article', 'discusses', 'a', 'code', 'of', 'practice', 'and', 'mentions', 'the', 'lt', 'a', 'href', 'quot', 'http', 'www', 'datascienceassn', 'org', 'code', 'of', 'conduct', 'html', 'quot', 'gt', 'data', 'science', 'association', 's', 'code', 'of', 'conduct', 'lt', 'a', 'gt', 'is', 'this', 'something', 'that', 'is', 'in', 'use', 'rule', 'is', 'of', 'particular', 'interest', 'quoted', 'from', 'their', 'website', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'a', 'a', 'person', 'who', 'consults', 'with', 'a', 'data', 'scientist', 'about', 'the', 'possibility', 'xa', 'of', 'forming', 'a', 'client', 'data', 'scientist', 'relationship', 'with', 'respect', 'to', 'a', 'xa', 'matter', 'is', 'a', 'prospective', 'client', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'b', 'even', 'when', 'no', 'client', 'data', 'scientist', 'relationship', 'ensues', 'a', 'data', 'xa', 'scientist', 'who', 'has', 'learned', 'information', 'from', 'a', 'prospective', 'client', 'shall', 'xa', 'not', 'use', 'or', 'reveal', 'that', 'information', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'c', 'a', 'data', 'scientist', 'subject', 'to', 'paragraph', 'b', 'shall', 'not', 'provide', 'xa', 'professional', 'data', 'science', 'services', 'for', 'a', 'client', 'with', 'interests', 'xa', 'materially', 'adverse', 'to', 'those', 'of', 'a', 'prospective', 'client', 'in', 'the', 'same', 'or', 'a', 'xa', 'substantially', 'related', 'industry', 'if', 'the', 'data', 'scientist', 'received', 'xa', 'information', 'from', 'the', 'prospective', 'client', 'that', 'could', 'be', 'significantly', 'xa', 'harmful', 'to', 'that', 'person', 'in', 'the', 'matter', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'something', 'that', 'is', 'practiced', 'professionally', 'many', 'users', 'blindly', 'accept', 'that', 'we', 'get', 'some', 'free', 'service', 'mail', 'social', 'network', 'image', 'hosting', 'blog', 'platform', 'etc', 'and', 'agree', 'to', 'an', 'eula', 'in', 'order', 'to', 'have', 'ads', 'pushed', 'at', 'us', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'finally', 'how', 'is', 'this', 'regulated', 'i', 'often', 'read', 'about', 'users', 'being', 'up', 'in', 'arms', 'when', 'the', 'terms', 'of', 'a', 'service', 'change', 'but', 'it', 'seems', 'that', 'it', 'requires', 'some', 'liberty', 'organisation', 'class', 'action', 'or', 'a', 'lt', 'a', 'href', 'quot', 'http', 'www', 'cnet', 'com', 'news', 'senator', 'asks', 'ftc', 'to', 'investigate', 'facebooks', 'mood', 'study', 'quot', 'gt', 'senator', 'lt', 'a', 'gt', 'to', 'react', 'to', 'such', 'things', 'before', 'something', 'happens', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'by', 'the', 'way', 'i', 'am', 'not', 'making', 'any', 'judgements', 'here', 'or', 'saying', 'that', 'all', 'data', 'scientists', 'behave', 'like', 'this', 'i', 'm', 'interested', 'in', 'what', 'is', 'taught', 'academically', 'and', 'practiced', 'professionally', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'most', 'vehicle', 'license', 'number', 'plate', 'extractors', 'i', 've', 'found', 'involve', 'reading', 'a', 'plate', 'from', 'an', 'image', 'ocr', 'but', 'i', 'm', 'interested', 'in', 'something', 'that', 'could', 'tag', 'instances', 'of', 'license', 'plates', 'in', 'a', 'body', 'of', 'text', 'are', 'there', 'any', 'such', 'annotators', 'out', 'there', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'while', 'running', 'the', 'below', 'pig', 'script', 'i', 'am', 'getting', 'error', 'in', 'line', 'xa', 'if', 'it', 'is', 'group', 'then', 'i', 'am', 'getting', 'error', 'xa', 'if', 'i', 'change', 'from', 'group', 'to', 'group', 'in', 'line', 'then', 'the', 'script', 'is', 'running', 'xa', 'what', 'is', 'the', 'difference', 'between', 'group', 'and', 'group', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lines', 'load', 'user', 'cloudera', 'datapeople', 'csv', 'using', 'pigstorage', 'as', 'firstname', 'chararray', 'lastname', 'chararray', 'address', 'chararray', 'city', 'chararray', 'state', 'chararray', 'zip', 'chararray', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'words', 'foreach', 'lines', 'generate', 'flatten', 'tokenize', 'zip', 'as', 'zips', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'wordsgrouped', 'group', 'words', 'by', 'zips', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'wordbycount', 'foreach', 'wordsgrouped', 'generate', 'group', 'as', 'zips', 'count', 'words', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'wordssort', 'order', 'wordbycount', 'by', 'desc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'dump', 'wordssort', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'have', 'you', 'heard', 'of', 'the', 'quot', 'data', 'science', 'association', 'quot', 'xa', 'lt', 'br', 'gt', 'url', 'lt', 'a', 'href', 'quot', 'http', 'www', 'datascienceassn', 'org', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'www', 'datascienceassn', 'org', 'lt', 'a', 'gt', 'xa', 'lt', 'br', 'gt', 'do', 'you', 'expect', 'it', 'to', 'become', 'a', 'professional', 'body', 'like', 'the', 'actuaries', 'institute', 'xa', 'lt', 'br', 'gt', 'if', 'yes', 'then', 'why', 'xa', 'lt', 'br', 'gt', 'if', 'no', 'then', 'why', 'not', 'and', 'do', 'you', 'see', 'anyone', 'else', 'becoming', 'the', 'professional', 'body', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lastly', 'is', 'this', 'question', 'quot', 'on', 'topic', 'quot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'my', 'data', 'contains', 'a', 'set', 'of', 'start', 'times', 'and', 'duration', 'for', 'an', 'action', 'i', 'would', 'like', 'to', 'plot', 'this', 'so', 'that', 'for', 'a', 'given', 'time', 'slice', 'i', 'can', 'see', 'how', 'many', 'actions', 'are', 'active', 'i', 'm', 'currently', 'thinking', 'of', 'this', 'as', 'a', 'histogram', 'with', 'time', 'on', 'the', 'x', 'axis', 'and', 'number', 'of', 'active', 'actions', 'on', 'the', 'y', 'axis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'how', 'should', 'i', 'adjust', 'the', 'data', 'so', 'that', 'this', 'is', 'able', 'to', 'be', 'plotted', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'times', 'for', 'an', 'action', 'can', 'be', 'between', 'seconds', 'and', 'a', 'minute', 'and', 'at', 'any', 'given', 'time', 'i', 'would', 'estimate', 'there', 'could', 'be', 'about', 'actions', 'taking', 'place', 'ideally', 'a', 'single', 'plot', 'would', 'be', 'able', 'to', 'show', 'hours', 'of', 'data', 'the', 'accuracy', 'of', 'the', 'data', 'is', 'in', 'milliseconds', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'the', 'past', 'the', 'way', 'that', 'i', 'have', 'done', 'this', 'is', 'to', 'count', 'for', 'each', 'second', 'how', 'many', 'actions', 'started', 'ended', 'or', 'were', 'active', 'this', 'gave', 'me', 'a', 'count', 'of', 'active', 'actions', 'for', 'each', 'second', 'the', 'issue', 'i', 'found', 'with', 'this', 'technique', 'was', 'that', 'it', 'made', 'it', 'difficult', 'to', 'adjust', 'the', 'time', 'slice', 'that', 'i', 'was', 'looking', 'at', 'looking', 'at', 'a', 'time', 'slice', 'of', 'a', 'minute', 'was', 'difficult', 'to', 'compute', 'and', 'looking', 'at', 'time', 'slices', 'of', 'less', 'than', 'a', 'second', 'was', 'impossible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'open', 'to', 'any', 'advice', 'on', 'how', 'to', 'think', 'about', 'this', 'issue', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'build', 'a', 'recommendation', 'engine', 'using', 'collaborative', 'filtering', 'i', 'have', 'the', 'usual', 'user', 'movie', 'rating', 'information', 'i', 'would', 'like', 'to', 'incorporate', 'an', 'additional', 'feature', 'like', 'language', 'or', 'duration', 'of', 'movie', 'i', 'am', 'not', 'sure', 'what', 'techniques', 'i', 'could', 'use', 'for', 'such', 'a', 'problem', 'please', 'suggest', 'references', 'or', 'packages', 'in', 'python', 'r', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'while', 'running', 'the', 'below', 'pig', 'script', 'i', 'am', 'getting', 'error', 'in', 'line', 'if', 'it', 'is', 'lt', 'code', 'gt', 'group', 'lt', 'code', 'gt', 'then', 'i', 'am', 'getting', 'error', 'if', 'i', 'change', 'from', 'lt', 'code', 'gt', 'group', 'lt', 'code', 'gt', 'to', 'lt', 'code', 'gt', 'group', 'lt', 'code', 'gt', 'in', 'line', 'then', 'the', 'script', 'is', 'running', 'what', 'is', 'the', 'difference', 'between', 'group', 'and', 'group', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'lines', 'load', 'user', 'cloudera', 'datapeople', 'csv', 'using', 'pigstorage', 'as', 'firstname', 'chararray', 'lastname', 'chararray', 'address', 'chararray', 'city', 'chararray', 'state', 'chararray', 'zip', 'chararray', 'xa', 'xa', 'words', 'foreach', 'lines', 'generate', 'flatten', 'tokenize', 'zip', 'as', 'zips', 'xa', 'xa', 'wordsgrouped', 'group', 'words', 'by', 'zips', 'xa', 'xa', 'wordbycount', 'foreach', 'wordsgrouped', 'generate', 'group', 'as', 'zips', 'count', 'words', 'xa', 'xa', 'wordssort', 'order', 'wordbycount', 'by', 'desc', 'xa', 'xa', 'dump', 'wordssort', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'been', 'trying', 'to', 'create', 'a', 'similarity', 'matrix', 'in', 'pandas', 'from', 'with', 'a', 'matrix', 'multiplication', 'operation', 'on', 'a', 'document', 'term', 'count', 'matrix', 'with', 'rows', 'and', 'columns', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'calculation', 'completes', 'in', 'ipython', 'but', 'inspection', 'shows', 'the', 'results', 'all', 'come', 'back', 'as', 'nan', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'also', 'tried', 'doing', 'the', 'same', 'job', 'in', 'numpy', 'tried', 'converting', 'the', 'original', 'matrix', 'to_sparse', 'and', 'even', 're', 'casting', 'the', 'values', 'as', 'integers', 'but', 'still', 'no', 'joy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anyone', 'suggest', 'the', 'best', 'approach', 'to', 'tackle', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'here', 's', 'my', 'code', 'thus', 'far', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'path', 'quot', 'reuters', 'db', 'quot', 'xa', 'pylab', 'inline', 'xa', 'import', 'pandas', 'as', 'pd', 'xa', 'import', 'numpy', 'as', 'np', 'xa', 'import', 'pandas', 'io', 'sql', 'as', 'psql', 'xa', 'import', 'sqlite', 'as', 'lite', 'xa', 'con', 'lite', 'connect', 'path', 'xa', 'with', 'con', 'xa', 'sql', 'quot', 'select', 'from', 'frequency', 'quot', 'xa', 'df', 'psql', 'frame_query', 'sql', 'con', 'xa', 'print', 'df', 'shape', 'xa', 'df', 'df', 'rename', 'columns', 'quot', 'term', 'quot', 'quot', 'term_id', 'quot', 'quot', 'count', 'quot', 'quot', 'count_id', 'quot', 'xa', 'pivoted', 'df', 'pivot', 'docid', 'term_id', 'count_id', 'xa', 'pivoted', 'to_sparse', 'xa', 'similarity_matrix', 'pivoted', 'dot', 'pivoted', 't', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'don', 't', 'know', 'if', 'this', 'is', 'a', 'right', 'place', 'to', 'ask', 'this', 'question', 'but', 'a', 'community', 'dedicated', 'to', 'data', 'science', 'should', 'be', 'the', 'most', 'appropriate', 'place', 'in', 'my', 'opinion', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'just', 'started', 'with', 'data', 'science', 'and', 'machine', 'learning', 'i', 'am', 'looking', 'for', 'long', 'term', 'project', 'ideas', 'which', 'i', 'can', 'work', 'on', 'for', 'like', 'months', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'mix', 'of', 'data', 'science', 'and', 'machine', 'learning', 'would', 'be', 'great', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'project', 'big', 'enough', 'to', 'help', 'me', 'understand', 'the', 'core', 'concepts', 'and', 'also', 'implement', 'them', 'at', 'the', 'same', 'time', 'would', 'be', 'very', 'beneficial', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'so', 'i', 'm', 'using', 'spark', 'to', 'do', 'sentiment', 'analysis', 'and', 'i', 'keep', 'getting', 'errors', 'with', 'the', 'serializers', 'it', 'uses', 'i', 'think', 'to', 'pass', 'python', 'objects', 'around', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'pyspark', 'worker', 'failed', 'with', 'exception', 'xa', 'traceback', 'most', 'recent', 'call', 'last', 'xa', 'file', 'quot', 'users', 'abdul', 'desktop', 'rsi', 'spark', 'bin', 'hadoop', 'python', 'pyspark', 'worker', 'py', 'quot', 'line', 'in', 'main', 'xa', 'serializer', 'dump_stream', 'func', 'split_index', 'iterator', 'outfile', 'xa', 'file', 'quot', 'users', 'abdul', 'desktop', 'rsi', 'spark', 'bin', 'hadoop', 'python', 'pyspark', 'serializers', 'py', 'quot', 'line', 'in', 'dump_stream', 'xa', 'self', 'serializer', 'dump_stream', 'self', '_batched', 'iterator', 'stream', 'xa', 'file', 'quot', 'users', 'abdul', 'desktop', 'rsi', 'spark', 'bin', 'hadoop', 'python', 'pyspark', 'serializers', 'py', 'quot', 'line', 'in', 'dump_stream', 'xa', 'for', 'obj', 'in', 'iterator', 'xa', 'file', 'quot', 'users', 'abdul', 'desktop', 'rsi', 'spark', 'bin', 'hadoop', 'python', 'pyspark', 'serializers', 'py', 'quot', 'line', 'in', '_batched', 'xa', 'for', 'item', 'in', 'iterator', 'xa', 'typeerror', '__init__', 'takes', 'exactly', 'arguments', 'given', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'the', 'code', 'for', 'serializers', 'is', 'available', 'lt', 'a', 'href', 'quot', 'https', 'spark', 'apache', 'org', 'docs', 'latest', 'api', 'python', 'pyspark', 'serializers', 'pysrc', 'html', 'pickleserializer', 'dumps', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'my', 'code', 'is', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'seashark', 'scalable', 'sentiment', 'analysis', 'blob', 'master', 'spark_test', 'py', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'h', 'gt', 'image', 'similarity', 'based', 'on', 'color', 'palette', 'distribution', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'compute', 'similarity', 'between', 'two', 'images', 'based', 'on', 'their', 'color', 'palette', 'distribution', 'let', 's', 'say', 'i', 'have', 'two', 'sets', 'of', 'key', 'value', 'pairs', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'img', 'lt', 'code', 'gt', 'brown', 'white', 'black', 'gray', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'img', 'lt', 'code', 'gt', 'pink', 'brown', 'white', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'the', 'numbers', 'denote', 'the', 'of', 'that', 'color', 'present', 'in', 'the', 'image', 'what', 'would', 'be', 'the', 'best', 'way', 'to', 'compute', 'similarity', 'on', 'a', 'scale', 'of', 'between', 'the', 'two', 'images', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'implemented', 'ner', 'system', 'with', 'the', 'use', 'of', 'crf', 'algorithm', 'with', 'my', 'handcrafted', 'features', 'that', 'gave', 'quite', 'good', 'results', 'the', 'thing', 'is', 'that', 'i', 'used', 'lots', 'of', 'different', 'features', 'including', 'pos', 'tags', 'and', 'lemmas', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'want', 'to', 'make', 'the', 'same', 'ner', 'for', 'different', 'language', 'the', 'problem', 'here', 'is', 'that', 'i', 'can', 't', 'use', 'pos', 'tags', 'and', 'lemmas', 'i', 'started', 'reading', 'articles', 'about', 'deep', 'learning', 'and', 'unsupervised', 'feature', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'possible', 'to', 'use', 'methods', 'for', 'unsupervised', 'feature', 'learning', 'with', 'crf', 'algorithm', 'did', 'anyone', 'try', 'this', 'and', 'got', 'any', 'good', 'result', 'is', 'there', 'any', 'article', 'or', 'tutorial', 'about', 'this', 'matter', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'still', 'don', 't', 'completely', 'understand', 'this', 'way', 'of', 'feature', 'creation', 'so', 'i', 'don', 't', 'want', 'to', 'spend', 'to', 'much', 'time', 'for', 'something', 'that', 'won', 't', 'work', 'so', 'any', 'information', 'would', 'be', 'really', 'helpful', 'to', 'create', 'whole', 'ner', 'system', 'based', 'on', 'deep', 'learning', 'is', 'a', 'bit', 'to', 'much', 'for', 'now', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'the', 'different', 'classes', 'of', 'data', 'science', 'problems', 'that', 'can', 'be', 'solved', 'using', 'mapreduce', 'programming', 'model', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'to', 'build', 'parse', 'tree', 'for', 'some', 'source', 'code', 'on', 'python', 'or', 'any', 'program', 'language', 'that', 'describe', 'by', 'cfg', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'have', 'source', 'code', 'on', 'some', 'programming', 'language', 'and', 'bnf', 'this', 'language', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anybody', 'give', 'some', 'advice', 'how', 'can', 'i', 'build', 'parse', 'tree', 'in', 'this', 'case', 'xa', 'preferably', 'with', 'tools', 'for', 'python', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'currently', 'working', 'with', 'a', 'large', 'set', 'of', 'health', 'insurance', 'claims', 'data', 'that', 'includes', 'some', 'laboratory', 'and', 'pharmacy', 'claims', 'the', 'most', 'consistent', 'information', 'in', 'the', 'data', 'set', 'however', 'is', 'made', 'up', 'of', 'diagnosis', 'icd', 'cm', 'and', 'procedure', 'codes', 'cpt', 'hcspcs', 'icd', 'cm', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'goals', 'are', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'identify', 'the', 'most', 'influential', 'precursor', 'conditions', 'comorbidities', 'for', 'a', 'medical', 'condition', 'like', 'chronic', 'kidney', 'disease', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'identify', 'the', 'likelihood', 'or', 'probability', 'that', 'a', 'patient', 'will', 'develop', 'a', 'medical', 'condition', 'based', 'on', 'the', 'conditions', 'they', 'have', 'had', 'in', 'the', 'past', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'do', 'the', 'same', 'as', 'and', 'but', 'with', 'procedures', 'and', 'or', 'diagnoses', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'preferably', 'the', 'results', 'would', 'be', 'interpretable', 'by', 'a', 'doctor', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'looked', 'at', 'things', 'like', 'the', 'lt', 'a', 'href', 'quot', 'https', 'www', 'heritagehealthprize', 'com', 'c', 'hhp', 'details', 'milestone', 'winners', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'heritage', 'health', 'prize', 'milestone', 'papers', 'lt', 'a', 'gt', 'and', 'have', 'learned', 'a', 'lot', 'from', 'them', 'but', 'they', 'are', 'focused', 'on', 'predicting', 'hospitalizations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'here', 'are', 'my', 'questions', 'what', 'methods', 'do', 'you', 'think', 'work', 'well', 'for', 'problems', 'like', 'this', 'and', 'what', 'resources', 'would', 'be', 'most', 'useful', 'for', 'learning', 'about', 'data', 'science', 'applications', 'and', 'methods', 'relevant', 'to', 'healthcare', 'and', 'clinical', 'medicine', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'to', 'add', 'plaintext', 'table', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'ckd', 'is', 'the', 'target', 'condition', 'quot', 'chronic', 'kidney', 'disease', 'quot', 'quot', 'any', 'quot', 'denotes', 'that', 'they', 'have', 'acquired', 'that', 'condition', 'at', 'any', 'time', 'quot', 'isbefore', 'ckd', 'quot', 'means', 'they', 'had', 'that', 'condition', 'before', 'their', 'first', 'diagnosis', 'of', 'ckd', 'the', 'other', 'abbreviations', 'correspond', 'with', 'other', 'conditions', 'identified', 'by', 'icd', 'cm', 'code', 'groupings', 'this', 'grouping', 'occurs', 'in', 'sql', 'during', 'the', 'import', 'process', 'each', 'variable', 'with', 'the', 'exception', 'of', 'patient_age', 'is', 'binary', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'does', 'anyone', 'know', 'what', 'from', 'your', 'experience', 'is', 'the', 'best', 'open', 'source', 'natural', 'language', 'generators', 'nlg', 'out', 'there', 'what', 'are', 'the', 'relative', 'merits', 'of', 'each', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'looking', 'to', 'do', 'sophisticated', 'text', 'summarization', 'and', 'would', 'like', 'to', 'use', 'theme', 'extraction', 'semantic', 'modeling', 'in', 'conjunction', 'with', 'nlg', 'tools', 'to', 'create', 'accurate', 'context', 'aware', 'and', 'natural', 'sounding', 'text', 'summaries', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'so', 'i', 'm', 'just', 'starting', 'to', 'learn', 'how', 'a', 'neural', 'network', 'can', 'operate', 'to', 'recognize', 'patterns', 'and', 'categorize', 'inputs', 'and', 'i', 've', 'seen', 'how', 'an', 'artificial', 'neural', 'network', 'can', 'parse', 'image', 'data', 'and', 'categorize', 'the', 'images', 'lt', 'a', 'href', 'quot', 'http', 'cs', 'stanford', 'edu', 'people', 'karpathy', 'convnetjs', 'demo', 'mnist', 'html', 'quot', 'gt', 'demo', 'with', 'convnetjs', 'lt', 'a', 'gt', 'and', 'the', 'key', 'there', 'is', 'to', 'downsample', 'the', 'image', 'and', 'each', 'pixel', 'stimulates', 'one', 'input', 'neuron', 'into', 'the', 'network', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'm', 'trying', 'to', 'wrap', 'my', 'head', 'around', 'if', 'this', 'is', 'possible', 'to', 'be', 'done', 'with', 'string', 'inputs', 'the', 'use', 'case', 'i', 've', 'got', 'is', 'a', 'quot', 'recommendation', 'engine', 'quot', 'for', 'movies', 'a', 'user', 'has', 'watched', 'movies', 'have', 'lots', 'of', 'string', 'data', 'title', 'plot', 'tags', 'and', 'i', 'could', 'imagine', 'quot', 'downsampling', 'quot', 'the', 'text', 'down', 'to', 'a', 'few', 'key', 'words', 'that', 'describe', 'that', 'movie', 'but', 'even', 'if', 'i', 'parse', 'out', 'the', 'top', 'five', 'words', 'that', 'describe', 'this', 'movie', 'i', 'think', 'i', 'd', 'need', 'input', 'neurons', 'for', 'every', 'english', 'word', 'in', 'order', 'to', 'compare', 'a', 'set', 'of', 'movies', 'i', 'could', 'limit', 'the', 'input', 'neurons', 'just', 'to', 'the', 'words', 'used', 'in', 'the', 'set', 'but', 'then', 'could', 'it', 'grow', 'learn', 'by', 'adding', 'new', 'movies', 'user', 'watches', 'a', 'new', 'movie', 'with', 'new', 'words', 'most', 'of', 'the', 'libraries', 'i', 've', 'seen', 'don', 't', 'allow', 'adding', 'new', 'neurons', 'after', 'the', 'system', 'has', 'been', 'trained', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'standard', 'way', 'to', 'map', 'string', 'word', 'character', 'data', 'to', 'inputs', 'into', 'a', 'neural', 'network', 'or', 'is', 'a', 'neural', 'network', 'really', 'not', 'the', 'right', 'tool', 'for', 'the', 'job', 'of', 'parsing', 'string', 'data', 'like', 'this', 'what', 's', 'a', 'better', 'tool', 'for', 'pattern', 'matching', 'in', 'string', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'attempting', 'to', 'use', 'the', 'tm', 'package', 'to', 'convert', 'a', 'vector', 'of', 'text', 'strings', 'to', 'a', 'corpus', 'element', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'code', 'looks', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'corpus', 'd', 'yes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'd', 'yes', 'is', 'a', 'factor', 'with', 'levels', 'each', 'containing', 'a', 'text', 'string', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'd', 'yes', 'quot', 'so', 'we', 'can', 'get', 'the', 'boat', 'out', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'receiving', 'the', 'following', 'error', 'xa', 'quot', 'error', 'inherits', 'x', 'quot', 'source', 'quot', 'is', 'not', 'true', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'not', 'sure', 'how', 'to', 'remedy', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'well', 'this', 'looks', 'like', 'the', 'most', 'suited', 'place', 'for', 'this', 'question', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'every', 'website', 'collect', 'data', 'of', 'the', 'user', 'some', 'just', 'for', 'usability', 'and', 'personalization', 'but', 'the', 'majority', 'like', 'social', 'networks', 'track', 'every', 'move', 'on', 'the', 'web', 'some', 'free', 'apps', 'on', 'your', 'phone', 'scan', 'text', 'messages', 'call', 'history', 'and', 'so', 'on', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'all', 'this', 'data', 'siphoning', 'is', 'just', 'for', 'selling', 'your', 'profile', 'for', 'advertisers', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'working', 'on', 'a', 'data', 'set', 'that', 'has', 'multiple', 'traffic', 'speed', 'measurements', 'per', 'day', 'my', 'data', 'is', 'from', 'the', 'city', 'of', 'chicago', 'and', 'it', 'is', 'taken', 'every', 'minute', 'for', 'about', 'six', 'months', 'i', 'wanted', 'to', 'consolidate', 'this', 'data', 'into', 'days', 'only', 'so', 'this', 'is', 'what', 'i', 'did', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'traffic', 'amp', 'lt', 'read', 'csv', 'quot', 'path', 'csv', 'quot', 'header', 'true', 'xa', 'traffic', 'amp', 'lt', 'aggregate', 'speed', 'date', 'data', 'traffic', 'fun', 'mean', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'was', 'perfect', 'because', 'it', 'took', 'all', 'of', 'my', 'data', 'and', 'averaged', 'it', 'by', 'date', 'for', 'example', 'my', 'original', 'data', 'looked', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'date', 'speed', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'the', 'final', 'looked', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'date', 'speed', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'problem', 'is', 'my', 'data', 'is', 'supposed', 'to', 'start', 'at', 'i', 'plotted', 'this', 'data', 'and', 'it', 'turns', 'out', 'the', 'data', 'goes', 'from', 'and', 'then', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'in', 'the', 'world', 'is', 'going', 'on', 'here', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'currently', 'on', 'a', 'project', 'that', 'will', 'build', 'a', 'model', 'train', 'and', 'test', 'on', 'client', 'side', 'web', 'data', 'but', 'evaluate', 'this', 'model', 'on', 'sever', 'side', 'web', 'data', 'unfortunately', 'building', 'the', 'model', 'on', 'server', 'side', 'data', 'is', 'not', 'an', 'option', 'nor', 'is', 'it', 'an', 'option', 'to', 'evaluate', 'this', 'model', 'on', 'client', 'side', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'model', 'will', 'be', 'based', 'on', 'metrics', 'collected', 'on', 'specific', 'visitors', 'this', 'is', 'a', 'real', 'time', 'system', 'that', 'will', 'be', 'calculating', 'a', 'likelihood', 'based', 'on', 'metrics', 'collected', 'while', 'visitors', 'browse', 'the', 'website', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'approaches', 'to', 'ensure', 'the', 'highest', 'possible', 'accuracy', 'on', 'the', 'model', 'evaluation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'far', 'i', 'have', 'the', 'following', 'ideas', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'clean', 'the', 'server', 'side', 'data', 'by', 'removing', 'webpages', 'that', 'are', 'never', 'seen', 'client', 'side', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'collect', 'additional', 'data', 'server', 'side', 'data', 'to', 'make', 'the', 'server', 'side', 'data', 'more', 'closely', 'resemble', 'client', 'side', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'collect', 'data', 'on', 'the', 'client', 'and', 'send', 'this', 'data', 'to', 'the', 'server', 'this', 'is', 'possible', 'and', 'may', 'be', 'the', 'best', 'solution', 'but', 'is', 'currently', 'undesirable', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'build', 'one', 'or', 'more', 'models', 'that', 'estimate', 'client', 'side', 'visitor', 'metrics', 'from', 'server', 'side', 'visitor', 'metrics', 'and', 'use', 'these', 'estimates', 'in', 'the', 'likelihood', 'model', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'other', 'thoughts', 'on', 'evaluating', 'over', 'one', 'population', 'while', 'training', 'and', 'testing', 'on', 'another', 'population', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'building', 'a', 'regression', 'model', 'and', 'i', 'need', 'to', 'calculate', 'the', 'below', 'to', 'check', 'for', 'correlations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'correlation', 'between', 'multi', 'level', 'categorical', 'variables', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'correlation', 'between', 'a', 'multi', 'level', 'categorical', 'variable', 'and', 'xa', 'continuous', 'variable', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'vif', 'variance', 'inflation', 'factor', 'for', 'a', 'multi', 'xa', 'level', 'categorical', 'variables', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'believe', 'its', 'wrong', 'to', 'use', 'pearson', 'correlation', 'coefficient', 'for', 'the', 'above', 'scenarios', 'because', 'pearson', 'only', 'works', 'for', 'continuous', 'variables', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'answer', 'the', 'below', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'which', 'correlation', 'coefficient', 'works', 'best', 'for', 'the', 'above', 'cases', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'vif', 'calculation', 'only', 'works', 'for', 'continuous', 'data', 'so', 'what', 'is', 'the', 'xa', 'alternative', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'are', 'the', 'assumptions', 'i', 'need', 'to', 'check', 'before', 'i', 'use', 'the', 'correlation', 'coefficient', 'you', 'suggest', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'to', 'implement', 'them', 'in', 'sas', 'amp', 'amp', 'r', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'assume', 'that', 'each', 'person', 'on', 'facebook', 'is', 'represented', 'as', 'a', 'node', 'of', 'a', 'graph', 'in', 'facebook', 'and', 'relationship', 'friendship', 'between', 'each', 'person', 'node', 'is', 'represented', 'as', 'an', 'edge', 'between', 'the', 'involved', 'nodes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'given', 'that', 'there', 'are', 'millions', 'of', 'people', 'on', 'facebook', 'how', 'is', 'the', 'graph', 'stored', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'implement', 'the', 'brown', 'clustering', 'algorithm', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'paper', 'details', 'quot', 'class', 'based', 'n', 'gram', 'models', 'of', 'natural', 'language', 'quot', 'by', 'brown', 'et', 'al', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'algorithm', 'is', 'supposed', 'to', 'in', 'lt', 'code', 'gt', 'o', 'v', 'k', 'lt', 'code', 'gt', 'where', 'lt', 'code', 'gt', 'v', 'lt', 'code', 'gt', 'is', 'the', 'size', 'of', 'the', 'vocabulary', 'and', 'k', 'is', 'the', 'number', 'of', 'clusters', 'i', 'am', 'unable', 'to', 'implement', 'it', 'this', 'efficiently', 'in', 'fact', 'the', 'best', 'i', 'can', 'manage', 'is', 'lt', 'code', 'gt', 'o', 'v', 'k', 'lt', 'code', 'gt', 'which', 'is', 'too', 'slow', 'my', 'current', 'implementation', 'for', 'the', 'main', 'part', 'of', 'the', 'algorithm', 'is', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'for', 'w', 'number', 'of', 'clusters', 'to', 'v', 'xa', 'xa', 'word', 'next', 'most', 'frequent', 'word', 'in', 'the', 'corpus', 'xa', 'xa', 'assign', 'word', 'to', 'a', 'new', 'cluster', 'xa', 'xa', 'initialize', 'maxquality', 'to', 'xa', 'xa', 'initialize', 'argmax', 'vector', 'to', 'xa', 'xa', 'for', 'i', 'to', 'number', 'of', 'clusters', 'xa', 'xa', 'for', 'j', 'i', 'to', 'number', 'of', 'clusters', 'xa', 'xa', 'quality', 'mutual', 'information', 'if', 'we', 'merge', 'cluster', 'i', 'and', 'cluster', 'j', 'xa', 'xa', 'if', 'quality', 'amp', 'gt', 'maxquality', 'xa', 'xa', 'maxquality', 'quality', 'xa', 'xa', 'argmax', 'i', 'j', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'compute', 'quality', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'before', 'entering', 'the', 'second', 'loop', 'compute', 'the', 'pre', 'merge', 'quality', 'i', 'e', 'quality', 'before', 'doing', 'any', 'merges', 'xa', 'every', 'time', 'a', 'cluster', 'pair', 'merge', 'step', 'is', 'considered', 'xa', 'i', 'assign', 'quality', 'pre', 'merge', 'quality', 'xa', 'ii', 'quality', 'quality', 'any', 'terms', 'in', 'the', 'mutual', 'information', 'equation', 'that', 'contain', 'cluster', 'i', 'or', 'cluster', 'j', 'pre', 'merge', 'xa', 'iii', 'quality', 'quality', 'any', 'terms', 'in', 'the', 'mutual', 'information', 'equation', 'that', 'contain', 'cluster', 'i', 'u', 'cluster', 'j', 'post', 'merge', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'my', 'implementation', 'the', 'first', 'loop', 'has', 'approx', 'v', 'iterations', 'the', 'second', 'and', 'third', 'loop', 'approx', 'k', 'iterations', 'each', 'to', 'compute', 'quality', 'at', 'each', 'step', 'requires', 'approx', 'a', 'further', 'k', 'iterations', 'in', 'total', 'it', 'runs', 'in', 'lt', 'code', 'gt', 'v', 'k', 'lt', 'code', 'gt', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'do', 'you', 'get', 'it', 'to', 'run', 'in', 'lt', 'code', 'gt', 'v', 'k', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'are', 'there', 'any', 'general', 'rules', 'that', 'one', 'can', 'use', 'to', 'infer', 'what', 'can', 'be', 'learned', 'generalized', 'from', 'a', 'particular', 'data', 'set', 'suppose', 'the', 'dataset', 'was', 'taken', 'from', 'a', 'sample', 'of', 'people', 'can', 'these', 'rules', 'be', 'stated', 'as', 'functions', 'of', 'the', 'sample', 'or', 'total', 'population', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'understand', 'the', 'above', 'may', 'be', 'vague', 'so', 'a', 'case', 'scenario', 'users', 'participate', 'in', 'a', 'search', 'task', 'where', 'the', 'data', 'are', 'their', 'queries', 'clicked', 'results', 'and', 'the', 'html', 'content', 'text', 'only', 'of', 'those', 'results', 'each', 'of', 'these', 'are', 'tagged', 'with', 'their', 'user', 'and', 'timestamp', 'a', 'user', 'may', 'generate', 'a', 'few', 'pages', 'for', 'a', 'simple', 'fact', 'finding', 'task', 'or', 'hundreds', 'of', 'pages', 'for', 'a', 'longer', 'term', 'search', 'task', 'like', 'for', 'class', 'report', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'in', 'addition', 'to', 'generalizing', 'about', 'a', 'population', 'given', 'a', 'sample', 'i', 'm', 'interested', 'in', 'generalizing', 'about', 'an', 'individual', 's', 'overall', 'search', 'behavior', 'given', 'a', 'time', 'slice', 'theory', 'and', 'paper', 'references', 'are', 'a', 'plus', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'set', 'of', 'results', 'from', 'an', 'a', 'b', 'test', 'one', 'control', 'group', 'one', 'feature', 'group', 'which', 'do', 'not', 'fit', 'a', 'normal', 'distribution', 'xa', 'in', 'fact', 'the', 'distribution', 'resembles', 'more', 'closely', 'the', 'landau', 'distribution', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'believe', 'the', 'independent', 't', 'test', 'requires', 'that', 'the', 'samples', 'be', 'at', 'least', 'approximately', 'normally', 'distributed', 'which', 'discourages', 'me', 'using', 'the', 't', 'test', 'as', 'a', 'valid', 'method', 'of', 'significance', 'testing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'my', 'question', 'is', 'xa', 'lt', 'strong', 'gt', 'at', 'what', 'point', 'can', 'one', 'say', 'that', 'the', 't', 'test', 'is', 'not', 'a', 'good', 'method', 'of', 'significance', 'testing', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'put', 'another', 'way', 'how', 'can', 'one', 'qualify', 'how', 'reliable', 'the', 'p', 'values', 'of', 'a', 't', 'test', 'are', 'given', 'only', 'the', 'data', 'set', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'to', 'generate', 'periodic', 'daily', 'monthly', 'web', 'analytics', 'dashboard', 'reports', 'they', 'will', 'be', 'static', 'and', 'don', 't', 'require', 'interaction', 'so', 'imagine', 'a', 'pdf', 'file', 'as', 'the', 'target', 'output', 'the', 'reports', 'will', 'mix', 'tables', 'and', 'charts', 'mainly', 'sparkline', 'and', 'bullet', 'graphs', 'created', 'with', 'ggplot', 'think', 'stephen', 'few', 'perceptual', 'edge', 'style', 'dashboards', 'such', 'as', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'edh', 'e', 'png', 'quot', 'alt', 'quot', 'sample', 'dashboard', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'applied', 'to', 'web', 'analytics', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'suggestions', 'on', 'what', 'packages', 'to', 'use', 'creating', 'these', 'dashboard', 'reports', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'first', 'intuition', 'is', 'to', 'use', 'r', 'markdown', 'and', 'knitr', 'but', 'perhaps', 'you', 've', 'found', 'a', 'better', 'solution', 'i', 'can', 't', 'seem', 'to', 'find', 'rich', 'examples', 'of', 'dashboards', 'generated', 'from', 'r', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'is', 'there', 'a', 'known', 'general', 'table', 'of', 'statistical', 'techniques', 'that', 'explain', 'how', 'they', 'scale', 'with', 'sample', 'size', 'and', 'dimension', 'for', 'example', 'a', 'friend', 'of', 'mine', 'told', 'me', 'the', 'other', 'day', 'that', 'the', 'computation', 'time', 'of', 'simply', 'quick', 'sorting', 'one', 'dimensional', 'data', 'of', 'size', 'n', 'goes', 'as', 'n', 'log', 'n', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'for', 'example', 'if', 'we', 'regress', 'y', 'against', 'x', 'where', 'x', 'is', 'a', 'd', 'dimensional', 'variable', 'does', 'it', 'go', 'as', 'o', 'n', 'd', 'how', 'does', 'it', 'scale', 'if', 'i', 'want', 'to', 'find', 'the', 'solution', 'via', 'exact', 'gauss', 'markov', 'solution', 'vs', 'numerical', 'least', 'squares', 'with', 'newton', 'method', 'or', 'simply', 'getting', 'the', 'solution', 'vs', 'using', 'significance', 'tests', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'guess', 'i', 'more', 'want', 'a', 'good', 'source', 'of', 'answers', 'like', 'a', 'paper', 'that', 'summarizes', 'the', 'scaling', 'of', 'various', 'statistical', 'techniques', 'than', 'a', 'good', 'answer', 'here', 'like', 'say', 'a', 'list', 'that', 'includes', 'the', 'scaling', 'of', 'multiple', 'regression', 'logistic', 'regression', 'pca', 'cox', 'proportional', 'hazard', 'regression', 'k', 'means', 'clustering', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'attempting', 'to', 'solve', 'a', 'set', 'of', 'equations', 'which', 'has', 'independent', 'variables', 'x', 'x', 'and', 'one', 'dependent', 'variable', 'y', 'the', 'total', 'number', 'of', 'equations', 'number', 'of', 'rows', 'is', 'and', 'i', 'want', 'to', 'solve', 'for', 'the', 'set', 'of', 'coefficients', 'that', 'minimizes', 'the', 'total', 'sum', 'of', 'square', 'error', 'between', 'y', 'and', 'the', 'predicted', 'value', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'problem', 'is', 'that', 'the', 'matrix', 'is', 'very', 'sparse', 'and', 'i', 'do', 'not', 'know', 'the', 'best', 'way', 'to', 'solve', 'the', 'system', 'of', 'equations', 'with', 'sparse', 'data', 'an', 'example', 'of', 'the', 'dataset', 'is', 'shown', 'below', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'y', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'currently', 'using', 'a', 'genetic', 'algorithm', 'to', 'solve', 'this', 'and', 'the', 'results', 'are', 'coming', 'out', 'xa', 'with', 'roughly', 'a', 'factor', 'of', 'two', 'difference', 'between', 'observed', 'and', 'expected', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anyone', 'suggest', 'different', 'methods', 'or', 'techniques', 'which', 'are', 'capable', 'of', 'solving', 'a', 'set', 'of', 'equations', 'with', 'sparse', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'data', 'set', 'looks', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'observations', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'up', 'to', 'predictors', 'of', 'different', 'types', 'numeric', 'multi', 'class', 'categorical', 'binary', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'target', 'variable', 'is', 'binary', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'cross', 'validation', 'method', 'is', 'typical', 'for', 'this', 'type', 'of', 'problems', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'by', 'default', 'i', 'm', 'using', 'k', 'fold', 'how', 'many', 'folds', 'is', 'enough', 'in', 'this', 'case', 'one', 'of', 'the', 'models', 'i', 'use', 'is', 'random', 'forest', 'which', 'is', 'time', 'consuming', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 'is', 'a', 'text', 'summarization', 'project', 'called', 'summarist', 'apparently', 'it', 'is', 'able', 'to', 'perform', 'abstractive', 'text', 'summarization', 'i', 'want', 'to', 'give', 'it', 'a', 'try', 'but', 'unfortunately', 'the', 'demo', 'links', 'on', 'the', 'website', 'do', 'not', 'work', 'does', 'anybody', 'have', 'any', 'information', 'regarding', 'this', 'how', 'can', 'i', 'test', 'this', 'tool', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'isi', 'edu', 'natural', 'language', 'projects', 'summarist', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'www', 'isi', 'edu', 'natural', 'language', 'projects', 'summarist', 'html', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'regards', 'xa', 'pasmod', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'fitting', 'a', 'model', 'in', 'r', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'use', 'lt', 'code', 'gt', 'createfolds', 'lt', 'code', 'gt', 'method', 'to', 'create', 'several', 'lt', 'code', 'gt', 'k', 'lt', 'code', 'gt', 'folds', 'from', 'the', 'data', 'set', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'loop', 'through', 'the', 'folds', 'repeating', 'the', 'following', 'on', 'each', 'iteration', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'code', 'gt', 'train', 'lt', 'code', 'gt', 'the', 'model', 'on', 'k', 'folds', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'code', 'gt', 'predict', 'lt', 'code', 'gt', 'the', 'outcomes', 'for', 'the', 'i', 'th', 'fold', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'calculate', 'prediction', 'accuracy', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'average', 'the', 'accuracy', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'r', 'have', 'a', 'function', 'that', 'makes', 'folds', 'itself', 'repeats', 'model', 'tuning', 'predictions', 'and', 'gives', 'the', 'average', 'accuracy', 'back', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'set', 'of', 'documents', 'and', 'i', 'want', 'classify', 'them', 'to', 'true', 'and', 'false', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'i', 'have', 'to', 'take', 'the', 'whole', 'words', 'in', 'the', 'documents', 'then', 'i', 'classify', 'them', 'depend', 'on', 'the', 'similarity', 'words', 'in', 'these', 'documents', 'or', 'i', 'can', 'take', 'only', 'some', 'words', 'that', 'i', 'interested', 'in', 'then', 'i', 'compare', 'it', 'with', 'the', 'documents', 'which', 'one', 'is', 'more', 'efficient', 'in', 'classify', 'document', 'and', 'can', 'work', 'with', 'svm', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'the', 'dataset', 'with', 'lots', 'of', 'na', 'values', 'with', 'sklearn', 'and', 'pandas', 'dataframe', 'i', 'implemented', 'different', 'imputation', 'strategies', 'for', 'different', 'columns', 'of', 'the', 'dataframe', 'based', 'column', 'names', 'for', 'example', 'nas', 'predictor', 'var', 'i', 'impute', 'with', 's', 'and', 'for', 'var', 'with', 'mean', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'i', 'try', 'to', 'cross', 'validate', 'my', 'model', 'using', 'train_test_split', 'it', 'returns', 'me', 'a', 'nparray', 'which', 'does', 'not', 'have', 'column', 'names', 'how', 'can', 'i', 'impute', 'missing', 'values', 'in', 'this', 'nparray', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'p', 's', 'i', 'do', 'not', 'impute', 'missing', 'values', 'in', 'the', 'original', 'data', 'set', 'before', 'splitting', 'on', 'purpose', 'so', 'i', 'keep', 'test', 'and', 'validation', 'sets', 'separately', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'train', 'and', 'test', 'sets', 'of', 'chronological', 'data', 'consisting', 'of', 'instances', 'and', 'appropriately', 'there', 'are', 'features', 'in', 'each', 'instance', 'and', 'only', 'possible', 'class', 'values', 'new', 'old', 'the', 'problem', 'is', 'that', 'there', 'are', 'only', 'old', 'instances', 'in', 'the', 'train', 'set', 'and', 'in', 'the', 'test', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'algorithm', 'which', 'succeeds', 'for', 'me', 'to', 'handle', 'imbalance', 'is', 'naivebayes', 'in', 'weka', 'precision', 'for', 'old', 'class', 'others', 'trees', 'classify', 'each', 'instance', 'as', 'new', 'xa', 'what', 'is', 'the', 'best', 'approach', 'to', 'handle', 'the', 'imbalance', 'and', 'the', 'appropriate', 'algorithm', 'in', 'such', 'a', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'attempting', 'to', 'compile', 'code', 'using', 'knitr', 'in', 'r', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'code', 'below', 'is', 'returning', 'the', 'following', 'error', 'and', 'causes', 'errors', 'in', 'the', 'rest', 'of', 'the', 'document', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'miss', 'amp', 'lt', 'sample', 'sensor_glucose', 'is', 'na', 'sample', 'sensor_glucose', 'xa', 'error', 'quot', 'warning', 'is', 'na', 'applied', 'to', 'non', 'list', 'or', 'vector', 'of', 'type', 'null', 'quot', 'xa', 'xa', 'str', 'miss', 'xa', 'int', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anyone', 'know', 'how', 'to', 'remedy', 'this', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'on', 'the', 'problem', 'with', 'too', 'many', 'features', 'and', 'training', 'my', 'models', 'takes', 'way', 'too', 'long', 'i', 'implemented', 'forward', 'selection', 'algorithm', 'to', 'choose', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'was', 'wondering', 'does', 'scikit', 'learn', 'have', 'forward', 'selection', 'stepwise', 'regression', 'algorithm', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 'are', 'storing', 'the', 'information', 'about', 'our', 'users', 'showing', 'interest', 'in', 'our', 'items', 'based', 'on', 'this', 'information', 'we', 'would', 'like', 'to', 'create', 'a', 'simple', 'recommendation', 'engine', 'that', 'will', 'take', 'the', 'items', 'i', 'i', 'i', 'etc', 'of', 'the', 'current', 'user', 'search', 'for', 'all', 'other', 'users', 'that', 'had', 'shown', 'interest', 'in', 'those', 'items', 'and', 'then', 'output', 'the', 'items', 'i', 'i', 'i', 'etc', 'of', 'the', 'other', 'users', 'sorted', 'by', 'their', 'decreasing', 'popularity', 'so', 'basically', 'the', 'standard', 'quot', 'other', 'buyer', 'were', 'also', 'interested', 'in', 'quot', 'functionality', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'asking', 'myself', 'what', 'kind', 'of', 'a', 'database', 'is', 'suitable', 'for', 'a', 'realtime', 'recommendation', 'engine', 'like', 'this', 'my', 'current', 'idea', 'is', 'to', 'build', 'a', 'trie', 'of', 'item', 'ids', 'then', 'sort', 'the', 'item', 'ids', 'of', 'the', 'current', 'user', 'as', 'the', 'order', 'of', 'items', 'is', 'irrelevant', 'and', 'to', 'go', 'down', 'the', 'trie', 'the', 'children', 'of', 'the', 'last', 'trie', 'node', 'will', 'build', 'the', 'needed', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'problem', 'is', 'that', 'we', 'have', 'million', 'items', 'so', 'that', 'according', 'to', 'our', 'estimation', 'the', 'trie', 'will', 'have', 'at', 'least', 'e', 'nodes', 'so', 'that', 'we', 'probably', 'need', 'a', 'distributed', 'sharded', 'database', 'to', 'store', 'it', 'before', 'we', 'reinvent', 'the', 'wheel', 'are', 'there', 'any', 'ready', 'to', 'use', 'databases', 'or', 'generally', 'non', 'cloud', 'solutions', 'for', 'recommendation', 'engines', 'out', 'there', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 'have', 'a', 'classification', 'algorithm', 'to', 'categorize', 'java', 'exceptions', 'in', 'production', 'xa', 'this', 'algorithm', 'is', 'based', 'on', 'hierarchical', 'human', 'defined', 'rules', 'so', 'when', 'a', 'bunch', 'of', 'text', 'forming', 'an', 'exception', 'comes', 'up', 'it', 'determines', 'what', 'kind', 'of', 'exception', 'is', 'development', 'availability', 'configuration', 'etc', 'and', 'the', 'responsible', 'component', 'the', 'most', 'inner', 'component', 'responsible', 'of', 'the', 'exception', 'in', 'java', 'an', 'exception', 'can', 'have', 'several', 'causing', 'exceptions', 'and', 'the', 'whole', 'must', 'be', 'analyzed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'given', 'the', 'following', 'example', 'exception', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'com', 'myapp', 'customexception', 'error', 'printing', 'xa', 'stack', 'xa', 'caused', 'by', 'com', 'foo', 'webservice', 'remoteexception', 'unable', 'to', 'communicate', 'xa', 'stack', 'xa', 'caused', 'by', 'com', 'acme', 'printexception', 'printserver', 'timeout', 'xa', 'stack', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'first', 'of', 'all', 'our', 'algorithm', 'splits', 'the', 'whole', 'stack', 'in', 'three', 'isolated', 'exceptions', 'afterwards', 'it', 'starts', 'analyzing', 'these', 'exceptions', 'starting', 'from', 'the', 'most', 'inner', 'one', 'in', 'this', 'case', 'it', 'determines', 'that', 'this', 'exception', 'the', 'second', 'caused', 'by', 'is', 'of', 'type', 'lt', 'code', 'gt', 'availability', 'lt', 'code', 'gt', 'and', 'that', 'the', 'responsible', 'component', 'is', 'a', 'quot', 'print', 'server', 'quot', 'this', 'is', 'because', 'there', 'is', 'a', 'rule', 'that', 'matches', 'containing', 'the', 'word', 'lt', 'code', 'gt', 'timeout', 'lt', 'code', 'gt', 'associated', 'to', 'the', 'lt', 'code', 'gt', 'availability', 'lt', 'code', 'gt', 'type', 'there', 'is', 'also', 'a', 'rule', 'that', 'matches', 'lt', 'code', 'gt', 'com', 'acme', 'printexception', 'lt', 'code', 'gt', 'and', 'determines', 'that', 'the', 'responsible', 'component', 'is', 'a', 'print', 'server', 'as', 'all', 'the', 'information', 'needed', 'is', 'determined', 'using', 'only', 'the', 'most', 'inner', 'exception', 'the', 'upper', 'exceptions', 'are', 'ignored', 'but', 'this', 'is', 'not', 'always', 'the', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'you', 'can', 'see', 'this', 'kind', 'of', 'approximation', 'is', 'very', 'complex', 'and', 'chaotic', 'as', 'a', 'human', 'have', 'to', 'create', 'new', 'rules', 'as', 'new', 'exceptions', 'appear', 'besides', 'the', 'new', 'rules', 'have', 'to', 'be', 'compatible', 'with', 'the', 'current', 'ones', 'because', 'a', 'new', 'rule', 'for', 'classifying', 'a', 'new', 'exception', 'must', 'not', 'change', 'the', 'classification', 'of', 'any', 'of', 'the', 'already', 'classified', 'exceptions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'we', 'are', 'thinking', 'about', 'using', 'machine', 'learning', 'to', 'automate', 'this', 'process', 'obviously', 'i', 'am', 'not', 'asking', 'for', 'a', 'solution', 'here', 'as', 'i', 'know', 'the', 'complexity', 'but', 'i', 'd', 'really', 'appreciate', 'some', 'advice', 'to', 'achieve', 'our', 'goal', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'cross', 'posting', 'this', 'from', 'cross', 'validated', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'seen', 'this', 'question', 'asked', 'before', 'but', 'i', 'have', 'yet', 'to', 'come', 'across', 'a', 'definitive', 'source', 'answering', 'the', 'specific', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 's', 'the', 'most', 'appropriate', 'statistical', 'test', 'to', 'apply', 'to', 'a', 'small', 'a', 'b', 'test', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 's', 'the', 'r', 'code', 'and', 'interpretation', 'to', 'analyze', 'a', 'small', 'a', 'b', 'test', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'running', 'a', 'small', 'test', 'to', 'figure', 'out', 'which', 'ads', 'perform', 'better', 'i', 'have', 'the', 'following', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'position', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'variation', 'impressions', 'clicks', 'xa', 'row', 'xa', 'row', 'xa', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'position', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'variation', 'impressions', 'clicks', 'xa', 'row', 'xa', 'row', 'xa', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'position', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'variation', 'impressions', 'clicks', 'xa', 'row', 'xa', 'row', 'xa', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'think', 'it', 's', 'safe', 'to', 'say', 'these', 'numbers', 'are', 'small', 'and', 'likely', 'to', 'be', 'not', 'normally', 'distributed', 'also', 'it', 's', 'click', 'data', 'so', 'there', 's', 'a', 'binary', 'outcome', 'of', 'clicked', 'or', 'not', 'and', 'the', 'trials', 'are', 'independent', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'appropriate', 'test', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'analyzing', 'each', 'position', 'for', 'significance', 'i', 'think', 'comparison', 'with', 'a', 'binomial', 'or', 'poisson', 'distribution', 'makes', 'the', 'most', 'sense', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'according', 'to', 'lt', 'a', 'href', 'quot', 'http', 'www', 'openintro', 'org', 'stat', 'textbook', 'php', 'stat_book', 'os', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'the', 'openintro', 'stats', 'lt', 'a', 'gt', 'and', 'other', 'sources', 'book', 'a', 'variable', 'follows', 'a', 'poisson', 'distribution', 'quot', 'if', 'the', 'event', 'being', 'considered', 'is', 'rare', 'the', 'population', 'is', 'large', 'and', 'the', 'events', 'occur', 'independently', 'of', 'each', 'other', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'same', 'source', 'classifies', 'a', 'binomial', 'variable', 'approximately', 'the', 'same', 'way', 'adding', 'that', 'the', 'probability', 'of', 'success', 'is', 'the', 'same', 'and', 'the', 'number', 'of', 'trials', 'is', 'fixed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'appreciate', 'this', 'is', 'not', 'an', 'either', 'or', 'decision', 'and', 'analysis', 'can', 'be', 'done', 'using', 'both', 'distributions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'given', 'a', 'b', 'split', 'testing', 'is', 'a', 'science', 'that', 'has', 'been', 'practiced', 'for', 'several', 'years', 'i', 'imagine', 'that', 'there', 'is', 'a', 'canonical', 'test', 'however', 'looking', 'around', 'the', 'internet', 'i', 'mostly', 'come', 'across', 'analysis', 'that', 'uses', 'the', 'standard', 'normal', 'distribution', 'that', 'just', 'seems', 'wrong', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'canonical', 'test', 'to', 'use', 'for', 'a', 'b', 'tests', 'with', 'small', 's', 'of', 'clicks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'interpretation', 'and', 'r', 'code', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'used', 'the', 'following', 'r', 'code', 'to', 'test', 'significance', 'for', 'each', 'position', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'position', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'binom', 'test', 'p', 'xa', 'xa', 'exact', 'binomial', 'test', 'xa', 'xa', 'data', 'and', 'xa', 'number', 'of', 'successes', 'number', 'of', 'trials', 'p', 'value', 'e', 'xa', 'alternative', 'hypothesis', 'true', 'probability', 'of', 'success', 'is', 'not', 'equal', 'to', 'xa', 'percent', 'confidence', 'interval', 'xa', 'xa', 'sample', 'estimates', 'xa', 'probability', 'of', 'success', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'interpret', 'this', 'result', 'to', 'mean', 'the', 'probability', 'of', 'success', 'in', 'the', 'test', 'group', 'is', 'indeed', 'different', 'than', 'the', 'control', 'group', 'with', 'a', 'confidence', 'interval', 'that', 'the', 'success', 'probability', 'is', 'between', 'and', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'ppois', 'lambda', 'lower', 'tail', 'f', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'interpret', 'this', 'result', 'to', 'mean', 'given', 'a', 'poisson', 'distribution', 'with', 'a', 'click', 'rate', 'of', 'per', 'trials', 'there', 'is', 'a', 'chance', 'of', 'having', 'a', 'click', 'rate', 'of', 'or', 'more', 'per', 'trials', 'in', 'the', 'same', 'distribution', 'contextualized', 'in', 'the', 'ad', 'example', 'xa', 'there', 'is', 'a', 'chance', 'that', 'the', 'control', 'ad', 'actually', 'performs', 'the', 'same', 'as', 'the', 'test', 'ad', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'the', 'above', 'interpretation', 'correct', 'does', 'the', 'test', 'and', 'interpretation', 'change', 'with', 'the', 'different', 'positions', 'i', 'e', 'are', 'the', 'results', 'of', 'the', 'poisson', 'test', 'more', 'appropriate', 'for', 'position', 'given', 'the', 'small', 'numbers', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'found', 'a', 'number', 'of', 'libraries', 'and', 'tools', 'for', 'data', 'science', 'in', 'scala', 'i', 'would', 'like', 'to', 'know', 'about', 'which', 'one', 'has', 'more', 'adoption', 'and', 'which', 'one', 'is', 'gaining', 'adoption', 'at', 'a', 'faster', 'pace', 'and', 'to', 'what', 'extent', 'this', 'is', 'the', 'case', 'basically', 'which', 'one', 'should', 'i', 'bet', 'for', 'if', 'any', 'at', 'this', 'point', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'of', 'the', 'tools', 'i', 've', 'found', 'are', 'in', 'no', 'particular', 'order', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'scalding', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'breeze', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'spark', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'saddle', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'h', 'o', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'spire', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'mahout', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'hadoop', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'mongodb', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'need', 'to', 'be', 'more', 'specific', 'to', 'make', 'the', 'question', 'answerable', 'i', 'm', 'not', 'particularly', 'interested', 'in', 'clusters', 'and', 'big', 'data', 'at', 'this', 'moment', 'but', 'i', 'm', 'interested', 'in', 'sizable', 'data', 'up', 'to', 'gb', 'for', 'information', 'integration', 'and', 'predictive', 'analytics', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'facing', 'this', 'bizarre', 'issue', 'while', 'using', 'lt', 'code', 'gt', 'apache', 'pig', 'lt', 'code', 'gt', 'lt', 'strong', 'gt', 'rank', 'lt', 'strong', 'gt', 'utility', 'i', 'am', 'executing', 'the', 'following', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'email_id_ranked', 'rank', 'email_id', 'xa', 'store', 'email_id_ranked', 'into', 'tmp', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'basically', 'i', 'am', 'trying', 'to', 'get', 'the', 'following', 'result', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'email', 'xa', 'email', 'xa', 'email', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'issue', 'is', 'sometime', 'pig', 'dumps', 'the', 'above', 'result', 'but', 'sometimes', 'it', 'dumps', 'only', 'the', 'emails', 'without', 'the', 'rank', 'also', 'when', 'i', 'dump', 'the', 'data', 'on', 'screen', 'using', 'lt', 'code', 'gt', 'dump', 'lt', 'code', 'gt', 'function', 'pig', 'returns', 'both', 'the', 'columns', 'i', 'don', 't', 'know', 'where', 'the', 'issue', 'is', 'kindly', 'advice', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'more', 'information', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'pig', 'version', 'apache', 'pig', 'version', 'cdh', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'to', 'do', 'coreference', 'resolution', 'for', 'german', 'texts', 'and', 'i', 'plan', 'to', 'use', 'opennlp', 'to', 'perform', 'this', 'task', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'far', 'as', 'i', 'know', 'opennlp', 'coreference', 'resolution', 'does', 'not', 'support', 'the', 'german', 'language', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'components', 'data', 'do', 'i', 'need', 'to', 'adapt', 'the', 'code', 'such', 'that', 'it', 'is', 'possible', 'to', 'perform', 'coreference', 'resolution', 'for', 'german', 'texts', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'does', 'anybody', 'know', 'a', 'libarary', 'for', 'performing', 'coreference', 'resolution', 'on', 'german', 'texts', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'far', 'as', 'i', 'know', 'opennlp', 'and', 'standord', 'nlp', 'are', 'not', 'able', 'to', 'perform', 'coreference', 'resolution', 'for', 'german', 'texts', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'tool', 'that', 'i', 'know', 'is', 'lt', 'a', 'href', 'quot', 'http', 'www', 'cl', 'uzh', 'ch', 'static', 'news', 'php', 'om', 'view', 'amp', 'amp', 'nid', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'corzu', 'lt', 'a', 'gt', 'which', 'is', 'a', 'python', 'library', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'know', 'that', 'arima', 'can', 't', 'detect', 'multiple', 'seasonality', 'but', 'it', 'is', 'possible', 'to', 'lt', 'a', 'href', 'quot', 'http', 'robjhyndman', 'com', 'hyndsight', 'dailydata', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'use', 'fourier', 'functions', 'to', 'add', 'a', 'second', 'seasonality', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'need', 'to', 'forecast', 'gas', 'consumption', 'composed', 'by', 'a', 'daily', 'weekly', 'week', 'days', 'weekend', 'yearly', 'seasonality', 'does', 'it', 'make', 'sense', 'to', 'apply', 'three', 'times', 'the', 'lt', 'a', 'href', 'quot', 'http', 'stat', 'ethz', 'ch', 'r', 'manual', 'r', 'devel', 'library', 'stats', 'html', 'stl', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'stl', 'decomposition', 'by', 'loess', 'lt', 'a', 'gt', 'the', 'reason', 'is', 'that', 'i', 'applied', 'the', 'fourier', 'method', 'and', 'i', 'have', 'bad', 'results', 'but', 'i', 'don', 't', 'know', 'if', 'it', 'is', 'only', 'because', 'i', 'applied', 'it', 'wrong', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'interested', 'in', 'the', 'theoretical', 'explanation', 'but', 'here', 'you', 'find', 'also', 'the', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'arima', 'stl', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'b', 'amp', 'lt', 'ts', 'drop', 'coredata', 'dat', 'ts', 'deltat', 'start', 'xa', 'fit', 'amp', 'lt', 'stl', 'b', 's', 'window', 'quot', 'periodic', 'quot', 'xa', 'b', 'amp', 'lt', 'seasadj', 'fit', 'xa', 'dat', 'ts', 'amp', 'lt', 'xts', 'b', 'index', 'dat', 'ts', 'xa', 'xa', 'the', 'weekdays', 'are', 'extracted', 'xa', 'dat', 'weekdays', 'amp', 'lt', 'dat', 'ts', 'indexwday', 'dat', 'ts', 'in', 'xa', 'dat', 'weekdaysts', 'amp', 'lt', 'ts', 'drop', 'coredata', 'dat', 'weekdays', 'frequency', 'start', 'xa', 'fit', 'amp', 'lt', 'stl', 'dat', 'weekdaysts', 's', 'window', 'quot', 'periodic', 'quot', 'xa', 'dat', 'weekdaysts', 'amp', 'lt', 'seasadj', 'fit', 'xa', 'xa', 'arima', 'amp', 'lt', 'arima', 'dat', 'weekdaysts', 'order', 'c', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'with', 'fourier', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'dat', 'weekdays', 'amp', 'lt', 'dat', 'ts', 'indexwday', 'dat', 'ts', 'in', 'xa', 'dat', 'weekdaysts', 'amp', 'lt', 'ts', 'drop', 'coredata', 'dat', 'weekdays', 'frequency', 'start', 'xa', 'z', 'amp', 'lt', 'fourier', 'ts', 'dat', 'weekdaysts', 'frequency', 'k', 'xa', 'arima', 'amp', 'lt', 'arima', 'dat', 'weekdaysts', 'order', 'c', 'xreg', 'z', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'lbxl', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'these', 'are', 'different', 'weight', 'matrices', 'that', 'i', 'got', 'after', 'training', 'a', 'lt', 'strong', 'gt', 'restricted', 'boltzman', 'machine', 'rbm', 'lt', 'strong', 'gt', 'with', 'k', 'visible', 'units', 'and', 'only', 'hidden', 'units', 'weight', 'vectors', 'as', 'you', 'can', 'see', 'weights', 'are', 'extremely', 'similar', 'even', 'black', 'pixels', 'on', 'the', 'face', 'are', 'reproduced', 'the', 'other', 'vectors', 'are', 'very', 'similar', 'too', 'though', 'lt', 'em', 'gt', 'none', 'lt', 'em', 'gt', 'of', 'weights', 'are', 'lt', 'em', 'gt', 'exactly', 'lt', 'em', 'gt', 'the', 'same', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'can', 'overcome', 'this', 'by', 'increasing', 'number', 'of', 'weight', 'vectors', 'to', 'or', 'more', 'but', 'i', 'encountered', 'this', 'problem', 'several', 'times', 'earlier', 'with', 'different', 'rbm', 'types', 'binary', 'gaussian', 'even', 'convolutional', 'different', 'number', 'of', 'hidden', 'units', 'including', 'pretty', 'large', 'different', 'hyper', 'parameters', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'what', 'is', 'the', 'lt', 'strong', 'gt', 'most', 'likely', 'reason', 'lt', 'strong', 'gt', 'for', 'weights', 'to', 'get', 'lt', 'strong', 'gt', 'very', 'similar', 'values', 'lt', 'strong', 'gt', 'do', 'they', 'all', 'just', 'get', 'to', 'some', 'local', 'minimum', 'or', 'is', 'it', 'a', 'sign', 'of', 'overfitting', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'currently', 'use', 'a', 'kind', 'of', 'gaussian', 'bernoulli', 'rbm', 'code', 'may', 'be', 'found', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'faithlessfriend', 'milk', 'jl', 'blob', 'master', 'src', 'rbm', 'jl', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'upd', 'lt', 'strong', 'gt', 'my', 'dataset', 'is', 'based', 'on', 'lt', 'a', 'href', 'quot', 'http', 'www', 'pitt', 'edu', 'emotion', 'ck', 'spread', 'htm', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'ck', 'lt', 'a', 'gt', 'which', 'contains', 'gt', 'k', 'images', 'of', 'individuals', 'however', 'i', 'do', 'pretty', 'heavy', 'preprocessing', 'first', 'i', 'clip', 'only', 'pixels', 'inside', 'of', 'outer', 'contour', 'of', 'a', 'face', 'second', 'i', 'transform', 'each', 'face', 'using', 'piecewise', 'affine', 'wrapping', 'to', 'the', 'same', 'grid', 'e', 'g', 'eyebrows', 'nose', 'lips', 'etc', 'are', 'in', 'the', 'same', 'x', 'y', 'position', 'on', 'all', 'images', 'after', 'preprocessing', 'images', 'look', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'fqmsp', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'hjcaw', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'training', 'rbm', 'i', 'take', 'only', 'non', 'zero', 'pixels', 'so', 'outer', 'black', 'region', 'is', 'ignored', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'know', 'the', 'difference', 'between', 'clustering', 'and', 'classification', 'in', 'machine', 'learning', 'but', 'i', 'don', 't', 'understand', 'the', 'difference', 'between', 'text', 'classification', 'and', 'topic', 'modeling', 'for', 'documents', 'can', 'i', 'use', 'topic', 'modeling', 'over', 'documents', 'to', 'identify', 'a', 'topic', 'can', 'i', 'use', 'classification', 'methods', 'to', 'classify', 'the', 'text', 'inside', 'these', 'documents', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'for', 'experimenting', 'we', 'd', 'like', 'to', 'use', 'the', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'emoji', 'quot', 'gt', 'emoji', 'lt', 'a', 'gt', 'embedded', 'in', 'many', 'tweets', 'as', 'a', 'ground', 'truth', 'training', 'data', 'for', 'simple', 'quantitative', 'senitment', 'analysis', 'tweets', 'are', 'usually', 'too', 'unstructured', 'for', 'nlp', 'to', 'work', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'anyway', 'there', 'are', 'emoji', 'in', 'unicode', 'and', 'probably', 'another', 'will', 'be', 'added', 'in', 'unicode', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'database', 'like', 'e', 'g', 'sentiwordnet', 'that', 'contains', 'sentiment', 'annotations', 'for', 'them', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'that', 'sentiwordnet', 'does', 'allow', 'for', 'lt', 'em', 'gt', 'ambiguous', 'lt', 'em', 'gt', 'meanings', 'too', 'consider', 'e', 'g', 'lt', 'a', 'href', 'quot', 'http', 'sentiwordnet', 'isti', 'cnr', 'it', 'search', 'php', 'q', 'funny', 'quot', 'gt', 'funny', 'lt', 'a', 'gt', 'which', 'is', 'not', 'just', 'positive', 'quot', 'this', 'tastes', 'funny', 'quot', 'is', 'probably', 'not', 'positive', 'same', 'will', 'hold', 'for', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'for', 'example', 'but', 'i', 'don', 't', 'think', 'this', 'is', 'harder', 'for', 'emoji', 'than', 'it', 'is', 'for', 'regular', 'words', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'if', 'you', 'have', 'experience', 'with', 'using', 'them', 'for', 'sentiment', 'analysis', 'i', 'd', 'be', 'interested', 'to', 'hear', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'a', 'product', 'that', 'allows', 'us', 'to', 'take', 'in', 'a', 'collection', 'of', 'datastreams', 'and', 'then', 'after', 'some', 'event', 'will', 'find', 'any', 'data', 'that', 'changes', 'or', 'correlates', 'with', 'that', 'event', 'for', 'example', 'having', 'a', 'headache', 'and', 'identifying', 'that', 'i', 'drank', 'too', 'much', 'beer', 'last', 'night', 'and', 'didn', 't', 'drink', 'enough', 'water', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'using', 'the', 'lt', 'code', 'gt', 'stepaic', 'lt', 'code', 'gt', 'function', 'in', 'r', 'to', 'do', 'a', 'bi', 'directional', 'forward', 'and', 'backward', 'stepwise', 'regression', 'i', 'do', 'not', 'understand', 'what', 'each', 'return', 'value', 'from', 'the', 'function', 'means', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'output', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'df', 'sum', 'of', 'sq', 'rss', 'aic', 'xa', 'amp', 'lt', 'none', 'amp', 'gt', 'xa', 'aaa', 'xa', 'bbb', 'xa', 'ccc', 'xa', 'ddd', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'are', 'the', 'values', 'listed', 'under', 'lt', 'code', 'gt', 'df', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'sum', 'of', 'sq', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'rss', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'aic', 'lt', 'code', 'gt', 'the', 'values', 'for', 'a', 'model', 'where', 'only', 'one', 'variable', 'would', 'be', 'considered', 'as', 'the', 'independent', 'variable', 'i', 'e', 'y', 'xa', 'aaa', 'y', 'bbb', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'while', 'finding', 'frequent', 'subgraphs', 'in', 'single', 'large', 'graph', 'subgraph', 'isomorphism', 'test', 'is', 'not', 'considered', 'because', 'its', 'not', 'anti', 'monotone', 'how', 'and', 'why', 'subgraph', 'isomorphism', 'is', 'not', 'anti', 'monotone', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'like', 'to', 'find', 'the', 'weight', 'vector', 'for', 'input', 'space', 'features', 'in', 'a', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'structured_support_vector_machine', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'structured', 'svm', 'lt', 'a', 'gt', 'the', 'idea', 'is', 'to', 'identify', 'the', 'most', 'important', 'set', 'of', 'input', 'space', 'features', 'based', 'on', 'the', 'magnitude', 'of', 'their', 'corresponding', 'weights', 'i', 'know', 'that', 'in', 'a', 'binary', 'svm', 'the', 'weight', 'vector', 'can', 'be', 'written', 'as', 'a', 'lt', 'a', 'href', 'quot', 'http', 'pyml', 'sourceforge', 'net', 'doc', 'howto', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'linear', 'combination', 'of', 'examples', 'lt', 'a', 'gt', 'and', 'the', 'magnitude', 'of', 'those', 'weights', 'represents', 'how', 'much', 'they', 'were', 'effective', 'for', 'the', 'prediction', 'problem', 'at', 'hand', 'but', 'how', 'do', 'you', 'compute', 'the', 'same', 'for', 'an', 'ssvm', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'big', 'sparse', 'matrix', 'of', 'users', 'and', 'items', 'they', 'like', 'in', 'the', 'order', 'of', 'm', 'users', 'and', 'k', 'items', 'with', 'a', 'very', 'low', 'level', 'of', 'sparsity', 'i', 'm', 'exploring', 'ways', 'in', 'which', 'i', 'could', 'perform', 'knn', 'search', 'on', 'it', 'given', 'the', 'size', 'of', 'my', 'dataset', 'and', 'some', 'initial', 'tests', 'i', 'performed', 'my', 'assumption', 'is', 'that', 'the', 'method', 'i', 'will', 'use', 'will', 'need', 'to', 'be', 'either', 'parallel', 'or', 'distributed', 'so', 'i', 'm', 'considering', 'two', 'classes', 'of', 'possible', 'solutions', 'one', 'that', 'is', 'either', 'available', 'or', 'implementable', 'in', 'a', 'reasonably', 'easy', 'way', 'on', 'a', 'single', 'multicore', 'machine', 'the', 'other', 'on', 'a', 'spark', 'cluster', 'i', 'e', 'as', 'a', 'mapreduce', 'program', 'here', 'are', 'three', 'broad', 'ideas', 'that', 'i', 'considered', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'assuming', 'a', 'cosine', 'similarity', 'metric', 'perform', 'the', 'full', 'multiplication', 'of', 'the', 'normalized', 'matrix', 'by', 'its', 'transpose', 'implemented', 'as', 'a', 'sum', 'of', 'outer', 'products', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'using', 'locality', 'sensitive', 'hashing', 'lsh', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'reducing', 'first', 'the', 'dimensionality', 'of', 'the', 'problem', 'with', 'a', 'pca', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'appreciate', 'any', 'thoughts', 'or', 'advices', 'about', 'possible', 'other', 'ways', 'in', 'which', 'i', 'could', 'tackle', 'this', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'let', 'me', 'show', 'you', 'an', 'example', 'of', 'a', 'hypothetical', 'online', 'clustering', 'application', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'zvoqn', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'time', 'n', 'points', 'are', 'allocated', 'to', 'the', 'blue', 'cluster', 'a', 'and', 'points', 'b', 'are', 'allocated', 'to', 'the', 'red', 'cluster', 'b', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'time', 'n', 'a', 'new', 'point', 'a', 'is', 'introduced', 'which', 'is', 'assigned', 'to', 'the', 'blue', 'cluster', 'a', 'but', 'also', 'causes', 'the', 'point', 'b', 'to', 'be', 'assigned', 'to', 'the', 'blue', 'cluster', 'a', 'as', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'the', 'end', 'points', 'a', 'b', 'belong', 'to', 'a', 'and', 'points', 'to', 'b', 'to', 'me', 'this', 'seems', 'reasonable', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'seems', 'simple', 'at', 'first', 'glance', 'is', 'actually', 'a', 'bit', 'tricky', 'to', 'maintain', 'identifiers', 'across', 'time', 'steps', 'let', 'me', 'try', 'to', 'make', 'this', 'point', 'clear', 'with', 'a', 'more', 'borderline', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'sqj', 'h', 'jpg', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'green', 'point', 'will', 'cause', 'two', 'blue', 'and', 'two', 'red', 'points', 'to', 'be', 'merged', 'into', 'one', 'cluster', 'which', 'i', 'arbitrarily', 'decided', 'to', 'color', 'blue', 'mind', 'this', 'is', 'already', 'my', 'human', 'heuristical', 'thinking', 'at', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'computer', 'to', 'make', 'this', 'decision', 'will', 'have', 'to', 'use', 'rules', 'for', 'example', 'when', 'points', 'are', 'merged', 'into', 'a', 'cluster', 'then', 'the', 'identity', 'of', 'the', 'cluster', 'is', 'determined', 'by', 'the', 'majority', 'in', 'this', 'case', 'we', 'would', 'face', 'a', 'draw', 'both', 'blue', 'and', 'red', 'might', 'be', 'valid', 'choices', 'for', 'the', 'new', 'here', 'blue', 'colored', 'cluster', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'imagine', 'a', 'fifth', 'red', 'point', 'close', 'to', 'the', 'green', 'one', 'then', 'the', 'majority', 'would', 'be', 'red', 'red', 'vs', 'blue', 'so', 'red', 'would', 'be', 'a', 'good', 'choice', 'for', 'the', 'new', 'cluster', 'but', 'this', 'would', 'contradict', 'the', 'even', 'clearer', 'choice', 'of', 'red', 'for', 'the', 'rightmost', 'cluster', 'as', 'those', 'have', 'been', 'red', 'and', 'probably', 'should', 'stay', 'that', 'way', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'find', 'it', 'fishy', 'to', 'think', 'about', 'this', 'at', 'the', 'end', 'of', 'the', 'day', 'i', 'guess', 'there', 'are', 'no', 'perfect', 'rules', 'for', 'this', 'rather', 'heuristics', 'optimizing', 'some', 'stability', 'criterea', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'finally', 'leads', 'to', 'my', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'does', 'this', 'quot', 'problem', 'quot', 'have', 'a', 'name', 'that', 'it', 'can', 'be', 'referred', 'to', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'are', 'there', 'quot', 'standard', 'quot', 'solutions', 'to', 'this', 'and', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'maybe', 'even', 'an', 'r', 'package', 'for', 'that', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'hr', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'www', 'joyofdata', 'de', 'blog', 'reasonable', 'inheritance', 'of', 'cluster', 'identities', 'in', 'repetitive', 'clustering', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'reasonable', 'inheritance', 'of', 'cluster', 'identities', 'in', 'repetitive', 'clustering', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'problem', 'of', 'clustering', 'huge', 'amount', 'of', 'sentences', 'into', 'groups', 'by', 'their', 'meanings', 'this', 'is', 'similar', 'to', 'a', 'problem', 'when', 'you', 'have', 'lots', 'of', 'sentences', 'and', 'want', 'to', 'group', 'them', 'by', 'their', 'meanings', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'algorithms', 'are', 'suggested', 'to', 'do', 'this', 'i', 'don', 't', 'know', 'number', 'of', 'clusters', 'in', 'advance', 'and', 'as', 'more', 'data', 'is', 'coming', 'clusters', 'can', 'change', 'as', 'well', 'what', 'features', 'are', 'normally', 'used', 'to', 'represent', 'each', 'sentence', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'trying', 'now', 'the', 'simplest', 'features', 'with', 'just', 'list', 'of', 'words', 'and', 'distance', 'between', 'sentences', 'defined', 'as', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'dhb', 'x', 'jpg', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'and', 'b', 'are', 'corresponding', 'sets', 'of', 'words', 'in', 'sentence', 'a', 'and', 'b', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'it', 'make', 'sense', 'at', 'all', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'apply', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'auto_examples', 'cluster', 'plot_mean_shift', 'html', 'example', 'cluster', 'plot', 'mean', 'shift', 'py', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'mean', 'shift', 'lt', 'a', 'gt', 'algorithm', 'from', 'scikit', 'library', 'to', 'this', 'distance', 'as', 'it', 'does', 'not', 'require', 'number', 'of', 'clusters', 'in', 'advance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'anyone', 'will', 'advise', 'better', 'methods', 'approaches', 'for', 'the', 'problem', 'it', 'will', 'be', 'very', 'much', 'appreciated', 'as', 'i', 'm', 'still', 'new', 'to', 'the', 'topic', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'no', 'knowledge', 'about', 'the', 'climate', 'or', 'soil', 'and', 'i', 'just', 'want', 'to', 'find', 'out', 'more', 'about', 'these', 'kind', 'of', 'dataset', 'i', 'heard', 'that', 'climate', 'corporation', 'asked', 'its', 'candidates', 'to', 'perform', 'statistical', 'analysis', 'on', 'various', 'climate', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'is', 'why', 'i', 'am', 'asking', 'this', 'question', 'please', 'do', 'not', 'get', 'me', 'wrong', 'i', 'am', 'not', 'trying', 'to', 'get', 'the', 'dataset', 'to', 'prepare', 'myself', 'for', 'an', 'interview', 'as', 'i', 'know', 'they', 'give', 'out', 'different', 'dataset', 'to', 'people', 'from', 'different', 'background', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'know', 'that', 'climate', 'corporation', 'only', 'hires', 'phd', 'which', 'i', 'am', 'not', 'i', 'only', 'want', 'to', 'play', 'around', 'with', 'their', 'dataset', 'such', 'that', 'i', 'can', 'learn', 'and', 'implement', 'lt', 'strong', 'gt', 'time', 'series', 'analysis', 'lt', 'strong', 'gt', 'that', 's', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'if', 'anyone', 'does', 'not', 'mind', 'sharing', 'their', 'dataset', 'please', 'post', 'the', 'link', 'them', 'below', 'thank', 'you', 'very', 'much', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'question', 'about', 'classifying', 'documents', 'using', 'supervised', 'learning', 'and', 'unsupervised', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'i', 'have', 'a', 'bunch', 'of', 'documents', 'talking', 'about', 'football', 'lt', 'br', 'gt', 'xa', 'as', 'we', 'know', 'football', 'has', 'different', 'meaning', 'in', 'uk', 'usa', 'and', 'australia', 'therefore', 'it', 'is', 'difficult', 'to', 'classify', 'these', 'documents', 'to', 'three', 'different', 'categorizations', 'which', 'are', 'soccer', 'american', 'football', 'and', 'australian', 'football', 'lt', 'br', 'gt', 'xa', 'my', 'approach', 'tries', 'to', 'use', 'cosine', 'similarity', 'terms', 'which', 'is', 'based', 'on', 'unsupervised', 'after', 'we', 'use', 'the', 'cluster', 'learning', 'we', 'are', 'able', 'to', 'create', 'a', 'number', 'of', 'clusters', 'based', 'on', 'cosine', 'similarity', 'which', 'each', 'cluster', 'will', 'contain', 'similar', 'documents', 'terms', 'after', 'we', 'create', 'the', 'clusters', 'we', 'can', 'use', 'a', 'semantic', 'feature', 'to', 'identify', 'these', 'clusters', 'depend', 'on', 'supervised', 'model', 'like', 'svm', 'to', 'make', 'accurate', 'categorizations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'goal', 'is', 'to', 'create', 'more', 'accurate', 'categorizations', 'because', 'if', 'i', 'want', 'to', 'test', 'a', 'new', 'document', 'i', 'want', 'know', 'if', 'this', 'document', 'can', 'be', 'related', 'to', 'these', 'categorizations', 'or', 'not', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'problem', 'i', 'am', 'tackling', 'is', 'categorizing', 'short', 'texts', 'into', 'multiple', 'classes', 'my', 'current', 'approach', 'is', 'to', 'use', 'tf', 'idf', 'weighted', 'term', 'frequencies', 'and', 'learn', 'a', 'simple', 'linear', 'classifier', 'logistic', 'regression', 'this', 'works', 'reasonably', 'well', 'around', 'macro', 'f', 'on', 'test', 'set', 'nearly', 'on', 'training', 'set', 'a', 'big', 'problem', 'are', 'unseen', 'words', 'n', 'grams', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'improve', 'the', 'classifier', 'by', 'adding', 'other', 'features', 'e', 'g', 'a', 'fixed', 'sized', 'vector', 'computed', 'using', 'distributional', 'similarities', 'as', 'computed', 'by', 'word', 'vec', 'or', 'other', 'categorical', 'features', 'of', 'the', 'examples', 'my', 'idea', 'was', 'to', 'just', 'add', 'the', 'features', 'to', 'the', 'sparse', 'input', 'features', 'from', 'the', 'bag', 'of', 'words', 'however', 'this', 'results', 'in', 'worse', 'performance', 'on', 'the', 'test', 'and', 'training', 'set', 'the', 'additional', 'features', 'by', 'themselves', 'give', 'about', 'f', 'on', 'the', 'test', 'set', 'so', 'they', 'aren', 't', 'garbage', 'scaling', 'the', 'features', 'didn', 't', 'help', 'as', 'well', 'my', 'current', 'thinking', 'is', 'that', 'these', 'kind', 'of', 'features', 'don', 't', 'mix', 'well', 'with', 'the', 'sparse', 'bag', 'of', 'words', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'the', 'question', 'is', 'assuming', 'the', 'additional', 'features', 'provide', 'additional', 'information', 'what', 'is', 'the', 'best', 'way', 'to', 'incorporate', 'them', 'could', 'training', 'separate', 'classifiers', 'and', 'combining', 'them', 'in', 'some', 'kind', 'of', 'ensemble', 'work', 'this', 'would', 'probably', 'have', 'the', 'drawback', 'that', 'no', 'interaction', 'between', 'the', 'features', 'of', 'the', 'different', 'classifiers', 'could', 'be', 'captured', 'are', 'there', 'other', 'more', 'complex', 'models', 'i', 'should', 'consider', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'run', 'svr', 'using', 'scikit', 'learn', 'python', 'on', 'a', 'training', 'dataset', 'having', 'rows', 'and', 'columns', 'features', 'and', 'test', 'dataset', 'having', 'rows', 'the', 'data', 'has', 'been', 'pre', 'processed', 'and', 'regularized', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'able', 'to', 'successfully', 'run', 'the', 'test', 'examples', 'but', 'on', 'executing', 'using', 'my', 'dataset', 'and', 'letting', 'it', 'run', 'for', 'over', 'an', 'hour', 'i', 'could', 'still', 'not', 'see', 'any', 'output', 'or', 'termination', 'of', 'program', 'i', 'have', 'tried', 'executing', 'using', 'a', 'different', 'ide', 'and', 'even', 'from', 'terminal', 'but', 'that', 'doesn', 't', 'seem', 'to', 'be', 'the', 'issue', 'xa', 'i', 'have', 'also', 'tried', 'changing', 'the', 'c', 'parameter', 'value', 'from', 'to', 'e', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'facing', 'similar', 'issues', 'with', 'all', 'svm', 'implementations', 'using', 'scikit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'am', 'i', 'not', 'waiting', 'enough', 'for', 'it', 'to', 'complete', 'xa', 'how', 'much', 'time', 'should', 'this', 'execution', 'take', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'from', 'my', 'experience', 'it', 'shouldn', 't', 'require', 'over', 'a', 'few', 'minutes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'my', 'system', 'configuration', 'xa', 'ubuntu', 'gb', 'ram', 'lots', 'of', 'free', 'memory', 'th', 'gen', 'i', 'processor', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'been', 'analyzing', 'a', 'data', 'set', 'of', 'k', 'records', 'and', 'variables', 'the', 'dependent', 'variable', 'is', 'binary', 'i', 've', 'fitted', 'a', 'logistic', 'regression', 'a', 'regression', 'tree', 'a', 'random', 'forest', 'and', 'a', 'gradient', 'boosted', 'tree', 'all', 'of', 'them', 'give', 'virtual', 'identical', 'goodness', 'of', 'fit', 'numbers', 'when', 'i', 'validate', 'them', 'on', 'another', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'is', 'this', 'so', 'i', 'm', 'guessing', 'that', 'it', 's', 'because', 'my', 'observations', 'to', 'variable', 'ratio', 'is', 'so', 'high', 'if', 'this', 'is', 'correct', 'at', 'what', 'observation', 'to', 'variable', 'ratio', 'will', 'different', 'models', 'start', 'to', 'give', 'different', 'results', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'where', 'can', 'i', 'find', 'free', 'spatio', 'temporal', 'dataset', 'for', 'download', 'so', 'that', 'i', 'can', 'play', 'with', 'it', 'in', 'r', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'caveat', 'i', 'am', 'a', 'complete', 'beginner', 'when', 'it', 'comes', 'to', 'machine', 'learning', 'but', 'eager', 'to', 'learn', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'a', 'large', 'dataset', 'and', 'i', 'm', 'trying', 'to', 'find', 'pattern', 'in', 'it', 'there', 'may', 'may', 'not', 'be', 'correlation', 'across', 'the', 'data', 'either', 'with', 'known', 'variables', 'or', 'variables', 'that', 'are', 'contained', 'in', 'the', 'data', 'but', 'which', 'i', 'haven', 't', 'yet', 'realised', 'are', 'actually', 'variables', 'relevant', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'guessing', 'this', 'would', 'be', 'a', 'familiar', 'problem', 'in', 'the', 'world', 'of', 'data', 'analysis', 'so', 'i', 'have', 'a', 'few', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'the', 'silver', 'bullet', 'would', 'be', 'to', 'throw', 'this', 'all', 'this', 'data', 'into', 'a', 'stats', 'data', 'analysis', 'program', 'and', 'for', 'it', 'to', 'crunch', 'the', 'data', 'looking', 'for', 'known', 'unknown', 'patterns', 'trying', 'to', 'find', 'relations', 'is', 'spss', 'suitable', 'or', 'are', 'there', 'other', 'applications', 'which', 'may', 'be', 'better', 'suited', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'should', 'i', 'learn', 'a', 'language', 'like', 'r', 'and', 'figure', 'out', 'how', 'to', 'manually', 'process', 'the', 'data', 'wouldn', 't', 'this', 'comprimise', 'finding', 'relations', 'as', 'i', 'would', 'have', 'to', 'manually', 'specify', 'what', 'and', 'how', 'to', 'analyse', 'the', 'data', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'how', 'would', 'a', 'professional', 'data', 'miner', 'approach', 'this', 'problem', 'and', 'what', 'steps', 'would', 's', 'he', 'take', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'recently', 'read', 'lt', 'a', 'href', 'quot', 'http', 'research', 'microsoft', 'com', 'en', 'us', 'um', 'people', 'sdumais', 'ecir', 'metzlerdumaismeek', 'final', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'similarity', 'measures', 'for', 'short', 'segments', 'of', 'text', 'lt', 'a', 'gt', 'metzler', 'et', 'al', 'it', 'describes', 'basic', 'methods', 'for', 'measuring', 'query', 'similarity', 'and', 'in', 'the', 'paper', 'the', 'data', 'consists', 'of', 'queries', 'and', 'their', 'top', 'results', 'results', 'are', 'lists', 'of', 'page', 'urls', 'page', 'titles', 'and', 'short', 'page', 'snippets', 'in', 'the', 'paper', 'the', 'authors', 'collect', 'results', 'per', 'query', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'using', 'the', 'public', 'google', 'apis', 'to', 'retrieve', 'results', 'i', 'was', 'only', 'able', 'to', 'collect', 'results', 'per', 'query', 'there', 's', 'a', 'substantial', 'difference', 'between', 'and', 'hence', 'how', 'much', 'data', 'is', 'commonly', 'used', 'in', 'practice', 'to', 'measure', 'query', 'similarity', 'e', 'g', 'how', 'many', 'results', 'per', 'query', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'references', 'are', 'a', 'plus', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'scrape', 'some', 'data', 'from', 'a', 'website', 'xa', 'i', 'have', 'used', 'import', 'io', 'but', 'still', 'not', 'much', 'satisfied', 'can', 'any', 'of', 'you', 'suggest', 'about', 'it', 'whats', 'the', 'best', 'tool', 'to', 'get', 'the', 'unstructured', 'data', 'from', 'web', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'just', 'started', 'reading', 'about', 'ab', 'testing', 'as', 'it', 'pertains', 'to', 'optimizing', 'website', 'design', 'i', 'find', 'it', 'interesting', 'that', 'most', 'of', 'the', 'methods', 'assume', 'that', 'changes', 'to', 'the', 'layout', 'and', 'appearance', 'are', 'independent', 'of', 'each', 'other', 'i', 'understand', 'that', 'the', 'most', 'common', 'method', 'of', 'optimization', 'is', 'the', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'multi', 'armed_bandit', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'multi', 'armed', 'bandit', 'lt', 'a', 'gt', 'procedure', 'while', 'i', 'grasp', 'the', 'concept', 'of', 'it', 'it', 'seems', 'to', 'ignore', 'the', 'fact', 'that', 'changes', 'changes', 'to', 'the', 'website', 'in', 'this', 'case', 'are', 'not', 'independent', 'to', 'each', 'other', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'company', 'is', 'testing', 'the', 'placement', 'and', 'color', 'of', 'the', 'logo', 'on', 'the', 'website', 'they', 'find', 'the', 'optimal', 'color', 'first', 'then', 'the', 'optimal', 'placement', 'not', 'that', 'i', 'm', 'some', 'expert', 'on', 'human', 'psychology', 'but', 'shouldn', 't', 'these', 'be', 'related', 'can', 'the', 'multi', 'armed', 'bandit', 'method', 'be', 'efficiently', 'used', 'in', 'this', 'case', 'or', 'more', 'complicated', 'cases', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'first', 'instinct', 'is', 'to', 'say', 'no', 'on', 'that', 'note', 'why', 'haven', 't', 'people', 'used', 'heuristic', 'algorithms', 'to', 'optimize', 'over', 'complicated', 'ab', 'testing', 'sample', 'spaces', 'for', 'an', 'example', 'i', 'thought', 'someone', 'might', 'have', 'used', 'a', 'genetic', 'algorithm', 'to', 'optimize', 'a', 'website', 'layout', 'but', 'i', 'can', 'find', 'no', 'examples', 'of', 'something', 'like', 'this', 'out', 'there', 'this', 'leads', 'me', 'to', 'believe', 'that', 'i', 'm', 'missing', 'something', 'important', 'in', 'my', 'understanding', 'of', 'ab', 'testing', 'as', 'it', 'applies', 'to', 'website', 'optimization', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'isn', 't', 'heuristic', 'optimization', 'used', 'on', 'more', 'complicated', 'websites', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'installed', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'factual', 'drake', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'drake', 'lt', 'a', 'gt', 'on', 'windows', 'bit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'using', 'jdk', '_', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'both', 'using', 'the', 'pre', 'compiled', 'jar', 'file', 'and', 'xa', 'compiling', 'from', 'the', 'clojure', 'source', 'using', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'technomancy', 'leiningen', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'leiningen', 'lt', 'a', 'gt', 'xa', 'the', 'resulting', 'drake', 'version', 'is', 'the', 'current', 'development', 'version', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'running', 'drake', 'i', 'get', 'the', 'current', 'version', 'number', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'next', 'i', 'tried', 'to', 'go', 'through', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'factual', 'drake', 'wiki', 'tutorial', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'the', 'tutorial', 'lt', 'a', 'gt', 'the', 'command', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'java', 'jar', 'drake', 'jar', 'w', 'workflow', 'd', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'results', 'in', 'the', 'following', 'exception', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'java', 'lang', 'exception', 'no', 'input', 'data', 'found', 'in', 'locations', 'd', 'tools', 'drake', 'in', 'c', 'xa', 'sv', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'even', 'though', 'the', 'file', 'exists', 'and', 'has', 'text', 'inside', 'it', 'xa', 'the', 'same', 'scenario', 'works', 'in', 'a', 'similar', 'installation', 'on', 'ubuntu', 'xa', 'am', 'i', 'doing', 'something', 'wrong', 'or', 'is', 'this', 'a', 'windows', 'specific', 'bug', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'studying', 'reinforcement', 'learning', 'in', 'order', 'to', 'implement', 'a', 'kind', 'of', 'time', 'series', 'pattern', 'analyzer', 'such', 'as', 'market', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'most', 'examples', 'i', 'have', 'seen', 'are', 'based', 'on', 'the', 'maze', 'environment', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'in', 'real', 'market', 'environment', 'the', 'signal', 'changes', 'endlessly', 'as', 'time', 'passes', 'and', 'i', 'can', 'not', 'guess', 'how', 'can', 'i', 'model', 'environment', 'and', 'states', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'question', 'is', 'about', 'buy', 'sell', 'modeling', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'let', 's', 'assume', 'that', 'the', 'agent', 'randomly', 'buy', 'at', 'time', 't', 'and', 'sell', 'at', 'time', 't', 'alpha', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 's', 'simple', 'to', 'calculate', 'reward', 'xa', 'the', 'problem', 'is', 'how', 'can', 'i', 'model', 'q', 'matrix', 'and', 'how', 'can', 'i', 'model', 'signals', 'between', 'buy', 'and', 'sell', 'actions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'you', 'share', 'some', 'source', 'code', 'or', 'guidance', 'for', 'similar', 'situation', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'hoping', 'to', 'model', 'the', 'characteristics', 'of', 'the', 'users', 'of', 'a', 'specific', 'page', 'on', 'facebook', 'which', 'has', 'roughly', 'million', 'likes', 'i', 'have', 'been', 'looking', 'at', 'the', 'facebook', 'sdk', 'api', 'but', 'i', 'can', 't', 'really', 'see', 'if', 'what', 'i', 'would', 'like', 'to', 'do', 'is', 'possible', 'it', 'seems', 'that', 'the', 'users', 'share', 'quite', 'different', 'amounts', 'of', 'data', 'so', 'i', 'probably', 'discard', 'a', 'lot', 'of', 'users', 'and', 'only', 'use', 'the', 'ones', 'with', 'a', 'quite', 'open', 'public', 'profile', 'i', 'would', 'like', 'to', 'have', 'the', 'following', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'see', 'the', 'individuals', 'that', 'have', 'liked', 'the', 'page', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'see', 'the', 'list', 'of', 'friends', 'for', 'each', 'person', 'that', 'have', 'liked', 'the', 'page', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'see', 'gender', 'for', 'each', 'person', 'optional', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'see', 'other', 'pages', 'that', 'each', 'person', 'has', 'liked', 'optional', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'could', 'anyone', 'tell', 'me', 'if', 'it', 'is', 'possible', 'to', 'get', 'this', 'data', 'as', 'mentioned', 'earlier', 'it', 'is', 'okay', 'if', 'i', 'discard', 'data', 'for', 'users', 'that', 'don', 't', 'like', 'to', 'share', 'this', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'thousands', 'of', 'lists', 'of', 'strings', 'and', 'each', 'list', 'has', 'about', 'strings', 'most', 'strings', 'in', 'a', 'given', 'list', 'are', 'very', 'similar', 'though', 'some', 'strings', 'are', 'rarely', 'completely', 'unrelated', 'to', 'the', 'others', 'and', 'some', 'strings', 'contain', 'irrelevant', 'words', 'they', 'can', 'be', 'considered', 'to', 'be', 'noisy', 'variations', 'of', 'a', 'canonical', 'string', 'i', 'am', 'looking', 'for', 'an', 'algorithm', 'or', 'a', 'library', 'that', 'will', 'convert', 'each', 'list', 'into', 'this', 'canonical', 'string', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'one', 'such', 'list', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'star', 'wars', 'episode', 'iv', 'a', 'new', 'hope', 'starwars', 'com', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'star', 'wars', 'episode', 'iv', 'a', 'new', 'hope', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'star', 'wars', 'episode', 'iv', 'a', 'new', 'hope', 'rotten', 'tomatoes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'watch', 'star', 'wars', 'episode', 'iv', 'a', 'new', 'hope', 'online', 'free', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'star', 'wars', 'greatest', 'films', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'rec', 'poster', 'promises', 'death', 'by', 'outboard', 'motor', 'scifinow', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'this', 'list', 'any', 'string', 'matching', 'the', 'regular', 'expression', 'lt', 'code', 'gt', 'star', 'wars', 'episode', 'iv', 'a', 'new', 'hope', 'lt', 'code', 'gt', 'would', 'be', 'acceptable', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'looked', 'at', 'andrew', 'ng', 's', 'course', 'on', 'machine', 'learning', 'on', 'coursera', 'but', 'i', 'was', 'not', 'able', 'to', 'find', 'a', 'similar', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'fit', 'a', 'glm', 'poisson', 'to', 'a', 'data', 'set', 'where', 'one', 'of', 'the', 'variables', 'is', 'categorical', 'for', 'the', 'year', 'a', 'customer', 'bought', 'a', 'product', 'from', 'my', 'company', 'ranging', 'from', 'to', 'there', 's', 'a', 'linear', 'trend', 'of', 'the', 'coefficients', 'for', 'the', 'values', 'of', 'the', 'variable', 'as', 'the', 'year', 'of', 'sale', 'increases', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'any', 'problem', 'with', 'trying', 'to', 'improve', 'predictions', 'for', 'and', 'maybe', 'by', 'extrapolating', 'to', 'get', 'the', 'coefficients', 'for', 'those', 'years', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'been', 'developing', 'a', 'chess', 'program', 'which', 'makes', 'use', 'of', 'alpha', 'beta', 'pruning', 'algorithm', 'and', 'an', 'evaluation', 'function', 'that', 'evaluates', 'positions', 'using', 'the', 'following', 'features', 'namely', 'material', 'kingsafety', 'mobility', 'pawn', 'structure', 'and', 'trapped', 'pieces', 'etc', 'my', 'evaluation', 'function', 'is', 'derived', 'from', 'the', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'f', 'p', 'w_', 'cdot', 'text', 'material', 'w_', 'cdot', 'text', 'kingsafety', 'w_', 'cdot', 'text', 'mobility', 'w_', 'cdot', 'text', 'pawn', 'structure', 'w_', 'cdot', 'text', 'trapped', 'pieces', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'w', 'is', 'the', 'weight', 'assigned', 'to', 'each', 'feature', 'at', 'this', 'point', 'i', 'want', 'to', 'tune', 'the', 'weights', 'of', 'my', 'evaluation', 'function', 'using', 'temporal', 'difference', 'where', 'the', 'agent', 'plays', 'against', 'itself', 'and', 'in', 'the', 'process', 'gather', 'training', 'data', 'from', 'its', 'environment', 'which', 'is', 'a', 'form', 'of', 'reinforcement', 'learning', 'i', 'have', 'read', 'some', 'books', 'and', 'articles', 'in', 'order', 'to', 'have', 'an', 'insight', 'on', 'how', 'to', 'implement', 'this', 'in', 'java', 'but', 'they', 'seem', 'to', 'be', 'theoretical', 'rather', 'than', 'practical', 'i', 'need', 'a', 'detailed', 'explanation', 'and', 'pseudo', 'codes', 'on', 'how', 'to', 'automatically', 'tune', 'the', 'weights', 'of', 'my', 'evaluation', 'function', 'based', 'on', 'previous', 'games', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'been', 'reading', 'around', 'about', 'random', 'forests', 'but', 'i', 'cannot', 'really', 'find', 'a', 'definitive', 'answer', 'about', 'the', 'problem', 'of', 'overfitting', 'according', 'to', 'the', 'original', 'paper', 'of', 'breiman', 'they', 'should', 'not', 'overfit', 'when', 'increasing', 'the', 'number', 'of', 'trees', 'in', 'the', 'forest', 'but', 'it', 'seems', 'that', 'there', 'is', 'not', 'consensus', 'about', 'this', 'this', 'is', 'creating', 'me', 'quite', 'some', 'confusion', 'about', 'the', 'issue', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'maybe', 'someone', 'more', 'expert', 'than', 'me', 'can', 'give', 'me', 'a', 'more', 'concrete', 'answer', 'or', 'point', 'me', 'in', 'the', 'right', 'direction', 'to', 'better', 'understand', 'the', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'which', 'one', 'will', 'be', 'the', 'dominating', 'programming', 'language', 'for', 'next', 'years', 'for', 'analytics', 'machine', 'learning', 'r', 'verses', 'python', 'verses', 'sas', 'advantage', 'and', 'disadvantage', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'exploring', 'different', 'types', 'of', 'parse', 'tree', 'structures', 'the', 'two', 'widely', 'known', 'parse', 'tree', 'structures', 'are', 'xa', 'a', 'constituency', 'based', 'parse', 'tree', 'and', 'xa', 'b', 'dependency', 'based', 'parse', 'tree', 'structures', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'able', 'to', 'use', 'generate', 'both', 'types', 'of', 'parse', 'tree', 'structures', 'using', 'stanford', 'nlp', 'package', 'however', 'i', 'am', 'not', 'sure', 'how', 'to', 'use', 'these', 'tree', 'structures', 'for', 'my', 'classification', 'task', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'e', 'g', 'if', 'i', 'want', 'to', 'do', 'sentiment', 'analysis', 'and', 'want', 'to', 'categorize', 'text', 'into', 'positive', 'and', 'negative', 'classes', 'what', 'features', 'can', 'i', 'derive', 'from', 'parse', 'tree', 'structures', 'for', 'my', 'classification', 'task', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'exploring', 'how', 'to', 'model', 'a', 'data', 'set', 'using', 'normal', 'distributions', 'with', 'both', 'mean', 'and', 'variance', 'defined', 'as', 'linear', 'functions', 'of', 'independent', 'variables', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'something', 'like', 'n', 'f', 'x', 'g', 'x', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'generate', 'a', 'random', 'sample', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'def', 'draw', 'x', 'xa', 'return', 'norm', 'x', 'x', 'rvs', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'want', 'to', 'retrieve', 'and', 'as', 'the', 'parameters', 'for', 'my', 'distribution', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'generate', 'my', 'sample', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'smp', 'np', 'zeros', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'for', 'i', 'in', 'range', 'len', 'smp', 'xa', 'smp', 'i', 'i', 'xa', 'smp', 'i', 'draw', 'i', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'likelihood', 'function', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'def', 'lh', 'p', 'xa', 'p_loc_b', 'p', 'xa', 'p_loc_b', 'p', 'xa', 'p_scl_b', 'p', 'xa', 'p_scl_b', 'p', 'xa', 'xa', 'l', 'xa', 'for', 'i', 'in', 'range', 'len', 'smp', 'xa', 'x', 'smp', 'i', 'xa', 'y', 'smp', 'i', 'xa', 'l', 'l', 'norm', 'p_loc_b', 'p_loc_b', 'x', 'p_scl_b', 'p_scl_b', 'x', 'pdf', 'y', 'xa', 'xa', 'return', 'l', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'the', 'parameters', 'for', 'the', 'linear', 'functions', 'used', 'in', 'the', 'model', 'are', 'given', 'in', 'the', 'p', 'variable', 'vector', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'using', 'scipy', 'optimize', 'i', 'can', 'solve', 'for', 'the', 'mle', 'parameters', 'using', 'an', 'extremely', 'low', 'xtol', 'and', 'already', 'giving', 'the', 'solution', 'as', 'the', 'starting', 'point', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'fmin', 'lh', 'x', 'xtol', 'e', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'does', 'not', 'work', 'to', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'warning', 'maximum', 'number', 'of', 'function', 'evaluations', 'has', 'been', 'exceeded', 'xa', 'array', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'raising', 'the', 'xtol', 'to', 'higher', 'values', 'does', 'no', 'good', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'try', 'using', 'a', 'starting', 'solution', 'far', 'from', 'the', 'real', 'solution', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'amp', 'gt', 'amp', 'gt', 'amp', 'gt', 'fmin', 'lh', 'x', 'xtol', 'e', 'xa', 'optimization', 'terminated', 'successfully', 'xa', 'current', 'function', 'value', 'xa', 'iterations', 'xa', 'function', 'evaluations', 'xa', 'array', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'makes', 'me', 'think', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'pdf', 'are', 'largely', 'clustered', 'around', 'the', 'mean', 'and', 'have', 'very', 'low', 'gradients', 'only', 'a', 'few', 'standard', 'deviations', 'away', 'from', 'the', 'mean', 'which', 'must', 'be', 'not', 'too', 'good', 'for', 'numerical', 'methods', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'how', 'does', 'one', 'go', 'about', 'doing', 'these', 'kind', 'of', 'numerical', 'estimation', 'in', 'functions', 'where', 'gradient', 'is', 'very', 'near', 'to', 'zero', 'away', 'from', 'the', 'solution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'created', 'external', 'table', 'in', 'hive', 'in', 'the', 'hdfs', 'path', 'hdfs', 'localhost', 'localdomain', 'user', 'hive', 'training', 'if', 'i', 'apply', 'describe', 'command', 'i', 'can', 'find', 'the', 'table', 'path', 'as', 'shown', 'below', 'but', 'when', 'i', 'browse', 'through', 'the', 'namenode', 'web', 'page', 'the', 'table', 'name', 'does', 'not', 'showing', 'up', 'in', 'the', 'path', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'hive', 'amp', 'gt', 'describe', 'extended', 'testtable', 'xa', 'ok', 'xa', 'firstname', 'string', 'xa', 'lastname', 'string', 'xa', 'address', 'string', 'xa', 'city', 'string', 'xa', 'state', 'string', 'xa', 'country', 'string', 'xa', 'xa', 'detailed', 'table', 'information', 'table', 'tablename', 'testtable', 'dbname', 'default', 'owner', 'cloudera', 'createtime', 'lastaccesstime', 'retention', 'sd', 'storagedescriptor', 'cols', 'fieldschema', 'name', 'firstname', 'type', 'string', 'comment', 'null', 'fieldschema', 'name', 'lastname', 'type', 'string', 'comment', 'null', 'fieldschema', 'name', 'address', 'type', 'string', 'comment', 'null', 'fieldschema', 'name', 'city', 'type', 'string', 'comment', 'null', 'fieldschema', 'name', 'state', 'type', 'string', 'comment', 'null', 'fieldschema', 'name', 'country', 'type', 'string', 'comment', 'null', 'location', 'hdfs', 'localhost', 'localdomain', 'user', 'hive', 'training', 'inputformat', 'org', 'apache', 'hadoop', 'mapred', 'textinputformat', 'outputformat', 'org', 'apache', 'hadoop', 'hive', 'ql', 'io', 'hiveignorekeytextoutputformat', 'compressed', 'false', 'numbuckets', 'serdeinfo', 'serdeinfo', 'name', 'null', 'serializationlib', 'org', 'apache', 'hadoop', 'hive', 'serde', 'lazy', 'lazysimpleserde', 'parameters', 'serialization', 'format', 'field', 'delim', 'line', 'delim', 'xa', 'bucketcols', 'sortcols', 'parameters', 'skewedinfo', 'skewedinfo', 'skewedcolnames', 'skewedcolvalues', 'skewedcolvaluelocationmaps', 'storedassubdirectories', 'false', 'partitionkeys', 'parameters', 'external', 'true', 'transient_lastddltime', 'vieworiginaltext', 'null', 'viewexpandedtext', 'null', 'tabletype', 'external_table', 'xa', 'time', 'taken', 'seconds', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'seeking', 'a', 'basic', 'list', 'of', 'key', 'data', 'analysis', 'methods', 'used', 'for', 'studying', 'social', 'media', 'platforms', 'online', 'are', 'there', 'such', 'key', 'methods', 'or', 'does', 'this', 'process', 'generally', 'vary', 'according', 'to', 'topic', 'and', 'is', 'there', 'a', 'standard', 'order', 'in', 'which', 'these', 'methods', 'are', 'applied', 'the', 'particular', 'context', 'i', 'm', 'interested', 'in', 'is', 'how', 'the', 'news', 'is', 'impacting', 'on', 'social', 'media', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'what', 'i', 'described', 'in', 'the', 'title', 'we', 'are', 'especially', 'interested', 'in', 'those', 'for', 'dealing', 'with', 'big', 'data', 'ts', 'efficiency', 'and', 'stability', 'and', 'used', 'in', 'industry', 'not', 'in', 'experiment', 'or', 'university', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'have', 'anyone', 'used', 'shark', 'as', 'repository', 'from', 'resulting', 'datasets', 'from', 'apache', 'spark', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'starting', 'some', 'tests', 'with', 'spark', 'and', 'read', 'about', 'this', 'database', 'tecnology', 'have', 'anyone', 'been', 'using', 'it', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'strong', 'gt', 'general', 'description', 'of', 'the', 'problem', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'a', 'graph', 'where', 'some', 'vertices', 'are', 'labeled', 'with', 'a', 'type', 'with', 'or', 'possible', 'values', 'for', 'the', 'other', 'vertices', 'the', 'type', 'is', 'unknown', 'xa', 'my', 'goal', 'is', 'to', 'use', 'the', 'graph', 'to', 'predict', 'the', 'type', 'for', 'vertices', 'that', 'are', 'unlabeled', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'possible', 'framework', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'suspect', 'this', 'fits', 'into', 'the', 'general', 'framework', 'of', 'label', 'propagation', 'problems', 'based', 'on', 'my', 'reading', 'of', 'the', 'literature', 'e', 'g', 'see', 'lt', 'a', 'href', 'quot', 'http', 'lvk', 'cs', 'msu', 'su', 'bruzz', 'articles', 'classification', 'zhu', 'learning', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'this', 'paper', 'lt', 'a', 'gt', 'and', 'lt', 'a', 'href', 'quot', 'http', 'www', 'csc', 'ncsu', 'edu', 'faculty', 'samatova', 'practical', 'graph', 'mining', 'with', 'r', 'slides', 'pdf', 'frequent_subgraph_mining', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'this', 'paper', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'method', 'that', 'is', 'mentioned', 'often', 'is', 'lt', 'code', 'gt', 'frequent', 'subgraph', 'mining', 'lt', 'code', 'gt', 'which', 'includes', 'algorithms', 'like', 'lt', 'code', 'gt', 'subdue', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'sleuth', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'gspan', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'found', 'in', 'r', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'only', 'label', 'propagation', 'implementation', 'i', 'managed', 'to', 'find', 'in', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'is', 'lt', 'code', 'gt', 'label', 'propagation', 'community', 'lt', 'code', 'gt', 'from', 'the', 'lt', 'code', 'gt', 'igraph', 'lt', 'code', 'gt', 'library', 'xa', 'however', 'as', 'the', 'name', 'suggests', 'it', 'is', 'mostly', 'used', 'to', 'find', 'communities', 'not', 'for', 'classifying', 'unlabeled', 'vertices', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'also', 'seems', 'to', 'be', 'several', 'references', 'to', 'a', 'lt', 'code', 'gt', 'subgraphmining', 'lt', 'code', 'gt', 'library', 'here', 'for', 'example', 'but', 'it', 'looks', 'like', 'it', 'is', 'missing', 'from', 'cran', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'question', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'do', 'you', 'know', 'of', 'a', 'library', 'or', 'framework', 'for', 'the', 'task', 'described', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'lt', 'a', 'href', 'quot', 'http', 'stat', 'ethz', 'ch', 'r', 'manual', 'r', 'devel', 'library', 'base', 'html', 'summary', 'html', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'summarize', 'lt', 'a', 'gt', 'as', 'in', 'r', 'the', 'contents', 'of', 'a', 'csv', 'possibly', 'after', 'lt', 'a', 'href', 'quot', 'http', 'www', 'endmemo', 'com', 'program', 'r', 'readcsv', 'php', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'loading', 'lt', 'a', 'gt', 'it', 'or', 'storing', 'it', 'somewhere', 'that', 's', 'not', 'a', 'problem', 'the', 'summary', 'should', 'contain', 'the', 'quartiles', 'mean', 'median', 'min', 'and', 'max', 'of', 'the', 'data', 'in', 'a', 'csv', 'file', 'for', 'each', 'numeric', 'integer', 'or', 'real', 'numbers', 'dimension', 'the', 'standard', 'deviation', 'would', 'be', 'cool', 'as', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'also', 'like', 'to', 'generate', 'some', 'plots', 'to', 'visualize', 'the', 'data', 'for', 'example', 'plots', 'for', 'the', 'pairs', 'of', 'variables', 'that', 'are', 'more', 'correlated', 'lt', 'a', 'href', 'quot', 'http', 'www', 'r', 'tutor', 'com', 'elementary', 'statistics', 'numerical', 'measures', 'correlation', 'coefficient', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'correlation', 'coefficient', 'lt', 'a', 'gt', 'and', 'plots', 'for', 'the', 'pairs', 'of', 'variables', 'that', 'are', 'least', 'correlated', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'r', 'requires', 'only', 'a', 'few', 'lines', 'to', 'implement', 'this', 'are', 'there', 'any', 'libraries', 'or', 'tools', 'that', 'would', 'allow', 'a', 'similarly', 'simple', 'and', 'efficient', 'if', 'possible', 'implementation', 'in', 'java', 'or', 'scala', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'pd', 'this', 'is', 'a', 'specific', 'use', 'case', 'for', 'a', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'any', 'clear', 'winner', 'for', 'data', 'science', 'in', 'scala', 'quot', 'gt', 'previous', 'too', 'broad', 'question', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'wondering', 'if', 'there', 'is', 'any', 'research', 'or', 'study', 'made', 'to', 'calculate', 'the', 'volume', 'of', 'space', 'is', 'used', 'by', 'all', 'scientific', 'articles', 'it', 'could', 'be', 'in', 'pdf', 'txt', 'compressed', 'or', 'any', 'other', 'format', 'is', 'there', 'even', 'a', 'way', 'to', 'measure', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'some', 'one', 'point', 'me', 'towards', 'realizing', 'this', 'study', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'regards', 'and', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'question', 'regarding', 'the', 'use', 'of', 'neural', 'network', 'i', 'am', 'currently', 'working', 'with', 'r', 'lt', 'a', 'href', 'quot', 'http', 'cran', 'r', 'project', 'org', 'web', 'packages', 'neuralnet', 'index', 'html', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'neuralnet', 'package', 'lt', 'a', 'gt', 'and', 'i', 'am', 'facing', 'the', 'following', 'issue', 'xa', 'my', 'testing', 'and', 'validation', 'set', 'are', 'always', 'late', 'with', 'respect', 'to', 'the', 'historical', 'data', 'is', 'there', 'a', 'way', 'of', 'correcting', 'the', 'result', 'xa', 'maybe', 'something', 'is', 'wrong', 'in', 'my', 'analysis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'use', 'the', 'daily', 'log', 'return', 'xa', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'a', 'r', 'gif', 'quot', 'alt', 'quot', 'r', 't', 'ln', 's', 't', 's', 't', 'quot', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'normalise', 'my', 'data', 'with', 'the', 'sigmoid', 'function', 'sigma', 'and', 'mu', 'computed', 'on', 'my', 'whole', 'set', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'train', 'my', 'neural', 'networks', 'with', 'dates', 'and', 'the', 'output', 'is', 'the', 'normalised', 'value', 'that', 'follows', 'these', 'dates', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'to', 'add', 'the', 'trend', 'but', 'there', 'is', 'no', 'improvement', 'i', 'observed', 'days', 'late', 'my', 'process', 'seems', 'ok', 'what', 'do', 'you', 'think', 'about', 'it', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'the', 'best', 'solution', 'to', 'manage', 'and', 'host', 'datasets', 'for', 'journalistic', 'pursuits', 'i', 'am', 'assessing', 'lt', 'a', 'href', 'quot', 'https', 'www', 'documentcloud', 'org', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'https', 'www', 'documentcloud', 'org', 'lt', 'a', 'gt', 'and', 'lt', 'a', 'href', 'quot', 'http', 'datahub', 'io', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'datahub', 'io', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anyone', 'explain', 'the', 'differences', 'between', 'them', 'or', 'recommend', 'a', 'superior', 'solution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'evaluate', 'and', 'compare', 'several', 'different', 'machine', 'learning', 'models', 'built', 'with', 'different', 'parameters', 'i', 'e', 'downsampling', 'outlier', 'removal', 'and', 'different', 'classifiers', 'i', 'e', 'bayes', 'net', 'svm', 'decision', 'tree', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'performing', 'a', 'type', 'of', 'cross', 'validation', 'where', 'i', 'randomly', 'select', 'of', 'the', 'data', 'for', 'use', 'in', 'the', 'training', 'set', 'and', 'of', 'the', 'data', 'for', 'use', 'in', 'the', 'testing', 'set', 'i', 'perform', 'this', 'for', 'several', 'iterations', 'say', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'from', 'each', 'iteration', 'i', 'am', 'able', 'to', 'generate', 'a', 'confusion', 'matrix', 'and', 'compute', 'a', 'kappa', 'my', 'question', 'is', 'what', 'are', 'some', 'ways', 'to', 'aggregate', 'these', 'across', 'the', 'iterations', 'i', 'am', 'also', 'interested', 'in', 'aggregating', 'accuracy', 'and', 'expected', 'accuracy', 'among', 'other', 'things', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'the', 'kappa', 'accuracy', 'and', 'expected', 'accuracy', 'i', 'have', 'just', 'been', 'taking', 'the', 'average', 'up', 'to', 'this', 'point', 'one', 'of', 'the', 'problems', 'is', 'that', 'when', 'i', 'recompute', 'kappa', 'with', 'the', 'aggregated', 'average', 'and', 'expected', 'average', 'it', 'is', 'not', 'the', 'same', 'with', 'the', 'aggregated', 'kappa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'the', 'confusion', 'matrix', 'i', 'have', 'been', 'first', 'normalizing', 'the', 'confusion', 'matrix', 'from', 'each', 'iteration', 'and', 'then', 'averaging', 'them', 'in', 'an', 'attempt', 'to', 'avoid', 'an', 'issue', 'of', 'confusion', 'matrices', 'with', 'different', 'numbers', 'of', 'total', 'cases', 'which', 'is', 'possible', 'with', 'my', 'cross', 'validation', 'scheme', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'i', 'recompute', 'the', 'kappa', 'from', 'this', 'aggregated', 'confusion', 'matrix', 'it', 'is', 'also', 'different', 'from', 'the', 'previous', 'two', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'one', 'is', 'most', 'correct', 'is', 'there', 'another', 'way', 'of', 'computing', 'an', 'average', 'kappa', 'that', 'is', 'more', 'correct', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'and', 'if', 'more', 'concrete', 'examples', 'are', 'needed', 'in', 'order', 'to', 'illustrate', 'my', 'question', 'please', 'let', 'me', 'know', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'interested', 'in', 'knowing', 'the', 'differences', 'in', 'lt', 'strong', 'gt', 'functionality', 'lt', 'strong', 'gt', 'between', 'sap', 'hana', 'and', 'exasol', 'since', 'this', 'is', 'a', 'bit', 'of', 'an', 'open', 'ended', 'question', 'let', 'me', 'be', 'clear', 'i', 'am', 'not', 'interested', 'in', 'people', 'debating', 'which', 'is', 'quot', 'better', 'quot', 'or', 'faster', 'i', 'am', 'only', 'interested', 'in', 'what', 'each', 'was', 'designed', 'to', 'do', 'so', 'please', 'keep', 'your', 'opinions', 'out', 'of', 'it', 'i', 'suspect', 'it', 'is', 'a', 'bit', 'like', 'comparing', 'hana', 'to', 'oracle', 'exalytics', 'where', 'there', 'is', 'some', 'overlap', 'but', 'the', 'functionality', 'goals', 'are', 'different', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'how', 'is', 'the', 'concept', 'of', 'data', 'different', 'for', 'different', 'disciplines', 'obviously', 'for', 'physicists', 'and', 'sociologists', 'quot', 'data', 'quot', 'is', 'something', 'different', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'packages', 'either', 'in', 'python', 'r', 'or', 'a', 'standalone', 'package', 'to', 'perform', 'online', 'learning', 'to', 'predict', 'stock', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'found', 'and', 'read', 'about', 'vowpal', 'wabbit', 'lt', 'a', 'href', 'quot', 'https', 'github', 'com', 'johnlangford', 'vowpal_wabbit', 'wiki', 'quot', 'gt', 'https', 'github', 'com', 'johnlangford', 'vowpal_wabbit', 'wiki', 'lt', 'a', 'gt', 'xa', 'which', 'seems', 'to', 'be', 'quite', 'promising', 'but', 'i', 'am', 'wondering', 'if', 'there', 'are', 'any', 'other', 'packages', 'out', 'there', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'svms', 'the', 'polynomial', 'kernel', 'is', 'defined', 'as', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'scale', 'crossprod', 'x', 'y', 'offset', 'degree', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'do', 'the', 'scale', 'and', 'offset', 'parameters', 'affect', 'the', 'model', 'and', 'what', 'range', 'should', 'they', 'be', 'in', 'intuitively', 'please', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'the', 'scale', 'and', 'offset', 'for', 'numeric', 'stability', 'only', 'that', 's', 'what', 'it', 'looks', 'like', 'to', 'me', 'or', 'do', 'they', 'influence', 'prediction', 'accuracy', 'as', 'well', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'good', 'values', 'for', 'scale', 'and', 'offset', 'be', 'calculated', 'estimated', 'when', 'the', 'data', 'is', 'known', 'or', 'is', 'a', 'grid', 'search', 'required', 'the', 'caret', 'package', 'always', 'sets', 'the', 'offset', 'to', 'but', 'it', 'does', 'a', 'grid', 'search', 'for', 'scale', 'why', 'is', 'an', 'offset', 'of', 'a', 'good', 'value', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'ps', 'wikipedia', 'didn', 't', 'really', 'help', 'my', 'understanding', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'for', 'degree', 'd', 'polynomials', 'the', 'polynomial', 'kernel', 'is', 'defined', 'as', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'stcr', 'png', 'quot', 'alt', 'quot', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'x', 'and', 'y', 'are', 'vectors', 'in', 'the', 'input', 'space', 'i', 'e', 'vectors', 'of', 'features', 'xa', 'computed', 'from', 'training', 'or', 'test', 'samples', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'dwi', 'png', 'quot', 'alt', 'quot', 'quot', 'gt', 'lt', 'strong', 'gt', 'is', 'a', 'constant', 'trading', 'lt', 'strong', 'gt', 'xa', 'lt', 'strong', 'gt', 'off', 'the', 'influence', 'of', 'higher', 'order', 'versus', 'lower', 'order', 'terms', 'lt', 'strong', 'gt', 'in', 'the', 'xa', 'polynomial', 'when', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'cdg', 'png', 'quot', 'alt', 'quot', 'quot', 'gt', 'the', 'kernel', 'is', 'called', 'homogeneous', 'lt', 'strong', 'gt', 'a', 'further', 'lt', 'strong', 'gt', 'xa', 'lt', 'strong', 'gt', 'generalized', 'polykernel', 'divides', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'ps', 'qt', 'png', 'quot', 'alt', 'quot', 'quot', 'gt', 'by', 'a', 'user', 'specified', 'scalar', 'lt', 'strong', 'gt', 'xa', 'lt', 'strong', 'gt', 'parameter', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'sjfs', 'png', 'quot', 'alt', 'quot', 'quot', 'gt', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'neither', 'did', 'polydot', 's', 'explanation', 'in', 'r', 's', 'help', 'system', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'scale', 'lt', 'strong', 'gt', 'the', 'scaling', 'parameter', 'of', 'the', 'polynomial', 'and', 'tangent', 'kernel', 'is', 'a', 'xa', 'convenient', 'way', 'of', 'normalizing', 'patterns', 'amp', 'lt', 'without', 'the', 'need', 'to', 'modify', 'the', 'xa', 'data', 'itself', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'offset', 'lt', 'strong', 'gt', 'the', 'offset', 'used', 'in', 'a', 'polynomial', 'or', 'hyperbolic', 'tangent', 'kernel', 'amp', 'lt', 'lol', 'thanks', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'background', 'xa', 'i', 'run', 'a', 'product', 'that', 'compares', 'sets', 'of', 'data', 'data', 'matching', 'and', 'data', 'reconciliation', 'xa', 'to', 'get', 'the', 'result', 'we', 'need', 'to', 'compare', 'each', 'row', 'in', 'a', 'data', 'set', 'with', 'every', 'n', 'rows', 'on', 'the', 'opposing', 'data', 'set', 'xa', 'now', 'however', 'we', 'get', 'sets', 'of', 'up', 'to', 'rows', 'of', 'data', 'in', 'each', 'set', 'to', 'compare', 'and', 'are', 'getting', 'billion', 'computations', 'to', 'handle', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'my', 'question', 'is', 'this', 'xa', 'even', 'though', 'we', 'dont', 'have', 'the', 'data', 'volumes', 'to', 'use', 'hadoop', 'we', 'have', 'the', 'computational', 'need', 'for', 'something', 'distributed', 'is', 'hadoop', 'a', 'good', 'choice', 'for', 'us', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'going', 'to', 'classify', 'unstructured', 'text', 'documents', 'namely', 'web', 'sites', 'of', 'unknown', 'structure', 'the', 'number', 'of', 'classes', 'to', 'which', 'i', 'am', 'classifying', 'is', 'limited', 'at', 'this', 'point', 'i', 'believe', 'there', 'is', 'no', 'more', 'than', 'three', 'does', 'anyone', 'have', 'a', 'suggested', 'for', 'how', 'i', 'might', 'get', 'started', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'the', 'quot', 'bag', 'of', 'words', 'quot', 'approach', 'feasible', 'here', 'later', 'i', 'could', 'add', 'another', 'classification', 'stage', 'based', 'on', 'document', 'structure', 'perhaps', 'decision', 'trees', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'somewhat', 'familiar', 'with', 'mahout', 'and', 'hadoop', 'so', 'i', 'prefer', 'java', 'based', 'solutions', 'if', 'needed', 'i', 'can', 'switch', 'to', 'scala', 'and', 'or', 'spark', 'engine', 'the', 'ml', 'library', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'searching', 'for', 'data', 'sets', 'for', 'evaluating', 'text', 'retrieval', 'quality', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'tf', 'idf', 'is', 'a', 'popular', 'similarity', 'measure', 'but', 'is', 'it', 'the', 'best', 'choice', 'and', 'which', 'lt', 'em', 'gt', 'variant', 'lt', 'em', 'gt', 'is', 'the', 'best', 'choice', 'lt', 'a', 'href', 'quot', 'https', 'lucene', 'apache', 'org', 'core', '_', '_', 'api', 'all', 'org', 'apache', 'lucene', 'search', 'similarity', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'lucenes', 'scoring', 'lt', 'a', 'gt', 'for', 'example', 'uses', 'idf', 'and', 'idf', 'defined', 'as', 'log', 'numdocs', 'docfreq', 'tf', 'in', 'lucene', 'is', 'defined', 'as', 'sqrt', 'frequency', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'many', 'more', 'variants', 'exist', 'including', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'okapi_bm', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'okapi', 'bm', 'lt', 'a', 'gt', 'which', 'is', 'used', 'by', 'the', 'lt', 'a', 'href', 'quot', 'http', 'xapian', 'org', 'docs', 'bm', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'xapian', 'search', 'engine', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'like', 'to', 'study', 'the', 'different', 'variants', 'and', 'i', 'm', 'looking', 'for', 'lt', 'strong', 'gt', 'evaluation', 'data', 'sets', 'lt', 'strong', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'curious', 'about', 'the', 'anova', 'rbf', 'kernel', 'provided', 'by', 'kernlab', 'package', 'available', 'in', 'r', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tested', 'it', 'with', 'a', 'numeric', 'dataset', 'of', 'input', 'variables', 'and', 'one', 'output', 'variable', 'for', 'each', 'variable', 'i', 'have', 'different', 'values', 'comparing', 'with', 'other', 'kernels', 'i', 'got', 'very', 'bad', 'results', 'with', 'this', 'kernel', 'xa', 'for', 'example', 'using', 'the', 'simple', 'rbf', 'kernel', 'i', 'could', 'predict', 'with', 'r', 'however', 'with', 'the', 'anova', 'rbf', 'i', 'could', 'only', 'get', 'r', 'xa', 'i', 'thought', 'that', 'anova', 'rbf', 'would', 'be', 'a', 'very', 'good', 'kernel', 'any', 'thoughts', 'thanks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'code', 'is', 'as', 'follows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'set', 'seed', 'use', 'the', 'same', 'seed', 'to', 'train', 'different', 'models', 'xa', 'svrfitanovaacv', 'amp', 'lt', 'train', 'r', 'xa', 'data', 'trainset', 'xa', 'method', 'svmanova', 'xa', 'preproc', 'c', 'quot', 'center', 'quot', 'quot', 'scale', 'quot', 'xa', 'trcontrol', 'ctrl', 'tunelength', 'by', 'default', 'rmse', 'and', 'r', 'are', 'computed', 'for', 'regression', 'in', 'all', 'cases', 'selects', 'the', 'tunning', 'and', 'cross', 'val', 'model', 'with', 'best', 'value', 'metric', 'quot', 'roc', 'quot', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'define', 'custom', 'model', 'in', 'caret', 'package', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'library', 'caret', 'xa', 'rbf', 'anova', 'kernel', 'xa', 'svmanova', 'amp', 'lt', 'list', 'type', 'quot', 'regression', 'quot', 'library', 'quot', 'kernlab', 'quot', 'loop', 'null', 'xa', 'prmanova', 'amp', 'lt', 'data', 'frame', 'parameter', 'c', 'quot', 'c', 'quot', 'quot', 'sigma', 'quot', 'quot', 'degree', 'quot', 'quot', 'epsilon', 'quot', 'xa', 'class', 'rep', 'quot', 'numeric', 'quot', 'xa', 'label', 'c', 'quot', 'cost', 'quot', 'quot', 'sigma', 'quot', 'quot', 'degree', 'quot', 'quot', 'epsilon', 'quot', 'xa', 'svmanova', 'parameters', 'amp', 'lt', 'prmanova', 'xa', 'svmgridanova', 'amp', 'lt', 'function', 'x', 'y', 'len', 'null', 'xa', 'library', 'kernlab', 'xa', 'sigmas', 'amp', 'lt', 'sigest', 'as', 'matrix', 'x', 'na', 'action', 'na', 'omit', 'scaled', 'true', 'frac', 'xa', 'expand', 'grid', 'sigma', 'mean', 'sigmas', 'epsilon', 'xa', 'c', 'len', 'degree', 'len', 'tunelength', 'in', 'train', 'xa', 'xa', 'svmanova', 'grid', 'amp', 'lt', 'svmgridanova', 'xa', 'svmfitanova', 'amp', 'lt', 'function', 'x', 'y', 'wts', 'param', 'lev', 'last', 'weights', 'classprobs', 'xa', 'ksvm', 'x', 'as', 'matrix', 'x', 'y', 'y', 'xa', 'kernel', 'quot', 'anovadot', 'quot', 'xa', 'kpar', 'list', 'sigma', 'param', 'sigma', 'degree', 'param', 'degree', 'xa', 'c', 'param', 'c', 'epsilon', 'param', 'epsilon', 'xa', 'prob', 'model', 'classprobs', 'xa', 'default', 'type', 'quot', 'eps', 'svr', 'quot', 'xa', 'xa', 'svmanova', 'fit', 'amp', 'lt', 'svmfitanova', 'xa', 'svmpredanova', 'amp', 'lt', 'function', 'modelfit', 'newdata', 'preproc', 'null', 'submodels', 'null', 'xa', 'predict', 'modelfit', 'newdata', 'xa', 'svmanova', 'predict', 'amp', 'lt', 'svmpredanova', 'xa', 'svmprob', 'amp', 'lt', 'function', 'modelfit', 'newdata', 'preproc', 'null', 'submodels', 'null', 'xa', 'predict', 'modelfit', 'newdata', 'type', 'quot', 'probabilities', 'quot', 'xa', 'svmanova', 'prob', 'amp', 'lt', 'svmprob', 'xa', 'svmsortanova', 'amp', 'lt', 'function', 'x', 'x', 'order', 'x', 'c', 'xa', 'svmanova', 'sort', 'amp', 'lt', 'svmsortanova', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'load', 'data', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'dataa', 'amp', 'lt', 'read', 'csv', 'quot', 'c', 'results', 'a', 'txt', 'quot', 'header', 'true', 'xa', 'blank', 'lines', 'skip', 'true', 'sep', 'quot', 'quot', 'xa', 'set', 'seed', 'xa', 'intrainset', 'amp', 'lt', 'createdatapartition', 'dataa', 'r', 'p', 'list', 'false', 'xa', 'trainset', 'amp', 'lt', 'dataa', 'intrainset', 'xa', 'testset', 'amp', 'lt', 'dataa', 'intrainset', 'xa', 'xa', 'k', 'folds', 'resampling', 'method', 'for', 'fitting', 'svr', 'xa', 'ctrl', 'amp', 'lt', 'traincontrol', 'method', 'quot', 'repeatedcv', 'quot', 'number', 'repeats', 'xa', 'allowparallel', 'true', 'separate', 'fold', 'cross', 'validations', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'link', 'to', 'data', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'wuala', 'com', 'jpcgandre', 'documents', 'data', 'svr', 'key', 'bod', 'ntinzrhg', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 'have', 'biomedical', 'documents', 'each', 'of', 'some', 'mb', 'we', 'want', 'to', 'use', 'a', 'non', 'query', 'based', 'method', 'to', 'rank', 'the', 'documents', 'in', 'order', 'of', 'their', 'unique', 'content', 'score', 'i', 'm', 'calling', 'it', 'quot', 'unique', 'content', 'quot', 'because', 'our', 'researchers', 'want', 'to', 'know', 'from', 'which', 'document', 'to', 'start', 'reading', 'all', 'the', 'documents', 'are', 'of', 'the', 'same', 'topic', 'in', 'the', 'biomedical', 'world', 'we', 'know', 'that', 'there', 'is', 'always', 'a', 'lot', 'of', 'content', 'overlap', 'so', 'all', 'we', 'want', 'to', 'do', 'is', 'to', 'arrange', 'the', 'documents', 'in', 'the', 'order', 'of', 'their', 'unique', 'content', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'most', 'information', 'retrieval', 'literature', 'suggest', 'query', 'based', 'ranking', 'which', 'does', 'not', 'fit', 'our', 'need', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'it', 'may', 'be', 'unlikely', 'that', 'anyone', 'knows', 'this', 'but', 'i', 'have', 'a', 'specific', 'question', 'about', 'freebase', 'here', 'is', 'the', 'freebase', 'page', 'from', 'the', 'lt', 'a', 'href', 'quot', 'http', 'www', 'freebase', 'com', 'm', '_d', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'ford', 'taurus', 'automotive', 'model', 'lt', 'a', 'gt', 'it', 'has', 'a', 'property', 'called', 'quot', 'related', 'models', 'quot', 'does', 'anyone', 'know', 'how', 'this', 'list', 'of', 'related', 'models', 'was', 'compiled', 'what', 'is', 'the', 'similarity', 'measure', 'that', 'they', 'use', 'i', 'don', 't', 'think', 'it', 'is', 'only', 'about', 'other', 'wikipedia', 'pages', 'that', 'link', 'to', 'or', 'from', 'this', 'page', 'alternatively', 'it', 'may', 'be', 'that', 'this', 'is', 'user', 'generated', 'does', 'anyone', 'know', 'for', 'sure', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'best', 'technology', 'to', 'be', 'used', 'to', 'create', 'my', 'custom', 'bag', 'of', 'words', 'with', 'n', 'grams', 'to', 'apply', 'to', 'i', 'want', 'to', 'know', 'a', 'functionality', 'that', 'can', 'be', 'achieved', 'over', 'gui', 'i', 'cannot', 'use', 'spot', 'fire', 'as', 'it', 'is', 'not', 'available', 'in', 'the', 'organization', 'though', 'i', 'can', 'get', 'sap', 'hana', 'or', 'r', 'hadoop', 'but', 'r', 'hadoop', 'is', 'bit', 'challenging', 'any', 'suggessions', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'are', 'there', 'any', 'machine', 'learning', 'libraries', 'for', 'ruby', 'that', 'are', 'relatively', 'complete', 'including', 'a', 'wide', 'variety', 'of', 'algorithms', 'for', 'supervised', 'and', 'unsupervised', 'learning', 'robustly', 'tested', 'and', 'well', 'documented', 'i', 'love', 'python', 's', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'scikit', 'learn', 'lt', 'a', 'gt', 'for', 'its', 'incredible', 'documentation', 'but', 'a', 'client', 'would', 'prefer', 'to', 'write', 'the', 'code', 'in', 'ruby', 'since', 'that', 's', 'what', 'they', 're', 'familiar', 'with', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'ideally', 'i', 'am', 'looking', 'for', 'a', 'library', 'or', 'set', 'of', 'libraries', 'which', 'like', 'scikit', 'and', 'numpy', 'can', 'implement', 'a', 'wide', 'variety', 'of', 'data', 'structures', 'like', 'sparse', 'matrices', 'as', 'well', 'as', 'learners', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'examples', 'of', 'things', 'we', 'll', 'need', 'to', 'do', 'are', 'binary', 'classification', 'using', 'svms', 'and', 'implementing', 'bag', 'of', 'words', 'models', 'which', 'we', 'hope', 'to', 'concatenate', 'with', 'arbitrary', 'numeric', 'data', 'as', 'described', 'in', 'lt', 'a', 'href', 'quot', 'https', 'stackoverflow', 'com', 'q', 'quot', 'gt', 'this', 'lt', 'a', 'gt', 'stackoverflow', 'post', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'strong', 'gt', 'problem', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'my', 'machine', 'learning', 'task', 'i', 'create', 'a', 'set', 'of', 'predictors', 'xa', 'predictors', 'come', 'in', 'quot', 'bundles', 'quot', 'multi', 'dimensional', 'measurements', 'or', 'dimensional', 'in', 'my', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'hole', 'quot', 'bundle', 'quot', 'makes', 'sense', 'only', 'if', 'it', 'has', 'been', 'measured', 'and', 'taken', 'all', 'together', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'problem', 'is', 'different', 'bundles', 'of', 'predictors', 'can', 'be', 'measured', 'only', 'for', 'small', 'part', 'of', 'the', 'sample', 'and', 'those', 'parts', 'don', 't', 'necessary', 'intersect', 'for', 'different', 'bundles', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'parts', 'are', 'small', 'imputing', 'leads', 'to', 'considerable', 'decrease', 'in', 'accuracy', 'catastrophical', 'to', 'be', 'more', 'accurate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'possible', 'solutions', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'could', 'create', 'dummy', 'variables', 'that', 'would', 'mark', 'whether', 'the', 'measurement', 'has', 'taken', 'place', 'for', 'each', 'variable', 'the', 'problem', 'is', 'when', 'random', 'forests', 'draws', 'random', 'variables', 'it', 'does', 'so', 'individually', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'there', 'are', 'two', 'basic', 'ways', 'to', 'solve', 'this', 'problem', 'xa', 'combine', 'each', 'quot', 'bundle', 'quot', 'into', 'one', 'predictor', 'that', 'is', 'possible', 'but', 'it', 'seems', 'information', 'will', 'be', 'lost', 'xa', 'make', 'random', 'forest', 'draw', 'variables', 'not', 'individually', 'but', 'by', 'obligatory', 'quot', 'bundles', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'problem', 'for', 'random', 'forest', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'random', 'forest', 'draws', 'variables', 'randomly', 'it', 'takes', 'features', 'that', 'are', 'useless', 'or', 'much', 'less', 'useful', 'without', 'other', 'from', 'their', 'quot', 'bundle', 'quot', 'i', 'have', 'a', 'feeling', 'that', 'leads', 'to', 'a', 'loss', 'of', 'accuracy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'example', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'i', 'have', 'variables', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'a_measure', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'b', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'b_measure', 'lt', 'code', 'gt', 'xa', 'the', 'problem', 'is', 'variables', 'lt', 'code', 'gt', 'a_measure', 'lt', 'code', 'gt', 'make', 'sense', 'only', 'if', 'variable', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'is', 'present', 'same', 'for', 'lt', 'code', 'gt', 'b', 'lt', 'code', 'gt', 'so', 'i', 'either', 'have', 'to', 'combine', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'a_measure', 'lt', 'code', 'gt', 'into', 'one', 'variable', 'or', 'make', 'random', 'forest', 'draw', 'both', 'in', 'case', 'at', 'least', 'one', 'of', 'them', 'is', 'drawn', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'question', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'the', 'best', 'practice', 'solutions', 'for', 'problems', 'when', 'different', 'sets', 'of', 'predictors', 'are', 'measured', 'for', 'small', 'parts', 'of', 'overall', 'population', 'and', 'these', 'sets', 'of', 'predictors', 'come', 'in', 'obligatory', 'quot', 'bundles', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'problem', 'refers', 'to', 'decision', 'trees', 'building', 'according', 'to', 'wikipedia', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'gini_coefficient', 'quot', 'gt', 'gini', 'coefficient', 'lt', 'a', 'gt', 'should', 'not', 'be', 'confused', 'with', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'decision_tree_learning', 'gini_impurity', 'quot', 'gt', 'gini', 'impurity', 'lt', 'a', 'gt', 'however', 'both', 'measures', 'can', 'be', 'used', 'when', 'building', 'a', 'decision', 'tree', 'these', 'can', 'support', 'our', 'choices', 'when', 'splitting', 'the', 'set', 'of', 'items', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'gini', 'impurity', 'it', 'is', 'a', 'standard', 'decision', 'tree', 'splitting', 'metric', 'see', 'in', 'the', 'link', 'above', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'gini', 'coefficient', 'each', 'splitting', 'can', 'be', 'assessed', 'based', 'on', 'the', 'auc', 'criterion', 'for', 'each', 'splitting', 'scenario', 'we', 'can', 'build', 'a', 'roc', 'curve', 'and', 'compute', 'auc', 'metric', 'according', 'to', 'wikipedia', 'auc', 'ginicoeff', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'question', 'is', 'are', 'both', 'these', 'measures', 'equivalent', 'on', 'the', 'one', 'hand', 'i', 'am', 'informed', 'that', 'gini', 'coefficient', 'should', 'not', 'be', 'confused', 'with', 'gini', 'impurity', 'on', 'the', 'other', 'hand', 'both', 'these', 'measures', 'can', 'be', 'used', 'in', 'doing', 'the', 'same', 'thing', 'assessing', 'the', 'quality', 'of', 'a', 'decision', 'tree', 'split', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'at', 'pybrain', 'for', 'taking', 'server', 'monitor', 'alarms', 'and', 'determining', 'the', 'root', 'cause', 'of', 'a', 'problem', 'i', 'm', 'happy', 'with', 'training', 'it', 'using', 'supervised', 'learning', 'and', 'curating', 'the', 'training', 'data', 'sets', 'the', 'data', 'is', 'structured', 'something', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'server', 'type', 'lt', 'strong', 'gt', 'a', 'lt', 'strong', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'server', 'type', 'lt', 'strong', 'gt', 'a', 'lt', 'strong', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'server', 'type', 'lt', 'strong', 'gt', 'b', 'lt', 'strong', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'strong', 'gt', 'lt', 'strong', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'alarm', 'type', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'there', 'are', 'lt', 'em', 'gt', 'n', 'lt', 'em', 'gt', 'servers', 'with', 'lt', 'em', 'gt', 'x', 'lt', 'em', 'gt', 'alarms', 'that', 'can', 'be', 'lt', 'code', 'gt', 'up', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'down', 'lt', 'code', 'gt', 'both', 'lt', 'code', 'gt', 'n', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'are', 'variable', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'server', 'a', 'has', 'lt', 'em', 'gt', 'alarm', 'amp', 'amp', 'lt', 'em', 'gt', 'as', 'lt', 'code', 'gt', 'down', 'lt', 'code', 'gt', 'then', 'we', 'can', 'say', 'that', 'lt', 'em', 'gt', 'service', 'a', 'lt', 'em', 'gt', 'is', 'down', 'on', 'that', 'server', 'and', 'is', 'the', 'cause', 'of', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'lt', 'em', 'gt', 'alarm', 'lt', 'em', 'gt', 'is', 'down', 'on', 'all', 'servers', 'then', 'we', 'can', 'say', 'that', 'lt', 'em', 'gt', 'service', 'a', 'lt', 'em', 'gt', 'is', 'the', 'cause', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'can', 'potentially', 'be', 'multiple', 'options', 'for', 'the', 'cause', 'so', 'straight', 'classification', 'doesn', 't', 'seem', 'appropriate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'also', 'like', 'to', 'tie', 'later', 'sources', 'of', 'data', 'to', 'the', 'net', 'such', 'as', 'just', 'scripts', 'that', 'ping', 'some', 'external', 'service', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'all', 'the', 'appropriate', 'alarms', 'may', 'not', 'be', 'triggered', 'at', 'once', 'due', 'to', 'serial', 'service', 'checks', 'so', 'it', 'can', 'start', 'with', 'one', 'server', 'down', 'and', 'then', 'another', 'server', 'down', 'minutes', 'later', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'do', 'some', 'basic', 'stuff', 'at', 'first', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'from', 'pybrain', 'tools', 'shortcuts', 'import', 'buildnetwork', 'xa', 'from', 'pybrain', 'datasets', 'import', 'superviseddataset', 'xa', 'from', 'pybrain', 'supervised', 'trainers', 'import', 'backproptrainer', 'xa', 'xa', 'xa', 'inputs', 'xa', 'outputs', 'xa', 'xa', 'build', 'network', 'xa', 'xa', 'inputs', 'hidden', 'output', 'neurons', 'xa', 'net', 'buildnetwork', 'inputs', 'outputs', 'xa', 'xa', 'xa', 'build', 'dataset', 'xa', 'xa', 'dataset', 'with', 'inputs', 'and', 'output', 'xa', 'ds', 'superviseddataset', 'inputs', 'outputs', 'xa', 'xa', 'xa', 'add', 'one', 'sample', 'iterable', 'of', 'inputs', 'and', 'iterable', 'of', 'outputs', 'xa', 'ds', 'addsample', 'xa', 'xa', 'xa', 'xa', 'train', 'the', 'network', 'with', 'the', 'dataset', 'xa', 'trainer', 'backproptrainer', 'net', 'ds', 'xa', 'xa', 'train', 'epochs', 'xa', 'for', 'x', 'in', 'xrange', 'xa', 'trainer', 'train', 'xa', 'xa', 'train', 'infinite', 'epochs', 'until', 'the', 'error', 'rate', 'is', 'low', 'xa', 'trainer', 'trainuntilconvergence', 'xa', 'xa', 'xa', 'run', 'an', 'input', 'over', 'the', 'network', 'xa', 'result', 'net', 'activate', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'i', 'm', 'having', 'a', 'hard', 'time', 'mapping', 'variable', 'numbers', 'of', 'alarms', 'to', 'static', 'numbers', 'of', 'inputs', 'for', 'example', 'if', 'we', 'add', 'an', 'alarm', 'to', 'a', 'server', 'or', 'add', 'a', 'server', 'the', 'whole', 'net', 'needs', 'to', 'be', 'rebuilt', 'if', 'that', 'is', 'something', 'that', 'needs', 'to', 'be', 'done', 'i', 'can', 'do', 'it', 'but', 'want', 'to', 'know', 'if', 'there', 's', 'a', 'better', 'way', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'option', 'i', 'm', 'trying', 'to', 'think', 'of', 'is', 'have', 'a', 'different', 'net', 'for', 'each', 'type', 'of', 'server', 'but', 'i', 'don', 't', 'see', 'how', 'i', 'can', 'draw', 'an', 'environment', 'wide', 'conclusion', 'since', 'it', 'will', 'just', 'make', 'evaluations', 'on', 'a', 'single', 'host', 'instead', 'of', 'all', 'hosts', 'at', 'once', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'type', 'of', 'algorithm', 'should', 'i', 'use', 'and', 'how', 'do', 'i', 'map', 'the', 'dataset', 'to', 'draw', 'environment', 'wide', 'conclusions', 'as', 'a', 'whole', 'with', 'variable', 'inputs', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'corenlp', 'parts', 'of', 'speech', 'tagger', 'and', 'name', 'entity', 'recognition', 'tagger', 'are', 'pretty', 'good', 'out', 'of', 'the', 'box', 'but', 'i', 'd', 'like', 'to', 'improve', 'the', 'accuracy', 'further', 'so', 'that', 'the', 'overall', 'program', 'runs', 'better', 'to', 'explain', 'more', 'about', 'accuracy', 'there', 'are', 'situations', 'in', 'which', 'the', 'pos', 'ner', 'is', 'wrongly', 'tagged', 'for', 'instance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'quot', 'oversaw', 'car', 'manufacturing', 'quot', 'gets', 'tagged', 'as', 'nnp', 'nn', 'nn', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'rather', 'than', 'vb', 'or', 'something', 'similar', 'since', 'it', 's', 'a', 'verb', 'like', 'phrase', 'i', 'm', 'not', 'a', 'linguist', 'so', 'take', 'this', 'with', 'a', 'grain', 'of', 'salt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'what', 's', 'the', 'best', 'way', 'to', 'accomplish', 'accuracy', 'improvement', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'are', 'there', 'better', 'models', 'out', 'there', 'for', 'pos', 'ner', 'that', 'can', 'be', 'incorporated', 'into', 'corenlp', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'should', 'i', 'switch', 'to', 'other', 'nlp', 'tools', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'or', 'create', 'training', 'models', 'with', 'exception', 'rules', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'been', 'tasked', 'with', 'creating', 'a', 'pipeline', 'chart', 'with', 'the', 'live', 'data', 'and', 'the', 'budgeted', 'numbers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'know', 'what', 'probability', 'of', 'each', 'phase', 'of', 'reaching', 'the', 'next', 'the', 'problem', 'is', 'i', 'have', 'no', 'idea', 'what', 'to', 'do', 'about', 'the', 'pipeline', 'budgeting', 'with', 'regards', 'to', 'time', 'for', 'instance', 'what', 'period', 'of', 'time', 'should', 'i', 'have', 'closed', 'sales', 'in', 'the', 'chart', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'honestly', 'been', 'working', 'on', 'trying', 'to', 'figure', 'it', 'out', 'each', 'successive', 'revision', 'gets', 'me', 'farther', 'from', 'the', 'answer', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'currently', 'trying', 'to', 'implement', 'logistic', 'regression', 'with', 'iteratively', 'reweightes', 'ls', 'according', 'to', 'quot', 'pattern', 'recognition', 'and', 'machine', 'learning', 'quot', 'by', 'c', 'bishop', 'in', 'a', 'first', 'approach', 'i', 'tried', 'to', 'implement', 'it', 'in', 'c', 'where', 'i', 'used', 'gauss', 'algorithm', 'to', 'solve', 'eq', 'for', 'a', 'single', 'feature', 'it', 'gave', 'very', 'promising', 'nearly', 'exact', 'results', 'but', 'whenever', 'i', 'tried', 'to', 'run', 'it', 'with', 'more', 'than', 'one', 'feature', 'my', 'system', 'matrix', 'became', 'singular', 'and', 'the', 'weights', 'did', 'not', 'converge', 'i', 'first', 'thought', 'that', 'it', 'was', 'my', 'implementation', 'but', 'when', 'i', 'implemented', 'it', 'in', 'scilab', 'the', 'results', 'sustained', 'the', 'scilab', 'more', 'concise', 'due', 'to', 'matrix', 'operators', 'code', 'i', 'used', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'phi', 'xa', 't', 'xa', 'w', 'xa', 'xa', 'w', 'phi', 'xa', 'xa', 'for', 'in', 'xa', 'y', 'xa', 'r', 'zeros', 'size', 'phi', 'xa', 'r_inv', 'zeros', 'size', 'phi', 'xa', 'xa', 'for', 'i', 'size', 'phi', 'xa', 'y', 'i', 'exp', 'w', 'phi', 'i', 'xa', 'r', 'i', 'i', 'y', 'i', 'y', 'i', 'xa', 'r_inv', 'i', 'i', 'r', 'i', 'i', 'xa', 'end', 'xa', 'xa', 'z', 'phi', 'w', 'r_inv', 'y', 't', 'xa', 'w', 'inv', 'phi', 'r', 'phi', 'phi', 'r', 'z', 'xa', 'end', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'with', 'the', 'values', 'for', 'phi', 'input', 'features', 'and', 't', 'output', 'classes', 'it', 'yields', 'a', 'weight', 'of', 'which', 'is', 'pretty', 'much', 'which', 'seems', 'fine', 'to', 'me', 'for', 'there', 'is', 'probability', 'of', 'beeing', 'assigned', 'to', 'class', 'if', 'feature', 'is', 'present', 'please', 'forgive', 'me', 'if', 'my', 'terms', 'do', 'not', 'comply', 'with', 'ml', 'language', 'completely', 'for', 'i', 'am', 'an', 'software', 'developer', 'if', 'i', 'now', 'added', 'an', 'intercept', 'feature', 'which', 'would', 'accord', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'phi', 'xa', 'w', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'r', 'matrix', 'becomes', 'singular', 'and', 'the', 'last', 'weights', 'value', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'w', 'xa', 'xa', 'd', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'to', 'my', 'reading', 'would', 'mean', 'that', 'the', 'probability', 'of', 'belonging', 'to', 'class', 'would', 'be', 'close', 'to', 'if', 'feature', 'is', 'present', 'about', 'for', 'the', 'rest', 'there', 'has', 'got', 'to', 'be', 'any', 'error', 'i', 'made', 'but', 'i', 'do', 'not', 'get', 'which', 'one', 'for', 'both', 'implementations', 'yield', 'the', 'same', 'results', 'i', 'suspect', 'that', 'there', 'is', 'some', 'point', 'i', 've', 'been', 'missing', 'or', 'gotten', 'wrong', 'but', 'i', 'do', 'not', 'understand', 'which', 'one', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'having', 'an', 'html', 'string', 'and', 'want', 'to', 'find', 'out', 'if', 'a', 'word', 'i', 'supply', 'is', 'relevant', 'in', 'that', 'string', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'relevancy', 'could', 'be', 'measured', 'based', 'on', 'frequency', 'in', 'the', 'text', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'an', 'example', 'to', 'illustrate', 'my', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'this', 'is', 'an', 'awesome', 'bike', 'store', 'xa', 'bikes', 'can', 'be', 'purchased', 'online', 'xa', 'the', 'bikes', 'we', 'own', 'rock', 'xa', 'check', 'out', 'our', 'bike', 'store', 'now', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'want', 'to', 'test', 'a', 'few', 'other', 'words', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'bike', 'repairs', 'xa', 'dog', 'poo', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'bike', 'repairs', 'lt', 'code', 'gt', 'should', 'be', 'marked', 'as', 'relevant', 'whereas', 'lt', 'code', 'gt', 'dog', 'poo', 'lt', 'code', 'gt', 'should', 'not', 'be', 'marked', 'as', 'relevant', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'could', 'this', 'be', 'done', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'to', 'i', 'filter', 'out', 'ambiguous', 'words', 'like', 'lt', 'code', 'gt', 'in', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'your', 'ideas', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'guess', 'it', 's', 'something', 'google', 'does', 'to', 'figure', 'out', 'what', 'keywords', 'are', 'relevant', 'to', 'a', 'website', 'i', 'am', 'basically', 'trying', 'to', 'reproduce', 'their', 'on', 'page', 'rankings', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'classification', 'problem', 'with', 'approximately', 'positive', 'and', 'negative', 'samples', 'in', 'training', 'set', 'so', 'this', 'data', 'set', 'is', 'quite', 'unbalanced', 'plain', 'random', 'forest', 'is', 'just', 'trying', 'to', 'mark', 'all', 'test', 'samples', 'as', 'a', 'majority', 'class', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'some', 'good', 'answers', 'about', 'sub', 'sampling', 'and', 'weighted', 'random', 'forest', 'are', 'given', 'here', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'what', 'are', 'the', 'implications', 'for', 'training', 'a', 'tree', 'ensemble', 'with', 'highly', 'biased', 'datase', 'quot', 'gt', 'what', 'are', 'the', 'implications', 'for', 'training', 'a', 'tree', 'ensemble', 'with', 'highly', 'biased', 'datasets', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'classification', 'methods', 'besides', 'rf', 'can', 'handle', 'the', 'problem', 'in', 'the', 'best', 'way', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'as', 'mentioned', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'quick', 'guide', 'into', 'training', 'highly', 'imbalanced', 'data', 'sets', 'quot', 'gt', 'before', 'lt', 'a', 'gt', 'i', 'have', 'a', 'classification', 'problem', 'and', 'unbalanced', 'data', 'set', 'the', 'majority', 'class', 'contains', 'of', 'all', 'samples', 'xa', 'i', 'have', 'trained', 'a', 'generalized', 'boosted', 'regression', 'model', 'using', 'lt', 'code', 'gt', 'gbm', 'lt', 'code', 'gt', 'from', 'the', 'lt', 'code', 'gt', 'gbm', 'lt', 'code', 'gt', 'package', 'in', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'and', 'get', 'the', 'following', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'interaction', 'depth', 'n', 'trees', 'accuracy', 'kappa', 'accuracy', 'sd', 'kappa', 'sd', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'looking', 'at', 'the', 'accuracy', 'i', 'assume', 'that', 'model', 'has', 'labeled', 'all', 'the', 'samples', 'as', 'majority', 'class', 'that', 's', 'clear', 'xa', 'and', 'what', 'is', 'not', 'transparent', 'how', 'kappa', 'is', 'calculated', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'does', 'this', 'kappa', 'values', 'near', 'to', 'really', 'mean', 'is', 'it', 'enough', 'to', 'say', 'that', 'the', 'model', 'is', 'not', 'classifying', 'them', 'just', 'by', 'chance', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'do', 'lt', 'code', 'gt', 'accuracy', 'sd', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'kappa', 'sd', 'lt', 'code', 'gt', 'mean', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'cluster', 'a', 'set', 'of', 'long', 'tailed', 'pareto', 'like', 'data', 'into', 'several', 'bins', 'actually', 'the', 'bin', 'number', 'is', 'not', 'determined', 'yet', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'algorithm', 'or', 'model', 'would', 'anyone', 'recommend', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'general', 'methodological', 'question', 'i', 'have', 'two', 'columns', 'of', 'data', 'with', 'one', 'a', 'column', 'a', 'numeric', 'variable', 'for', 'age', 'and', 'another', 'column', 'a', 'short', 'character', 'variable', 'for', 'text', 'responses', 'to', 'a', 'question', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'goal', 'is', 'to', 'group', 'the', 'age', 'variable', 'that', 'is', 'create', 'cut', 'points', 'for', 'the', 'age', 'variable', 'based', 'on', 'the', 'text', 'responses', 'i', 'm', 'unfamiliar', 'with', 'any', 'general', 'approaches', 'for', 'doing', 'this', 'sort', 'of', 'analysis', 'what', 'general', 'approaches', 'would', 'you', 'recommend', 'ideally', 'i', 'd', 'like', 'to', 'categorize', 'the', 'age', 'variable', 'based', 'on', 'linguistic', 'similarity', 'of', 'the', 'text', 'responses', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'rfm_', 'customer_value', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'rfm', 'lt', 'a', 'gt', 'is', 'a', 'ranking', 'model', 'when', 'all', 'customers', 'are', 'ranked', 'according', 'to', 'their', 'purchasing', 'lt', 'strong', 'gt', 'f', 'lt', 'strong', 'gt', 'requency', 'lt', 'strong', 'gt', 'r', 'lt', 'strong', 'gt', 'recency', 'and', 'lt', 'strong', 'gt', 'm', 'lt', 'strong', 'gt', 'monetary', 'value', 'this', 'indicator', 'is', 'highly', 'used', 'by', 'marketing', 'departments', 'of', 'various', 'organizations', 'to', 'segment', 'customers', 'into', 'groups', 'according', 'to', 'customer', 'value', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'question', 'is', 'following', 'are', 'there', 'any', 'substantial', 'models', 'based', 'on', 'rfm', 'scoring', 'or', 'related', 'to', 'which', 'have', 'solid', 'predictive', 'power', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'update', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'predicting', 'which', 'customer', 'will', 'most', 'likely', 'spend', 'more', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'who', 'is', 'going', 'to', 'upgrade', 'renew', 'subscribtion', 'refund', 'etc', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'update', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'understand', 'this', 'is', 'simple', 'problem', 'with', 'three', 'independent', 'variable', 'and', 'one', 'classifier', 'my', 'guess', 'and', 'experience', 'say', 'these', 'pure', 'three', 'factors', 'do', 'not', 'predict', 'future', 'customer', 'value', 'but', 'they', 'can', 'be', 'used', 'together', 'with', 'another', 'data', 'or', 'can', 'be', 'an', 'additional', 'input', 'into', 'some', 'model', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'please', 'share', 'which', 'methodologies', 'worked', 'for', 'you', 'personally', 'and', 'are', 'likely', 'to', 'have', 'high', 'predictive', 'ability', 'what', 'kind', 'of', 'data', 'you', 'used', 'together', 'with', 'rfm', 'indicators', 'and', 'it', 'worked', 'well', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'suppose', 'i', 'am', 'interested', 'in', 'classifying', 'a', 'set', 'of', 'instances', 'composed', 'by', 'different', 'content', 'types', 'e', 'g', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'piece', 'of', 'lt', 'strong', 'gt', 'text', 'lt', 'strong', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'an', 'lt', 'strong', 'gt', 'image', 'lt', 'strong', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'lt', 'code', 'gt', 'relevant', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'non', 'relevant', 'lt', 'code', 'gt', 'for', 'a', 'specific', 'class', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'my', 'classification', 'process', 'i', 'perform', 'the', 'following', 'steps', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'given', 'a', 'sample', 'i', 'subdivide', 'it', 'in', 'text', 'and', 'image', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'first', 'svm', 'binary', 'classifier', 'lt', 'code', 'gt', 'svm', 'text', 'lt', 'code', 'gt', 'trained', 'only', 'on', 'text', 'classifies', 'the', 'text', 'as', 'lt', 'code', 'gt', 'relevant', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'non', 'relevant', 'lt', 'code', 'gt', 'for', 'the', 'class', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'a', 'second', 'svm', 'binary', 'classifier', 'lt', 'code', 'gt', 'svm', 'image', 'lt', 'code', 'gt', 'trained', 'only', 'on', 'images', 'classifies', 'the', 'image', 'as', 'lt', 'code', 'gt', 'relevant', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'non', 'relevant', 'lt', 'code', 'gt', 'for', 'the', 'class', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'both', 'lt', 'code', 'gt', 'svm', 'text', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'svm', 'image', 'lt', 'code', 'gt', 'produce', 'an', 'estimate', 'of', 'the', 'probability', 'of', 'the', 'analyzed', 'content', 'text', 'or', 'image', 'of', 'being', 'relevant', 'for', 'the', 'class', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'given', 'this', 'i', 'am', 'able', 'to', 'state', 'whether', 'the', 'text', 'is', 'relevant', 'for', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'and', 'the', 'image', 'is', 'relevant', 'for', 'lt', 'code', 'gt', 'c', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'these', 'estimates', 'are', 'valid', 'for', 'segments', 'of', 'the', 'original', 'sample', 'either', 'the', 'text', 'or', 'the', 'image', 'while', 'it', 'is', 'not', 'clear', 'how', 'to', 'obtain', 'a', 'general', 'opinion', 'on', 'the', 'whole', 'original', 'sample', 'text', 'image', 'how', 'can', 'i', 'combine', 'conveniently', 'the', 'opinions', 'of', 'the', 'two', 'classifiers', 'so', 'as', 'to', 'obtain', 'a', 'classification', 'for', 'the', 'whole', 'original', 'sample', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'working', 'on', 'a', 'text', 'classification', 'problem', 'using', 'random', 'forest', 'as', 'classifiers', 'and', 'a', 'bag', 'of', 'words', 'approach', 'xa', 'i', 'am', 'using', 'the', 'basic', 'implementation', 'of', 'random', 'forests', 'the', 'one', 'present', 'in', 'scikit', 'that', 'creates', 'a', 'binary', 'condition', 'on', 'a', 'single', 'variable', 'at', 'each', 'split', 'given', 'this', 'is', 'there', 'a', 'difference', 'between', 'using', 'simple', 'tf', 'term', 'frequency', 'features', 'where', 'each', 'word', 'has', 'an', 'associated', 'weight', 'that', 'represents', 'the', 'number', 'of', 'occurrences', 'in', 'the', 'document', 'or', 'tf', 'idf', 'term', 'frequency', 'inverse', 'document', 'frequency', 'where', 'the', 'term', 'frequency', 'is', 'also', 'multiplied', 'by', 'a', 'value', 'that', 'represents', 'the', 'ratio', 'between', 'the', 'total', 'number', 'of', 'documents', 'and', 'the', 'number', 'of', 'documents', 'containing', 'the', 'word', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'my', 'opinion', 'there', 'should', 'not', 'be', 'any', 'difference', 'between', 'these', 'two', 'approaches', 'because', 'the', 'only', 'difference', 'is', 'a', 'scaling', 'factor', 'on', 'each', 'feature', 'but', 'since', 'the', 'split', 'is', 'done', 'at', 'the', 'level', 'of', 'single', 'features', 'this', 'should', 'not', 'make', 'a', 'difference', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'am', 'i', 'right', 'in', 'my', 'reasoning', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'interested', 'in', 'the', 'field', 'of', 'lt', 'strong', 'gt', 'named', 'entity', 'disambiguation', 'lt', 'strong', 'gt', 'and', 'want', 'to', 'learn', 'more', 'about', 'it', 'i', 'have', 'heard', 'that', 'there', 'are', 'contests', 'organised', 'by', 'various', 'associations', 'on', 'these', 'kind', 'of', 'research', 'topics', 'these', 'contests', 'are', 'very', 'helpful', 'as', 'they', 'give', 'a', 'practical', 'experience', 'in', 'these', 'fields', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'found', 'one', 'such', 'contest', 'organised', 'by', 'microsoft', 'research', 'lt', 'a', 'href', 'quot', 'http', 'web', 'ngram', 'research', 'microsoft', 'com', 'erd', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'though', 'the', 'dates', 'have', 'already', 'passed', 'can', 'anyone', 'point', 'me', 'to', 'any', 'other', 'such', 'contests', 'also', 'is', 'there', 'a', 'site', 'which', 'catalogues', 'these', 'contests', 'so', 'that', 'one', 'can', 'just', 'go', 'there', 'and', 'know', 'about', 'all', 'upcoming', 'contests', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'information', 'on', 'formal', 'algebraic', 'systems', 'that', 'can', 'be', 'used', 'to', 'transform', 'time', 'series', 'in', 'either', 'a', 'practical', 'or', 'academic', 'context', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'hope', 'that', 'there', 'exists', 'at', 'least', 'one', 'small', 'expressive', 'set', 'of', 'operators', 'ranging', 'over', 'finite', 'time', 'series', 'i', 'want', 'to', 'compare', 'and', 'contrast', 'different', 'systems', 'with', 'respect', 'to', 'algebraic', 'completeness', 'and', 'brevity', 'of', 'representation', 'of', 'common', 'time', 'series', 'transformations', 'in', 'various', 'domains', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'realise', 'this', 'question', 'is', 'broad', 'but', 'hope', 'it', 'is', 'not', 'too', 'vague', 'for', 'datascience', 'stackexchange', 'i', 'welcome', 'any', 'pointers', 'to', 'relevant', 'literature', 'for', 'specific', 'scenarios', 'or', 'the', 'general', 'subject', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'attempt', 'to', 'better', 'explain', 'what', 'i', 'meant', 'by', 'an', 'algebraic', 'system', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'was', 'thinking', 'about', 'quot', 'abstract', 'algebras', 'quot', 'as', 'discussed', 'in', 'wikipedia', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'algebra', 'abstract_algebra', 'quot', 'gt', 'http', 'en', 'wikipedia', 'org', 'wiki', 'algebra', 'abstract_algebra', 'lt', 'a', 'gt', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'abstract_algebra', 'basic_concepts', 'quot', 'gt', 'http', 'en', 'wikipedia', 'org', 'wiki', 'abstract_algebra', 'basic_concepts', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'boolean', 'algebras', 'are', 'very', 'simple', 'algebras', 'that', 'range', 'over', 'boolean', 'values', 'a', 'simple', 'example', 'of', 'such', 'an', 'algebra', 'would', 'consist', 'the', 'values', 'true', 'and', 'false', 'and', 'the', 'operations', 'and', 'or', 'and', 'not', 'one', 'might', 'argue', 'this', 'algebra', 'is', 'complete', 'as', 'from', 'these', 'two', 'constants', 'free', 'variables', 'and', 'three', 'basic', 'operations', 'arbitrary', 'boolean', 'functions', 'can', 'be', 'constructed', 'described', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'interested', 'to', 'discover', 'algebras', 'where', 'the', 'values', 'are', 'time', 'domain', 'time', 'series', 'i', 'd', 'like', 'it', 'to', 'be', 'possible', 'to', 'construct', 'quot', 'arbitrary', 'quot', 'functions', 'that', 'map', 'time', 'series', 'to', 'time', 'series', 'from', 'a', 'few', 'operations', 'which', 'individually', 'map', 'time', 'series', 'to', 'time', 'series', 'i', 'am', 'open', 'to', 'liberal', 'interpretations', 'of', 'quot', 'arbitrary', 'quot', 'i', 'would', 'be', 'especially', 'interested', 'in', 'examples', 'of', 'these', 'algebras', 'where', 'the', 'operations', 'consist', 'higher', 'order', 'functions', 'where', 'such', 'operations', 'have', 'been', 'developed', 'for', 'a', 'specific', 'domain', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'research', 'scholar', 'in', 'data', 'mining', 'i', 'm', 'interested', 'in', 'c', 'implementation', 'of', 'k', 'means', 'clustering', 'algorithm', 'for', 'mixed', 'numeric', 'and', 'categorical', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'rather', 'large', 'commute', 'every', 'day', 'it', 'ranges', 'between', 'about', 'an', 'hour', 'and', 'about', 'an', 'hour', 'and', 'half', 'of', 'driving', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'been', 'tracking', 'my', 'driving', 'times', 'and', 'want', 'to', 'continue', 'to', 'do', 'so', 'i', 'am', 'capturing', 'the', 'date', 'my', 'time', 'of', 'departure', 'my', 'time', 'of', 'arrival', 'the', 'route', 'i', 'took', 'there', 'are', 'two', 'or', 'three', 'possible', 'ones', 'weather', 'conditions', 'wet', 'dry', 'and', 'clear', 'hazy', 'foggy', 'and', 'whether', 'i', 'stopped', 'and', 'if', 'so', 'for', 'what', 'reason', 'fuel', 'toilet', 'break', 'food', 'break', 'and', 'for', 'how', 'long', 'for', 'every', 'journey', 'to', 'and', 'from', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'create', 'a', 'system', 'to', 'analyse', 'this', 'data', 'and', 'suggest', 'an', 'optimal', 'departure', 'time', 'for', 'the', 'next', 'journey', 'based', 'on', 'day', 'of', 'the', 'week', 'weather', 'conditions', 'and', 'whether', 'i', 'need', 'to', 'stop', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'anecdotally', 'i', 'can', 'see', 'that', 'tuesday', 'mornings', 'are', 'worse', 'than', 'other', 'mornings', 'the', 'earlier', 'i', 'leave', 'the', 'more', 'likely', 'i', 'am', 'to', 'take', 'a', 'toilet', 'break', 'or', 'a', 'food', 'break', 'and', 'obviously', 'that', 'the', 'journey', 'takes', 'longer', 'on', 'rainy', 'or', 'foggy', 'days', 'than', 'on', 'clear', 'and', 'dry', 'days', 'but', 'i', 'would', 'like', 'the', 'system', 'to', 'empirically', 'tell', 'me', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'assume', 'this', 'is', 'a', 'machine', 'learning', 'and', 'statistical', 'analysis', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'have', 'absolutely', 'no', 'knowledge', 'of', 'machine', 'learning', 'or', 'statistical', 'methods', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'statistical', 'methods', 'should', 'i', 'use', 'to', 'do', 'this', 'kind', 'of', 'analysis', 'to', 'the', 'point', 'where', 'the', 'data', 'will', 'lead', 'to', 'suggestions', 'like', 'quot', 'tomorrow', 'is', 'tuesday', 'and', 'it', 'is', 'going', 'to', 'rain', 'so', 'you', 'must', 'leave', 'home', 'between', 'and', 'and', 'take', 'route', 'xyz', 'to', 'get', 'the', 'optimal', 'driving', 'time', 'oh', 'and', 'chances', 'are', 'you', 'will', 'need', 'a', 'toilet', 'break', 'and', 'i', 'have', 'factored', 'that', 'in', 'quot', 'assume', 'that', 'i', 'manually', 'enter', 'tomorrow', 's', 'weather', 'forecast', 'i', 'll', 'look', 'into', 'integrating', 'with', 'a', 'weather', 'service', 'later', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'that', 'this', 'is', 'life', 'hacking', 'for', 'me', 'trying', 'to', 'optimise', 'the', 'hell', 'out', 'of', 'a', 'tedious', 'process', 'and', 'it', 'is', 'very', 'personal', 'specific', 'to', 'me', 'and', 'my', 'habits', 'specific', 'to', 'this', 'route', 'and', 'specific', 'to', 'the', 'morning', 'evening', 'commute', 'times', 'google', 'maps', 'with', 'traffic', 'tomtom', 'with', 'iq', 'and', 'waze', 'do', 'very', 'well', 'in', 'the', 'more', 'open', 'ended', 'situations', 'of', 'ad', 'hoc', 'driving', 'time', 'prediction', 'even', 'apple', 'is', 'happy', 'to', 'tell', 'me', 'on', 'my', 'iphone', 'notification', 'screen', 'how', 'long', 'it', 'will', 'take', 'me', 'to', 'get', 'home', 'if', 'i', 'leave', 'right', 'now', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'note', 'it', 'appears', 'to', 'me', 'that', 'traffic', 'is', 'not', 'a', 'consideration', 'that', 'is', 'to', 'say', 'i', 'do', 'not', 'think', 'i', 'need', 'to', 'know', 'the', 'actual', 'traffic', 'conditions', 'traffic', 'is', 'a', 'function', 'of', 'day', 'of', 'the', 'week', 'and', 'weather', 'for', 'example', 'there', 'are', 'more', 'people', 'on', 'the', 'roads', 'on', 'monday', 'and', 'tuesday', 'mornings', 'and', 'people', 'drive', 'more', 'slowly', 'and', 'more', 'people', 'are', 'in', 'cars', 'opting', 'to', 'drive', 'instead', 'of', 'cycle', 'or', 'take', 'public', 'transport', 'when', 'it', 'rains', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'what', 'extent', 'can', 'i', 'let', 'the', 'data', 'do', 'all', 'the', 'talking', 'i', 'have', 'a', 'somewhat', 'ambiguous', 'hidden', 'agenda', 'which', 'may', 'not', 'be', 'apparent', 'from', 'the', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'should', 'be', 'at', 'work', 'at', 'i', 'e', 'minutes', 'every', 'day', 'but', 'the', 'occasional', 'am', 'arrival', 'is', 'ok', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'want', 'to', 'leave', 'home', 'as', 'late', 'as', 'possible', 'and', 'yet', 'arrive', 'at', 'work', 'as', 'early', 'as', 'possible', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'want', 'to', 'leave', 'work', 'as', 'early', 'as', 'possible', 'and', 'yet', 'have', 'done', 'at', 'least', 'hours', 'work', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'it', 'is', 'ok', 'for', 'me', 'to', 'say', 'leave', 'half', 'an', 'hour', 'early', 'on', 'one', 'day', 'but', 'stay', 'late', 'on', 'another', 'to', 'compensate', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'think', 'i', 'can', 'come', 'up', 'with', 'a', 'procedural', 'formula', 'that', 'can', 'encompass', 'all', 'of', 'these', 'rules', 'but', 'my', 'gut', 'feeling', 'is', 'that', 'statistical', 'analysis', 'can', 'make', 'it', 'a', 'lot', 'smarter', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'apart', 'from', 'the', 'methods', 'of', 'analysis', 'the', 'technology', 'stack', 'is', 'not', 'an', 'issue', 'java', 'is', 'my', 'language', 'of', 'choice', 'i', 'am', 'quite', 'familiar', 'with', 'programming', 'in', 'it', 'and', 'in', 'creating', 'web', 'applications', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'assuming', 'that', 'it', 'is', 'possible', 'are', 'there', 'java', 'libraries', 'that', 'can', 'provide', 'the', 'requisite', 'methods', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'limitations', 'are', 'there', 'i', 'want', 'to', 'keep', 'capturing', 'more', 'and', 'more', 'data', 'every', 'day', 'making', 'the', 'data', 'set', 'bigger', 'hopefully', 'making', 'the', 'prediction', 'more', 'accurate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'other', 'ways', 'are', 'there', 'to', 'do', 'it', 'can', 'i', 'push', 'this', 'data', 'into', 'say', 'wolfram', 'programming', 'cloud', 'or', 'maybe', 'something', 'google', 'provides', 'to', 'get', 'the', 'desired', 'results', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'time', 'series', 'data', 'from', 'mobile', 'sensors', 'for', 'different', 'motions', 'such', 'as', 'walking', 'pushups', 'dumbellifts', 'rowing', 'and', 'so', 'on', 'all', 'these', 'motions', 'have', 'different', 'length', 'of', 'time', 'series', 'for', 'classifying', 'them', 'using', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'dynamic_time_warping', 'quot', 'gt', 'dynamic', 'time', 'warping', 'dtw', 'lt', 'a', 'gt', 'how', 'do', 'i', 'choose', 'an', 'appropriate', 'window', 'size', 'that', 'will', 'give', 'good', 'results', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'looking', 'to', 'choose', 'my', 'career', 'in', 'the', 'area', 'of', 'decision', 'science', 'or', 'predictive', 'modeling', 'and', 'i', 'am', 'aware', 'that', 'this', 'is', 'kind', 'of', 'opinion', 'based', 'but', 'i', 'would', 'like', 'to', 'have', 'some', 'suggestion', 'from', 'experts', 'that', 'i', 'can', 'use', 'it', 'to', 'build', 'my', 'career', 'in', 'correct', 'path', 'what', 'are', 'the', 'tools', 'should', 'i', 'know', 'like', 'r', 'sas', 'or', 'any', 'other', 'what', 'are', 'the', 'thinks', 'i', 'should', 'know', 'to', 'work', 'in', 'a', 'data', 'science', 'or', 'machine', 'learning', 'or', 'predictive', 'modeling', 'for', 'me', 'i', 'am', 'having', 'problem', 'in', 'identifying', 'steps', 'that', 'i', 'should', 'follow', 'please', 'suggest', 'me', 'some', 'steps', 'to', 'follow', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'recently', 'i', 'was', 'introduced', 'to', 'the', 'field', 'of', 'data', 'science', 'its', 'been', 'months', 'approx', 'and', 'ii', 'started', 'the', 'journey', 'with', 'machine', 'learning', 'course', 'by', 'andrew', 'ng', 'and', 'post', 'that', 'started', 'working', 'on', 'the', 'data', 'science', 'specialization', 'by', 'jhu', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'on', 'practical', 'application', 'front', 'i', 'have', 'been', 'working', 'on', 'building', 'a', 'predictive', 'model', 'that', 'would', 'predict', 'attrition', 'so', 'far', 'i', 'have', 'used', 'glm', 'bayesglm', 'rf', 'in', 'an', 'effort', 'to', 'learn', 'and', 'apply', 'these', 'methods', 'but', 'i', 'find', 'a', 'lot', 'of', 'gap', 'in', 'my', 'understanding', 'of', 'these', 'algorithms', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'basic', 'dilemma', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'em', 'gt', 'whether', 'i', 'should', 'focus', 'more', 'on', 'learning', 'the', 'intricacies', 'of', 'a', 'few', 'algorithms', 'or', 'should', 'i', 'use', 'the', 'approach', 'of', 'knowing', 'a', 'lot', 'of', 'them', 'as', 'and', 'when', 'and', 'as', 'much', 'as', 'required', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'guide', 'me', 'in', 'the', 'right', 'direction', 'maybe', 'by', 'suggesting', 'books', 'or', 'articles', 'or', 'anything', 'that', 'you', 'think', 'would', 'help', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'be', 'grateful', 'if', 'you', 'would', 'reply', 'with', 'an', 'idea', 'of', 'guiding', 'someone', 'who', 'has', 'just', 'started', 'his', 'career', 'in', 'the', 'field', 'of', 'data', 'science', 'and', 'wants', 'to', 'be', 'a', 'person', 'who', 'solves', 'practical', 'issues', 'for', 'the', 'business', 'world', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'read', 'as', 'many', 'as', 'possible', 'resources', 'books', 'articles', 'suggested', 'in', 'this', 'post', 'and', 'would', 'provide', 'a', 'personal', 'feed', 'back', 'on', 'the', 'pros', 'and', 'cons', 'of', 'the', 'same', 'so', 'as', 'to', 'make', 'this', 'a', 'helpful', 'post', 'for', 'people', 'who', 'come', 'across', 'a', 'similar', 'question', 'in', 'future', 'and', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'people', 'suggesting', 'these', 'books', 'can', 'do', 'the', 'same', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'one', 'of', 'the', 'discussed', 'nice', 'aspects', 'of', 'the', 'procedure', 'that', 'vowpal', 'wabbit', 'uses', 'for', 'updates', 'to', 'sgd', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'lowrank', 'net', 'nikos', 'pubs', 'liw', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'pdf', 'lt', 'a', 'gt', 'is', 'so', 'called', 'weight', 'invariance', 'described', 'in', 'the', 'linked', 'as', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'among', 'these', 'updates', 'we', 'mainly', 'focus', 'on', 'a', 'novel', 'xa', 'set', 'of', 'updates', 'that', 'satisfies', 'an', 'additional', 'invariance', 'xa', 'property', 'for', 'all', 'importance', 'weights', 'of', 'h', 'the', 'update', 'xa', 'is', 'equivalent', 'to', 'two', 'updates', 'with', 'importance', 'weight', 'xa', 'h', 'we', 'call', 'these', 'updates', 'importance', 'invariant', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'does', 'this', 'mean', 'and', 'why', 'is', 'it', 'useful', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'working', 'on', 'a', 'text', 'classification', 'problem', 'on', 'tweets', 'at', 'the', 'moment', 'i', 'was', 'only', 'considering', 'the', 'content', 'of', 'the', 'tweets', 'as', 'a', 'source', 'of', 'information', 'and', 'i', 'was', 'using', 'a', 'simple', 'bag', 'of', 'words', 'approach', 'using', 'term', 'frequencies', 'as', 'features', 'using', 'random', 'forests', 'this', 'is', 'something', 'i', 'cannot', 'change', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'my', 'idea', 'is', 'to', 'try', 'to', 'incorporate', 'information', 'present', 'in', 'the', 'urls', 'used', 'in', 'tweets', 'now', 'not', 'all', 'the', 'tweets', 'have', 'urls', 'and', 'if', 'i', 'decide', 'to', 'use', 'the', 'same', 'term', 'frequency', 'representation', 'also', 'for', 'urls', 'i', 'will', 'have', 'a', 'huge', 'number', 'of', 'features', 'only', 'from', 'urls', 'for', 'this', 'reason', 'i', 'suppose', 'that', 'having', 'a', 'single', 'set', 'of', 'features', 'containing', 'both', 'the', 'tweet', 'term', 'frequencies', 'and', 'the', 'url', 'term', 'frequencies', 'could', 'be', 'bad', 'besides', 'i', 'll', 'have', 'to', 'fill', 'some', 'impossible', 'values', 'like', 'for', 'the', 'url', 'features', 'for', 'tweets', 'that', 'do', 'not', 'have', 'urls', 'and', 'i', 'will', 'probably', 'worsen', 'the', 'classification', 'for', 'this', 'tweets', 'as', 'i', 'will', 'have', 'a', 'huge', 'number', 'of', 'uninformative', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'do', 'you', 'have', 'any', 'suggestions', 'regarding', 'this', 'issue', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'a', 'java', 'developer', 'and', 'i', 'want', 'to', 'pursue', 'career', 'in', 'data', 'science', 'and', 'machine', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'advise', 'me', 'where', 'and', 'how', 'to', 'begin', 'what', 'subjects', 'and', 'mathematical', 'statistical', 'skills', 'are', 'required', 'and', 'so', 'on', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'problem', 'and', 'i', 'm', 'having', 'trouble', 'representing', 'it', 'first', 'i', 'thought', 'i', 'should', 'use', 'graph', 'theory', 'nodes', 'and', 'edges', 'and', 'now', 'i', 'm', 'not', 'sure', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'data', 'is', 'some', 'tanks', 'names', 'and', 'it', 's', 'volumes', 'those', 'tanks', 'are', 'connected', 'by', 'pipelines', 'which', 'i', 'have', 'the', 'names', 'and', 'length', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'pipeline', 'pipeline', 'pipeline', 'xa', 'xa', 'r', 'tank', 's', 'tank', 's', 'tank', 'pipeline', 's', 'tank', 'xa', 'xa', 's', 'tank', 'xa', 'xa', 'r', 'tank', 'is', 'sink', 'receiver', 'and', 's', 'tank', 'is', 'source', 'sender', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'problem', 'is', 'the', 'pipe', 'names', 'change', 'doesn', 't', 'occur', 'where', 'there', 'is', 'a', 'tank', 'they', 'change', 'name', 'because', 'historical', 'reasons', 'size', 'or', 'connections', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'if', 'i', 'want', 'to', 'graphically', 'show', 'that', 's', 'tank', 'is', 'connected', 'to', 'pipeline', 'at', 'point', 'x', 'and', 'pipeline', 'connects', 'to', 'pipeline', 'and', 'the', 'content', 'goes', 'to', 'r', 'tank', 'how', 'should', 'i', 'do', 'this', 'i', 'think', 'the', 'point', 'x', 'may', 'not', 'be', 'relevant', 'but', 'if', 'i', 'had', 'some', 'way', 'to', 'get', 'the', 'distance', 'travelled', 'would', 'be', 'great', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'couldn', 't', 'quite', 'think', 'of', 'how', 'best', 'to', 'title', 'this', 'so', 'recommendations', 'are', 'welcome', 'same', 'goes', 'for', 'the', 'tags', 'i', 'don', 't', 'have', 'the', 'reputation', 'to', 'use', 'the', 'tags', 'that', 'i', 'thought', 'were', 'appropriate', 'the', 'question', 'is', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'suppose', 'you', 'have', 'n', 'pairs', 'of', 'observations', 'x', 'y', 'and', 'you', 'have', 'a', 'model', 'with', 'some', 'unknown', 'parameters', 'b', 'that', 'estimates', 'the', 'relationship', 'between', 'x', 'and', 'y', 'f', 'x', 'b', 'gt', 'y', 'now', 'suppose', 'you', 'determine', 'b', 'using', 'the', 'method', 'of', 'least', 'squares', 'and', 'implicitly', 'that', 'all', 'the', 'assumptions', 'of', 'least', 'squares', 'are', 'satisfied', 'the', 'parameters', 'b', 'are', 'themselves', 'random', 'variables', 'each', 'with', 'its', 'own', 'variance', 'is', 'there', 'any', 'way', 'to', 'estimate', 'the', 'reduction', 'or', 'increase', 'in', 'the', 'variance', 'of', 'b', 'that', 'would', 'result', 'from', 'applying', 'the', 'same', 'method', 'of', 'least', 'squares', 'to', 'n', 'pairs', 'of', 'observations', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'question', 'is', 'asked', 'in', 'the', 'context', 'of', 'experimentation', 'if', 'each', 'data', 'point', 'costs', 'x', 'an', 'affirmative', 'answer', 'to', 'the', 'question', 'would', 'go', 'a', 'long', 'way', 'in', 'determining', 'whether', 'or', 'not', 'to', 'continue', 'testing', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'large', 'set', 'of', 'data', 'about', 'gb', 'i', 'want', 'to', 'use', 'machine', 'learning', 'to', 'analyze', 'it', 'so', 'i', 'think', 'i', 'should', 'do', 'svd', 'then', 'pca', 'to', 'reduce', 'the', 'data', 'dimension', 'for', 'efficiency', 'but', 'matlab', 'and', 'octave', 'cannot', 'load', 'such', 'a', 'large', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'tools', 'i', 'can', 'use', 'to', 'do', 'svd', 'with', 'such', 'a', 'large', 'amount', 'of', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'going', 'to', 'start', 'a', 'computer', 'science', 'phd', 'this', 'year', 'and', 'for', 'that', 'i', 'need', 'a', 'research', 'topic', 'i', 'am', 'interested', 'in', 'predictive', 'analytics', 'in', 'the', 'context', 'of', 'big', 'data', 'i', 'am', 'interested', 'by', 'the', 'area', 'of', 'education', 'moocs', 'online', 'courses', 'in', 'that', 'field', 'what', 'are', 'the', 'unexplored', 'areas', 'that', 'can', 'help', 'me', 'choose', 'a', 'strong', 'topic', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'matlab', 'is', 'a', 'great', 'tool', 'for', 'some', 'mathematical', 'experiments', 'neural', 'networks', 'image', 'processing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'know', 'if', 'there', 'is', 'such', 'a', 'comprehensive', 'and', 'strong', 'tool', 'for', 'data', 'manipulation', 'and', 'nlp', 'tasks', 'such', 'as', 'tokenization', 'pos', 'tagging', 'parsing', 'training', 'testing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'am', 'new', 'to', 'nlp', 'and', 'i', 'need', 'a', 'tool', 'which', 'let', 'me', 'experiment', 'get', 'familiar', 'and', 'progress', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'are', 'there', 'any', 'general', 'open', 'source', 'programs', 'or', 'libraries', 'e', 'g', 'a', 'python', 'library', 'for', 'analyzing', 'user', 'search', 'behavior', 'by', 'quot', 'search', 'behavior', 'quot', 'i', 'mean', 'a', 'user', 's', 'interaction', 'with', 'a', 'search', 'engine', 'such', 'as', 'querying', 'clicking', 'relevant', 'results', 'and', 'spending', 'time', 'on', 'those', 'results', 'i', 'd', 'like', 'something', 'with', 'the', 'following', 'properties', 'it', 'doesn', 't', 'have', 'to', 'be', 'all', 'of', 'them', 'but', 'the', 'more', 'the', 'merrier', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'models', 'individual', 'user', 'behavior', 'aggregate', 'and', 'time', 'based', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'models', 'group', 'user', 'behavior', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'simulates', 'individual', 'user', 'behavior', 'given', 'a', 'model', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'easily', 'extensible', 'to', 'accept', 'data', 'input', 'formats', 'user', 'models', 'document', 'models', 'etc', 'that', 'end', 'users', 'define', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'links', 'are', 'a', 'plus', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'not', 'sure', 'if', 'this', 'is', 'math', 'stats', 'or', 'data', 'science', 'but', 'i', 'figured', 'i', 'would', 'post', 'it', 'here', 'to', 'get', 'the', 'site', 'used', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'a', 'programmer', 'when', 'you', 'have', 'a', 'system', 'component', 'implemented', 'you', 'might', 'want', 'to', 'allow', 'some', 'performance', 'monitoring', 'for', 'example', 'to', 'query', 'how', 'often', 'a', 'function', 'call', 'was', 'used', 'how', 'long', 'it', 'took', 'and', 'so', 'on', 'so', 'typically', 'you', 'care', 'about', 'count', 'means', 'percentile', 'max', 'min', 'and', 'similiar', 'statistics', 'this', 'could', 'be', 'measurements', 'since', 'startup', 'but', 'also', 'a', 'rolling', 'average', 'or', 'window', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'wonder', 'if', 'there', 'is', 'a', 'good', 'data', 'structure', 'which', 'can', 'be', 'updated', 'efficiently', 'concurrently', 'which', 'can', 'be', 'used', 'as', 'the', 'source', 'for', 'most', 'of', 'those', 'queries', 'for', 'example', 'having', 'a', 'ringbuffer', 'of', 'rollup', 'metrics', 'count', 'sum', 'min', 'max', 'over', 'increasing', 'periods', 'of', 'time', 'and', 'a', 'background', 'aggregate', 'process', 'triggered', 'regularly', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'focus', 'here', 'for', 'me', 'is', 'on', 'in', 'memory', 'data', 'structures', 'with', 'limited', 'memory', 'consumption', 'for', 'other', 'things', 'i', 'would', 'use', 'a', 'rrd', 'type', 'of', 'library', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'post', 'already', 'the', 'question', 'few', 'months', 'ago', 'about', 'my', 'project', 'that', 'i', 'm', 'starting', 'to', 'work', 'on', 'this', 'post', 'can', 'be', 'see', 'here', 'xa', 'lt', 'a', 'href', 'quot', 'https', 'datascience', 'stackexchange', 'com', 'questions', 'human', 'activity', 'recognition', 'using', 'smartphone', 'data', 'set', 'problem', 'quot', 'gt', 'human', 'activity', 'recognition', 'using', 'smartphone', 'data', 'set', 'problem', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'know', 'this', 'is', 'based', 'around', 'multivariate', 'time', 'series', 'analysis', 'and', 'tasks', 'are', 'to', 'classify', 'and', 'cluster', 'the', 'data', 'i', 'have', 'gathered', 'some', 'materials', 'e', 'books', 'tutorials', 'etc', 'on', 'this', 'but', 'still', 'can', 't', 'see', 'a', 'more', 'detailed', 'picture', 'of', 'how', 'even', 'i', 'should', 'start', 'here', 's', 'the', 'tutorial', 'that', 'looks', 'like', 'it', 'might', 'be', 'helpful', 'but', 'the', 'thing', 'is', 'my', 'data', 'looks', 'differently', 'and', 'i', 'm', 'not', 'really', 'sure', 'if', 'this', 'can', 'be', 'applied', 'to', 'my', 'work', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'little', 'book', 'of', 'r', 'for', 'multivariate', 'analysis', 'readthedocs', 'org', 'en', 'latest', 'src', 'multivariateanalysis', 'html', 'scatterplots', 'of', 'the', 'principal', 'components', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'http', 'little', 'book', 'of', 'r', 'for', 'multivariate', 'analysis', 'readthedocs', 'org', 'en', 'latest', 'src', 'multivariateanalysis', 'html', 'scatterplots', 'of', 'the', 'principal', 'components', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'basically', 'my', 'questions', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'i', 'can', 'start', 'on', 'some', 'very', 'basic', 'analysis', 'how', 'to', 'read', 'data', 'so', 'it', 'any', 'meaning', 'for', 'me', 'xa', 'any', 'tips', 'and', 'advises', 'will', 'be', 'much', 'appreciated', 'xa', 'note', 'i', 'm', 'just', 'the', 'beginner', 'in', 'data', 'science', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'implement', 'item', 'based', 'collaborative', 'filtering', 'do', 'any', 'distance', 'calculations', 'allow', 'for', 'weighting', 'of', 'certain', 'ranges', 'of', 'values', 'within', 'each', 'vector', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'i', 'would', 'like', 'to', 'be', 'able', 'to', 'say', 'values', 'within', 'each', 'vector', 'are', 'more', 'significant', 'than', 'values', 'within', 'the', 'range', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'been', 'experimenting', 'with', 'pearson', 'tanimoto', 'and', 'euclidean', 'algorithms', 'but', 'they', 'all', 'seem', 'to', 'assume', 'equal', 'weighting', 'for', 'each', 'value', 'within', 'the', 'vector', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'am', 'i', 'approaching', 'this', 'problem', 'in', 'the', 'right', 'way', 'and', 'if', 'not', 'how', 'do', 'others', 'deal', 'with', 'this', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'the', 'text', 'based', 'semantic', 'clustering', 'emd', 'do', 'xa', 'is', 'there', 'a', 'better', 'way', 'of', 'using', 'lda', 'to', 'detect', 'topics', 'in', 'text', 'there', 'are', 'so', 'provide', 'better', 'results', 'xa', 'i', 'm', 'going', 'to', 'do', 'my', 'emd', 'on', 'discovery', 'topics', 'xa', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'interested', 'in', 'discovering', 'some', 'kind', 'of', 'dis', 'associations', 'between', 'the', 'periods', 'of', 'a', 'time', 'series', 'based', 'on', 'its', 'data', 'e', 'g', 'find', 'some', 'unknown', 'number', 'of', 'periods', 'where', 'the', 'data', 'is', 'not', 'similar', 'with', 'the', 'data', 'from', 'another', 'period', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'i', 'would', 'like', 'to', 'compare', 'the', 'same', 'data', 'but', 'over', 'years', 'something', 'like', 'dtw', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'get', 'my', 'data', 'excel', 'as', 'a', 'two', 'column', 'list', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'c', 'date', 'one', 'per', 'each', 'day', 'of', 'the', 'year', 'c', 'data', 'to', 'analyze', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'what', 'algorithms', 'could', 'i', 'use', 'and', 'in', 'what', 'software', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'update', 'later', 'edit', 'lt', 'strong', 'gt', 'xa', 'i', 'm', 'looking', 'for', 'dates', 'as', 'cut', 'off', 'points', 'from', 'which', 'the', 'datatoanalyze', 'could', 'be', 'part', 'of', 'another', 'cluster', 'of', 'consecutive', 'dates', 'for', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'amp', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'part', 'of', 'lt', 'em', 'gt', 'cluster_', 'lt', 'em', 'gt', 'based', 'on', 'datatoanalyze', 'and', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'amp', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'part', 'of', 'lt', 'em', 'gt', 'cluster_', 'lt', 'em', 'gt', 'based', 'on', 'datatoanalyze', 'and', 'so', 'on', 'so', 'clusters', 'of', 'consecutive', 'dates', 'should', 'be', 'automatically', 'determined', 'based', 'on', 'some', 'algorithms', 'which', 'is', 'what', 'i', 'm', 'looking', 'for', 'which', 'ones', 'or', 'which', 'software', 'would', 'be', 'applicable', 'to', 'this', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'implement', 'gd', 'for', 'standard', 'task', 'of', 'nn', 'training', 'the', 'best', 'papers', 'for', 'practioneer', 'i', 've', 'founded', 'so', 'far', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'efficient', 'backprop', 'quot', 'by', 'yann', 'lecun', 'et', 'al', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'stochastic', 'gradient', 'descent', 'tricks', 'quot', 'by', 'leon', 'bottou', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'some', 'other', 'must', 'read', 'papers', 'on', 'this', 'topic', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'planning', 'to', 'use', 'scikit', 'linear', 'support', 'vector', 'machine', 'svm', 'classifier', 'for', 'text', 'classification', 'on', 'a', 'corpus', 'consisting', 'of', 'million', 'labeled', 'documents', 'what', 'i', 'am', 'planning', 'to', 'do', 'is', 'when', 'a', 'user', 'enters', 'some', 'keyword', 'the', 'classifier', 'will', 'first', 'classify', 'it', 'in', 'a', 'category', 'and', 'then', 'a', 'subsequent', 'information', 'retrieval', 'query', 'will', 'happen', 'in', 'within', 'the', 'documents', 'of', 'that', 'category', 'catagory', 'i', 'have', 'a', 'few', 'questions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'do', 'i', 'confirm', 'that', 'classification', 'will', 'not', 'take', 'much', 'time', 'i', 'don', 't', 'want', 'users', 'to', 'have', 'to', 'spend', 'time', 'waiting', 'for', 'a', 'classification', 'to', 'finish', 'in', 'order', 'to', 'get', 'better', 'results', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'using', 'python', 's', 'scikit', 'library', 'for', 'websites', 'web', 'applications', 'suitable', 'for', 'this', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'does', 'anyone', 'know', 'how', 'amazon', 'or', 'flipkart', 'perform', 'classification', 'on', 'user', 'queries', 'or', 'do', 'they', 'use', 'a', 'completely', 'different', 'logic', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'client', 'that', 'is', 'managing', 'several', 'campaigns', 'however', 'i', 'm', 'not', 'clear', 'what', 'percentage', 'should', 'be', 'applied', 'to', 'each', 'channel', 'that', 'bring', 'traffic', 'to', 'my', 'website', 'when', 'assessing', 'their', 'participation', 'in', 'the', 'objectives', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'those', 'interested', 'here', 'i', 'leave', 'the', 'link', 'to', 'my', 'profile', 'on', 'linkedin', 'lt', 'a', 'href', 'quot', 'https', 'www', 'linkedin', 'com', 'pub', 'mario', 'mu', 'c', 'b', 'oz', 'ahumada', 'b', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'specialist', 'online', 'markegin', 'in', 'bogot', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'below', 'is', 'the', 'dataset', 'where', 'the', 'response', 'variable', 'is', 'play', 'with', 'two', 'labels', 'yes', 'and', 'no', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'no', 'outlook', 'temperature', 'humidity', 'windy', 'play', 'xa', 'sunny', 'hot', 'high', 'false', 'no', 'xa', 'sunny', 'hot', 'high', 'true', 'no', 'xa', 'overcast', 'hot', 'high', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'true', 'no', 'xa', 'overcast', 'cool', 'normal', 'true', 'yes', 'xa', 'sunny', 'mild', 'high', 'false', 'no', 'xa', 'sunny', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'normal', 'false', 'yes', 'xa', 'sunny', 'mild', 'normal', 'true', 'yes', 'xa', 'overcast', 'mild', 'high', 'true', 'yes', 'xa', 'overcast', 'hot', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'true', 'no', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'are', 'the', 'decisions', 'with', 'their', 'respective', 'classifications', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'outlook', 'overcast', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'humidity', 'normal', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'sunny', 'humidity', 'high', 'amp', 'gt', 'play', 'no', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'rainy', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'sunny', 'humidity', 'normal', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'rainy', 'windy', 'true', 'amp', 'gt', 'play', 'no', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'statement', 'of', 'problem', 'an', 'ambulance', 'is', 'at', 'the', 'hospital', 'dropping', 'off', 'a', 'patient', 'the', 'goal', 'of', 'the', 'paramedic', 'is', 'to', 'get', 'released', 'from', 'the', 'hospital', 'as', 'soon', 'as', 'possible', 'i', 'am', 'curious', 'what', 'are', 'the', 'factors', 'in', 'how', 'long', 'an', 'ambulance', 'off', 'loads', 'a', 'patient', 'at', 'the', 'hospital', 'can', 'i', 'predict', 'how', 'long', 'an', 'offload', 'will', 'take', 'given', 'certain', 'variables', 'and', 'how', 'confident', 'can', 'i', 'be', 'in', 'this', 'model', 'the', 'dependent', 'variable', 'is', 'hospitaltime', 'it', 'is', 'a', 'ratio', 'type', 'of', 'data', 'and', 'is', 'measured', 'in', 'seconds', 'the', 'independent', 'variables', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'hospital', 'a', 'nominal', 'type', 'of', 'data', 'recoded', 'into', 'integers', 'would', 'stand', 'xa', 'for', 'lee', 'memorial', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'ambulance', 'a', 'nominal', 'type', 'of', 'data', 'recoded', 'into', 'integers', 'would', 'xa', 'stand', 'for', 'ambulance', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'patientpriority', 'is', 'an', 'ordinal', 'type', 'of', 'data', 'recoded', 'into', 'integers', 'a', 'xa', 'is', 'a', 'high', 'priority', 'is', 'a', 'medium', 'priority', 'and', 'is', 'low', 'acuity', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'monthofcall', 'is', 'an', 'interval', 'type', 'of', 'data', 'recoded', 'into', 'integers', 'a', 'xa', 'would', 'be', 'june', 'and', 'is', 'december', 'a', 'december', 'is', 'not', 'twice', 'as', 'xa', 'much', 'as', 'a', 'june', 'in', 'this', 'case', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'hourofcall', 'is', 'an', 'interval', 'type', 'of', 'data', 'recoded', 'into', 'integers', 'once', 'xa', 'again', 'an', 'offload', 'happening', 'at', 'pm', 'is', 'not', 'more', 'than', 'something', 'xa', 'happening', 'at', 'am', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'officer', 'and', 'officer', 'are', 'nominal', 'data', 'and', 'are', 'integers', 'representing', 'xa', 'an', 'emt', 'and', 'a', 'paramedic', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'this', 'given', 'this', 'type', 'of', 'data', 'and', 'my', 'goal', 'to', 'predict', 'the', 'off', 'loading', 'time', 'at', 'the', 'hospital', 'what', 'kind', 'of', 'regression', 'model', 'should', 'i', 'look', 'into', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'looked', 'at', 'my', 'statistics', 'books', 'from', 'university', 'days', 'and', 'they', 'are', 'all', 'using', 'ratio', 'data', 'my', 'data', 'is', 'mixed', 'with', 'nominal', 'ordinal', 'interval', 'and', 'ratio', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'as', 'much', 'data', 'as', 'you', 'could', 'ask', 'for', 'i', 'have', 'at', 'least', 'observations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'you', 'please', 'push', 'me', 'in', 'the', 'right', 'direction', 'what', 'kind', 'of', 'model', 'should', 'i', 'use', 'with', 'this', 'type', 'of', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'shown', 'below', 'are', 'observations', 'to', 'give', 'you', 'a', 'tiny', 'peek', 'at', 'my', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'incidentid', 'hospitaltime', 'hospital', 'ambulance', 'patientpriority', 'monthofcall', 'hourofcall', 'officer', 'officer', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'chr', 'chr', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'p', 's', 'this', 'question', 'is', 'cross', 'posted', 'in', 'stack', 'overflow', 'under', 'the', 'same', 'title', 'and', 'author', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'dataset', 'that', 'i', 'am', 'experimenting', 'with', 'is', 'in', 'the', 'form', 'of', 'a', 'table', 'with', 'columns', 'userid', 'and', 'itemid', 'if', 'there', 'is', 'a', 'row', 'for', 'a', 'given', 'user', 'and', 'a', 'given', 'item', 'that', 'means', 'the', 'user', 'accessed', 'the', 'item', 'like', 'in', 'an', 'online', 'store', 'i', 'am', 'trying', 'to', 'cluster', 'similar', 'items', 'based', 'on', 'this', 'data', 'if', 'a', 'pair', 'of', 'items', 'is', 'accessed', 'together', 'often', 'then', 'the', 'items', 'are', 'similar', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'because', 'this', 'is', 'a', 'case', 'of', 'a', 'high', 'dimensionality', 'of', 'users', 'and', 'items', 'will', 'be', 'in', 's', 'i', 'think', 'i', 'am', 'justified', 'in', 'trying', 'to', 'use', 'svd', 'as', 'a', 'pre', 'clustering', 'step', 'and', 'then', 'do', 'some', 'classical', 'clustering', 'when', 'i', 'tried', 'doing', 'this', 'i', 'got', 'poor', 'clustering', 'results', 'when', 'compared', 'with', 'simple', 'hierarchical', 'clustering', 'items', 'that', 'weren', 't', 'very', 'similar', 'were', 'being', 'bucketed', 'together', 'in', 'one', 'dimension', 'while', 'there', 'were', 'available', 'dimensions', 'that', 'weren', 't', 'used', 'the', 'results', 'weren', 't', 'completely', 'random', 'but', 'they', 'were', 'definitely', 'worse', 'than', 'the', 'output', 'from', 'the', 'hierarchical', 'clustering', 'i', 'attempted', 'the', 'svd', 'step', 'with', 'mahaut', 'and', 'octave', 'and', 'the', 'results', 'were', 'similar', 'for', 'the', 'hierarchical', 'clustering', 'i', 'used', 'the', 'jaccard', 'measure', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'at', 'this', 'point', 'i', 'am', 'starting', 'to', 'doubt', 'the', 'notion', 'of', 'svd', 'as', 'a', 'way', 'to', 'reduce', 'dimensionality', 'do', 'you', 'think', 'that', 'svd', 'cannot', 'be', 'used', 'effectively', 'in', 'this', 'case', 'and', 'why', 'or', 'do', 'you', 'think', 'that', 'i', 'made', 'some', 'mistake', 'along', 'the', 'way', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'several', 'datasets', 'with', 'thousands', 'of', 'variables', 'this', 'different', 'datasets', 'have', 'different', 'variables', 'for', 'the', 'same', 'thing', 'is', 'there', 'a', 'way', 'to', 'automatically', 'semi', 'automatically', 'check', 'compatible', 'variables', 'and', 'make', 'them', 'consistent', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'there', 'is', 'such', 'thing', 'that', 'would', 'save', 'me', 'months', 'of', 'tedious', 'work', 'the', 'data', 'is', 'stored', 'in', 'spss', 'format', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'coding', 'a', 'program', 'that', 'tests', 'several', 'classifiers', 'over', 'a', 'database', 'weather', 'arff', 'i', 'found', 'rules', 'below', 'i', 'want', 'classify', 'test', 'objects', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'do', 'not', 'understand', 'how', 'the', 'classification', 'it', 'is', 'described', 'xa', 'quot', 'in', 'classification', 'let', 'r', 'be', 'the', 'set', 'of', 'generated', 'rules', 'and', 't', 'the', 'training', 'data', 'the', 'basic', 'idea', 'of', 'the', 'proposed', 'method', 'is', 'to', 'choose', 'a', 'set', 'of', 'high', 'confidence', 'rules', 'in', 'r', 'to', 'cover', 't', 'in', 'classifying', 'a', 'test', 'object', 'the', 'first', 'rule', 'in', 'the', 'set', 'of', 'rules', 'that', 'matches', 'the', 'test', 'object', 'condition', 'classifies', 'it', 'this', 'process', 'ensures', 'that', 'only', 'the', 'highest', 'ranked', 'rules', 'classify', 'test', 'objects', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'classify', 'test', 'objects', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'no', 'outlook', 'temperature', 'humidity', 'windy', 'play', 'xa', 'sunny', 'hot', 'high', 'false', 'no', 'xa', 'sunny', 'hot', 'high', 'true', 'no', 'xa', 'overcast', 'hot', 'high', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'true', 'no', 'xa', 'overcast', 'cool', 'normal', 'true', 'yes', 'xa', 'sunny', 'mild', 'high', 'false', 'no', 'xa', 'sunny', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'normal', 'false', 'yes', 'xa', 'sunny', 'mild', 'normal', 'true', 'yes', 'xa', 'overcast', 'mild', 'high', 'true', 'yes', 'xa', 'overcast', 'hot', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'true', 'no', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'rule', 'found', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'outlook', 'overcast', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'humidity', 'normal', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'sunny', 'humidity', 'high', 'amp', 'gt', 'play', 'no', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'rainy', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'sunny', 'humidity', 'normal', 'amp', 'gt', 'play', 'yes', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'xa', 'outlook', 'rainy', 'windy', 'true', 'amp', 'gt', 'play', 'no', 'xa', 'support', 'confidence', 'correctly', 'classify', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'xa', 'dung', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'many', 'discussions', 'of', 'missing', 'data', 'in', 'supervised', 'and', 'unsupervised', 'learning', 'deal', 'with', 'various', 'methods', 'of', 'imputation', 'like', 'mean', 'values', 'or', 'em', 'but', 'in', 'some', 'cases', 'the', 'data', 'will', 'be', 'missing', 'as', 'a', 'necessary', 'consequence', 'of', 'the', 'data', 'generation', 'process', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'let', 's', 'say', 'i', 'm', 'trying', 'to', 'predict', 'students', 'grades', 'and', 'one', 'of', 'the', 'inputs', 'i', 'want', 'to', 'analyze', 'is', 'the', 'average', 'grades', 'of', 'the', 'student', 's', 'siblings', 'if', 'a', 'particular', 'student', 'is', 'an', 'only', 'child', 'then', 'that', 'value', 'will', 'be', 'missing', 'not', 'because', 'we', 'failed', 'to', 'collect', 'the', 'data', 'but', 'because', 'logically', 'there', 'is', 'no', 'data', 'to', 'collect', 'this', 'is', 'distinct', 'from', 'cases', 'where', 'the', 'student', 'has', 'siblings', 'but', 'we', 'can', 't', 'find', 'their', 'grades', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'other', 'examples', 'abound', 'say', 'we', 're', 'in', 'college', 'admissions', 'and', 'we', 'want', 'to', 'include', 'students', 'ap', 'exam', 'results', 'but', 'not', 'all', 'students', 'took', 'ap', 'exams', 'or', 'we', 're', 'looking', 'at', 'social', 'network', 'data', 'but', 'not', 'all', 'subjects', 'have', 'facebook', 'and', 'or', 'twitter', 'accounts', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'these', 'data', 'are', 'missing', 'but', 'they', 're', 'certainly', 'not', 'missing', 'at', 'random', 'and', 'many', 'algorithms', 'such', 'as', 'all', 'supervised', 'learning', 'packages', 'in', 'scikit', 'learn', 'simply', 'demand', 'that', 'there', 'be', 'no', 'missing', 'values', 'at', 'all', 'in', 'the', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'have', 'people', 'dealt', 'with', 'this', 'in', 'the', 'past', 'and', 'what', 'off', 'the', 'shelf', 'solutions', 'are', 'there', 'for', 'instance', 'i', 'believe', 'the', 'gradient', 'boosting', 'algorithm', 'in', 'r', 'uses', 'trees', 'with', 'three', 'possible', 'branches', 'left', 'right', 'and', 'missing', 'any', 'other', 'alternatives', 'out', 'there', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'made', 'a', 'naive', 'bayes', 'classifier', 'that', 'uses', 'the', 'bag', 'of', 'words', 'technique', 'to', 'classify', 'spam', 'posts', 'on', 'a', 'message', 'board', 'it', 'works', 'but', 'i', 'think', 'i', 'could', 'get', 'much', 'better', 'results', 'if', 'my', 'models', 'considered', 'the', 'word', 'orderings', 'and', 'phrases', 'ex', 'girls', 'and', 'live', 'may', 'not', 'trigger', 'a', 'high', 'spam', 'score', 'even', 'though', 'live', 'girls', 'is', 'most', 'likely', 'junk', 'how', 'can', 'i', 'build', 'a', 'model', 'that', 'takes', 'word', 'ordering', 'into', 'account', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'considered', 'storing', 'n', 'grams', 'check', 'out', 'these', 'out', 'these', 'live', 'these', 'live', 'girls', 'but', 'this', 'seems', 'to', 'radically', 'increase', 'the', 'size', 'of', 'the', 'dictionary', 'i', 'keep', 'score', 'in', 'and', 'causes', 'inconsistency', 'as', 'phrases', 'with', 'very', 'similar', 'wording', 'but', 'different', 'order', 'will', 'slip', 'through', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'not', 'tied', 'to', 'bayesian', 'classification', 'but', 'i', 'd', 'like', 'something', 'that', 'someone', 'without', 'a', 'strong', 'background', 'in', 'statistics', 'could', 'grok', 'and', 'implement', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'do', 'some', 'data', 'mining', 'and', 'nlp', 'experiments', 'to', 'do', 'some', 'research', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'decided', 'to', 'use', 'nltk', 'or', 'related', 'tools', 'and', 'software', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'environment', 'or', 'operating', 'system', 'do', 'you', 'suggest', 'for', 'my', 'purpose', 'i', 'mean', 'doing', 'research', 'on', 'nlp', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'windows', 'or', 'linux', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'a', 'user', 'of', 'windows', 'but', 'i', 'thought', 'if', 'linux', 'has', 'better', 'shell', 'and', 'related', 'software', 'for', 'nlp', 'tasks', 'then', 'i', 'switch', 'to', 'linux', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'is', 'your', 'experience', 'and', 'your', 'preferred', 'os', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'nltk', 'is', 'in', 'python', 'i', 'thought', 'python', 'is', 'a', 'good', 'language', 'for', 'my', 'purpose', 'do', 'you', 'suggest', 'python', 'too', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'next', 'week', 'i', 'm', 'going', 'to', 'begin', 'prototyping', 'a', 'recommendation', 'engine', 'for', 'work', 'i', 've', 'implemented', 'completed', 'the', 'netflix', 'challenge', 'in', 'java', 'before', 'for', 'college', 'but', 'have', 'no', 'real', 'idea', 'what', 'to', 'use', 'for', 'a', 'production', 'enterprise', 'level', 'recommendation', 'engine', 'taking', 'into', 'consideration', 'everything', 'from', 'a', 'standalone', 'programming', 'language', 'to', 'things', 'like', 'apache', 'mahout', 'and', 'neo', 'j', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'how', 'to', 'proceed', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'okay', 'here', 'is', 'the', 'background', 'xa', 'i', 'am', 'doing', 'text', 'mining', 'and', 'my', 'basic', 'flow', 'is', 'like', 'this', 'xa', 'extract', 'feature', 'n', 'gram', 'reduce', 'feature', 'count', 'score', 'tf', 'idf', 'and', 'classify', 'for', 'my', 'own', 'sake', 'i', 'am', 'doing', 'comparison', 'between', 'svm', 'and', 'neural', 'network', 'classifiers', 'here', 'is', 'the', 'weird', 'part', 'or', 'am', 'i', 'wrong', 'and', 'this', 'is', 'reasonable', 'if', 'i', 'use', 'gram', 'the', 'classifiers', 'result', 'accuracy', 'precision', 'is', 'different', 'and', 'the', 'svm', 'is', 'the', 'better', 'one', 'but', 'when', 'i', 'use', 'gram', 'the', 'results', 'are', 'exactly', 'the', 'same', 'what', 'causes', 'this', 'is', 'there', 'any', 'explanation', 'is', 'it', 'the', 'case', 'of', 'very', 'separable', 'classes', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'a', 'supervised', 'learning', 'algorithm', 'that', 'can', 'take', 'd', 'data', 'for', 'input', 'and', 'output', 'as', 'an', 'example', 'of', 'something', 'similar', 'to', 'my', 'data', 'consider', 'a', 'black', 'image', 'with', 'some', 'sparse', 'white', 'dots', 'blur', 'that', 'image', 'using', 'a', 'full', 'range', 'of', 'grayscale', 'then', 'create', 'a', 'machine', 'that', 'can', 'take', 'the', 'blurred', 'image', 'as', 'input', 'and', 'produce', 'the', 'original', 'sharp', 'image', 'as', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'could', 'make', 'some', 'sample', 'd', 'data', 'by', 'taking', 'a', 'region', 'radius', 'around', 'the', 'original', 'sharp', 'point', 'but', 'i', 'don', 't', 'know', 'the', 'exact', 'radius', 'it', 'would', 'be', 'significant', 'data', 'duplication', 'and', 'a', 'lot', 'of', 'guessing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'good', 'algorithm', 'suggestions', 'for', 'this', 'problem', 'thanks', 'for', 'your', 'time', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'hands', 'on', 'researcher', 'and', 'i', 'like', 'testing', 'out', 'viable', 'solutions', 'so', 'i', 'tend', 'to', 'run', 'a', 'lot', 'of', 'experiments', 'for', 'example', 'if', 'i', 'am', 'calculating', 'a', 'similarity', 'score', 'between', 'documents', 'i', 'might', 'want', 'to', 'try', 'out', 'many', 'measures', 'in', 'fact', 'for', 'each', 'measure', 'i', 'might', 'need', 'to', 'make', 'several', 'runs', 'to', 'test', 'the', 'effect', 'of', 'some', 'parameters', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'far', 'i', 've', 'been', 'tracking', 'the', 'runs', 'inputs', 'and', 'their', 'results', 'by', 'writing', 'out', 'the', 'results', 'into', 'files', 'with', 'as', 'much', 'info', 'about', 'the', 'inputs', 'the', 'problem', 'is', 'that', 'retrieving', 'a', 'specific', 'result', 'becomes', 'a', 'challenge', 'sometimes', 'even', 'if', 'i', 'try', 'to', 'add', 'the', 'input', 'info', 'to', 'th', 'filename', 'i', 'tried', 'using', 'a', 'spreadsheet', 'with', 'links', 'to', 'results', 'but', 'this', 'isn', 't', 'making', 'a', 'huge', 'difference', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'tools', 'process', 'do', 'you', 'use', 'for', 'the', 'book', 'keeping', 'of', 'your', 'experiments', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'year', 'old', 'it', 'professional', 'who', 'is', 'purely', 'technical', 'i', 'am', 'good', 'at', 'programming', 'learning', 'new', 'technologies', 'understanding', 'them', 'and', 'implementing', 'i', 'did', 'not', 'like', 'mathematics', 'at', 'school', 'so', 'i', 'didn', 't', 'score', 'well', 'in', 'mathematics', 'i', 'am', 'very', 'much', 'interested', 'in', 'pursuing', 'a', 'career', 'in', 'big', 'data', 'analytics', 'i', 'am', 'more', 'interested', 'in', 'analytics', 'rather', 'than', 'big', 'data', 'technologies', 'hadoop', 'etc', 'though', 'i', 'do', 'not', 'dislike', 'it', 'however', 'when', 'i', 'look', 'around', 'in', 'the', 'internet', 'i', 'see', 'that', 'people', 'who', 'are', 'good', 'in', 'analytics', 'data', 'scientists', 'are', 'mainly', 'mathematics', 'graduates', 'who', 'have', 'done', 'their', 'phds', 'and', 'sound', 'like', 'intelligent', 'creatures', 'who', 'are', 'far', 'far', 'ahead', 'of', 'me', 'i', 'get', 'scared', 'sometimes', 'to', 'think', 'whether', 'my', 'decision', 'is', 'correct', 'because', 'learning', 'advance', 'statistics', 'on', 'your', 'own', 'is', 'very', 'tough', 'and', 'requires', 'a', 'of', 'hard', 'work', 'and', 'time', 'investment', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'know', 'whether', 'my', 'decision', 'is', 'correct', 'or', 'should', 'i', 'leave', 'this', 'piece', 'of', 'work', 'to', 'only', 'intellectuals', 'who', 'have', 'spend', 'their', 'life', 'in', 'studying', 'in', 'prestigious', 'colleges', 'and', 'earned', 'their', 'degrees', 'and', 'phds', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'curious', 'if', 'anyone', 'else', 'has', 'run', 'into', 'this', 'i', 'have', 'a', 'data', 'set', 'with', 'about', 'k', 'samples', 'each', 'with', 'k', 'sparse', 'features', 'the', 'sparse', 'fill', 'rate', 'is', 'about', 'the', 'data', 'is', 'stored', 'in', 'a', 'lt', 'code', 'gt', 'scipy', 'sparse', 'csr', 'csr_matrix', 'lt', 'code', 'gt', 'object', 'with', 'lt', 'code', 'gt', 'dtype', 'numpy', 'float', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'using', 'this', 'as', 'an', 'input', 'to', 'sklearn', 's', 'logistic', 'regression', 'classifier', 'the', 'lt', 'a', 'href', 'quot', 'http', 'scikit', 'learn', 'org', 'stable', 'modules', 'generated', 'sklearn', 'linear_model', 'logisticregression', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'documentation', 'lt', 'a', 'gt', 'indicates', 'that', 'sparse', 'csr', 'matrices', 'are', 'acceptable', 'inputs', 'to', 'this', 'classifier', 'however', 'when', 'i', 'train', 'the', 'classifier', 'i', 'get', 'extremely', 'bad', 'memory', 'performance', 'the', 'memory', 'usage', 'of', 'my', 'process', 'explodes', 'from', 'mb', 'to', 'fill', 'all', 'the', 'available', 'memory', 'and', 'then', 'everything', 'grinds', 'to', 'a', 'halt', 'as', 'memory', 'swapping', 'to', 'disk', 'takes', 'over', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anyone', 'know', 'why', 'this', 'classifier', 'might', 'expand', 'the', 'sparse', 'matrix', 'to', 'a', 'dense', 'matrix', 'i', 'm', 'using', 'the', 'default', 'parameters', 'for', 'the', 'classifier', 'at', 'the', 'moment', 'within', 'an', 'updated', 'anacoda', 'distribution', 'thanks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'scipy', '__version__', 'xa', 'sklearn', '__version__', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'product', 'purchase', 'count', 'data', 'which', 'looks', 'likes', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'user', 'item', 'item', 'xa', 'a', 'xa', 'b', 'xa', 'c', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'these', 'data', 'are', 'imported', 'into', 'lt', 'code', 'gt', 'python', 'lt', 'code', 'gt', 'using', 'lt', 'code', 'gt', 'numpy', 'genfromtxt', 'lt', 'code', 'gt', 'now', 'i', 'want', 'to', 'process', 'it', 'to', 'get', 'the', 'correlation', 'between', 'lt', 'code', 'gt', 'item', 'lt', 'code', 'gt', 'purchase', 'amount', 'and', 'lt', 'code', 'gt', 'item', 'lt', 'code', 'gt', 'purchase', 'amount', 'basically', 'for', 'each', 'value', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'of', 'lt', 'code', 'gt', 'item', 'lt', 'code', 'gt', 'i', 'want', 'to', 'find', 'all', 'the', 'users', 'who', 'bought', 'lt', 'code', 'gt', 'item', 'lt', 'code', 'gt', 'in', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'quantity', 'then', 'average', 'the', 'lt', 'code', 'gt', 'item', 'lt', 'code', 'gt', 'over', 'the', 'same', 'users', 'what', 'is', 'the', 'best', 'way', 'to', 'do', 'this', 'i', 'can', 'do', 'this', 'by', 'using', 'lt', 'code', 'gt', 'for', 'lt', 'code', 'gt', 'loops', 'but', 'i', 'thought', 'there', 'might', 'be', 'something', 'more', 'efficient', 'than', 'that', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'figure', 'out', 'a', 'good', 'and', 'fast', 'solution', 'to', 'the', 'following', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'two', 'models', 'i', 'm', 'working', 'with', 'let', 's', 'call', 'them', 'players', 'and', 'teams', 'a', 'player', 'can', 'be', 'on', 'multiple', 'teams', 'and', 'a', 'team', 'can', 'have', 'multiple', 'players', 'i', 'm', 'working', 'on', 'creating', 'a', 'ui', 'element', 'on', 'a', 'form', 'that', 'allows', 'a', 'user', 'to', 'select', 'multiple', 'teams', 'checkboxes', 'as', 'the', 'user', 'is', 'selecting', 'or', 'deselecting', 'teams', 'i', 'd', 'like', 'to', 'display', 'the', 'teams', 'grouped', 'by', 'the', 'players', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'for', 'examples', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'if', 'the', 'selected', 'teams', 'have', 'no', 'players', 'that', 'intersect', 'each', 'team', 'would', 'have', 'its', 'own', 'section', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'if', 'the', 'user', 'selects', 'two', 'teams', 'and', 'they', 'have', 'the', 'same', 'players', 'there', 'would', 'be', 'one', 'section', 'containing', 'the', 'names', 'of', 'the', 'two', 'teams', 'and', 'all', 'the', 'players', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'lt', 'p', 'gt', 'if', 'team_a', 'has', 'players', 'and', 'team_b', 'has', 'players', 'there', 'would', 'be', 'the', 'following', 'sections', 'section_x', 'team_a', 'team_b', 'section_y', 'team_a', 'section', '_z', 'team_b', 'lt', 'p', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'hope', 'that', 's', 'clear', 'essentially', 'i', 'want', 'to', 'find', 'the', 'teams', 'that', 'players', 'have', 'in', 'common', 'and', 'group', 'by', 'that', 'i', 'was', 'thinking', 'maybe', 'there', 'is', 'a', 'way', 'to', 'do', 'this', 'by', 'navigating', 'a', 'bipartite', 'graph', 'not', 'exactly', 'sure', 'how', 'though', 'and', 'i', 'might', 'be', 'overthinking', 'it', 'i', 'was', 'hoping', 'to', 'do', 'this', 'by', 'creating', 'some', 'type', 'of', 'data', 'structure', 'on', 'the', 'server', 'and', 'using', 'it', 'on', 'the', 'client', 'i', 'would', 'love', 'to', 'hear', 'your', 'suggestions', 'and', 'i', 'appreciate', 'any', 'help', 'you', 'can', 'give', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lets', 'say', 'i', 'have', 'a', 'database', 'of', 'users', 'who', 'rate', 'different', 'products', 'on', 'a', 'scale', 'of', 'our', 'recommendation', 'engine', 'recommends', 'products', 'to', 'users', 'based', 'on', 'the', 'preferences', 'of', 'other', 'users', 'who', 'are', 'highly', 'similar', 'my', 'first', 'approach', 'to', 'finding', 'similar', 'users', 'was', 'to', 'use', 'cosine', 'similarity', 'and', 'just', 'treat', 'user', 'ratings', 'as', 'vector', 'components', 'the', 'main', 'problem', 'with', 'this', 'approach', 'is', 'that', 'it', 'just', 'measures', 'vector', 'angles', 'and', 'doesn', 't', 'take', 'rating', 'scale', 'or', 'magnitude', 'into', 'consideration', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'my', 'question', 'is', 'this', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'are', 'there', 'any', 'drawbacks', 'to', 'lt', 'strong', 'gt', 'lt', 'strong', 'gt', 'just', 'using', 'the', 'percentage', 'difference', 'between', 'the', 'vector', 'components', 'of', 'two', 'vectors', 'as', 'a', 'measure', 'of', 'similarity', 'lt', 'strong', 'gt', 'what', 'disadvantages', 'if', 'any', 'would', 'i', 'encounter', 'if', 'i', 'used', 'that', 'method', 'instead', 'of', 'cosine', 'similarity', 'or', 'euclidean', 'distance', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'for', 'example', 'why', 'not', 'just', 'do', 'this', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'n', 'stars', 'xa', 'a', 'xa', 'b', 'xa', 'xa', 'similarity', 'a', 'b', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'instead', 'of', 'cosine', 'similarity', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'a', 'xa', 'b', 'xa', 'xa', 'cossimilarity', 'a', 'b', 'xa', 'sqrt', 'sqrt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'how', 'to', 'get', 'the', 'polysemes', 'of', 'a', 'word', 'in', 'wordnet', 'or', 'any', 'other', 'api', 'i', 'am', 'looking', 'for', 'any', 'api', 'with', 'java', 'any', 'idea', 'is', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'know', 'there', 'is', 'the', 'normal', 'lt', 'em', 'gt', 'subtract', 'the', 'mean', 'and', 'divide', 'by', 'the', 'standard', 'deviation', 'lt', 'em', 'gt', 'for', 'standardizing', 'your', 'data', 'but', 'i', 'm', 'interested', 'to', 'know', 'if', 'there', 'are', 'more', 'appropriate', 'methods', 'for', 'this', 'kind', 'of', 'discrete', 'data', 'consider', 'the', 'following', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'items', 'that', 'have', 'been', 'ranked', 'by', 'customers', 'first', 'items', 'were', 'ranked', 'on', 'a', 'scale', 'others', 'are', 'and', 'to', 'transform', 'everything', 'to', 'a', 'to', 'scale', 'is', 'there', 'another', 'method', 'better', 'suited', 'for', 'this', 'case', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'the', 'data', 'has', 'a', 'central', 'tendency', 'then', 'the', 'standard', 'would', 'work', 'fine', 'but', 'what', 'about', 'when', 'you', 'have', 'more', 'of', 'a', 'halo', 'effect', 'or', 'some', 'more', 'exponential', 'distribution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'do', 'movement', 'building', 'work', 'for', 'effective', 'altruism', 'lt', 'a', 'href', 'quot', 'http', 'en', 'm', 'wikipedia', 'org', 'wiki', 'effective_altruism', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'en', 'm', 'wikipedia', 'org', 'wiki', 'effective_altruism', 'lt', 'a', 'gt', 'and', 'would', 'like', 'to', 'level', 'up', 'our', 'growth', 'strategy', 'it', 'occurred', 'to', 'me', 'that', 'a', 'social', 'network', 'visualization', 'tool', 'which', 'allowed', 'us', 'to', 'strategically', 'find', 'and', 'recruit', 'new', 'influencers', 'donors', 'would', 'be', 'mega', 'useful', 'i', 'd', 'love', 'to', 'find', 'something', 'preferably', 'free', 'similar', 'to', 'inmaps', 'which', 'would', 'allow', 'us', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'combine', 'all', 'of', 'our', 'social', 'media', 'connections', 'into', 'a', 'single', 'map', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'easily', 'see', 'who', 'the', 'superconnectors', 'are', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'weight', 'each', 'person', 'by', 'their', 'degree', 'of', 'social', 'influence', 'perhaps', 'some', 'function', 'of', 'things', 'like', 'klout', 'score', 'amount', 'of', 'social', 'media', 'connections', 'number', 'of', 'google', 'mentions', 'etc', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'such', 'a', 'thing', 'exist', 'if', 'not', 'is', 'anyone', 'interested', 'in', 'pro', 'bono', 'work', 'for', 'an', 'amazing', 'cause', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'disclaimer', 'i', 'am', 'a', 'data', 'science', 'noob', 'so', 'preferably', 'the', 'solution', 'would', 'be', 'one', 'with', 'a', 'nice', 'gui', 'and', 'minimal', 'involvement', 'of', 'r', 'or', 'python', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'suppose', 'for', 'example', 'that', 'the', 'first', 'search', 'result', 'on', 'a', 'page', 'of', 'google', 'search', 'results', 'is', 'swapped', 'with', 'the', 'second', 'result', 'how', 'much', 'would', 'this', 'change', 'the', 'click', 'through', 'probabilities', 'of', 'the', 'two', 'results', 'how', 'much', 'would', 'its', 'click', 'through', 'probability', 'drop', 'if', 'the', 'fifth', 'search', 'result', 'was', 'swapped', 'with', 'the', 'sixth', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'we', 'say', 'something', 'with', 'some', 'level', 'of', 'assurance', 'about', 'how', 'expected', 'click', 'through', 'probabilities', 'change', 'if', 'we', 'do', 'these', 'types', 'of', 'pairwise', 'swaps', 'within', 'pages', 'of', 'search', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'we', 'seek', 'is', 'a', 'measure', 'of', 'the', 'contribution', 'to', 'click', 'through', 'rates', 'made', 'specifically', 'by', 'position', 'bias', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'likely', 'how', 'position', 'ranking', 'would', 'affect', 'the', 'sales', 'in', 'amazon', 'or', 'other', 'online', 'shopping', 'website', 'if', 'we', 'cast', 'the', 'sales', 'into', 'two', 'parts', 'the', 'product', 'quality', 'and', 'its', 'ranking', 'effect', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'xa', 'sales', 'alpha', 'quality', 'beta', 'position', 'epsilon', 'xa', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'we', 'quantify', 'the', 'beta', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'when', 'ml', 'algorithms', 'e', 'g', 'vowpal', 'wabbit', 'or', 'some', 'of', 'the', 'factorization', 'machines', 'winning', 'click', 'through', 'rate', 'competitions', 'lt', 'a', 'href', 'quot', 'https', 'www', 'kaggle', 'com', 'c', 'criteo', 'display', 'ad', 'challenge', 'forums', 't', 'idiots', 'solution', 'post', 'quot', 'gt', 'kaggle', 'lt', 'a', 'gt', 'mention', 'that', 'features', 'are', 'hashed', 'what', 'does', 'that', 'actually', 'mean', 'for', 'the', 'model', 'lets', 'say', 'there', 'is', 'a', 'variable', 'that', 'represents', 'the', 'id', 'of', 'an', 'internet', 'add', 'which', 'takes', 'on', 'values', 'such', 'as', 'bg', 'then', 'i', 'understand', 'that', 'this', 'feature', 'is', 'hashed', 'to', 'a', 'random', 'integer', 'but', 'my', 'question', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'the', 'integer', 'now', 'used', 'in', 'the', 'model', 'as', 'an', 'integer', 'numeric', 'or', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'the', 'hashed', 'value', 'actually', 'still', 'treated', 'like', 'a', 'categorical', 'variable', 'and', 'one', 'hot', 'encoded', 'thus', 'the', 'hashing', 'trick', 'is', 'just', 'to', 'save', 'space', 'somehow', 'with', 'large', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'let', 's', 'assume', 'that', 'i', 'want', 'to', 'train', 'a', 'stochastic', 'gradient', 'descent', 'regression', 'algorithm', 'using', 'a', 'dataset', 'that', 'has', 'n', 'samples', 'since', 'the', 'size', 'of', 'the', 'dataset', 'is', 'fixed', 'i', 'will', 'reuse', 'the', 'data', 't', 'times', 'at', 'each', 'iteration', 'or', 'quot', 'epoch', 'quot', 'i', 'use', 'each', 'training', 'sample', 'exactly', 'once', 'after', 'randomly', 'reordering', 'the', 'whole', 'training', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'implementation', 'is', 'based', 'on', 'python', 'and', 'numpy', 'therefore', 'using', 'vector', 'operations', 'can', 'remarkably', 'decrease', 'computation', 'time', 'coming', 'up', 'with', 'a', 'vectorized', 'implementation', 'of', 'batch', 'gradient', 'descent', 'is', 'quite', 'straightforward', 'however', 'in', 'the', 'case', 'of', 'stochastic', 'gradient', 'descent', 'i', 'can', 'not', 'figure', 'out', 'how', 'to', 'avoid', 'the', 'outer', 'loop', 'that', 'iterates', 'through', 'all', 'the', 'samples', 'at', 'each', 'epoch', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anybody', 'know', 'any', 'vectorized', 'implementation', 'of', 'stochastic', 'gradient', 'descent', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'i', 've', 'been', 'asked', 'why', 'would', 'i', 'like', 'to', 'use', 'online', 'gradient', 'descent', 'if', 'the', 'size', 'of', 'my', 'dataset', 'is', 'fixed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'from', 'one', 'can', 'see', 'that', 'online', 'gradient', 'descent', 'converges', 'slower', 'than', 'batch', 'gradient', 'descent', 'to', 'the', 'minimum', 'of', 'the', 'empirical', 'cost', 'however', 'it', 'converges', 'faster', 'to', 'the', 'minimum', 'of', 'the', 'expected', 'cost', 'which', 'measures', 'generalization', 'performance', 'i', 'd', 'like', 'to', 'test', 'the', 'impact', 'of', 'these', 'theoretical', 'results', 'in', 'my', 'particular', 'problem', 'by', 'means', 'of', 'cross', 'validation', 'without', 'a', 'vectorized', 'implementation', 'my', 'online', 'gradient', 'descent', 'code', 'is', 'much', 'slower', 'than', 'the', 'batch', 'gradient', 'descent', 'one', 'that', 'remarkably', 'increases', 'the', 'time', 'it', 'takes', 'to', 'the', 'cross', 'validation', 'process', 'to', 'be', 'completed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'edit', 'lt', 'strong', 'gt', 'i', 'include', 'here', 'the', 'pseudocode', 'of', 'my', 'on', 'line', 'gradient', 'descent', 'implementation', 'as', 'requested', 'by', 'ffriend', 'i', 'am', 'solving', 'a', 'regression', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'method', 'on', 'line', 'gradient', 'descent', 'regression', 'xa', 'input', 'x', 'nxp', 'matrix', 'each', 'line', 'contains', 'a', 'training', 'sample', 'represented', 'as', 'a', 'length', 'p', 'vector', 'y', 'length', 'n', 'vector', 'output', 'of', 'the', 'training', 'samples', 'xa', 'output', 'a', 'length', 'p', 'vector', 'of', 'coefficients', 'xa', 'xa', 'initialize', 'coefficients', 'assign', 'value', 'to', 'all', 'coefficients', 'xa', 'calculate', 'outputs', 'f', 'xa', 'prev_error', 'inf', 'xa', 'error', 'sum', 'f', 'y', 'n', 'xa', 'it', 'xa', 'while', 'abs', 'error', 'prev_error', 'amp', 'gt', 'error_threshold', 'and', 'it', 'amp', 'lt', 'max_iterations', 'xa', 'randomly', 'shuffle', 'training', 'samples', 'xa', 'for', 'each', 'training', 'sample', 'i', 'xa', 'compute', 'error', 'for', 'training', 'sample', 'i', 'xa', 'update', 'coefficients', 'based', 'on', 'the', 'error', 'above', 'xa', 'prev_error', 'error', 'xa', 'calculate', 'outputs', 'f', 'xa', 'error', 'sum', 'f', 'y', 'n', 'xa', 'it', 'it', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'large', 'scale', 'online', 'learning', 'quot', 'l', 'bottou', 'y', 'le', 'cunn', 'nips', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'hope', 'you', 'can', 'help', 'me', 'as', 'i', 'have', 'some', 'questions', 'on', 'this', 'topic', 'i', 'm', 'new', 'in', 'the', 'field', 'of', 'deep', 'learning', 'and', 'while', 'i', 'did', 'some', 'tutorials', 'i', 'can', 't', 'relate', 'or', 'distinguish', 'concepts', 'from', 'one', 'another', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'some', 'help', 'with', 'a', 'single', 'layered', 'perceptron', 'with', 'multiple', 'classes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'i', 'need', 'to', 'do', 'is', 'classify', 'a', 'dataset', 'with', 'three', 'different', 'classes', 'by', 'now', 'i', 'just', 'learnt', 'how', 'to', 'do', 'it', 'with', 'two', 'classes', 'so', 'i', 'have', 'no', 'really', 'a', 'good', 'clue', 'how', 'to', 'do', 'it', 'with', 'three', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'dataset', 'have', 'three', 'different', 'classes', 'iris', 'setosa', 'iris', 'versicolor', 'and', 'iris', 'versicolor', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'url', 'with', 'the', 'dataset', 'and', 'the', 'information', 'is', 'in', 'lt', 'a', 'href', 'quot', 'http', 'ftp', 'ics', 'uci', 'edu', 'pub', 'machine', 'learning', 'databases', 'iris', 'iris', 'data', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'ftp', 'ics', 'uci', 'edu', 'pub', 'machine', 'learning', 'databases', 'iris', 'iris', 'data', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'really', 'appreciate', 'any', 'help', 'anyone', 'can', 'give', 'to', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'a', 'lot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'some', 'very', 'complicated', 'data', 'about', 'some', 'movie', 'sales', 'online', 'first', 'for', 'each', 'data', 'entry', 'i', 'have', 'a', 'key', 'which', 'is', 'a', 'combination', 'of', 'five', 'keys', 'which', 'are', 'territory', 'day', 'etc', 'and', 'then', 'for', 'each', 'key', 'i', 'have', 'the', 'sales', 'for', 'a', 'period', 'of', 'time', 'and', 'other', 'information', 'like', 'the', 'movie', 's', 'box', 'office', 'and', 'genre', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'each', 'day', 'there', 'is', 'a', 'delay', 'for', 'the', 'data', 'loading', 'to', 'the', 'database', 'around', 'ten', 'hours', 'i', 'try', 'to', 'fill', 'the', 'gap', 'do', 'some', 'data', 'extrapolations', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'each', 'movie', 'we', 'sell', 'there', 'is', 'some', 'decay', 'of', 'selling', 'since', 'the', 'new', 'release', 'of', 'the', 'movie', 'i', 'e', 'usually', 'for', 'each', 'movie', 'it', 'follows', 'some', 'sales', 'decay', 'pattern', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'a', 'recent', 'day', 'i', 'pulled', 'some', 'data', 'and', 'i', 'found', 'that', 'some', 'decay', 'pattern', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'h', 'lsj', 'png', 'quot', 'alt', 'quot', 'decay', 'curve', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'l', 'daf', 'png', 'quot', 'alt', 'quot', 'decay', 'curve', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'rjpjk', 'png', 'quot', 'alt', 'quot', 'decay', 'curve', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'for', 'that', 'day', 'the', 'sales', 'for', 'each', 'key', 'can', 'range', 'from', 'around', 'to', 'the', 'pic', 'is', 'as', 'follow', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'o', 'tsq', 'png', 'quot', 'alt', 'quot', 'one', 'day', 'sales', 'curve', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'the', 'picture', 'the', 'means', 'there', 'are', 'around', 'keys', 'for', 'each', 'day', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'found', 'this', 'article', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'homepage', 'stat', 'uiowa', 'edu', 'kcowles', 's', '_', 'project_lee', 'amp', 'amp', 'pyo', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'http', 'homepage', 'stat', 'uiowa', 'edu', 'kcowles', 's', '_', 'project_lee', 'amp', 'amp', 'pyo', 'pdf', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'predict', 'for', 'each', 'key', 'the', 'sales', 'amount', 'like', 'for', 'a', 'movie', 'territory', 'day', 'etc', 'combination', 'the', 'sales', 'amount', 'how', 'much', 'dollars', 'means', 'for', 'that', 'movie', 'that', 'territory', 'that', 'day', 'how', 'much', 'money', 'we', 'get', 'from', 'selling', 'online', 'i', 'tried', 'arima', 'time', 'series', 'model', 'but', 'there', 'is', 'some', 'concerns', 'for', 'that', 'model', 'seen', 'from', 'the', 'pics', 'there', 'is', 'some', 'seasonal', 'thing', 'and', 'decay', 'thing', 'for', 'the', 'movie', 'so', 'the', 'sales', 'prediction', 'can', 'not', 'be', 'always', 'flat', 'there', 'may', 'be', 'a', 'pump', 'after', 'a', 'going', 'down', 'it', 'may', 'happens', 'on', 'a', 'weekend', 'since', 'there', 'is', 'seasonal', 'thing', 'and', 'the', 'decay', 'trend', 'etc', 'how', 'to', 'capture', 'these', 'things', 'thank', 'you', 'for', 'your', 'reply', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'not', 'sure', 'whether', 'can', 'be', 'applied', 'and', 'how', 'to', 'be', 'applied', 'here', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'a', 'lot', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'first', 'of', 'all', 'i', 'know', 'the', 'question', 'may', 'be', 'not', 'suitable', 'for', 'the', 'website', 'but', 'i', 'd', 'really', 'appreciate', 'it', 'if', 'you', 'just', 'gave', 'me', 'some', 'pointers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'a', 'years', 'old', 'programmer', 'i', 've', 'had', 'experience', 'with', 'many', 'different', 'programming', 'languages', 'a', 'while', 'ago', 'i', 'started', 'a', 'course', 'at', 'coursera', 'titled', 'introduction', 'to', 'machine', 'learning', 'and', 'since', 'that', 'moment', 'i', 'got', 'very', 'motivated', 'to', 'learn', 'about', 'ai', 'i', 'started', 'reading', 'about', 'neural', 'networks', 'and', 'i', 'made', 'a', 'working', 'perceptron', 'using', 'java', 'and', 'it', 'was', 'really', 'fun', 'but', 'when', 'i', 'started', 'to', 'do', 'something', 'a', 'little', 'more', 'challenging', 'building', 'a', 'digit', 'recognition', 'software', 'i', 'found', 'out', 'that', 'i', 'have', 'to', 'learn', 'a', 'lot', 'of', 'math', 'i', 'love', 'math', 'but', 'the', 'schools', 'here', 'don', 't', 'teach', 'us', 'much', 'now', 'i', 'happen', 'to', 'know', 'someone', 'who', 'is', 'a', 'math', 'teacher', 'do', 'you', 'think', 'learning', 'math', 'specifically', 'calculus', 'is', 'necessary', 'for', 'me', 'to', 'learn', 'ai', 'or', 'should', 'i', 'wait', 'until', 'i', 'learn', 'those', 'stuff', 'at', 'school', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'what', 'other', 'things', 'would', 'be', 'helpful', 'in', 'the', 'path', 'of', 'me', 'learning', 'ai', 'and', 'machine', 'learning', 'do', 'other', 'techniques', 'like', 'svm', 'also', 'require', 'strong', 'math', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'sorry', 'if', 'my', 'question', 'is', 'long', 'i', 'd', 'really', 'appreciate', 'if', 'you', 'could', 'share', 'with', 'me', 'any', 'experience', 'you', 'have', 'had', 'with', 'learning', 'ai', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'wondering', 'if', 'someone', 'could', 'point', 'me', 'to', 'suitable', 'database', 'formats', 'for', 'building', 'up', 'a', 'user', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'basically', 'i', 'am', 'collecting', 'logs', 'of', 'impressions', 'data', 'and', 'i', 'want', 'to', 'compile', 'a', 'user', 'database', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'sites', 'user', 'visits', 'country', 'gender', 'and', 'other', 'categorisations', 'with', 'the', 'aim', 'of', 'xa', 'a', 'doing', 'searches', 'give', 'me', 'all', 'users', 'visiting', 'games', 'sites', 'from', 'france', 'xa', 'b', 'machine', 'learning', 'eg', 'clustering', 'users', 'by', 'the', 'sites', 'they', 'visit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'am', 'interested', 'in', 'storing', 'info', 'about', 's', 'of', 'millions', 'of', 'users', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'with', 'indexes', 'on', 'user', 'sites', 'geo', 'location', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'the', 'idea', 'would', 'be', 'that', 'this', 'data', 'would', 'be', 'continually', 'updated', 'eg', 'nightly', 'update', 'to', 'user', 'database', 'of', 'new', 'sites', 'visited', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'suitable', 'database', 'systems', 'can', 'someone', 'suggest', 'suitable', 'reading', 'material', 'xa', 'i', 'was', 'imagining', 'hbase', 'might', 'be', 'suitable', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'new', 'to', 'natural', 'language', 'processing', 'i', 'think', 'nlp', 'is', 'a', 'challenging', 'field', 'the', 'syntax', 'and', 'semantic', 'ambiguities', 'could', 'cause', 'a', 'lot', 'of', 'problems', 'for', 'example', 'i', 'think', 'for', 'these', 'problems', 'machine', 'translation', 'is', 'a', 'hard', 'task', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'therefore', 'there', 'are', 'probably', 'many', 'approaches', 'and', 'methods', 'that', 'have', 'been', 'applied', 'to', 'this', 'field', 'but', 'what', 'are', 'the', 'latest', 'and', 'most', 'promising', 'approaches', 'and', 'methods', 'in', 'the', 'field', 'of', 'nlp', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'these', 'techniques', 'highly', 'dependent', 'on', 'the', 'target', 'language', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'an', 'online', 'console', 'for', 'the', 'language', 'r', 'like', 'i', 'write', 'the', 'code', 'and', 'the', 'server', 'should', 'execute', 'and', 'provide', 'me', 'with', 'the', 'output', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'similar', 'to', 'the', 'website', 'datacamp', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'variable', 'whose', 'value', 'i', 'would', 'like', 'to', 'predict', 'and', 'i', 'would', 'like', 'to', 'use', 'only', 'one', 'variable', 'as', 'predictor', 'for', 'instance', 'predict', 'traffic', 'density', 'based', 'on', 'weather', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'initially', 'i', 'thought', 'about', 'using', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'self', 'organizing_map', 'quot', 'gt', 'self', 'organizing', 'maps', 'lt', 'a', 'gt', 'som', 'which', 'performs', 'unsupervised', 'clustering', 'regression', 'however', 'since', 'it', 'has', 'an', 'important', 'component', 'of', 'dimensionality', 'reduction', 'i', 'see', 'it', 'as', 'more', 'appropriated', 'for', 'a', 'large', 'number', 'of', 'variables', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'it', 'make', 'sense', 'to', 'use', 'it', 'for', 'a', 'single', 'variable', 'as', 'predictor', 'maybe', 'there', 'are', 'more', 'adequate', 'techniques', 'for', 'this', 'lt', 'em', 'gt', 'simple', 'lt', 'em', 'gt', 'case', 'i', 'used', 'quot', 'data', 'mining', 'quot', 'instead', 'of', 'quot', 'machine', 'learning', 'quot', 'in', 'the', 'title', 'of', 'my', 'question', 'because', 'i', 'think', 'maybe', 'a', 'linear', 'regression', 'could', 'do', 'the', 'job', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'papers', 'ssrn', 'com', 'sol', 'papers', 'cfm', 'abstract_id', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'automated', 'time', 'series', 'forecasting', 'for', 'biosurveillance', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'the', 'above', 'paper', 'page', 'two', 'models', 'non', 'adaptive', 'regression', 'model', 'adaptive', 'regression', 'model', 'the', 'non', 'adaptive', 'regression', 'model', 's', 'parameter', 'estimation', 'method', 'is', 'quot', 'least', 'squares', 'quot', 'what', 'is', 'the', 'parameter', 'estimation', 'for', 'the', 'adaptive', 'regression', 'model', 'is', 'there', 'any', 'package', 'in', 'r', 'to', 'do', 'parameter', 'estimation', 'for', 'this', 'kind', 'of', 'adaptive', 'regression', 'model', 'if', 'i', 'add', 'more', 'predictors', 'in', 'the', 'adaptive', 'regression', 'model', 'can', 'r', 'still', 'solve', 'it', 'and', 'how', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'how', 'would', 'i', 'do', 'parameter', 'estimation', 'and', 'prediction', 'for', 'the', 'adaptive', 'regression', 'model', 'using', 'r', 'as', 'in', 'the', 'th', 'page', 'of', 'the', 'paper', 'linked', 'below', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'papers', 'ssrn', 'com', 'sol', 'papers', 'cfm', 'abstract_id', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'papers', 'ssrn', 'com', 'sol', 'papers', 'cfm', 'abstract_id', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'could', 'anyone', 'clarify', 'this', 'for', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'you', 'know', 'adaptive', 'regression', 'models', 'very', 'well', 'share', 'some', 'useful', 'link', 'or', 'describe', 'the', 'model', 'parameter', 'estimation', 'prediction', 'that', 'would', 'be', 'very', 'helpful', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'so', 'much', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'identifies', 'different', 'queries', 'in', 'sentences', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'like', 'lt', 'code', 'gt', 'who', 'is', 'bill', 'gates', 'and', 'where', 'he', 'was', 'born', 'lt', 'code', 'gt', 'or', 'lt', 'code', 'gt', 'who', 'is', 'bill', 'gates', 'where', 'he', 'was', 'born', 'lt', 'code', 'gt', 'contains', 'two', 'queries', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'who', 'is', 'bill', 'gates', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'where', 'bill', 'gates', 'was', 'born', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'worked', 'on', 'coreference', 'resolution', 'so', 'i', 'can', 'identify', 'that', 'lt', 'code', 'gt', 'he', 'lt', 'code', 'gt', 'points', 'to', 'lt', 'code', 'gt', 'bill', 'gates', 'lt', 'code', 'gt', 'so', 'resolved', 'sentence', 'is', 'quot', 'who', 'is', 'bill', 'gates', 'where', 'bill', 'gates', 'was', 'born', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'like', 'wise', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'mgandhi', 'is', 'good', 'guys', 'where', 'he', 'was', 'born', 'xa', 'single', 'query', 'xa', 'who', 'is', 'mgandhi', 'and', 'where', 'was', 'he', 'born', 'xa', 'queries', 'xa', 'who', 'is', 'mgandhi', 'where', 'he', 'was', 'born', 'and', 'died', 'xa', 'quries', 'xa', 'india', 'won', 'world', 'cup', 'against', 'australia', 'when', 'xa', 'query', 'when', 'india', 'won', 'wc', 'against', 'auz', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'can', 'perform', 'coreference', 'resolution', 'identifying', 'and', 'converting', 'lt', 'code', 'gt', 'he', 'lt', 'code', 'gt', 'to', 'lt', 'code', 'gt', 'gandhi', 'lt', 'code', 'gt', 'but', 'not', 'getting', 'how', 'can', 'i', 'distinguish', 'queries', 'in', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'to', 'do', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'checked', 'various', 'sentence', 'parser', 'but', 'as', 'this', 'is', 'pure', 'nlp', 'stuff', 'sentence', 'parser', 'does', 'not', 'identify', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'to', 'find', 'quot', 'sentence', 'disambiguation', 'quot', 'like', 'quot', 'word', 'sense', 'disambiguation', 'quot', 'but', 'nothing', 'exist', 'like', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'help', 'or', 'suggestion', 'would', 'be', 'much', 'appreciable', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'recently', 'in', 'a', 'data', 'analytic', 'job', 'interview', 'for', 'an', 'e', 'commerce', 'site', 'they', 'asked', 'me', 'do', 'i', 'have', 'some', 'knowledge', 'of', 'buyer', 'classification', 'problem', 'unfortunately', 'i', 'heard', 'this', 'term', 'for', 'the', 'first', 'time', 'lt', 'br', 'gt', 'xa', 'after', 'interview', 'i', 'tried', 'to', 'search', 'a', 'lot', 'about', 'it', 'over', 'google', 'but', 'didn', 't', 'find', 'something', 'meaningful', 'please', 'any', 'one', 'let', 'me', 'know', 'if', 'you', 'have', 'heard', 'this', 'term', 'before', 'and', 'paste', 'some', 'links', 'explaining', 'this', 'concept', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'kind', 'of', 'a', 'newbie', 'on', 'machine', 'learning', 'and', 'i', 'would', 'like', 'to', 'ask', 'some', 'questions', 'based', 'on', 'a', 'problem', 'i', 'have', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'let', 's', 'say', 'i', 'have', 'x', 'y', 'z', 'as', 'variable', 'and', 'i', 'have', 'values', 'of', 'these', 'variables', 'as', 'time', 'progresses', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 't', 'x', 'y', 'z', 'lt', 'br', 'gt', 'xa', 't', 'x', 'y', 'z', 'lt', 'br', 'gt', 'xa', 'tn', 'xn', 'yn', 'zn', 'lt', 'br', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'want', 'a', 'model', 'that', 'when', 'it', 's', 'given', 'values', 'of', 'x', 'y', 'z', 'i', 'want', 'a', 'prediction', 'of', 'them', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'input', 'x_test', 'y_test', 'z_test', 'xa', 'output', 'x_prediction', 'y_prediction', 'z_prediction', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'these', 'values', 'are', 'float', 'numbers', 'what', 'is', 'the', 'best', 'model', 'for', 'this', 'kind', 'of', 'problem', 'xa', 'thanks', 'in', 'advance', 'for', 'all', 'the', 'answers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'more', 'details', 'xa', 'ok', 'so', 'let', 'me', 'give', 'some', 'more', 'details', 'about', 'the', 'problems', 'so', 'as', 'to', 'be', 'more', 'specific', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'run', 'certain', 'benchmarks', 'and', 'taken', 'values', 'of', 'performance', 'counters', 'from', 'the', 'cores', 'of', 'a', 'system', 'per', 'interval', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'performance', 'counters', 'are', 'the', 'x', 'y', 'z', 'in', 'the', 'above', 'example', 'they', 'are', 'dependent', 'to', 'each', 'other', 'simple', 'example', 'is', 'x', 'ipc', 'y', 'cache', 'misses', 'z', 'energy', 'at', 'core', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'got', 'this', 'dataset', 'of', 'all', 'these', 'performance', 'counters', 'per', 'interval', 'what', 'i', 'want', 'to', 'do', 'is', 'create', 'a', 'model', 'that', 'after', 'learning', 'from', 'the', 'training', 'dataset', 'it', 'will', 'be', 'given', 'a', 'certain', 'state', 'of', 'the', 'core', 'the', 'performance', 'counters', 'and', 'predict', 'the', 'performance', 'counters', 'that', 'the', 'core', 'will', 'have', 'in', 'the', 'next', 'interval', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'setup', 'a', 'big', 'data', 'infrastructure', 'using', 'hadoop', 'hive', 'elastic', 'search', 'amongst', 'others', 'and', 'i', 'would', 'like', 'to', 'run', 'some', 'algorithms', 'over', 'certain', 'datasets', 'i', 'would', 'like', 'the', 'algorithms', 'themselves', 'to', 'be', 'scalable', 'so', 'this', 'excludes', 'using', 'tools', 'such', 'as', 'weka', 'r', 'or', 'even', 'rhadoop', 'the', 'lt', 'a', 'href', 'quot', 'https', 'mahout', 'apache', 'org', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'apache', 'mahout', 'library', 'lt', 'a', 'gt', 'seems', 'to', 'be', 'a', 'good', 'option', 'and', 'it', 'features', 'lt', 'a', 'href', 'quot', 'https', 'mahout', 'apache', 'org', 'users', 'basics', 'algorithms', 'html', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'algorithms', 'for', 'regression', 'and', 'clustering', 'tasks', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'i', 'am', 'struggling', 'to', 'find', 'is', 'a', 'solution', 'for', 'anomaly', 'or', 'outlier', 'detection', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'since', 'mahout', 'features', 'hidden', 'markov', 'models', 'and', 'a', 'variety', 'of', 'clustering', 'techniques', 'including', 'k', 'means', 'i', 'was', 'wondering', 'if', 'it', 'would', 'be', 'possible', 'to', 'build', 'a', 'model', 'to', 'detect', 'outliers', 'in', 'time', 'series', 'using', 'any', 'of', 'this', 'i', 'would', 'be', 'grateful', 'if', 'somebody', 'experienced', 'on', 'this', 'could', 'advice', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'if', 'it', 'is', 'possible', 'and', 'in', 'case', 'it', 'is', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'to', 'do', 'it', 'plus', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'an', 'estimation', 'of', 'the', 'effort', 'involved', 'and', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'accuracy', 'problems', 'of', 'this', 'approach', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'a', 'topic', 'for', 'my', 'masters', 'thesis', 'machine', 'learning', 'is', 'my', 'primary', 'domain', 'and', 'i', 'want', 'to', 'work', 'on', 'probabilistic', 'models', 'and', 'applied', 'probability', 'in', 'machine', 'learning', 'please', 'suggest', 'some', 'exciting', 'new', 'topics', 'that', 'would', 'make', 'for', 'a', 'good', 'masters', 'thesis', 'subject', 'xa', 'anything', 'related', 'to', 'markov', 'chains', 'monte', 'carlo', 'bayesian', 'methods', 'probabilistic', 'graphical', 'models', 'markov', 'models', 'and', 'so', 'on', 'in', 'context', 'of', 'machine', 'learning', 'would', 'be', 'great', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'built', 'a', 'toy', 'random', 'forest', 'model', 'in', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'using', 'the', 'lt', 'code', 'gt', 'german', 'credit', 'lt', 'code', 'gt', 'dataset', 'from', 'the', 'lt', 'code', 'gt', 'caret', 'lt', 'code', 'gt', 'package', 'exported', 'it', 'in', 'lt', 'code', 'gt', 'pmml', 'lt', 'code', 'gt', 'and', 'deployed', 'onto', 'hadoop', 'using', 'the', 'lt', 'code', 'gt', 'cascading', 'pattern', 'lt', 'code', 'gt', 'library', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'run', 'into', 'an', 'issue', 'where', 'lt', 'code', 'gt', 'cascading', 'pattern', 'lt', 'code', 'gt', 'scores', 'the', 'same', 'data', 'differently', 'in', 'a', 'binary', 'classification', 'problem', 'than', 'the', 'same', 'model', 'in', 'lt', 'code', 'gt', 'r', 'lt', 'code', 'gt', 'out', 'of', 'observations', 'are', 'scored', 'differently', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'is', 'this', 'could', 'it', 'be', 'due', 'to', 'a', 'difference', 'in', 'the', 'implementation', 'of', 'random', 'forests', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'analyze', 'the', 'effectiveness', 'and', 'efficiency', 'of', 'kernel', 'methods', 'for', 'which', 'i', 'would', 'require', 'different', 'data', 'set', 'in', 'dimensional', 'space', 'for', 'each', 'of', 'the', 'following', 'cases', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'bad_kmeans', 'the', 'data', 'set', 'for', 'which', 'the', 'kmeans', 'clustering', 'algorithm', 'xa', 'will', 'not', 'perform', 'well', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'bad_pca', 'the', 'data', 'set', 'for', 'which', 'the', 'principal', 'component', 'analysis', 'xa', 'pca', 'dimension', 'reduction', 'method', 'upon', 'projection', 'of', 'the', 'original', 'xa', 'points', 'into', 'dimensional', 'space', 'i', 'e', 'the', 'first', 'eigenvector', 'will', 'xa', 'not', 'perform', 'well', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'bad_svm', 'the', 'data', 'set', 'for', 'which', 'the', 'linear', 'support', 'vector', 'machine', 'xa', 'svm', 'supervised', 'classification', 'method', 'using', 'two', 'classes', 'of', 'points', 'xa', 'positive', 'and', 'negative', 'will', 'not', 'perform', 'well', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'which', 'packages', 'can', 'i', 'use', 'in', 'r', 'to', 'generate', 'the', 'random', 'd', 'data', 'set', 'for', 'each', 'of', 'the', 'above', 'cases', 'a', 'sample', 'script', 'in', 'r', 'would', 'help', 'in', 'understanding', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'learn', 'both', 'python', 'and', 'r', 'for', 'usage', 'in', 'data', 'science', 'projects', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'currently', 'unemployed', 'fresh', 'out', 'of', 'university', 'scouting', 'around', 'for', 'jobs', 'and', 'thought', 'it', 'would', 'be', 'good', 'if', 'i', 'get', 'some', 'kaggle', 'projects', 'under', 'my', 'profile', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'have', 'very', 'little', 'knowledge', 'in', 'either', 'language', 'have', 'used', 'matlab', 'and', 'c', 'c', 'in', 'the', 'past', 'but', 'i', 'haven', 't', 'produced', 'production', 'quality', 'code', 'or', 'developed', 'an', 'application', 'or', 'software', 'in', 'either', 'language', 'it', 'has', 'been', 'dirty', 'coding', 'for', 'academic', 'usage', 'all', 'along', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'used', 'a', 'little', 'bit', 'of', 'python', 'in', 'a', 'university', 'project', 'but', 'i', 'dont', 'know', 'the', 'fundamentals', 'like', 'what', 'is', 'a', 'package', 'etc', 'etc', 'ie', 'havent', 'read', 'the', 'intricacies', 'of', 'the', 'language', 'using', 'a', 'standard', 'python', 'textbook', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'have', 'done', 'some', 'amount', 'of', 'coding', 'in', 'c', 'c', 'way', 'back', 'years', 'back', 'then', 'switched', 'over', 'to', 'matlab', 'octave', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'get', 'started', 'in', 'python', 'numpy', 'scipy', 'scikit', 'learn', 'and', 'pandas', 'etc', 'but', 'just', 'reading', 'up', 'wikipedia', 'articles', 'or', 'python', 'textbooks', 'is', 'going', 'to', 'be', 'infeasible', 'for', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'same', 'goes', 'with', 'r', 'except', 'that', 'i', 'have', 'zero', 'knowledge', 'of', 'r', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anyone', 'have', 'any', 'suggestions', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'is', 'there', 'a', 'good', 'java', 'library', 'for', 'doing', 'time', 'series', 'energy', 'consumption', 'forecasting', 'based', 'on', 'weather', 'data', 'and', 'other', 'variables', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'most', 'treebank', 'conversion', 'which', 'i', 'found', 'in', 'the', 'web', 'are', 'from', 'constituency', 'treebank', 'to', 'dependency', 'treebank', 'i', 'wonder', 'why', 'there', 'is', 'little', 'jobs', 'in', 'the', 'opposite', 'direction', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'looking', 'for', 'a', 'thesis', 'to', 'complete', 'my', 'master', 'm', 'i', 'will', 'work', 'on', 'a', 'topic', 'in', 'the', 'big', 'data', 's', 'field', 'creation', 'big', 'data', 'applications', 'using', 'hadoop', 'mapreduce', 'and', 'ecosystem', 'visualisation', 'analysis', 'please', 'suggest', 'some', 'topics', 'or', 'project', 'that', 'would', 'make', 'for', 'a', 'good', 'masters', 'thesis', 'subject', 'lt', 'br', 'gt', 'xa', 'i', 'add', 'that', 'i', 'have', 'bases', 'in', 'data', 'warehouses', 'databases', 'data', 'mining', 'good', 'skills', 'in', 'programming', 'system', 'administration', 'and', 'cryptography', 'lt', 'br', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'where', 'is', 'the', 'difference', 'between', 'one', 'class', 'binary', 'class', 'and', 'multinominal', 'class', 'classification', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'like', 'to', 'classify', 'text', 'in', 'lets', 'say', 'four', 'classes', 'and', 'also', 'want', 'the', 'system', 'to', 'be', 'able', 'to', 'tell', 'me', 'that', 'none', 'of', 'these', 'classes', 'matches', 'the', 'unknown', 'untrained', 'test', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'couldn', 't', 'i', 'just', 'use', 'all', 'the', 'methods', 'that', 'i', 'mentioned', 'above', 'to', 'reach', 'my', 'goal', 'xa', 'e', 'g', 'i', 'could', 'describe', 'c', 'c', 'c', 'and', 'c', 'as', 'four', 'different', 'trainings', 'sets', 'for', 'binary', 'classification', 'and', 'use', 'the', 'trained', 'models', 'to', 'label', 'an', 'unknow', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'just', 'by', 'saying', 'training', 'set', 'for', 'c', 'contains', 'class', 'all', 'good', 'samples', 'for', 'c', 'and', 'class', 'mix', 'of', 'all', 'c', 'c', 'and', 'c', 'as', 'bad', 'samples', 'for', 'c', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'unlabeled', 'data', 'c', 'gt', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'unlabeled', 'data', 'c', 'gt', 'or', 'xa', 'and', 'so', 'on', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'multinominal', 'classification', 'i', 'could', 'just', 'define', 'a', 'training', 'set', 'containing', 'all', 'good', 'sample', 'data', 'for', 'c', 'c', 'c', 'and', 'c', 'in', 'one', 'training', 'set', 'and', 'then', 'use', 'the', 'one', 'resulting', 'model', 'for', 'classification', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'where', 'is', 'the', 'difference', 'between', 'this', 'two', 'methods', 'except', 'of', 'that', 'i', 'have', 'to', 'use', 'different', 'algorithms', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'how', 'would', 'i', 'define', 'a', 'training', 'set', 'for', 'the', 'described', 'problem', 'of', 'categorizing', 'data', 'in', 'those', 'four', 'classes', 'using', 'one', 'class', 'classfication', 'is', 'that', 'even', 'possible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'excuse', 'me', 'if', 'i', 'm', 'completely', 'wrong', 'in', 'my', 'thinking', 'would', 'appreciate', 'an', 'answer', 'that', 'makes', 'the', 'methodology', 'a', 'little', 'bit', 'clearer', 'to', 'me', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'a', 'random', 'forest', 'rf', 'is', 'created', 'by', 'an', 'ensemble', 'of', 'decision', 'trees', 's', 'dt', 'by', 'using', 'bagging', 'each', 'dt', 'is', 'trained', 'in', 'a', 'different', 'data', 'subset', 'hence', 'lt', 'strong', 'gt', 'is', 'there', 'any', 'way', 'of', 'implementing', 'an', 'on', 'line', 'random', 'forest', 'by', 'adding', 'more', 'decision', 'tress', 'on', 'new', 'data', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'we', 'have', 'k', 'samples', 'and', 'train', 'dt', 's', 'then', 'we', 'get', 'k', 'samples', 'and', 'instead', 'of', 'training', 'again', 'the', 'full', 'rf', 'we', 'add', 'a', 'new', 'dt', 'the', 'prediction', 'is', 'done', 'now', 'by', 'the', 'bayesian', 'average', 'of', 'dt', 's', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'addition', 'if', 'we', 'keep', 'all', 'the', 'previous', 'data', 'the', 'new', 'dt', 's', 'can', 'be', 'trained', 'mainly', 'in', 'the', 'new', 'data', 'where', 'the', 'probability', 'of', 'picking', 'a', 'sample', 'is', 'weighted', 'depending', 'how', 'many', 'times', 'have', 'been', 'already', 'picked', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'where', 'is', 'the', 'difference', 'between', 'one', 'class', 'binary', 'class', 'and', 'multinominal', 'class', 'classification', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'like', 'to', 'classify', 'text', 'in', 'lets', 'say', 'four', 'classes', 'and', 'also', 'want', 'the', 'system', 'to', 'be', 'able', 'to', 'tell', 'me', 'that', 'none', 'of', 'these', 'classes', 'matches', 'the', 'unknown', 'untrained', 'test', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'couldn', 't', 'i', 'just', 'use', 'all', 'the', 'methods', 'that', 'i', 'mentioned', 'above', 'to', 'reach', 'my', 'goal', 'xa', 'e', 'g', 'i', 'could', 'describe', 'c', 'c', 'c', 'and', 'c', 'as', 'four', 'different', 'trainings', 'sets', 'for', 'binary', 'classification', 'and', 'use', 'the', 'trained', 'models', 'to', 'label', 'an', 'unknow', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'just', 'by', 'saying', 'training', 'set', 'for', 'c', 'contains', 'class', 'all', 'good', 'samples', 'for', 'c', 'and', 'class', 'mix', 'of', 'all', 'c', 'c', 'and', 'c', 'as', 'bad', 'samples', 'for', 'c', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'unlabeled', 'data', 'c', 'gt', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'unlabeled', 'data', 'c', 'gt', 'or', 'xa', 'and', 'so', 'on', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'multinominal', 'classification', 'i', 'could', 'just', 'define', 'a', 'training', 'set', 'containing', 'all', 'good', 'sample', 'data', 'for', 'c', 'c', 'c', 'and', 'c', 'in', 'one', 'training', 'set', 'and', 'then', 'use', 'the', 'one', 'resulting', 'model', 'for', 'classification', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'where', 'is', 'the', 'difference', 'between', 'this', 'two', 'methods', 'except', 'of', 'that', 'i', 'have', 'to', 'use', 'different', 'algorithms', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'how', 'would', 'i', 'define', 'a', 'training', 'set', 'for', 'the', 'described', 'problem', 'of', 'categorizing', 'data', 'in', 'those', 'four', 'classes', 'using', 'one', 'class', 'classfication', 'is', 'that', 'even', 'possible', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'excuse', 'me', 'if', 'i', 'm', 'completely', 'wrong', 'in', 'my', 'thinking', 'would', 'appreciate', 'an', 'answer', 'that', 'makes', 'the', 'methodology', 'a', 'little', 'bit', 'clearer', 'to', 'me', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'drawing', 'samples', 'from', 'two', 'classes', 'in', 'the', 'two', 'dimensional', 'cartesian', 'space', 'each', 'of', 'which', 'has', 'the', 'same', 'covariance', 'matrix', 'one', 'class', 'has', 'a', 'mean', 'of', 'and', 'the', 'other', 'has', 'a', 'mean', 'of', 'if', 'the', 'priors', 'are', 'for', 'the', 'former', 'and', 'for', 'the', 'later', 'how', 'would', 'i', 'derive', 'the', 'equation', 'for', 'the', 'ideal', 'decision', 'boundary', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'it', 'turns', 'out', 'that', 'misclassifying', 'the', 'second', 'class', 'is', 'twice', 'as', 'expensive', 'as', 'the', 'first', 'class', 'and', 'the', 'objective', 'is', 'to', 'minimize', 'the', 'expected', 'cost', 'what', 'equation', 'would', 'i', 'use', 'for', 'the', 'best', 'decision', 'boundary', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'are', 'there', 'any', 'good', 'sources', 'that', 'explain', 'how', 'decision', 'trees', 'can', 'be', 'implemented', 'in', 'a', 'scalable', 'way', 'on', 'a', 'distributed', 'computing', 'system', 'where', 'in', 'a', 'given', 'source', 'is', 'this', 'explained', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'can', 'anyone', 'explain', 'how', 'field', 'aware', 'factorization', 'machines', 'ffm', 'compare', 'to', 'standard', 'factorization', 'machines', 'fm', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'standard', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'www', 'ismll', 'uni', 'hildesheim', 'de', 'pub', 'pdfs', 'rendle', 'fm', 'pdf', 'quot', 'gt', 'http', 'www', 'ismll', 'uni', 'hildesheim', 'de', 'pub', 'pdfs', 'rendle', 'fm', 'pdf', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quot', 'field', 'aware', 'quot', 'xa', 'lt', 'a', 'href', 'quot', 'http', 'www', 'csie', 'ntu', 'edu', 'tw', 'r', 'kaggle', 'criteo', 'pdf', 'quot', 'gt', 'http', 'www', 'csie', 'ntu', 'edu', 'tw', 'r', 'kaggle', 'criteo', 'pdf', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'there', 're', 'many', 'data', 'points', 'each', 'of', 'which', 'is', 'associated', 'with', 'two', 'coordinates', 'and', 'a', 'numeral', 'value', 'or', 'three', 'coordinates', 'and', 'i', 'wish', 'it', 'is', 'coloured', 'xa', 'i', 'checked', 'packages', 'quot', 'scatterplot', 'd', 'quot', 'and', 'quot', 'plot', 'd', 'quot', 'but', 'i', 'couldn', 't', 'find', 'one', 'like', 'the', 'example', 'i', 'give', 'it', 'is', 'like', 'it', 'has', 'a', 'fitting', 'surface', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'data', 'is', 'basically', 'like', 'the', 'following', 'in', 'this', 'way', 'i', 'think', 'this', 'kind', 'of', 'plot', 'is', 'gonna', 'be', 'perfectly', 'suitble', 'for', 'this', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'ki', 'kt', 'top', 'averagef', 'score', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'i', 'also', 'may', 'have', 'one', 'more', 'additional', 'variable', 'which', 'makes', 'it', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'neighborhoodsize', 'ki', 'kt', 'top', 'averagef', 'score', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'do', 'you', 'also', 'have', 'any', 'good', 'idea', 'for', 'visualizing', 'the', 'second', 'case', 'what', 'kind', 'of', 'plot', 'and', 'which', 'packages', 'and', 'functions', 'etc', 'lt', 'strong', 'gt', 'xa', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'whm', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'analyze', 'lt', 'a', 'href', 'quot', 'http', 'grouplens', 'org', 'datasets', 'movielens', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'movielens', 'data', 'set', 'lt', 'a', 'gt', 'and', 'load', 'on', 'my', 'machine', 'the', 'm', 'file', 'i', 'combine', 'actually', 'two', 'data', 'files', 'ratings', 'dat', 'and', 'movies', 'dat', 'and', 'sort', 'the', 'table', 'according', 'lt', 'code', 'gt', 'userid', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'time', 'lt', 'code', 'gt', 'columns', 'the', 'head', 'of', 'my', 'dataframe', 'looks', 'like', 'here', 'all', 'columns', 'values', 'are', 'corresponding', 'to', 'the', 'original', 'data', 'sets', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'in', 'df', 'head', 'xa', 'out', 'xa', 'userid', 'movieid', 'rating', 'time', 'moviename', 'xa', 'apollo', 'xa', 'mary', 'poppins', 'xa', 'bambi', 'xa', 'driving', 'miss', 'daisy', 'xa', 'sound', 'of', 'music', 'the', 'xa', 'gigi', 'xa', 'awakenings', 'xa', 'saving', 'private', 'ryan', 'xa', 'rain', 'man', 'xa', 'run', 'lola', 'run', 'lola', 'rennt', 'xa', 'xa', 'genre', 'xa', 'drama', 'xa', 'children', 's', 'comedy', 'musical', 'xa', 'animation', 'children', 's', 'xa', 'drama', 'xa', 'musical', 'xa', 'musical', 'xa', 'drama', 'xa', 'action', 'drama', 'war', 'xa', 'drama', 'xa', 'action', 'crime', 'romance', 'xa', 'xa', 'rows', 'x', 'columns', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'can', 'not', 'understand', 'that', 'the', 'same', 'user', 'with', 'user', 'id', 'see', 'or', 'rated', 'the', 'different', 'movies', 'apollo', 'id', 'mary', 'poppins', 'id', 'and', 'bambi', 'id', 'exactly', 'at', 'the', 'same', 'time', 'up', 'to', 'the', 'milleseconds', 'if', 'somebody', 'works', 'already', 'with', 'this', 'data', 'set', 'please', 'clear', 'this', 'situation', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'not', 'sure', 'whether', 'i', 'formulated', 'the', 'question', 'correctly', 'basically', 'what', 'i', 'want', 'to', 'do', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'let', 's', 'suppose', 'i', 'have', 'a', 'list', 'of', 'strings', 'which', 'look', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'cvzxcvzx', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'cvzcxvz', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'otortorotr', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'grptprt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'vmvmvmeop', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'vmrprp', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'vccermpqp', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'rowerm', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'proororor', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'potrprt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'mprto', 'lt', 'strong', 'gt', 'string', 'lt', 'strong', 'gt', 'famerpaer', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'like', 'to', 'extract', 'these', 'reoccuring', 'strings', 'that', 'occur', 'on', 'the', 'list', 'what', 'solution', 'should', 'i', 'use', 'does', 'anyone', 'know', 'about', 'algorithm', 'that', 'could', 'do', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'some', 'serious', 'help', 'i', 'am', 'supposed', 'to', 'implement', 'a', 'project', 'non', 'existing', 'as', 'of', 'now', 'for', 'my', 'machine', 'learning', 'course', 'i', 'have', 'no', 'basics', 'in', 'ai', 'or', 'data', 'mining', 'or', 'machine', 'learning', 'i', 'have', 'been', 'searching', 'for', 'a', 'while', 'and', 'unable', 'to', 'find', 'something', 'that', 'i', 'can', 'finish', 'implementing', 'in', 'weeks', 'time', 'it', 'carries', 'a', 'huge', 'chunk', 'of', 'my', 'final', 'marks', 'and', 'no', 'matter', 'how', 'much', 'i', 'try', 'i', 'am', 'unable', 'to', 'understand', 'how', 'it', 'works', 'xa', 'can', 'the', 'machine', 'learning', 'masters', 'please', 'help', 'me', 'out', 'with', 'this', 'i', 'need', 'a', 'project', 'suggestion', 'to', 'start', 'with', 'and', 'i', 'want', 'to', 'know', 'how', 'to', 'proceed', 'after', 'gathering', 'the', 'data', 'set', 'i', 'am', 'totally', 'blank', 'and', 'running', 'out', 'of', 'time', 'for', 'my', 'graduation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'appreciate', 'your', 'suggestions', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'question', 'about', 'memory', 'usage', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'do', 'things', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'make', 'a', 'dataframe', 'from', 'one', 'of', 'several', 'columns', 'from', 'a', 'datasource', 'say', 'a', 'json', 'string', 'xa', 'make', 'the', 'third', 'column', 'of', 'the', 'original', 'dataset', 'the', 'index', 'to', 'the', 'dataframe', 'xa', 'change', 'the', 'name', 'of', 'another', 'column', 'xa', 'change', 'the', 'series', 'i', 've', 'created', 'to', 'a', 'dataframe', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'about', 'memory', 'efficiency', 'it', 'seems', 'that', 'for', 'step', 'i', 'am', 'first', 'loading', 'a', 'whole', 'dataframe', 'then', 'run', 'a', 'concat', 'command', 'to', 'concatenate', 'the', 'columns', 'i', 'want', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'step', 'i', 'again', 'need', 'to', 'resave', 'the', 'new', 'dataframe', 'as', 'another', 'object', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'step', 'it', 'seems', 'to', 'stick', 'so', 'nothing', 'there', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'advise', 'on', 'a', 'more', 'efficient', 'way', 'to', 'go', 'about', 'this', 'if', 'that', 'exists', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'command', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'df', 'pd', 'dataframe', 'jsonobject', 'xa', 'df', 'df', 'set_index', 'quot', 'columnc', 'quot', 'xa', 'df', 'index', 'names', 'quot', 'foo', 'quot', 'xa', 'df', 'df', 'quot', 'foo', 'quot', 'map', 'lambda', 'x', 'x', 'quot', 'id', 'quot', 'xa', 'df', 'pd', 'dataframe', 'df', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'work', 'in', 'an', 'analytical', 'role', 'at', 'a', 'a', 'large', 'financial', 'services', 'firm', 'we', 'do', 'a', 'ton', 'of', 'daily', 'reporting', 'over', 'metrics', 'that', 'rarely', 'change', 'in', 'a', 'meaningful', 'way', 'from', 'day', 'to', 'day', 'from', 'this', 'daily', 'reporting', 'our', 'management', 'is', 'required', 'to', 'extract', 'what', 'was', 'important', 'yesterday', 'and', 'what', 'important', 'trends', 'have', 'developed', 'are', 'developing', 'over', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'change', 'this', 'to', 'a', 'model', 'of', 'daily', 'exception', 'reporting', 'and', 'weekly', 'trend', 'reporting', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'features', 'might', 'include', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'user', 'report', 'consolidation', 'so', 'there', 's', 'only', 'one', 'daily', 'email', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'report', 'ordering', 'based', 'upon', 'level', 'of', 'variance', 'from', 'past', 'performance', 'see', 'the', 'most', 'important', 'stuff', 'first', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'html', 'email', 'support', 'with', 'my', 'audience', 'pretty', 'counts', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'web', 'interface', 'to', 'allow', 'preference', 'changes', 'including', 'ldap', 'support', 'make', 'administration', 'easier', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'unsubscribe', 'feature', 'at', 'the', 'report', 'level', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 's', 'what', 'i', 'd', 'like', 'to', 'know', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'are', 'the', 'practical', 'problems', 'i', 'might', 'run', 'into', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'is', 'the', 'best', 'way', 'to', 'display', 'the', 'new', 'reports', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'how', 'should', 'i', 'define', 'an', 'quot', 'exception', 'quot', 'how', 'can', 'i', 'know', 'if', 'my', 'definition', 'is', 'a', 'good', 'one', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'assume', 'i', 'd', 'be', 'using', 'a', 'mix', 'of', 'python', 'sql', 'and', 'powershell', 'anything', 'else', 'i', 'should', 'consider', 'e', 'g', 'r', 'what', 'are', 'some', 'good', 'resources', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'train', 'and', 'test', 'data', 'how', 'to', 'calculate', 'classification', 'accuracy', 'with', 'confusion', 'matrix', 'thanks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'attribute', 'outlook', 'sunny', 'overcast', 'rainy', 'xa', 'attribute', 'temperature', 'hot', 'mild', 'cool', 'xa', 'attribute', 'humidity', 'high', 'normal', 'xa', 'attribute', 'windy', 'true', 'false', 'xa', 'attribute', 'play', 'yes', 'no', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'train', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'sunny', 'hot', 'high', 'false', 'no', 'xa', 'sunny', 'hot', 'high', 'true', 'no', 'xa', 'overcast', 'hot', 'high', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'cool', 'normal', 'true', 'no', 'xa', 'sunny', 'cool', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'normal', 'false', 'yes', 'xa', 'sunny', 'mild', 'normal', 'true', 'yes', 'xa', 'overcast', 'mild', 'high', 'true', 'yes', 'xa', 'overcast', 'hot', 'normal', 'false', 'yes', 'xa', 'rainy', 'mild', 'high', 'true', 'no', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'test', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'overcast', 'cool', 'normal', 'true', 'yes', 'xa', 'sunny', 'mild', 'high', 'false', 'no', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'rules', 'found', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'humidity', 'normal', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'support', 'confidence', 'correctly', 'classify', 'xa', 'outlook', 'overcast', 'amp', 'gt', 'play', 'yes', 'support', 'confidence', 'correctly', 'classify', 'xa', 'outlook', 'rainy', 'windy', 'false', 'amp', 'gt', 'play', 'yes', 'support', 'confidence', 'correctly', 'classify', 'xa', 'outlook', 'sunny', 'temperature', 'hot', 'amp', 'gt', 'play', 'no', 'support', 'confidence', 'correctly', 'classify', 'xa', 'outlook', 'sunny', 'humidity', 'normal', 'amp', 'gt', 'play', 'yes', 'support', 'confidence', 'correctly', 'classify', 'xa', 'outlook', 'rainy', 'windy', 'true', 'amp', 'gt', 'play', 'no', 'support', 'confidence', 'correctly', 'classify', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'for', 'my', 'computational', 'intelligence', 'class', 'i', 'm', 'working', 'on', 'classifying', 'short', 'text', 'one', 'of', 'the', 'papers', 'that', 'i', 've', 'found', 'makes', 'a', 'lot', 'of', 'use', 'of', 'lt', 'em', 'gt', 'granular', 'computing', 'lt', 'em', 'gt', 'but', 'i', 'm', 'struggling', 'to', 'find', 'a', 'decent', 'explanation', 'of', 'what', 'exactly', 'it', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'from', 'what', 'i', 'can', 'gather', 'from', 'the', 'paper', 'it', 'sounds', 'to', 'me', 'like', 'granular', 'computing', 'is', 'very', 'similar', 'to', 'fuzzy', 'sets', 'so', 'what', 'exactly', 'is', 'the', 'difference', 'i', 'm', 'asking', 'about', 'rough', 'sets', 'as', 'well', 'because', 'i', 'm', 'curious', 'about', 'them', 'and', 'how', 'they', 'relate', 'to', 'fuzzy', 'sets', 'if', 'at', 'all', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'lt', 'a', 'href', 'quot', 'http', 'ijcai', 'org', 'papers', 'papers', 'ijcai', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'is', 'the', 'paper', 'i', 'm', 'referencing', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'data', 'collected', 'from', 'a', 'computer', 'simulation', 'of', 'football', 'games', 'which', 'seem', 'to', 'have', 'recurring', 'patterns', 'of', 'the', 'following', 'form', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'em', 'gt', 'if', 'madrid', 'plays', 'arsernal', 'and', 'the', 'match', 'ends', 'under', 'goal', 'then', 'on', 'their', 'next', 'match', 'against', 'each', 'others', 'madrid', 'will', 'win', 'if', 'madrid', 'happens', 'to', 'loose', 'and', 'then', 'plays', 'against', 'chelsea', 'next', 'they', 'will', 'win', 'of', 'the', 'time', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'do', 'i', 'find', 'such', 'inferences', 'from', 'simulation', 'generated', 'data', 'like', 'this', 'there', 'are', 'other', 'forms', 'of', 'hidden', 'patterns', 'that', 'i', 'believe', 'exists', 'in', 'the', 'dataset', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'cant', 'seem', 'to', 'figure', 'out', 'why', 'i', 'have', 'a', 'high', 'percentage', 'error', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'get', 'a', 'perceptron', 'between', 'x', 'and', 'x', 'which', 'are', 'gaussian', 'distributed', 'data', 'sets', 'with', 'distinct', 'means', 'and', 'identical', 'co', 'variances', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'below', 'is', 'my', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'n', 'xa', 'c', 'covariance', 'xa', 'm', 'xa', 'm', 'mean', 'xa', 'x', 'mvnrnd', 'm', 'c', 'n', 'xa', 'x', 'mvnrnd', 'm', 'c', 'n', 'xa', 'xa', 'x', 'x', 'x', 'xa', 'x', 'x', 'ones', 'n', 'bias', 'xa', 'y', 'ones', 'n', 'ones', 'n', 'classification', 'xa', 'xa', 'split', 'data', 'into', 'training', 'and', 'test', 'xa', 'ii', 'randperm', 'n', 'xa', 'xtr', 'x', 'ii', 'n', 'xa', 'ytr', 'x', 'ii', 'n', 'xa', 'xts', 'x', 'ii', 'n', 'n', 'xa', 'yts', 'y', 'ii', 'n', 'n', 'xa', 'nts', 'n', 'xa', 'xa', 'w', 'randn', 'xa', 'eta', 'xa', 'learn', 'from', 'training', 'set', 'xa', 'for', 'iter', 'xa', 'j', 'ceil', 'rand', 'n', 'xa', 'if', 'ytr', 'j', 'xtr', 'j', 'w', 'amp', 'lt', 'xa', 'w', 'w', 'eta', 'xtr', 'j', 'xa', 'end', 'xa', 'end', 'xa', 'xa', 'apply', 'what', 'you', 'have', 'learnt', 'to', 'test', 'set', 'xa', 'yhts', 'xts', 'w', 'xa', 'disp', 'yts', 'yhts', 'xa', 'percentageerror', 'sum', 'yts', 'yhts', 'amp', 'lt', 'nts', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'am', 'i', 'doing', 'wrong', 'and', 'how', 'can', 'i', 'address', 'this', 'challenge', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'sorry', 'if', 'this', 'topic', 'is', 'not', 'connected', 'directly', 'to', 'data', 'science', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'understand', 'how', 'the', 'lt', 'a', 'href', 'quot', 'http', 'graphlab', 'com', 'learn', 'gallery', 'index', 'html', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'graphlab', 'tool', 'lt', 'a', 'gt', 'works', 'firstly', 'i', 'want', 'to', 'execute', 'the', 'toy', 'examples', 'from', 'the', 'gallery', 'site', 'when', 'i', 'try', 'to', 'execute', 'the', 'example', 'code', 'everything', 'is', 'ok', 'except', 'one', 'command', 'i', 'can', 'not', 'see', 'the', 'graphlab', 'plot', 'after', 'lt', 'code', 'gt', 'show', 'lt', 'code', 'gt', 'the', 'command', 'lt', 'code', 'gt', 'show', 'lt', 'code', 'gt', 'returns', 'to', 'me', 'some', 'kind', 'of', 'object', 'in', 'ipython', 'and', 'nothing', 'in', 'the', 'ipython', 'notebook', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'the', 'example', 'code', 'has', 'the', 'plot', 'which', 'depends', 'directly', 'on', 'the', 'matplotlib', 'module', 'i', 'can', 'produce', 'the', 'real', 'plots', 'and', 'save', 'it', 'on', 'my', 'machine', 'consequently', 'i', 'suppose', 'the', 'main', 'error', 'depends', 'on', 'the', 'graphlab', 'or', 'object', 'from', 'its', 'class', 'xa', 'if', 'somebody', 'already', 'used', 'this', 'tool', 'and', 'rendered', 'the', 'plot', 'can', 'he', 'she', 'tell', 'me', 'how', 'i', 'can', 'execute', 'the', 'plots', 'command', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'in', 'import', 'graphlab', 'xa', 'xa', 'in', 'from', 'ipython', 'display', 'import', 'display', 'xa', 'xa', 'from', 'ipython', 'display', 'import', 'image', 'xa', 'xa', 'graphlab', 'canvas', 'set_target', 'ipynb', 'xa', 'xa', 'in', 'import', 'urllib', 'xa', 'xa', 'url', 'https', 's', 'amazonaws', 'com', 'graphlab', 'datasets', 'americanmovies', 'freebase_performances', 'csv', 'xa', 'xa', 'urllib', 'urlretrieve', 'url', 'filename', 'freebase_performances', 'csv', 'downloads', 'an', 'mb', 'file', 'to', 'the', 'working', 'directory', 'xa', 'xa', 'out', 'freebase_performances', 'csv', 'amp', 'lt', 'httplib', 'httpmessage', 'instance', 'at', 'x', 'f', 'e', 'cf', 'amp', 'gt', 'xa', 'xa', 'in', 'data', 'graphlab', 'sframe', 'read_csv', 'remote', 'freebase_performances', 'csv', 'column_type_hints', 'year', 'int', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'in', 'data', 'show', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'no', 'plot', 'after', 'this', 'line', 'xa', 'xa', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'in', 'print', 'data', 'show', 'xa', 'xa', 'amp', 'lt', 'ipython', 'core', 'display', 'javascript', 'object', 'at', 'x', 'f', 'e', 'c', 'amp', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'object', 'of', 'graphlab', 'after', 'print', 'command', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'our', 'system', 'allows', 'an', 'admin', 'to', 'manage', 'a', 'database', 'of', 'university', 'courses', 'these', 'courses', 'have', 'multiple', 'fields', 'like', 'the', 'department', 'a', 'title', 'and', 'a', 'description', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'adding', 'the', 'ability', 'to', 'add', 'learning', 'objectives', 'to', 'a', 'course', 'to', 'simplify', 'the', 'problem', 'let', 's', 'say', 'that', 'learning', 'objectives', 'are', 'just', 'tags', 'courses', 'can', 'have', 'more', 'than', 'one', 'learning', 'objective', 'associated', 'with', 'them', 'so', 'a', 'course', 'like', 'chem', 'might', 'have', 'quot', 'chemistry', 'quot', 'quot', 'technology', 'quot', 'quot', 'science', 'quot', 'and', 'several', 'others', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'assuming', 'i', 'can', 'reduce', 'a', 'course', 'to', 'a', 'set', 'of', 'features', 'using', 'keywords', 'stemming', 'nlp', 'i', 'suppose', 'what', 'kind', 'of', 'problem', 'is', 'this', 'and', 'what', 'algorithm', 'would', 'you', 'suggest', 'it', 'seems', 'very', 'similar', 'to', 'a', 'classification', 'problem', 'but', 'i', 'want', 'to', 'provide', 'a', 'sorted', 'list', 'of', 'suggestions', 'with', 'the', 'most', 'relevant', 'at', 'the', 'top', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'training', 'a', 'nn', 'with', 'features', 'and', 'training', 'examples', 'with', 'a', 'single', 'output', 'using', 'the', 'scipy', 'optimise', 'cg', 'algorithm', 'and', 'the', 'results', 'are', 'somewhat', 'inconsistent', 'the', 'goal', 'is', 'to', 'get', 'the', 'nn', 'to', 'be', 'as', 'precise', 'as', 'possible', 'recall', 'doesn', 't', 'really', 'matter', 'too', 'much', 'so', 'i', 've', 'set', 'the', 'threshold', 'for', 'y', 'value', 'quite', 'high', 'most', 'of', 'the', 'time', 'it', 'gets', 'a', 'precision', 'of', 'around', 'however', 'sometimes', 'it', 'fails', 'using', 'exactly', 'the', 'same', 'parameters', 'lambda', 'etc', 'to', 'generate', 'any', 'outputs', 'which', 'are', 'above', 'the', 'threshold', 'meaning', 'the', 'precision', 'equals', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 've', 'successfully', 'trained', 'nns', 'before', 'without', 'these', 'unusual', 'results', 'albeit', 'the', 'goal', 'was', 'a', 'somewhat', 'more', 'conventional', 'multi', 'class', 'classifier', 'with', 'many', 'more', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'wondering', 'if', 'the', 'training', 'nns', 'with', 'fewer', 'features', 'increases', 'the', 'chances', 'of', 'it', 'getting', 'stuck', 'at', 'a', 'local', 'optima', 'or', 'getting', 'stuck', 'at', 'local', 'optima', 'has', 'a', 'more', 'significant', 'impact', 'on', 'nns', 'with', 'fewer', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'thoughts', 'on', 'what', 's', 'going', 'on', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'the', 'common', 'best', 'practices', 'to', 'handle', 'time', 'data', 'for', 'machine', 'learning', 'application', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'in', 'data', 'set', 'there', 'is', 'a', 'column', 'with', 'timestamp', 'of', 'event', 'such', 'as', 'quot', 'quot', 'how', 'you', 'can', 'extract', 'useful', 'features', 'from', 'this', 'column', 'if', 'any', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'most', 'online', 'tutorials', 'like', 'to', 'use', 'a', 'simple', 'example', 'to', 'introduce', 'to', 'machine', 'learning', 'by', 'classify', 'unknown', 'text', 'in', 'spam', 'or', 'not', 'spam', 'they', 'say', 'that', 'this', 'is', 'a', 'binary', 'class', 'problem', 'but', 'why', 'is', 'this', 'a', 'binary', 'class', 'problem', 'i', 'think', 'it', 'is', 'a', 'one', 'class', 'problem', 'i', 'do', 'only', 'need', 'positive', 'samples', 'of', 'my', 'inbox', 'to', 'learn', 'what', 'is', 'not', 'spam', 'if', 'i', 'do', 'take', 'a', 'bunch', 'of', 'not', 'spam', 'textes', 'as', 'positiv', 'samples', 'and', 'a', 'bunch', 'of', 'spam', 'mails', 'as', 'negativ', 'samples', 'then', 'of', 'course', 'it', 's', 'possible', 'to', 'train', 'a', 'binary', 'classifier', 'and', 'make', 'predictions', 'from', 'unlabeled', 'data', 'but', 'where', 'is', 'the', 'difference', 'to', 'the', 'onc', 'class', 'approach', 'there', 'i', 'would', 'just', 'define', 'a', 'training', 'set', 'of', 'all', 'non', 'spam', 'examples', 'and', 'train', 'some', 'one', 'class', 'classifier', 'what', 'do', 'you', 'think', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'doing', 'some', 'data', 'analysis', 'in', 'a', 'statistical', 'pattern', 'recognition', 'course', 'using', 'prml', 'we', 'analyzed', 'a', 'lot', 'of', 'matrix', 'properties', 'like', 'eigenvalues', 'column', 'independence', 'positive', 'semi', 'definite', 'matrix', 'etc', 'when', 'we', 'are', 'doing', 'for', 'example', 'linear', 'regression', 'we', 'need', 'to', 'calculate', 'some', 'of', 'those', 'properties', 'and', 'fit', 'them', 'into', 'the', 'equation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'my', 'question', 'is', 'my', 'question', 'is', 'about', 'the', 'intuition', 'behind', 'these', 'matrix', 'properties', 'and', 'their', 'implications', 'in', 'the', 'ml', 'dm', 'literature', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'anyone', 'could', 'answer', 'can', 'you', 'teach', 'me', 'what', 'is', 'the', 'importance', 'of', 'eigenvalue', 'positive', 'semi', 'definite', 'matrix', 'and', 'column', 'independence', 'for', 'ml', 'dm', 'and', 'possibly', 'other', 'important', 'matrix', 'properties', 'you', 'think', 'important', 'in', 'study', 'the', 'dataset', 'and', 'why', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'be', 'really', 'appreciated', 'if', 'someone', 'can', 'answer', 'this', 'question', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'examining', 'the', 'activity', 'of', 'customers', 'over', 'the', 'years', 'which', 'have', 'about', 'one', 'event', 'per', 'year', 'this', 'results', 'is', 'many', 'short', 'time', 'series', 'for', 'which', 'i', 'found', 'the', 'distributions', 'hit', 'miss', 'over', 'years', 'sorted', 'by', 'probability', 'in', 'the', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'apparently', 'a', 'purely', 'uncorrelated', 'binomial', 'model', 'wouldn', 't', 'do', 'but', 'one', 'can', 'observe', 'that', 'if', 'both', 'the', 'number', 'of', 's', 'and', 's', 'coincide', 'then', 'the', 'probabilities', 'are', 'approximately', 'equal', 'apart', 'from', 'a', 'small', 'recency', 'effect', 'of', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'you', 'see', 'a', 'way', 'to', 'approach', 'such', 'data', 'to', 'deduce', 'a', 'probabilistic', 'model', 'basically', 'where', 'i', 'have', 'only', 'a', 'few', 'probability', 'parameters', 'which', 'roughly', 'explain', 'this', 'distribution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'planing', 'to', 'write', 'a', 'classification', 'program', 'that', 'is', 'able', 'to', 'classify', 'unknown', 'text', 'in', 'around', 'different', 'categories', 'and', 'if', 'none', 'of', 'them', 'fits', 'it', 'would', 'be', 'nice', 'to', 'know', 'that', 'it', 'is', 'also', 'possible', 'that', 'more', 'then', 'one', 'category', 'is', 'right', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'predefined', 'categories', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'c', 'quot', 'politics', 'quot', 'xa', 'c', 'quot', 'biology', 'quot', 'xa', 'c', 'quot', 'food', 'quot', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'thinking', 'about', 'the', 'right', 'approach', 'in', 'how', 'to', 'represent', 'my', 'training', 'data', 'or', 'what', 'kind', 'of', 'classification', 'is', 'the', 'right', 'one', 'the', 'first', 'challenge', 'is', 'about', 'finding', 'the', 'right', 'features', 'if', 'i', 'only', 'have', 'text', 'words', 'each', 'what', 'method', 'would', 'you', 'recommend', 'to', 'find', 'the', 'right', 'features', 'my', 'first', 'approach', 'is', 'to', 'remove', 'all', 'stop', 'words', 'and', 'use', 'the', 'pos', 'tagger', 'lt', 'a', 'href', 'quot', 'http', 'nlp', 'stanford', 'edu', 'software', 'tagger', 'shtml', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'stanford', 'nlp', 'pos', 'tagger', 'lt', 'a', 'gt', 'to', 'find', 'nouns', 'adjective', 'etc', 'i', 'count', 'them', 'an', 'use', 'all', 'frequently', 'appeared', 'words', 'as', 'features', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'e', 'g', 'politics', 'i', 've', 'around', 'text', 'entities', 'with', 'the', 'mentioned', 'pos', 'tagger', 'i', 'found', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'law', 'xa', 'capitalism', 'xa', 'president', 'xa', 'democracy', 'xa', 'executive', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'would', 'it', 'be', 'right', 'to', 'use', 'only', 'that', 'as', 'features', 'the', 'trainings', 'set', 'would', 'then', 'look', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'training', 'set', 'for', 'politics', 'xa', 'feature', 'law', 'numeric', 'xa', 'feature', 'capitalism', 'numeric', 'xa', 'feature', 'president', 'numeric', 'xa', 'feature', 'democracy', 'numeric', 'xa', 'feature', 'executive', 'numeric', 'xa', 'class', 'politics', 'all_others', 'xa', 'xa', 'sample', 'data', 'xa', 'politics', 'xa', 'politics', 'xa', 'politics', 'xa', 'politics', 'xa', 'xa', 'all_others', 'xa', 'all_others', 'xa', 'all_others', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'would', 'this', 'be', 'a', 'right', 'approach', 'for', 'binary', 'classification', 'or', 'how', 'would', 'i', 'define', 'my', 'sets', 'or', 'is', 'multi', 'class', 'classification', 'the', 'right', 'approach', 'then', 'it', 'would', 'look', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'training', 'set', 'for', 'politics', 'xa', 'feature', 'law', 'numeric', 'xa', 'feature', 'capitalism', 'numeric', 'xa', 'feature', 'president', 'numeric', 'xa', 'feature', 'democracy', 'numeric', 'xa', 'feature', 'executive', 'numeric', 'xa', 'feature', 'genetics', 'numeric', 'xa', 'feature', 'muscle', 'numeric', 'xa', 'feature', 'blood', 'numeric', 'xa', 'feature', 'burger', 'numeric', 'xa', 'feature', 'salad', 'numeric', 'xa', 'feature', 'cooking', 'numeric', 'xa', 'class', 'politics', 'biology', 'food', 'xa', 'xa', 'sample', 'data', 'xa', 'politics', 'xa', 'politics', 'xa', 'politics', 'xa', 'politics', 'xa', 'xa', 'biology', 'xa', 'biology', 'xa', 'biology', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'would', 'you', 'say', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'a', 'newbie', 'to', 'data', 'science', 'with', 'a', 'typical', 'problem', 'i', 'have', 'a', 'data', 'set', 'with', 'metric', 'metric', 'and', 'metric', 'all', 'these', 'metrics', 'are', 'interdependent', 'on', 'each', 'other', 'i', 'want', 'to', 'detect', 'anomalies', 'in', 'metric', 'currently', 'i', 'am', 'using', 'nupic', 'from', 'numenta', 'org', 'for', 'my', 'analysis', 'and', 'it', 'doesn', 't', 'seem', 'to', 'be', 'effective', 'is', 'there', 'any', 'ml', 'library', 'which', 'can', 'detect', 'anomalies', 'in', 'multiple', 'parameters', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'tried', 'to', 'use', 'omp', 'algorithm', 'available', 'in', 'scikit', 'learn', 'my', 'net', 'datasize', 'which', 'includes', 'both', 'target', 'signal', 'and', 'dictionary', 'g', 'however', 'when', 'i', 'ran', 'the', 'code', 'it', 'exited', 'with', 'mem', 'error', 'xa', 'the', 'machine', 'has', 'g', 'ram', 'so', 'i', 'don', 't', 'think', 'this', 'should', 'have', 'happened', 'i', 'tried', 'with', 'some', 'logging', 'where', 'the', 'error', 'came', 'and', 'found', 'that', 'the', 'data', 'got', 'loaded', 'completely', 'into', 'numpy', 'arrays', 'and', 'it', 'was', 'the', 'algorithm', 'itself', 'that', 'caused', 'the', 'error', 'can', 'someone', 'help', 'me', 'with', 'this', 'xa', 'or', 'sugggest', 'more', 'memory', 'efficient', 'algorithm', 'for', 'feature', 'selection', 'or', 'is', 'subsampling', 'the', 'xa', 'data', 'my', 'only', 'option', 'are', 'there', 'some', 'deterministic', 'good', 'subsampling', 'techniques', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'xa', 'relevant', 'code', 'piece', 'lt', 'p', 'gt', 'xa', 'xa', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'n', 'xa', 'y', 'mydata', 'xa', 'x', 'mydata', 'xa', 'print', 'y', 'xa', 'print', 'x', 'xa', 'print', 'quot', 'here', 'quot', 'xa', 'omp', 'orthogonalmatchingpursuit', 'n_nonzero_coefs', 'copy_x', 'false', 'normalize', 'true', 'xa', 'omp', 'fit', 'x', 'y', 'xa', 'coef', 'omp', 'coef_', 'xa', 'print', 'omp', 'coef_', 'xa', 'idx_r', 'coef', 'nonzero', 'xa', 'for', 'id', 'in', 'idx_r', 'xa', 'print', 'coef', 'id', 'vars', 'id', 'quot', 'n', 'quot', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'error', 'i', 'get', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'file', 'quot', 'usr', 'local', 'lib', 'python', 'dist', 'packages', 'sklearn', 'base', 'py', 'quot', 'line', 'in', 'score', 'xa', 'return', 'r', '_score', 'y', 'self', 'predict', 'x', 'sample_weight', 'sample_weight', 'xa', 'file', 'quot', 'usr', 'local', 'lib', 'python', 'dist', 'packages', 'sklearn', 'metrics', 'metrics', 'py', 'quot', 'line', 'in', 'r', '_score', 'xa', 'numerator', 'weight', 'y_true', 'y_pred', 'sum', 'dtype', 'np', 'float', 'xa', 'memoryerror', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'most', 'data', 'acquisition', 'settings', 'it', 'is', 'useful', 'to', 'tag', 'your', 'data', 'with', 'time', 'and', 'location', 'if', 'i', 'write', 'the', 'data', 'to', 'csv', 'file', 'what', 'are', 'the', 'best', 'formats', 'that', 'i', 'can', 'use', 'for', 'this', 'two', 'variables', 'if', 'i', 'want', 'to', 'create', 'a', 'heatmap', 'on', 'google', 'maps', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'continuous', 'variable', 'sampled', 'over', 'a', 'period', 'of', 'a', 'year', 'at', 'irregular', 'intervals', 'some', 'days', 'have', 'more', 'than', 'one', 'observation', 'per', 'hour', 'while', 'other', 'periods', 'have', 'nothing', 'for', 'days', 'this', 'makes', 'it', 'particularly', 'difficult', 'to', 'detect', 'patterns', 'in', 'the', 'time', 'series', 'because', 'some', 'months', 'for', 'instance', 'october', 'are', 'highly', 'sampled', 'while', 'others', 'are', 'not', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'mext', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'question', 'is', 'what', 'would', 'be', 'the', 'best', 'approach', 'to', 'model', 'this', 'time', 'series', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'believe', 'most', 'time', 'series', 'analysis', 'techniques', 'like', 'arma', 'need', 'a', 'fixed', 'frequency', 'i', 'could', 'aggregate', 'the', 'data', 'in', 'order', 'to', 'have', 'a', 'constant', 'sample', 'or', 'choose', 'a', 'sub', 'set', 'of', 'the', 'data', 'that', 'is', 'very', 'detailed', 'with', 'both', 'options', 'i', 'would', 'be', 'missing', 'some', 'information', 'from', 'the', 'original', 'dataset', 'that', 'could', 'unveil', 'distinct', 'patterns', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'instead', 'of', 'decomposing', 'the', 'series', 'in', 'cycles', 'i', 'could', 'feed', 'the', 'model', 'xa', 'with', 'the', 'entire', 'dataset', 'and', 'expect', 'it', 'to', 'pick', 'up', 'the', 'patterns', 'for', 'xa', 'instance', 'i', 'transformed', 'the', 'hour', 'weekday', 'and', 'month', 'in', 'categorical', 'xa', 'variables', 'and', 'tried', 'a', 'multiple', 'regression', 'with', 'good', 'results', 'r', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'the', 'idea', 'that', 'machine', 'learning', 'techniques', 'such', 'as', 'ann', 'can', 'also', 'pick', 'these', 'patterns', 'from', 'uneven', 'time', 'series', 'but', 'i', 'was', 'wondering', 'if', 'anybody', 'has', 'tried', 'that', 'and', 'could', 'provide', 'me', 'some', 'advice', 'about', 'the', 'best', 'way', 'of', 'representing', 'time', 'patterns', 'in', 'a', 'neural', 'network', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'd', 'like', 'to', 'apply', 'some', 'of', 'the', 'more', 'complex', 'supervised', 'machine', 'learning', 'techniques', 'in', 'python', 'deep', 'learning', 'generalized', 'addative', 'models', 'proper', 'implementation', 'of', 'regularization', 'other', 'cool', 'stuff', 'i', 'dont', 'even', 'know', 'about', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'recommendations', 'how', 'i', 'could', 'find', 'expert', 'ml', 'folks', 'that', 'would', 'like', 'to', 'collaborate', 'on', 'projects', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'had', 'a', 'conversation', 'with', 'someone', 'recently', 'and', 'mentioned', 'my', 'interest', 'in', 'data', 'analysis', 'and', 'who', 'i', 'intended', 'to', 'learn', 'the', 'necessary', 'skills', 'and', 'tools', 'they', 'suggested', 'to', 'me', 'that', 'while', 'it', 'is', 'great', 'to', 'learn', 'the', 'tools', 'and', 'build', 'the', 'skills', 'there', 'is', 'little', 'point', 'in', 'doing', 'so', 'unless', 'i', 'have', 'specialized', 'knowledge', 'in', 'a', 'specific', 'field', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'they', 'basically', 'summed', 'it', 'to', 'that', 'i', 'd', 'just', 'be', 'like', 'a', 'builder', 'with', 'a', 'pile', 'of', 'tools', 'who', 'could', 'build', 'a', 'few', 'wooden', 'boxes', 'and', 'may', 'be', 'build', 'better', 'things', 'cabins', 'cupboards', 'etc', 'but', 'without', 'knowledge', 'in', 'a', 'specific', 'field', 'i', 'd', 'never', 'be', 'a', 'builder', 'people', 'would', 'come', 'to', 'for', 'a', 'specific', 'product', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'has', 'anyone', 'found', 'this', 'or', 'have', 'any', 'input', 'on', 'what', 'to', 'make', 'of', 'this', 'it', 'would', 'seem', 'if', 'it', 'was', 'true', 'one', 'would', 'have', 'to', 'learn', 'the', 'data', 'science', 'aspects', 'of', 'things', 'and', 'then', 'learn', 'a', 'new', 'field', 'just', 'to', 'become', 'specialized', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'h', 'gt', 'my', 'background', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'a', 'graduate', 'student', 'in', 'civil', 'engineering', 'for', 'the', 'analyses', 'of', 'road', 'traffic', 'data', 'vehicle', 'trajectories', 'as', 'time', 'series', 'i', 'work', 'with', 'big', 'data', 'sets', 'mostly', 'about', 'a', 'million', 'data', 'points', 'or', 'more', 'lt', 'br', 'gt', 'xa', 'i', 'started', 'using', 'r', 'language', 'when', 'ms', 'excel', 'could', 'not', 'open', 'the', 'big', 'data', 'files', 'using', 'basic', 'statistics', 'knowledge', 'and', 'r', 'code', 'i', 'developed', 'few', 'algorithms', 'to', 'identify', 'certain', 'patterns', 'in', 'the', 'data', 'which', 'worked', 'for', 'many', 'applications', 'but', 'i', 'still', 'lack', 'serious', 'programming', 'skills', 'in', 'r', 'lt', 'br', 'gt', 'xa', 'now', 'i', 'am', 'familiar', 'with', 'basic', 'inferential', 'statistics', 'and', 'r', 'packages', 'plyr', 'dplyr', 'ggplot', 'etc', 'recently', 'i', 'came', 'to', 'know', 'that', 'machine', 'learning', 'algorithms', 'also', 'help', 'in', 'defining', 'patterns', 'in', 'the', 'data', 'through', 'supervised', 'unsupervised', 'learning', 'and', 'their', 'application', 'might', 'improve', 'the', 'accuracy', 'of', 'prediction', 'of', 'certain', 'behaviors', 'of', 'drivers', 'using', 'the', 'traffic', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'h', 'gt', 'question', 'lt', 'h', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'having', 'the', 'basic', 'knowledge', 'of', 'statistics', 'and', 'r', 'i', 'want', 'to', 'learn', 'about', 'the', 'data', 'science', 'machine', 'learning', 'as', 'a', 'beginner', 'i', 'know', 'that', 'some', 'concepts', 'in', 'stats', 'and', 'ml', 'overlap', 'and', 'that', 'might', 'bridge', 'the', 'gap', 'in', 'my', 'learning', 'of', 'ml', 'keeping', 'my', 'background', 'in', 'mind', 'what', 'resources', 'books', 'online', 'courses', 'would', 'you', 'recommend', 'me', 'to', 'start', 'learning', 'data', 'science', 'and', 'apply', 'it', 'in', 'my', 'field', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'ask', 'your', 'opinion', 'on', 'how', 'to', 'choose', 'a', 'similarity', 'measure', 'i', 'have', 'a', 'set', 'of', 'vectors', 'of', 'length', 'n', 'each', 'element', 'of', 'which', 'can', 'contain', 'either', 'or', 'the', 'vectors', 'are', 'actually', 'ordered', 'sequences', 'so', 'the', 'position', 'of', 'each', 'element', 'is', 'important', 'suppose', 'i', 'have', 'three', 'vectors', 'of', 'length', 'x_', 'x', 'x', 'x', 'has', 'three', 'at', 'positions', 'indexes', 'start', 'from', 'both', 'x', 'and', 'x', 'have', 'an', 'additional', 'but', 'x', 'has', 'it', 'in', 'position', 'while', 'x', 'has', 'it', 'in', 'position', 'i', 'am', 'looking', 'for', 'a', 'metric', 'according', 'to', 'which', 'x', 'is', 'more', 'similar', 'to', 'x', 'than', 'to', 'x', 'in', 'that', 'the', 'additional', 'is', 'closer', 'to', 'the', 'quot', 'bulk', 'quot', 'of', 'ones', 'i', 'guess', 'this', 'is', 'a', 'relatively', 'common', 'problem', 'but', 'i', 'am', 'confused', 'on', 'the', 'best', 'way', 'to', 'approach', 'it', 'xa', 'many', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'just', 'starting', 'to', 'work', 'on', 'a', 'relatively', 'large', 'dataset', 'after', 'ml', 'course', 'in', 'coursera', 'xa', 'trying', 'to', 'work', 'on', 'lt', 'a', 'href', 'quot', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'datasets', 'yearpredictionmsd', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'datasets', 'yearpredictionmsd', 'lt', 'a', 'gt', 'xa', 'got', 'an', 'accuracy', 'of', 'in', 'training', 'and', 'test', 'set', 'with', 'linear', 'regression', 'using', 'gradient', 'descent', 'in', 'octave', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'tried', 'adding', 'all', 'possible', 'quadratic', 'features', 'instances', 'and', 'features', 'but', 'the', 'code', 'just', 'won', 't', 'stop', 'executing', 'in', 'my', 'hp', 'pavilion', 'g', 'tx', 'with', 'gb', 'ram', 'in', 'ubuntu', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'this', 'beyond', 'the', 'data', 'size', 'capacity', 'of', 'octave', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'data', 'set', 'that', 'is', 'pivoted', 'in', 'to', 'the', 'following', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'key', 'id', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'key', 'could', 'be', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'products', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'products', 'is', 'the', 'category', 'of', 'item', 'being', 'reviewed', 'and', 'key', 'is', 'the', 'identifier', 'for', 'the', 'product', 'the', 'only', 'fields', 'are', 'individual', 'daily', 'samples', 'identify', 'how', 'many', 'of', 'quot', 'x', 'quot', 'product', 'were', 'logged', 'as', 'either', 'sold', 'in', 'stock', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'i', 'need', 'to', 'do', 'is', 'perform', 'some', 'kind', 'of', 'analysis', 'on', 'an', 'entire', 'slew', 'of', 'products', 'and', 'their', 'inventory', 'levels', 'i', 'e', 'each', 'import', 'of', 'data', 'i', 'need', 'to', 'make', 'sure', 'the', 'incoming', 'data', 'is', 'accurate', 'or', 'predictably', 'accurate', 'and', 'that', 'some', 'human', 'did', 'not', 'typo', 'a', 'stock', 'level', 'the', 'problem', 'is', 'using', 'a', 'simple', 'average', 'or', 'rolling', 'average', 'can', 'introduce', 'significant', 'variance', 'and', 'smoothing', 'out', 'the', 'average', 'renders', 'my', 'analysis', 'less', 'reliable', 'ideally', 'this', 'analysis', 'would', 'trigger', 'an', 'alarm', 'that', 'someone', 'would', 'have', 'to', 'investigate', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'better', 'and', 'more', 'accurate', 'way', 'of', 'performing', 'this', 'analysis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'meaning', 'of', 'multi', 'class', 'classification', 'rules', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'example', 'i', 'have', 'two', 'classification', 'rules', 'refund', 'is', 'a', 'predictor', 'and', 'cheat', 'is', 'a', 'binary', 'response', 'xa', 'refund', 'no', 'cheat', 'no', 'support', 'confidence', 'xa', 'refund', 'no', 'cheat', 'yes', 'support', 'confidence', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'gt', 'multi', 'class', 'classification', 'rules', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'refund', 'no', 'cheat', 'no', 'v', 'cheat', 'yes', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'when', 'predicted', 'classification', 'for', 'test', 'data', 'cheat', 'no', 'will', 'be', 'selected', 'priority', 'so', 'why', 'we', 'need', 'to', 'have', 'cheat', 'yes', 'in', 'multi', 'class', 'classification', 'rules', 'here', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'by', 'quot', 'large', 'quot', 'i', 'mean', 'in', 'the', 'range', 'of', 'm', 'to', 'b', 'rows', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'currently', 'using', 'both', 'hadoop', 'mapreduce', 'and', 'amazon', 'redshift', 'mapreduce', 'has', 'been', 'a', 'little', 'disappointing', 'here', 'redshift', 'works', 'very', 'well', 'if', 'the', 'data', 'is', 'distributed', 'well', 'for', 'the', 'given', 'query', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'other', 'technologies', 'that', 'i', 'should', 'be', 'looking', 'at', 'here', 'if', 'so', 'what', 'are', 'the', 'trade', 'offs', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'going', 'through', 'an', 'ieee', 'research', 'paper', 'which', 'has', 'used', 'fuzzy', 'artmap', 'for', 'predicting', 'the', 'price', 'of', 'electricity', 'given', 'some', 'highly', 'correlated', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'per', 'my', 'basic', 'understanding', 'about', 'fuzzy', 'artmap', 'it', 'is', 'a', 'classification', 'algorithm', 'so', 'how', 'will', 'it', 'be', 'able', 'to', 'predict', 'continuous', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'text', 'from', 'research', 'paper', 'is', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'in', 'the', 'architecture', 'of', 'the', 'fa', 'network', 'the', 'preprocessing', 'stages', 'take', 'xa', 'the', 'input', 'vector', 'and', 'contribute', 'to', 'produce', 'complement', 'coding', 'which', 'xa', 'avoids', 'category', 'proliferation', 'i', 'e', 'the', 'creation', 'of', 'a', 'relatively', 'xa', 'large', 'number', 'of', 'categories', 'to', 'represent', 'the', 'training', 'data', 'a', 'sequence', 'xa', 'of', 'input', 'vectors', 'price', 'and', 'demand', 'and', 'their', 'respective', 'target', 'xa', 'vectors', 'are', 'introduced', 'to', 'the', 'fa', 'network', 'in', 'order', 'to', 'classify', 'the', 'xa', 'input', 'pattern', 'correctly', 'the', 'classi', 'ed', 'input', 'patterns', 'are', 'then', 'grouped', 'xa', 'into', 'labels', 'using', 'membership', 'functions', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'was', 'using', 'matlab', 'to', 'implement', 'the', 'same', 'so', 'is', 'there', 'a', 'library', 'in', 'matlab', 'to', 'approach', 'towards', 'the', 'solution', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'to', 'collect', 'several', 'large', 'datasets', 'thousands', 'of', 'samples', 'dozens', 'of', 'features', 'for', 'regression', 'with', 'only', 'categorical', 'inputs', 'i', 'already', 'look', 'for', 'such', 'datasets', 'in', 'the', 'uci', 'repository', 'but', 'i', 'did', 'not', 'find', 'any', 'suitable', 'one', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'does', 'anybody', 'know', 'of', 'any', 'such', 'dataset', 'or', 'of', 'any', 'additional', 'dataset', 'repository', 'on', 'the', 'internet', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'n', 'this', 'paper', 'lt', 'a', 'href', 'quot', 'http', 'ronan', 'collobert', 'com', 'pub', 'matos', '_nlp_icml', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'they', 'suppose', 'a', 'quot', 'unified', 'architecture', 'for', 'nlp', 'quot', 'with', 'deep', 'neural', 'networks', 'with', 'multitask', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'problem', 'is', 'to', 'understand', 'the', 'layer', 'architecture', 'in', 'figure', 'see', 'below', 'xa', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'rou', 'p', 'png', 'quot', 'alt', 'quot', 'unified', 'deep', 'learning', 'architecture', 'for', 'nlp', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'someone', 'able', 'to', 'give', 'me', 'a', 'concrete', 'reproducible', 'example', 'how', 'this', 'architecture', 'processing', 'sentences', 'through', 'their', 'layers', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'are', 'the', 'outputs', 'after', 'each', 'layer', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'why', 'they', 'choose', 'which', 'layer', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thans', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'lt', 'a', 'href', 'quot', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'datasets', 'molecular', 'biology', 'splice', 'junction', 'gene', 'sequences', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'database', 'lt', 'a', 'gt', 'of', 'instances', 'of', 'dna', 'consisting', 'of', 'sequential', 'dna', 'nucleotide', 'positions', 'classified', 'according', 'to', 'types', 'ei', 'ie', 'other', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'formulate', 'a', 'supervised', 'classifier', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'present', 'approach', 'is', 'to', 'formulate', 'a', 'nd', 'order', 'markov', 'transition', 'matrix', 'for', 'each', 'instance', 'and', 'apply', 'the', 'resulting', 'data', 'to', 'a', 'neural', 'network', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'best', 'to', 'approach', 'this', 'classification', 'problem', 'given', 'that', 'the', 'sequence', 'of', 'the', 'data', 'should', 'be', 'relevant', 'is', 'there', 'a', 'better', 'approach', 'than', 'the', 'one', 'i', 'came', 'up', 'with', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'need', 'to', 'analyse', 'a', 'dataset', 'about', 'mobile', 'phone', 'usage', 'calls', 'sms', 'internetconnections', 'per', 'each', 'cell', 'and', 'hour', 'in', 'the', 'different', 'days', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'date', 'cdr', 'position', 'calls', 'sms', 'internetconnections', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'purpose', 'is', 'detecting', 'similarities', 'in', 'the', 'data', 'monday', 'tuesday', 'is', 'similar', 'or', 'monday', 'night', 'is', 'different', 'after', 'this', 'i', 'd', 'like', 'to', 'find', 'the', 'reason', 'they', 'are', 'similar', 'dissimilar', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'can', 'i', 'apply', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'biosemi', 'bdf', 'of', 'eeg', 'data', 'which', 'contains', 'channel', 'xa', 'i', 've', 'opened', 'it', 'using', 'biosig', 'everything', 'works', 'great', 'a', 'first', 'list', 'is', 'channel', 'and', 'inside', 'each', 'list', 'there', 'are', 'eeg', 'data', 'xa', 'but', 'if', 'i', 'open', 'it', 'using', 'mne', 'it', 'the', 'first', 'list', 'is', 'eeg', 'data', 'and', 'the', 'second', 'list', 'inside', 'the', 'list', 'of', 'eeg', 'data', 'are', 'two', 'list', 'of', 'eeg', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'is', 'how', 'i', 'open', 'the', 'data', 'using', 'mne', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'raw_file', 'read_raw_edf', 'quot', 'e', 'eegdata', 's', '_reduced', 'bdf', 'quot', 'preload', 'true', 'verbose', 'true', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'am', 'i', 'missing', 'something', 'here', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'create', 'a', 'logistic', 'regression', 'model', 'in', 'jpmml', 'then', 'write', 'the', 'pmml', 'to', 'a', 'file', 'the', 'problem', 'i', 'm', 'having', 'is', 'that', 'i', 'can', 't', 'find', 'any', 'way', 'to', 'create', 'a', 'custom', 'tag', 'such', 'as', 'quot', 'shortform', 'quot', 'and', 'quot', 'longform', 'quot', 'in', 'the', 'following', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'amp', 'lt', 'mapvalues', 'outputcolumn', 'quot', 'longform', 'quot', 'amp', 'gt', 'xa', 'amp', 'lt', 'fieldcolumnpair', 'field', 'quot', 'gender', 'quot', 'column', 'quot', 'shortform', 'quot', 'amp', 'gt', 'xa', 'amp', 'lt', 'inlinetable', 'amp', 'gt', 'xa', 'amp', 'lt', 'row', 'amp', 'gt', 'amp', 'lt', 'shortform', 'amp', 'gt', 'm', 'amp', 'lt', 'shortform', 'amp', 'gt', 'amp', 'lt', 'longform', 'amp', 'gt', 'male', 'amp', 'lt', 'longform', 'amp', 'gt', 'xa', 'amp', 'lt', 'row', 'amp', 'gt', 'xa', 'amp', 'lt', 'row', 'amp', 'gt', 'amp', 'lt', 'shortform', 'amp', 'gt', 'f', 'amp', 'lt', 'shortform', 'amp', 'gt', 'amp', 'lt', 'longform', 'amp', 'gt', 'female', 'amp', 'lt', 'longform', 'amp', 'gt', 'xa', 'amp', 'lt', 'row', 'amp', 'gt', 'xa', 'amp', 'lt', 'inlinetable', 'amp', 'gt', 'xa', 'amp', 'lt', 'mapvalues', 'amp', 'gt', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 's', 'what', 'i', 'have', 'so', 'far', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'mapvalues', 'mv', 'new', 'mapvalues', 'quot', 'output', 'quot', 'xa', 'withfieldcolumnpairs', 'xa', 'new', 'fieldcolumnpair', 'new', 'fieldname', 'quot', 'gender', 'quot', 'quot', 'shortform', 'quot', 'xa', 'withinlinetable', 'xa', 'new', 'inlinetable', 'withrows', 'xa', 'new', 'row', 'with', 'new', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'short', 'i', 'am', 'asking', 'for', 'an', 'api', 'call', 'i', 'can', 'use', 'to', 'instantiate', 'the', 'quot', 'shortform', 'quot', 'element', 'in', 'the', 'example', 'and', 'attach', 'it', 'to', 'the', 'quot', 'row', 'quot', 'object', 'i', 've', 'been', 'all', 'through', 'the', 'api', 'examples', 'and', 'google', 'so', 'and', 'can', 't', 'find', 'a', 'thing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'your', 'help', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 'have', 'a', 'ruby', 'on', 'rails', 'platform', 'w', 'postgresql', 'db', 'for', 'people', 'to', 'upload', 'various', 'products', 'to', 'trade', 'of', 'course', 'many', 'of', 'these', 'products', 'listed', 'are', 'the', 'same', 'while', 'they', 'are', 'described', 'differently', 'by', 'the', 'consumer', 'either', 'through', 'spelling', 'case', 'etc', 'quot', 'lots', 'of', 'duplicates', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'the', 'purposes', 'of', 'analytics', 'and', 'a', 'better', 'ux', 'we', 're', 'aiming', 'to', 'create', 'an', 'evolving', 'quot', 'master', 'product', 'list', 'quot', 'or', 'quot', 'whitelist', 'quot', 'if', 'you', 'will', 'that', 'will', 'have', 'users', 'select', 'from', 'an', 'existing', 'list', 'of', 'products', 'they', 'are', 'uploading', 'or', 'request', 'to', 'add', 'a', 'new', 'one', 'we', 'also', 'plan', 'to', 'enrich', 'each', 'product', 'entry', 'with', 'additional', 'information', 'from', 'the', 'web', 'that', 'would', 'be', 'tied', 'to', 'the', 'quot', 'master', 'product', 'quot', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'are', 'some', 'methods', 'we', 're', 'proposing', 'to', 'solve', 'this', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'take', 'all', 'the', 'quot', 'items', 'quot', 'listed', 'in', 'the', 'website', 'de', 'dupe', 'as', 'much', 'as', 'possible', 'by', 'running', 'select', 'quot', 'distinct', 'quot', 'queries', 'while', 'maintaining', 'a', 'key', 'map', 'back', 'to', 'original', 'data', 'by', 'generating', 'an', 'array', 'of', 'item', 'keys', 'from', 'each', 'distinct', 'listing', 'in', 'a', 'group', 'by', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'then', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'running', 'this', 'data', 'through', 'mechanical', 'turk', 'and', 'asking', 'each', 'turk', 'user', 'to', 'list', 'data', 'in', 'a', 'uniform', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'running', 'each', 'product', 'entry', 'through', 'the', 'amazon', 'products', 'api', 'and', 'asking', 'the', 'user', 'to', 'identify', 'a', 'match', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'a', 'better', 'method', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'for', 'problems', 'where', 'the', 'data', 'represents', 'online', 'fraud', 'or', 'insurance', 'where', 'each', 'row', 'represents', 'a', 'transaction', 'it', 'is', 'typical', 'for', 'the', 'response', 'variable', 'to', 'denote', 'the', 'value', 'of', 'fraud', 'committed', 'in', 'dollars', 'such', 'a', 'response', 'value', 'might', 'have', 'less', 'than', 'non', 'zero', 'values', 'denoting', 'fraudulent', 'transactions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'two', 'questions', 'regarding', 'such', 'a', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'algorithms', 'can', 'we', 'use', 'to', 'ensure', 'that', 'the', 'model', 'not', 'only', 'predicts', 'the', 'fraudulent', 'transactions', 'accurately', 'but', 'also', 'predicts', 'the', 'value', 'of', 'fraud', 'associated', 'with', 'these', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'assuming', 'that', 'we', 'can', 'quantify', 'the', 'cost', 'involved', 'in', 'each', 'false', 'positive', 'tagging', 'a', 'non', 'fraudulent', 'transaction', 'as', 'fraudulent', 'and', 'cost', 'incurred', 'due', 'to', 'a', 'false', 'negative', 'tagging', 'a', 'fraudulent', 'transaction', 'as', 'non', 'fraudulent', 'how', 'can', 'we', 'optimize', 'the', 'model', 'to', 'maximize', 'savings', 'or', 'minimize', 'losses', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'we', 're', 'currently', 'using', 'redshift', 'as', 'our', 'data', 'warehouse', 'which', 'we', 're', 'very', 'happy', 'with', 'however', 'we', 'now', 'have', 'a', 'requirement', 'to', 'do', 'machine', 'learning', 'against', 'the', 'data', 'in', 'our', 'warehouse', 'given', 'the', 'volume', 'of', 'data', 'involved', 'ideally', 'i', 'd', 'want', 'to', 'run', 'the', 'computation', 'in', 'the', 'same', 'location', 'as', 'the', 'data', 'rather', 'than', 'shifting', 'the', 'data', 'around', 'but', 'this', 'doesn', 't', 'seem', 'possible', 'with', 'redshift', 'i', 've', 'looked', 'at', 'madlib', 'but', 'this', 'is', 'not', 'an', 'option', 'as', 'redshift', 'does', 'not', 'support', 'udfs', 'which', 'madlib', 'requires', 'i', 'm', 'currently', 'looking', 'at', 'shifting', 'the', 'data', 'over', 'to', 'emr', 'and', 'processing', 'it', 'with', 'the', 'apache', 'spark', 'machine', 'learning', 'library', 'or', 'maybe', 'h', 'or', 'mahout', 'or', 'whatever', 'so', 'my', 'questions', 'are', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'there', 'a', 'better', 'way', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'if', 'not', 'how', 'should', 'i', 'make', 'the', 'data', 'accessible', 'to', 'spark', 'the', 'options', 'i', 've', 'identified', 'so', 'far', 'include', 'use', 'sqoop', 'to', 'load', 'it', 'into', 'hdfs', 'use', 'dbinputformat', 'do', 'a', 'redshift', 'export', 'to', 's', 'and', 'have', 'spark', 'grab', 'it', 'from', 'there', 'what', 'are', 'the', 'pros', 'cons', 'for', 'these', 'different', 'approaches', 'and', 'any', 'others', 'when', 'using', 'spark', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'note', 'that', 'this', 'is', 'off', 'line', 'batch', 'learning', 'but', 'we', 'd', 'like', 'to', 'be', 'able', 'to', 'do', 'this', 'as', 'quickly', 'as', 'possible', 'so', 'that', 'we', 'can', 'iterate', 'experiments', 'quickly', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'data', 'set', 'that', 'keeps', 'track', 'of', 'who', 'referred', 'someone', 'to', 'a', 'program', 'and', 'includes', 'the', 'geo', 'coordinates', 'of', 'both', 'parties', 'for', 'each', 'record', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'will', 'be', 'the', 'best', 'way', 'to', 'visualize', 'this', 'kind', 'of', 'data', 'set', 'this', 'visualization', 'should', 'also', 'be', 'able', 'to', 'use', 'the', 'geo', 'coordinates', 'to', 'place', 'this', 'entities', 'in', 'the', 'map', 'to', 'form', 'clusters', 'or', 'to', 'superimpose', 'them', 'on', 'a', 'real', 'map', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'interested', 'in', 'an', 'algorithm', 'and', 'or', 'a', 'library', 'that', 'will', 'help', 'me', 'do', 'this', 'library', 'should', 'be', 'preferably', 'written', 'in', 'java', 'python', 'scala', 'or', 'nodejs', 'the', 'record', 'count', 'can', 'be', 'as', 'big', 'as', 'a', 'thousand', 'or', 'hundreds', 'of', 'thousands', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'use', 'latent', 'dirichlet', 'allocation', 'for', 'a', 'project', 'and', 'i', 'am', 'using', 'python', 'with', 'the', 'gensim', 'library', 'after', 'finding', 'the', 'topics', 'i', 'would', 'like', 'to', 'cluster', 'the', 'documents', 'using', 'an', 'algorithm', 'such', 'as', 'k', 'means', 'ideally', 'i', 'would', 'like', 'to', 'use', 'a', 'good', 'one', 'for', 'overlapping', 'clusters', 'so', 'any', 'recommendation', 'is', 'welcomed', 'i', 'managed', 'to', 'get', 'the', 'topics', 'but', 'they', 'are', 'in', 'the', 'form', 'of', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'minister', 'key', 'moments', 'controversial', 'prime', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'order', 'to', 'apply', 'a', 'clustering', 'algorithm', 'and', 'correct', 'me', 'if', 'i', 'm', 'wrong', 'i', 'believe', 'i', 'should', 'find', 'a', 'way', 'to', 'represent', 'each', 'word', 'as', 'a', 'number', 'using', 'either', 'tfidf', 'or', 'word', 'vec', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'do', 'you', 'have', 'any', 'ideas', 'of', 'how', 'i', 'could', 'quot', 'strip', 'quot', 'the', 'textual', 'information', 'from', 'e', 'g', 'a', 'list', 'in', 'order', 'to', 'do', 'so', 'and', 'then', 'place', 'them', 'back', 'in', 'order', 'to', 'make', 'the', 'appropriate', 'multiplication', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'the', 'way', 'i', 'see', 'it', 'if', 'the', 'word', 'minister', 'has', 'a', 'tfidf', 'weight', 'of', 'and', 'so', 'on', 'for', 'any', 'other', 'word', 'within', 'the', 'same', 'topic', 'i', 'should', 'be', 'to', 'compute', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'tfidf', 'prime', 'and', 'get', 'a', 'result', 'that', 'will', 'be', 'later', 'on', 'used', 'in', 'order', 'to', 'cluster', 'the', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'for', 'your', 'time', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'measurements', 'of', 'devices', 'at', 'two', 'different', 'points', 'of', 'time', 'a', 'measurement', 'basically', 'consists', 'of', 'an', 'array', 'of', 'ones', 'and', 'zeros', 'corresponding', 'to', 'a', 'bit', 'value', 'at', 'the', 'corresponding', 'location', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'whos', 'measurement', '_dev', '_time', 'xa', 'xa', 'name', 'size', 'bytes', 'class', 'attributes', 'xa', 'xa', 'measurement', '_dev', '_time', 'x', 'logical', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'assume', 'that', 'for', 'a', 'specific', 'device', 'the', 'changes', 'between', 'time', 'and', 'of', 'the', 'measurements', 'are', 'unique', 'however', 'since', 'i', 'am', 'dealing', 'with', 'bits', 'at', 'different', 'locations', 'it', 'is', 'quite', 'hard', 'to', 'visualize', 'if', 'there', 'is', 'some', 'kind', 'of', 'dependency', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'every', 'bit', 'at', 'location', 'lt', 'code', 'gt', 'x', 'lt', 'code', 'gt', 'can', 'be', 'regarded', 'as', 'one', 'dimension', 'of', 'an', 'observation', 'i', 'thought', 'to', 'use', 'pca', 'to', 'reduce', 'the', 'number', 'of', 'dimensions', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thus', 'for', 'every', 'of', 'the', 'devices', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'randomly', 'sample', 'lt', 'code', 'gt', 'n', 'lt', 'code', 'gt', 'measurements', 'at', 'point', 'lt', 'code', 'gt', 't', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 't', 'lt', 'code', 'gt', 'seperatly', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'i', 'prepare', 'an', 'array', 'as', 'input', 'for', 'lt', 'code', 'gt', 'pca', 'lt', 'code', 'gt', 'with', 'lt', 'code', 'gt', 'm', 'lt', 'code', 'gt', 'n', 'columns', 'lt', 'code', 'gt', 'm', 'lt', 'code', 'gt', 'amp', 'lt', 'its', 'a', 'subset', 'of', 'all', 'the', 'observed', 'bits', 'as', 'the', 'original', 'data', 'might', 'be', 'too', 'big', 'for', 'pca', 'and', 'rows', 'one', 'row', 'for', 'each', 'device', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'on', 'this', 'array', 'lt', 'code', 'gt', 'a', 'lt', 'code', 'gt', 'i', 'calculate', 'the', 'pca', 'coeff', 'score', 'latent', 'pca', 'zscore', 'a', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'then', 'i', 'try', 'to', 'visualize', 'it', 'using', 'lt', 'code', 'gt', 'biplot', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'biplot', 'coeff', 'score', 'score', 'lt', 'code', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'this', 'gives', 'me', 'really', 'strange', 'results', 'maybe', 'pca', 'is', 'not', 'the', 'right', 'approach', 'for', 'this', 'problem', 'i', 'also', 'modified', 'the', 'input', 'data', 'to', 'do', 'the', 'pca', 'not', 'on', 'the', 'logical', 'bit', 'array', 'itself', 'instead', 'i', 'created', 'a', 'vector', 'which', 'holds', 'the', 'indices', 'where', 'there', 'is', 'a', 'in', 'the', 'original', 'measurement', 'array', 'also', 'this', 'produces', 'strange', 'results', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'i', 'am', 'completely', 'new', 'to', 'pca', 'i', 'want', 'to', 'ask', 'you', 'if', 'you', 'either', 'see', 'a', 'flaw', 'in', 'the', 'process', 'or', 'if', 'pca', 'is', 'just', 'not', 'the', 'right', 'approach', 'for', 'my', 'goal', 'and', 'i', 'better', 'look', 'for', 'other', 'dimension', 'reduction', 'approaches', 'or', 'clustering', 'algorithms', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'did', 'small', 'survey', 'and', 'get', 'such', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'yes', 'no', 'dont_know', 'xa', 'xa', 'employee', 'xa', 'workers', 'xa', 'businessmen', 'xa', 'jobless', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'r', 'code', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'dt', 'amp', 'lt', 'data', 'frame', 'workers', 'c', 'quot', 'employee', 'quot', 'xa', 'quot', 'workers', 'quot', 'xa', 'quot', 'businessmen', 'quot', 'xa', 'quot', 'jobless', 'quot', 'xa', 'yes', 'c', 'xa', 'no', 'c', 'xa', 'dont_know', 'c', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'kind', 'of', 'test', 'i', 'must', 'do', 'if', 'i', 'want', 'to', 'show', 'that', 'the', 'jobless', 'people', 'are', 'often', 'choosing', 'lt', 'strong', 'gt', 'no', 'lt', 'strong', 'gt', 'answer', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'is', 'the', 'difference', 'between', 'jobless', 'and', 'businessmen', 'answers', 'significant', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'and', 'what', 'is', 'about', 'other', 'groups', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'what', 'another', 'information', 'i', 'can', 'get', 'from', 'such', 'data', 'or', 'what', 'kind', 'questions', 'i', 'can', 'ask', 'from', 'such', 'data', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'two', 'classes', 'a', 'b', 'that', 'i', 'want', 'to', 'classify', 'using', 'a', 'svm', 'say', 'that', 'i', 'have', 'a', 'class', 'c', 'and', 'a', 'function', 'f', 'can', 'i', 'do', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'xa', 'a', 'f', 'a', 'c', 'a', 'c', 'xa', 'b', 'f', 'b', 'c', 'b', 'c', 'xa', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'then', 'perform', 'the', 'classification', 'on', 'a', 'and', 'b', 'instead', 'in', 'the', 'context', 'of', 'my', 'problem', 'a', 'and', 'b', 'are', 'classes', 'where', 'elements', 'are', 'vectors', 'the', 'f', 'function', 'measures', 'the', 'mahalanobis', 'distance', 'of', 'each', 'vector', 'with', 'respect', 'to', 'the', 'distribution', 'imposed', 'by', 'c', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'dealing', 'with', 'a', 'lot', 'of', 'categorical', 'data', 'right', 'now', 'and', 'i', 'would', 'like', 'to', 'use', 'an', 'appropriate', 'data', 'mining', 'method', 'in', 'any', 'tool', 'preferably', 'r', 'to', 'find', 'the', 'effect', 'of', 'each', 'parameter', 'categorical', 'parameters', 'over', 'my', 'target', 'variable', 'to', 'give', 'a', 'brief', 'notion', 'about', 'the', 'data', 'that', 'am', 'dealing', 'with', 'my', 'target', 'variable', 'denotes', 'the', 'product', 'type', 'say', 'disposables', 'and', 'non', 'disposables', 'and', 'i', 'have', 'parameters', 'like', 'root', 'cause', 'symptom', 'customer', 'name', 'product', 'name', 'etc', 'as', 'my', 'target', 'can', 'be', 'considered', 'as', 'a', 'binary', 'value', 'i', 'tried', 'to', 'find', 'the', 'combination', 'of', 'values', 'leading', 'to', 'the', 'desired', 'categories', 'using', 'apriori', 'but', 'i', 'have', 'more', 'than', 'categories', 'in', 'that', 'attribute', 'and', 'i', 'want', 'to', 'use', 'all', 'of', 'them', 'and', 'find', 'the', 'effect', 'of', 'the', 'mentioned', 'parameters', 'over', 'each', 'category', 'i', 'really', 'wanted', 'to', 'try', 'svm', 'and', 'use', 'hyperplanes', 'to', 'separate', 'the', 'content', 'and', 'get', 'n', 'dimensional', 'view', 'but', 'i', 'do', 'not', 'have', 'enough', 'knowledge', 'to', 'validate', 'the', 'technique', 'functions', 'am', 'using', 'to', 'do', 'the', 'analysis', 'currently', 'i', 'have', 'like', 'records', 'and', 'each', 'of', 'them', 'represents', 'a', 'complaint', 'from', 'the', 'user', 'there', 'are', 'lot', 'of', 'columns', 'available', 'in', 'the', 'dataset', 'which', 'is', 'what', 'i', 'am', 'trying', 'to', 'use', 'to', 'determine', 'the', 'target', 'variable', 'myforumla', 'amp', 'lt', 'target', 'i', 'tried', 'with', 'just', 'categorical', 'columns', 'too', 'not', 'getting', 'a', 'proper', 'result', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'just', 'the', 'categorical', 'variables', 'be', 'used', 'to', 'develop', 'a', 'svm', 'model', 'and', 'get', 'visualization', 'with', 'n', 'hyper', 'planes', 'is', 'there', 'any', 'appropriate', 'data', 'mining', 'technique', 'available', 'for', 'dealing', 'with', 'just', 'the', 'categorical', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'think', 'this', 'is', 'something', 'that', 'experienced', 'programmers', 'do', 'all', 'the', 'time', 'but', 'given', 'my', 'limited', 'programming', 'experience', 'please', 'bear', 'with', 'me', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'an', 'excel', 'file', 'which', 'has', 'particular', 'cell', 'entries', 'that', 'read', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'em', 'gt', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'false', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'false', 'quot', 'value', 'quot', 'quot', 'from', 'quot', 'quot', 'quot', 'quot', 'response', 'quot', 'true', 'quot', 'value', 'quot', 'lt', 'em', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'for', 'each', 'such', 'entry', 'i', 'want', 'to', 'take', 'the', 'information', 'in', 'each', 'of', 'the', 'curly', 'brackets', 'and', 'make', 'a', 'row', 'of', 'data', 'out', 'of', 'it', 'each', 'such', 'row', 'would', 'have', 'columns', 'for', 'example', 'the', 'row', 'formed', 'from', 'the', 'first', 'entry', 'within', 'curly', 'brackets', 'should', 'have', 'the', 'entries', 'quot', 'quot', 'quot', 'true', 'quot', 'and', 'quot', 'quot', 'respectively', 'the', 'part', 'i', 'posted', 'should', 'give', 'me', 'such', 'rows', 'and', 'for', 'n', 'such', 'repetitions', 'i', 'should', 'end', 'up', 'with', 'a', 'matrix', 'of', 'n', 'rows', 'and', 'columns', 'an', 'identifier', 'plus', 'the', 'columns', 'mentioned', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'would', 'be', 'most', 'efficient', 'way', 'to', 'do', 'this', 'by', 'quot', 'doing', 'this', 'quot', 'i', 'mean', 'learning', 'the', 'trick', 'and', 'then', 'implementing', 'it', 'i', 'have', 'access', 'to', 'quite', 'a', 'few', 'software', 'packages', 'excel', 'stata', 'matlab', 'r', 'in', 'my', 'laboratory', 'so', 'that', 'should', 'not', 'be', 'an', 'issue', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'new', 'to', 'd', 'programming', 'any', 'programming', 'for', 'that', 'matter', 'i', 'have', 'protein', 'protein', 'interaction', 'data', 'in', 'json', 'format', 'and', 'csv', 'format', 'i', 'would', 'like', 'to', 'use', 'that', 'data', 'for', 'network', 'visualization', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'data', 'attributes', 'protein', 'name', 'protein', 'group', 'protein', 'type', 'protein', 'source', 'node', 'protein', 'target', 'node', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anyone', 'suggest', 'good', 'network', 'visualizations', 'for', 'such', 'data', 'how', 'does', 'it', 'work', 'with', 'hive', 'plots', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'was', 'wondering', 'if', 'anyone', 'was', 'aware', 'of', 'any', 'methods', 'for', 'visualizing', 'an', 'svm', 'model', 'where', 'there', 'are', 'more', 'than', 'three', 'continuous', 'explanatory', 'variables', 'in', 'my', 'particular', 'situation', 'my', 'response', 'variable', 'is', 'binomial', 'with', 'continuous', 'explanatory', 'variables', 'predictors', 'one', 'categorical', 'explanatory', 'variable', 'predictor', 'i', 'have', 'already', 'reduced', 'the', 'number', 'of', 'predictors', 'and', 'i', 'am', 'primarily', 'using', 'r', 'for', 'my', 'analysis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'unaware', 'if', 'such', 'a', 'task', 'is', 'possible', 'worth', 'pursuing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'your', 'time', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'this', 'is', 'my', 'first', 'ever', 'stack', 'exchange', 'question', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'build', 'a', 'tool', 'right', 'now', 'and', 'one', 'of', 'the', 'features', 'of', 'the', 'tool', 'is', 'the', 'ability', 'to', 'break', 'down', 'a', 'product', 'or', 'service', 'into', 'it', 's', 'associated', 'attributes', 'properties', 'classes', 'keywords', 'entities', 'choose', 'which', 'word', 'best', 'suits', 'as', 'i', 'have', 'no', 'idea', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'we', 'had', 'a', 'camera', 'as', 'the', 'product', 'i', 'would', 'like', 'to', 'be', 'able', 'to', 'generate', 'a', 'breakdown', 'of', 'everything', 'that', 'is', 'associated', 'to', 'a', 'camera', 'such', 'as', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'digital', 'film', 'optical', 'lcd', 'glass', 'ccd', 'cmos', 'rgb', 'lens', 'shutter', 'negative', 'polaroid', 'darkroom', 'flash', 'resolution', 'stabilisation', 'batteries', 'zoom', 'angle', 'telephoto', 'macro', 'filters', 'memory', 'cf', 'sd', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'list', 'could', 'go', 'on', 'for', 'quite', 'some', 'time', 'those', 'were', 'jsut', 'a', 'few', 'off', 'the', 'top', 'of', 'my', 'head', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'on', 'earth', 'could', 'i', 'go', 'about', 'retrieving', 'such', 'attributes', 'automatically', 'is', 'there', 'a', 'database', 'out', 'there', 'that', 'has', 'such', 'info', 'are', 'there', 'any', 'special', 'tricks', 'anyone', 'has', 'up', 'their', 'sleeve', 'to', 'be', 'able', 'to', 'accumulate', 'datasets', 'such', 'as', 'the', 'example', 'above', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'very', 'interested', 'in', 'your', 'answers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'the', 'questionnaire', 'for', 'the', 'data', 'is', 'lt', 'a', 'href', 'quot', 'http', 'www', 'cc', 'gatech', 'edu', 'gvu', 'user_surveys', 'survey', 'questions', 'general', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'first', 'question', 'takes', 'multiple', 'entry', 'for', 'the', 'same', 'question', 'i', 'want', 'to', 'reduce', 'this', 'to', 'a', 'single', 'variable', 'how', 'do', 'i', 'do', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'clean', 'data', 'is', 'available', 'lt', 'a', 'href', 'quot', 'http', 'wikisend', 'com', 'download', 'dataraw', 'arff', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'xa', 'nb', 'the', 'column', 'compuplat', 'has', 'missing', 'values', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'part', 'of', 'dataset', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'cmfam', 'cmhobb', 'cmnone', 'cmother', 'cmpol', 'cmprof', 'cmrel', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'xa', 'community', 'membership_family', 'xa', 'community', 'membership_hobbies', 'xa', 'community', 'membership_none', 'xa', 'community', 'membership_other', 'xa', 'community', 'membership_political', 'xa', 'community', 'membership_professional', 'xa', 'community', 'membership_religious', 'xa', 'community', 'membership_support', 'xa', 'lt', 'code', 'gt', 'xa', 'i', 'want', 'to', 'club', 'all', 'of', 'them', 'in', 'a', 'variable', 'cm', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'data', 'sample', 'contains', 'a', 'single', 'feature', 'random', 'integer', 'number', 'from', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'it', 'possble', 'to', 'change', 'lt', 'code', 'gt', 'lt', 'code', 'gt', 'representation', 'on', 'the', 'filter', 'card', 'to', 'some', 'custom', 'names', 'say', 'lt', 'code', 'gt', 'type', 'type', 'type', 'type', 'lt', 'code', 'gt', 'not', 'changing', 'data', 'set', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'zpym', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'currently', 'we', 'are', 'regularly', 'analyzing', 'sets', 'of', 'paragraphs', 'every', 'month', 'i', 'would', 'like', 'to', 'automate', 'this', 'and', 'split', 'each', 'paragraphs', 'into', 'chunks', 'of', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'to', 'do', 'this', 'i', 'would', 'like', 'to', 'employ', 'a', 'neural', 'network', 'however', 'i', 'am', 'not', 'really', 'very', 'familiar', 'with', 'creating', 'neural', 'networks', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'ideas', 'or', 'starting', 'point', 'on', 'how', 'to', 'do', 'this', 'using', 'neuroph', 'or', 'maybe', 'in', 'other', 'framework', 'approaches', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'for', 'more', 'info', 'as', 'suggested', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'very', 'little', 'experience', 'on', 'neural', 'networks', 'though', 'i', 'have', 'some', 'introduction', 'with', 'it', 'in', 'college', 'however', 'i', 'am', 'very', 'much', 'familar', 'with', 'java', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'data', 'is', 'around', 'megabytes', 'only', 'and', 'consists', 'of', 'rules', 'and', 'relationships', 'for', 'a', 'single', 'domain', 'this', 'means', 'that', 'the', 'data', 'is', 'complex', 'but', 'relatively', 'limited', 'though', 'still', 'free', 'form', 'english', 'language', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'gram', 'text', 'data', 'from', 'wikipedia', 'for', 'categories', 'which', 'i', 'am', 'using', 'for', 'ne', 'classification', 'xa', 'i', 'feed', 'named', 'entity', 'from', 'sentence', 'to', 'lucene', 'indexer', 'which', 'searches', 'named', 'entity', 'from', 'these', 'categories', 'xa', 'issue', 'i', 'am', 'facing', 'is', 'for', 'single', 'entity', 'i', 'get', 'multiple', 'classes', 'as', 'a', 'result', 'with', 'same', 'score', 'xa', 'like', 'while', 'search', 'lt', 'code', 'gt', 'titanic', 'lt', 'code', 'gt', 'indexer', 'gives', 'this', 'result', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'score', 'xa', 'title', 'titanic', 'xa', 'category', 'book', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'score', 'xa', 'title', 'titanic', 'xa', 'category', 'movie', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'score', 'xa', 'title', 'titanic', 'xa', 'category', 'product', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'problem', 'is', 'which', 'class', 'to', 'be', 'considered', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'already', 'tried', 'with', 'classifiers', 'nb', 'me', 'in', 'nltk', 'scikit', 'learn', 'but', 'as', 'it', 'consider', 'each', 'entity', 'from', 'dataset', 'as', 'feature', 'it', 'works', 'as', 'indexer', 'only', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'lucene', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'tz', 'uy', 'jpg', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'running', 'svm', 'algorithm', 'in', 'r', 'it', 'is', 'taking', 'long', 'time', 'to', 'run', 'the', 'algorithm', 'i', 'have', 'system', 'with', 'gb', 'ram', 'how', 'can', 'i', 'use', 'that', 'whole', 'ram', 'memory', 'to', 'speed', 'my', 'process', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'an', 'ideally', 'free', 'api', 'that', 'would', 'have', 'time', 'series', 'avg', 'median', 'housing', 'prices', 'by', 'zip', 'code', 'or', 'city', 'state', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'quandl', 'almost', 'fits', 'the', 'bill', 'but', 'it', 'returns', 'inconsistent', 'results', 'across', 'different', 'zip', 'codes', 'and', 'the', 'data', 'is', 'not', 'as', 'up', 'to', 'date', 'as', 'i', 'd', 'like', 'it', 's', 'mid', 'november', 'and', 'the', 'last', 'month', 'is', 'august', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'also', 'looked', 'at', 'zillow', 'but', 'storing', 'their', 'data', 'is', 'against', 'tos', 'and', 'at', 'calls', 'daily', 'it', 'would', 'take', 'forever', 'to', 'pull', 'in', 'the', 'necessary', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'suggestions', 'even', 'if', 'they', 'aren', 't', 'free', 'would', 'be', 'much', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'list', 'user', 'data', 'user', 'name', 'age', 'sex', 'address', 'location', 'etc', 'and', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'a', 'set', 'of', 'product', 'data', 'product', 'name', 'cost', 'description', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'i', 'would', 'like', 'to', 'build', 'a', 'recommendation', 'engine', 'that', 'will', 'be', 'able', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'figure', 'out', 'similar', 'products', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'eg', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'name', 'category', 'cost', 'ingredients', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'x', 'x', 'xx', 'xx', 'xx', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'y', 'y', 'yy', 'yy', 'yy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'z', 'x', 'xx', 'xy', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'x', 'and', 'z', 'are', 'similar', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'recommend', 'relevant', 'products', 'from', 'the', 'product', 'list', 'to', 'a', 'user', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'i', 'implement', 'this', 'using', 'mahout', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'do', 'at', 'the', 'moment', 'some', 'data', 'experiments', 'with', 'the', 'lt', 'a', 'href', 'quot', 'http', 'graphlab', 'com', 'products', 'create', 'docs', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'graphlab', 'toolkit', 'lt', 'a', 'gt', 'i', 'have', 'at', 'the', 'first', 'next', 'sframe', 'with', 'the', 'three', 'columns', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'users', 'items', 'rating', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'pair', 'in', 'the', 'same', 'row', 'from', 'every', 'lt', 'code', 'gt', 'users', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'items', 'lt', 'code', 'gt', 'values', 'build', 'the', 'unique', 'key', 'and', 'the', 'lt', 'code', 'gt', 'rating', 'lt', 'code', 'gt', 'is', 'the', 'corresponded', 'float', 'value', 'these', 'values', 'are', 'not', 'normalised', 'first', 'of', 'all', 'i', 'do', 'someself', 'next', 'normalisation', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'division', 'of', 'every', 'rating', 'value', 'of', 'specific', 'user', 'by', 'the', 'rating', 'maximum', 'from', 'this', 'user', 'scale', 'between', 'and', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'take', 'the', 'logarithm', 'by', 'every', 'rating', 'value', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'afterward', 'i', 'create', 'a', 'recommender', 'model', 'and', 'evaluate', 'the', 'basic', 'metrics', 'for', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'in', 'this', 'topic', 'i', 'invite', 'everybody', 'to', 'discuss', 'another', 'interesting', 'normalisation', 'methods', 'if', 'anybody', 'could', 'tell', 'some', 'good', 'method', 'for', 'data', 'preparation', 'it', 'would', 'be', 'great', 'the', 'results', 'could', 'be', 'evaluated', 'because', 'of', 'the', 'metrics', 'and', 'i', 'can', 'publish', 'it', 'here', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'ps', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'dataset', 'is', 'comming', 'from', 'some', 'music', 'site', 'the', 'users', 'rated', 'some', 'tracks', 'i', 'have', 'approximately', 'users', 'and', 'tracks', 'total', 'number', 'of', 'ratings', 'is', 'over', 'millions', 'actually', 'the', 'matrix', 'is', 'sparse', 'this', 'is', 'the', 'most', 'simple', 'data', 'set', 'which', 'i', 'analyze', 'now', 'in', 'the', 'future', 'i', 'can', 'and', 'will', 'use', 'some', 'additional', 'information', 'about', 'the', 'users', 'and', 'tracks', 'f', 'e', 'duration', 'year', 'genre', 'band', 'etc', 'at', 'the', 'moment', 'i', 'just', 'interest', 'to', 'collect', 'some', 'methods', 'for', 'rating', 'normalisation', 'without', 'to', 'use', 'additional', 'information', 'users', 'amp', 'amp', 'items', 'features', 'my', 'problem', 'is', 'the', 'data', 'set', 'doesn', 't', 'have', 'any', 'lt', 'code', 'gt', 'rating', 'lt', 'code', 'gt', 'at', 'the', 'first', 'i', 'create', 'someself', 'the', 'column', 'lt', 'code', 'gt', 'rating', 'lt', 'code', 'gt', 'based', 'on', 'the', 'number', 'of', 'events', 'for', 'unique', 'lt', 'code', 'gt', 'user', 'item', 'lt', 'code', 'gt', 'pair', 'i', 'have', 'this', 'information', 'you', 'can', 'of', 'course', 'understand', 'that', 'some', 'users', 'can', 'hear', 'some', 'tracks', 'many', 'times', 'and', 'another', 'users', 'only', 'one', 'time', 'consequently', 'the', 'dispersion', 'is', 'very', 'high', 'and', 'i', 'want', 'to', 'reduce', 'it', 'normalise', 'the', 'ratings', 'value', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'working', 'through', 'the', 'lt', 'a', 'href', 'quot', 'https', 'www', 'coursera', 'org', 'course', 'nlp', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'coursera', 'nlp', 'course', 'by', 'jurafsky', 'amp', 'amp', 'manning', 'lt', 'a', 'gt', 'and', 'the', 'lt', 'a', 'href', 'quot', 'https', 'class', 'coursera', 'org', 'nlp', 'lecture', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'lecture', 'on', 'good', 'turing', 'smoothing', 'lt', 'a', 'gt', 'struck', 'me', 'odd', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'example', 'given', 'was', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'you', 'are', 'fishing', 'a', 'scenario', 'from', 'josh', 'goodman', 'and', 'caught', 'lt', 'br', 'gt', 'xa', 'carp', 'perch', 'whitefish', 'trout', 'salmon', 'eel', 'fish', 'lt', 'br', 'gt', 'xa', 'lt', 'br', 'gt', 'xa', 'how', 'likely', 'is', 'it', 'that', 'the', 'next', 'species', 'is', 'new', 'i', 'e', 'catfish', 'or', 'bass', 'lt', 'br', 'gt', 'xa', 'let', 's', 'use', 'our', 'estimate', 'of', 'things', 'we', 'saw', 'once', 'to', 'estimate', 'the', 'new', 'things', 'lt', 'br', 'gt', 'xa', 'because', 'n_', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'get', 'the', 'intuition', 'of', 'using', 'the', 'count', 'of', 'uniquely', 'seen', 'items', 'to', 'estimate', 'the', 'number', 'of', 'unseen', 'item', 'types', 'n', 'but', 'the', 'next', 'steps', 'seem', 'counterintuitive', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'is', 'the', 'denominator', 'left', 'unchanged', 'instead', 'of', 'incremented', 'by', 'the', 'estimate', 'of', 'unseen', 'item', 'types', 'i', 'e', 'i', 'would', 'expect', 'the', 'probabilities', 'to', 'become', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'lt', 'p', 'gt', 'carp', 'lt', 'br', 'gt', 'xa', 'perch', 'lt', 'br', 'gt', 'xa', 'whitefish', 'lt', 'br', 'gt', 'xa', 'trout', 'lt', 'br', 'gt', 'xa', 'salmon', 'lt', 'br', 'gt', 'xa', 'eel', 'lt', 'br', 'gt', 'xa', 'something', 'new', 'lt', 'p', 'gt', 'xa', 'lt', 'blockquote', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 'seems', 'like', 'the', 'good', 'turing', 'count', 'penalizes', 'seen', 'items', 'too', 'much', 'trout', 'salmon', 'amp', 'amp', 'eel', 'are', 'each', 'taken', 'down', 'to', 'coupled', 'with', 'the', 'need', 'to', 'adjust', 'the', 'formula', 'for', 'gaps', 'in', 'the', 'counts', 'e', 'g', 'perch', 'amp', 'amp', 'carp', 'would', 'be', 'zeroed', 'out', 'otherwise', 'it', 'just', 'feels', 'like', 'a', 'bad', 'hack', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'build', 'an', 'item', 'item', 'similarity', 'matching', 'recommendation', 'engine', 'with', 'mahout', 'the', 'data', 'set', 'is', 'as', 'in', 'the', 'following', 'format', 'attributes', 'are', 'in', 'text', 'not', 'in', 'numerals', 'format', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'name', 'category', 'cost', 'ingredients', 'xa', 'xa', 'x', 'xx', 'xxx', 'xxx', 'xxx', 'xa', 'xa', 'y', 'yy', 'yyy', 'yyy', 'yyy', 'xa', 'xa', 'z', 'xx', 'xxx', 'xxy', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'in', 'order', 'to', 'use', 'this', 'data', 'set', 'for', 'mahout', 'to', 'train', 'what', 'is', 'the', 'right', 'way', 'to', 'convert', 'this', 'in', 'to', 'numeric', 'as', 'csv', 'boolean', 'data', 'set', 'format', 'accepted', 'by', 'mahout', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'big', 'data', 'problem', 'with', 'a', 'large', 'dataset', 'take', 'for', 'example', 'million', 'rows', 'and', 'columns', 'the', 'dataset', 'consists', 'of', 'about', 'numerical', 'columns', 'and', 'categorical', 'columns', 'and', 'a', 'response', 'column', 'that', 'represents', 'a', 'binary', 'class', 'problem', 'the', 'cardinality', 'of', 'each', 'of', 'the', 'categorical', 'columns', 'is', 'less', 'than', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'want', 'to', 'know', 'a', 'priori', 'whether', 'i', 'should', 'go', 'for', 'deep', 'learning', 'methods', 'or', 'ensemble', 'tree', 'based', 'methods', 'for', 'example', 'gradient', 'boosting', 'adaboost', 'or', 'random', 'forests', 'are', 'there', 'some', 'exploratory', 'data', 'analysis', 'or', 'some', 'other', 'techniques', 'that', 'can', 'help', 'me', 'decide', 'for', 'one', 'method', 'over', 'the', 'other', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'trying', 'to', 'build', 'a', 'data', 'set', 'on', 'several', 'log', 'files', 'of', 'one', 'of', 'our', 'products', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'different', 'log', 'files', 'have', 'their', 'own', 'layout', 'and', 'own', 'content', 'i', 'successfully', 'grouped', 'them', 'together', 'only', 'one', 'step', 'remaining', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'indeed', 'the', 'log', 'quot', 'messages', 'quot', 'are', 'the', 'best', 'information', 'i', 'don', 't', 'have', 'the', 'comprehensive', 'list', 'of', 'all', 'those', 'messages', 'and', 'it', 's', 'a', 'bad', 'idea', 'to', 'hard', 'code', 'based', 'on', 'those', 'because', 'that', 'list', 'can', 'change', 'every', 'day', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'i', 'would', 'like', 'to', 'do', 'is', 'to', 'separate', 'the', 'indentification', 'text', 'from', 'the', 'value', 'text', 'for', 'example', 'quot', 'loaded', 'file', 'xxx', 'quot', 'becomes', 'identification', 'quot', 'loaded', 'file', 'quot', 'value', 'quot', 'xxx', 'quot', 'unfortunately', 'this', 'example', 'is', 'simple', 'and', 'in', 'real', 'world', 'there', 'are', 'different', 'layouts', 'and', 'sometimes', 'multiple', 'values', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'was', 'thinking', 'about', 'using', 'string', 'kernels', 'but', 'it', 'is', 'intended', 'for', 'clustering', 'and', 'cluseting', 'is', 'not', 'applicable', 'here', 'i', 'don', 't', 'know', 'the', 'number', 'of', 'different', 'types', 'of', 'messages', 'and', 'eventhough', 'it', 'would', 'be', 'too', 'much', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'do', 'you', 'have', 'any', 'idea', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'your', 'help', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'p', 's', 'for', 'those', 'who', 'programs', 'this', 'can', 'be', 'easier', 'to', 'understand', 'let', 's', 'say', 'that', 'the', 'code', 'contains', 'as', 'logs', 'printf', 'quot', 'blabla', 's', 'quot', 'quot', 'xxx', 'quot', 'gt', 'i', 'would', 'like', 'to', 'have', 'quot', 'blabla', 'quot', 'and', 'quot', 'xxx', 'quot', 'seperatated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'run', 'an', 'r', 'script', 'using', 'a', 'single', 'command', 'e', 'g', 'bat', 'file', 'or', 'shortcut', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'r', 'script', 'asks', 'the', 'user', 'to', 'choose', 'a', 'file', 'and', 'then', 'plots', 'information', 'about', 'that', 'file', 'all', 'is', 'done', 'via', 'dialog', 'boxes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'don', 't', 'want', 'the', 'user', 'to', 'go', 'inside', 'r', 'because', 'they', 'don', 't', 'know', 'it', 'at', 'all', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'was', 'using', 'r', 'cmd', 'and', 'other', 'similar', 'stuffs', 'but', 'as', 'soon', 'as', 'the', 'plots', 'are', 'displayed', 'r', 'exits', 'and', 'closes', 'the', 'plots', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'can', 'i', 'do', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'for', 'your', 'help', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'currently', 'finishing', 'up', 'a', 'b', 's', 'in', 'mathematics', 'and', 'would', 'like', 'to', 'attend', 'graduate', 'school', 'a', 'master', 's', 'degree', 'for', 'starters', 'with', 'the', 'possibility', 'of', 'a', 'subsequent', 'ph', 'd', 'with', 'an', 'eye', 'toward', 'entering', 'the', 'field', 'of', 'data', 'science', 'i', 'm', 'also', 'particularly', 'interested', 'in', 'machine', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'are', 'the', 'graduate', 'degree', 'choices', 'that', 'would', 'get', 'me', 'to', 'where', 'i', 'want', 'to', 'go', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'consensus', 'as', 'to', 'whether', 'a', 'graduate', 'degree', 'in', 'applied', 'mathematics', 'statistics', 'or', 'computer', 'science', 'would', 'put', 'me', 'in', 'a', 'better', 'position', 'to', 'enter', 'the', 'field', 'of', 'data', 'science', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thank', 'you', 'all', 'for', 'the', 'help', 'this', 'is', 'a', 'big', 'choice', 'for', 'me', 'and', 'any', 'input', 'is', 'very', 'much', 'appreciated', 'typically', 'i', 'ask', 'my', 'questions', 'on', 'mathematics', 'stack', 'exchange', 'but', 'i', 'thought', 'asking', 'here', 'would', 'give', 'me', 'a', 'broader', 'and', 'better', 'rounded', 'perspective', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'from', 'lt', 'a', 'href', 'quot', 'http', 'nshorter', 'com', 'researchpapers', 'machinelearning', 'a_roadmap_to_svm_smo', 'pdf', 'quot', 'rel', 'quot', 'nofollow', 'noreferrer', 'quot', 'gt', 'a_roadmap_to_svm_smo', 'pdf', 'lt', 'a', 'gt', 'pg', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 's', 'postimg', 'org', 'dx', 't', 'w', 'whatwhat', 'png', 'quot', 'alt', 'quot', 'a', 'busy', 'cat', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'assume', 'i', 'am', 'using', 'linear', 'kernel', 'how', 'will', 'i', 'be', 'able', 'to', 'get', 'both', 'the', 'first', 'and', 'second', 'inner', 'product', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'my', 'guess', 'inner', 'product', 'of', 'datapoint', 'with', 'datapoint', 'j', 'labelled', 'class', 'a', 'for', 'the', 'first', 'inner', 'product', 'of', 'the', 'equation', 'and', 'inner', 'product', 'of', 'datapoint', 'j', 'with', 'datapoints', 'labelled', 'class', 'b', 'for', 'second', 'inner', 'product', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'to', 'all', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'been', 'wracking', 'my', 'brain', 'at', 'this', 'for', 'a', 'while', 'and', 'thought', 'maybe', 'someone', 'here', 'would', 'know', 'of', 'a', 'package', 'or', 'algorithm', 'to', 'handle', 'the', 'following', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'nominal', 'multivariant', 'timeseries', 'that', 'look', 'like', 'the', 'following', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'time', 'var', 'var', 'var', 'var', 'var', 'varn', 'xa', 'a', 'a', 'b', 'c', 'a', 'h', 'xa', 'a', 'a', 'b', 'd', 'd', 'h', 'xa', 'b', 'a', 'c', 'd', 'd', 'h', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'so', 'on', 'from', 'times', 'to', 'what', 'i', 'would', 'like', 'to', 'do', 'is', 'search', 'the', 'time', 'series', 'for', 'rules', 'of', 'the', 'type', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'given', 'var', 'is', 'in', 'state', 'b', 'in', 'the', 'previous', 'step', 'and', 'var', 'is', 'in', 'state', 'd', 'in', 'the', 'previous', 'step', 'than', 'var', 'will', 'be', 'in', 'state', 'b', 'what', 'i', 'want', 'to', 'do', 'is', 'have', 'the', 'rules', 'that', 'include', 'the', 'time', 'interval', 'explicitly', 'a', 'simpler', 'case', 'of', 'interest', 'would', 'simply', 'be', 'to', 'reduce', 'the', 'time', 'series', 'to', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'time', 'var', 'var', 'var', 'var', 'var', 'varn', 'xa', 'xa', 'xa', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'where', 'the', 'the', 'variable', 'is', 'if', 'its', 'state', 'is', 'different', 'from', 'the', 'previous', 'step', 'and', 'zero', 'otherwise', 'then', 'i', 'just', 'want', 'to', 'have', 'rules', 'that', 'say', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'var', 'and', 'var', 'changed', 'in', 'the', 'previous', 'step', 'than', 'var', 'will', 'change', 'in', 'the', 'current', 'step', 'which', 'would', 'be', 'easy', 'for', 'a', 'lag', 'of', 'one', 'as', 'i', 'could', 'just', 'make', 'the', 'data', 'into', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'var', 'var', 'var', 'var', 'var', 'varn', 'var', '_t', 'var', '_t', 'var', '_t', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'and', 'then', 'do', 'sequence', 'mining', 'but', 'if', 'i', 'want', 'to', 'have', 'rules', 'that', 'aren', 't', 'just', 'a', 'single', 'lag', 'but', 'could', 'be', 'lags', 'from', 'to', 'than', 'my', 'data', 'set', 'begins', 'to', 'be', 'a', 'little', 'difficult', 'to', 'work', 'with', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'help', 'would', 'be', 'greatly', 'appreciated', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'edit', 'to', 'respond', 'to', 'comment', 'xa', 'each', 'column', 'could', 'be', 'in', 'one', 'of', 'different', 'states', 'as', 'far', 'as', 'a', 'target', 'it', 'is', 'non', 'specific', 'any', 'rules', 'between', 'the', 'columns', 'would', 'be', 'of', 'interest', 'however', 'predicting', 'columns', 'and', 'would', 'be', 'particularly', 'interesting', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'browsed', 'a', 'sample', 'for', 'available', 'data', 'at', 'lt', 'a', 'href', 'quot', 'http', 'dbpedia', 'org', 'page', 'sachin_tendulkar', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'dbpedia', 'org', 'page', 'sachin_tendulkar', 'lt', 'a', 'gt', 'i', 'wanted', 'these', 'properties', 'as', 'columns', 'so', 'i', 'downloaded', 'the', 'csv', 'files', 'from', 'lt', 'a', 'href', 'quot', 'http', 'wiki', 'dbpedia', 'org', 'dbpediaastables', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'http', 'wiki', 'dbpedia', 'org', 'dbpediaastables', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'now', 'when', 'i', 'browse', 'the', 'data', 'for', 'the', 'same', 'entity', 'quot', 'sachin_tendulkar', 'quot', 'i', 'find', 'that', 'many', 'of', 'the', 'properties', 'are', 'not', 'available', 'e', 'g', 'the', 'property', 'quot', 'dbpprop', 'bestbowling', 'quot', 'is', 'not', 'present', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'how', 'can', 'i', 'get', 'all', 'the', 'properties', 'that', 'i', 'can', 'browse', 'through', 'the', 'direct', 'resource', 'page', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'common', 'model', 'validation', 'statistics', 'like', 'the', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'kolmogorov', 'e', 'smirnov_test', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'kolmogorov', 'smirnov', 'test', 'lt', 'a', 'gt', 'ks', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'receiver_operating_characteristic', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'auroc', 'lt', 'a', 'gt', 'and', 'lt', 'a', 'href', 'quot', 'https', 'en', 'wikipedia', 'org', 'wiki', 'gini_coefficient', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'gini', 'coefficient', 'lt', 'a', 'gt', 'are', 'all', 'functionally', 'related', 'however', 'my', 'question', 'has', 'to', 'do', 'with', 'proving', 'how', 'these', 'are', 'all', 'related', 'i', 'am', 'curious', 'if', 'anyone', 'can', 'help', 'me', 'prove', 'these', 'relationships', 'i', 'haven', 't', 'been', 'able', 'to', 'find', 'anything', 'online', 'but', 'i', 'am', 'just', 'genuinely', 'interested', 'how', 'the', 'proofs', 'work', 'for', 'example', 'i', 'know', 'gini', 'auroc', 'but', 'my', 'best', 'proof', 'involves', 'pointing', 'at', 'a', 'graph', 'i', 'am', 'interested', 'in', 'formal', 'proofs', 'any', 'help', 'would', 'be', 'greatly', 'appreciated', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'having', 'some', 'difficulty', 'in', 'seeing', 'connection', 'between', 'pca', 'on', 'second', 'order', 'moment', 'matrix', 'in', 'estimating', 'parameters', 'of', 'gaussian', 'mixture', 'models', 'can', 'anyone', 'connect', 'the', 'above', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'hi', 'this', 'is', 'my', 'first', 'question', 'in', 'the', 'data', 'science', 'stack', 'i', 'want', 'to', 'create', 'an', 'algorithm', 'for', 'text', 'classification', 'suppose', 'i', 'have', 'a', 'large', 'set', 'of', 'text', 'and', 'articles', 'lets', 'say', 'around', 'plain', 'texts', 'i', 'first', 'use', 'a', 'simple', 'function', 'to', 'determine', 'the', 'frequency', 'of', 'all', 'the', 'four', 'and', 'above', 'character', 'words', 'i', 'then', 'use', 'this', 'as', 'the', 'feature', 'of', 'each', 'training', 'sample', 'now', 'i', 'want', 'my', 'algorithm', 'to', 'be', 'able', 'to', 'cluster', 'the', 'training', 'sets', 'to', 'according', 'to', 'their', 'features', 'which', 'here', 'is', 'the', 'frequency', 'of', 'each', 'word', 'in', 'the', 'article', 'note', 'that', 'in', 'this', 'example', 'each', 'article', 'would', 'have', 'its', 'own', 'unique', 'feature', 'since', 'each', 'article', 'has', 'a', 'different', 'feature', 'for', 'example', 'an', 'article', 'has', 'quot', 'water', 'and', 'quot', 'pure', 'quot', 'and', 'another', 'has', 'quot', 'politics', 'quot', 'and', 'quot', 'leverage', 'quot', 'can', 'you', 'suggest', 'the', 'best', 'possible', 'clustering', 'algorithm', 'for', 'this', 'example', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'datasets', 'yearpredictionmsd', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'datasets', 'yearpredictionmsd', 'lt', 'a', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'according', 'to', 'the', 'description', 'given', 'in', 'the', 'above', 'link', 'xa', 'the', 'attribute', 'information', 'specifies', 'quot', 'average', 'and', 'covariance', 'over', 'all', 'segments', 'each', 'segment', 'being', 'described', 'by', 'a', 'dimensional', 'timbre', 'vector', 'quot', 'so', 'the', 'covariance', 'matrix', 'should', 'have', 'elements', 'but', 'why', 'is', 'the', 'number', 'of', 'timbre', 'covariance', 'features', 'only', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'gbms', 'like', 'random', 'forests', 'build', 'each', 'tree', 'on', 'a', 'different', 'sample', 'of', 'the', 'dataset', 'and', 'hence', 'going', 'by', 'the', 'spirit', 'of', 'ensemble', 'models', 'produce', 'higher', 'accuracies', 'however', 'i', 'have', 'not', 'seen', 'gbm', 'being', 'used', 'with', 'dimension', 'sampling', 'at', 'every', 'split', 'of', 'the', 'tree', 'like', 'is', 'common', 'practice', 'with', 'random', 'forests', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'are', 'there', 'some', 'tests', 'that', 'show', 'that', 'dimensional', 'sampling', 'with', 'gbm', 'would', 'decrease', 'its', 'accuracy', 'because', 'of', 'which', 'this', 'is', 'avoided', 'either', 'in', 'literature', 'form', 'or', 'in', 'practical', 'experience', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'historic', 'error', 'of', 'time', 'series', 'i', 'want', 'to', 'analyze', 'error', 'series', 'to', 'improve', 'forecast', 'series', 'are', 'there', 'any', 'methods', 'to', 'do', 'this', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'is', 'the', 'standard', 'way', 'for', 'evaluating', 'and', 'comparing', 'different', 'algorithms', 'while', 'developing', 'recommendation', 'system', 'whether', 'we', 'need', 'to', 'have', 'a', 'predetermined', 'annotated', 'ranked', 'dataset', 'and', 'then', 'compare', 'with', 'precision', 'recall', 'f', 'measure', 'of', 'different', 'algorithms', 'is', 'this', 'the', 'best', 'way', 'for', 'evaluation', 'or', 'is', 'there', 'any', 'other', 'way', 'to', 'compare', 'results', 'of', 'various', 'recommendation', 'algorithms', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'from', 'programming', 'background', 'i', 'm', 'now', 'learning', 'analytics', 'i', 'm', 'learning', 'concepts', 'from', 'basic', 'statistics', 'to', 'model', 'building', 'like', 'linear', 'regression', 'logistic', 'regression', 'time', 'series', 'analysis', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'as', 'my', 'previous', 'experience', 'is', 'completely', 'on', 'programming', 'i', 'would', 'like', 'to', 'do', 'some', 'analysis', 'on', 'the', 'data', 'which', 'programmer', 'has', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'say', 'lets', 'have', 'the', 'details', 'below', 'i', 'm', 'using', 'svn', 'repository', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'personname', 'code', 'check', 'in', 'date', 'file', 'checked', 'in', 'number', 'of', 'times', 'checkedin', 'branch', 'check', 'in', 'date', 'and', 'time', 'build', 'version', 'number', 'of', 'defects', 'defect', 'date', 'file', 'that', 'has', 'defect', 'build', 'version', 'defect', 'fix', 'date', 'defect', 'fix', 'hours', 'please', 'feel', 'free', 'to', 'add', 'remove', 'how', 'many', 'ever', 'variables', 'needed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'just', 'need', 'a', 'trigger', 'starting', 'point', 'on', 'what', 'can', 'be', 'done', 'with', 'these', 'data', 'can', 'i', 'bring', 'any', 'insights', 'with', 'this', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'or', 'can', 'you', 'provide', 'any', 'links', 'that', 'has', 'information', 'about', 'similar', 'type', 'of', 'work', 'done', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'determine', 'whether', 'or', 'not', 'we', 'are', 'confident', 'that', 'the', 'mean', 'of', 'a', 'lt', 'em', 'gt', 'proposed', 'lt', 'em', 'gt', 'lt', 'em', 'gt', 'population', 'lt', 'em', 'gt', 'is', 'at', 'least', 'times', 'that', 'of', 'the', 'mean', 'of', 'the', 'lt', 'em', 'gt', 'incumbant', 'lt', 'em', 'gt', 'lt', 'em', 'gt', 'population', 'lt', 'em', 'gt', 'based', 'on', 'samples', 'from', 'each', 'population', 'which', 'is', 'all', 'the', 'data', 'i', 'have', 'right', 'now', 'here', 'are', 'the', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'incumbantvalues', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'proposedvalues', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'no', 'idea', 'if', 'either', 'population', 'is', 'or', 'will', 'be', 'normal', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'ratio', 'of', 'the', 'lt', 'em', 'gt', 'sample', 'lt', 'em', 'gt', 'means', 'does', 'exceed', 'but', 'how', 'does', 'that', 'translate', 'to', 'confidence', 'that', 'the', 'proposed', 'population', 'mean', 'will', 'be', 'at', 'least', 'twice', 'that', 'of', 'the', 'mean', 'of', 'the', 'incumbant', 'population', 'with', 'confidence', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 're', 'sampling', 'bootstrapping', 'with', 'replacement', 'help', 'answer', 'this', 'question', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'doing', 'a', 'text', 'classification', 'task', 'essays', 'evenly', 'distributed', 'by', 'labels', 'i', 'explored', 'lt', 'code', 'gt', 'linearsvc', 'lt', 'code', 'gt', 'and', 'got', 'an', 'accuracy', 'of', 'now', 'i', 'guess', 'whether', 'accuracy', 'could', 'be', 'raised', 'by', 'using', 'lt', 'code', 'gt', 'ensemble', 'lt', 'code', 'gt', 'classifier', 'with', 'lt', 'code', 'gt', 'svm', 'lt', 'code', 'gt', 'as', 'base', 'estimator', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'do', 'not', 'know', 'how', 'to', 'employ', 'an', 'lt', 'code', 'gt', 'ensemble', 'lt', 'code', 'gt', 'classifier', 'incorporating', 'all', 'the', 'features', 'please', 'note', 'that', 'i', 'do', 'not', 'want', 'to', 'combine', 'the', 'different', 'features', 'directly', 'in', 'a', 'single', 'vector', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'therefore', 'lt', 'strong', 'gt', 'my', 'first', 'question', 'lt', 'strong', 'gt', 'in', 'order', 'to', 'improve', 'the', 'current', 'accuracy', 'is', 'it', 'possible', 'to', 'use', 'lt', 'code', 'gt', 'ensemble', 'lt', 'code', 'gt', 'classifier', 'with', 'lt', 'code', 'gt', 'svm', 'lt', 'code', 'gt', 'as', 'base', 'estimator', 'xa', 'lt', 'strong', 'gt', 'my', 'second', 'question', 'lt', 'strong', 'gt', 'how', 'to', 'employ', 'an', 'lt', 'code', 'gt', 'ensemble', 'lt', 'code', 'gt', 'classifier', 'incorporating', 'all', 'features', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'what', 'are', 'some', 'possible', 'techniques', 'for', 'smoothing', 'proportions', 'across', 'very', 'large', 'categories', 'in', 'order', 'to', 'take', 'into', 'account', 'the', 'sample', 'size', 'the', 'application', 'of', 'interest', 'here', 'is', 'to', 'use', 'the', 'proportions', 'as', 'input', 'into', 'a', 'predictive', 'model', 'but', 'i', 'am', 'wary', 'of', 'using', 'the', 'raw', 'proportions', 'in', 'cases', 'where', 'there', 'is', 'little', 'evidence', 'and', 'i', 'don', 't', 'want', 'to', 'overfit', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'here', 'is', 'an', 'example', 'where', 'the', 'id', 'denotes', 'a', 'customer', 'and', 'impressions', 'and', 'clicks', 'are', 'the', 'number', 'of', 'ads', 'shown', 'and', 'clicks', 'the', 'customer', 'has', 'made', 'respectively', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'ohzq', 'jpg', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'a', 'visualization', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'creating', 'a', 'comparison', 'report', 'of', 'pr', 'event', 'efficiency', 'say', 'show', 'or', 'exhibition', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'are', 'two', 'dimensions', 'of', 'comparison', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'compare', 'vs', 'the', 'same', 'event', 'performance', 'in', 'the', 'past', 'years', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'compare', 'vs', 'another', 'type', 'of', 'analogical', 'competitive', 'events', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'there', 'is', 'also', 'a', 'number', 'of', 'comparison', 'aspects', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'audience', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'media', 'coverage', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'social', 'buzz', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'roi', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'etc', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'each', 'aspect', 'is', 'a', 'set', 'of', 'some', 'final', 'kpi', 's', 'just', 'numbers', 'which', 'can', 'be', 'compared', 'vs', 'another', 'quot', 'dimensions', 'quot', 'plus', 'maybe', 'some', 'descriptive', 'text', 'and', 'pictures', 'which', 'couldn', 't', 'be', 'a', 'metric', 'but', 'should', 'be', 'attached', 'to', 'the', 'report', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'finaly', 'it', 'looks', 'like', 'a', 'three', 'dimensional', 'coube', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'years', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'another', 'events', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'aspects', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'if', 'i', 'put', 'it', 'in', 'plain', 'word', 'or', 'ppt', 'it', 'will', 'look', 'like', 'a', 'document', 'with', 'dozen', 'of', 'slides', 'papers', 'and', 'linear', 'structure', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'any', 'ideas', 'how', 'to', 'compile', 'an', 'elegant', 'user', 'friendly', 'report', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'a', 'short', 'while', 'ago', 'i', 'came', 'across', 'this', 'ml', 'framework', 'that', 'has', 'implemented', 'several', 'different', 'algorithms', 'ready', 'for', 'use', 'the', 'site', 'also', 'provides', 'a', 'handy', 'api', 'that', 'you', 'can', 'access', 'with', 'an', 'api', 'key', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'need', 'of', 'the', 'framework', 'to', 'solve', 'a', 'website', 'classification', 'problem', 'where', 'i', 'basically', 'need', 'to', 'categorize', 'several', 'thousand', 'websites', 'based', 'on', 'their', 'html', 'content', 'as', 'i', 'don', 't', 'want', 'to', 'be', 'bound', 'to', 'their', 'existing', 'api', 'i', 'wanted', 'to', 'use', 'the', 'framework', 'to', 'implement', 'my', 'own', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'besides', 'some', 'introductory', 'level', 'data', 'mining', 'courses', 'and', 'associated', 'reading', 'i', 'know', 'very', 'little', 'as', 'to', 'what', 'exactly', 'i', 'would', 'need', 'to', 'use', 'specifically', 'i', 'm', 'at', 'a', 'loss', 'as', 'to', 'what', 'exactly', 'i', 'need', 'to', 'do', 'to', 'train', 'the', 'classifier', 'and', 'then', 'model', 'the', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'the', 'framework', 'already', 'includes', 'some', 'classification', 'algorithms', 'like', 'naivebayes', 'which', 'i', 'know', 'is', 'well', 'suited', 'to', 'the', 'task', 'of', 'text', 'classification', 'but', 'i', 'm', 'not', 'exactly', 'sure', 'how', 'to', 'apply', 'it', 'to', 'the', 'problem', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'can', 'anyone', 'give', 'me', 'a', 'rough', 'guidelines', 'as', 'to', 'what', 'exactly', 'i', 'would', 'need', 'to', 'do', 'to', 'accomplish', 'this', 'task', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'make', 'a', 'prediction', 'for', 'the', 'result', 'of', 'the', 'parliamentary', 'elections', 'my', 'output', 'will', 'be', 'the', 'each', 'party', 'receives', 'there', 'is', 'more', 'than', 'parties', 'so', 'logistic', 'regression', 'is', 'not', 'a', 'viable', 'option', 'i', 'could', 'make', 'a', 'separate', 'regression', 'for', 'each', 'party', 'but', 'in', 'that', 'case', 'the', 'results', 'would', 'be', 'in', 'some', 'manner', 'independent', 'from', 'each', 'other', 'it', 'would', 'not', 'ensure', 'that', 'the', 'sum', 'of', 'the', 'results', 'would', 'be', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'regression', 'or', 'other', 'method', 'should', 'i', 'use', 'is', 'it', 'possible', 'to', 'use', 'this', 'method', 'in', 'r', 'or', 'python', 'via', 'a', 'specific', 'library', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'some', 'text', 'files', 'containing', 'moview', 'reviews', 'i', 'need', 'to', 'find', 'out', 'whether', 'the', 'review', 'is', 'good', 'or', 'bad', 'i', 'tried', 'the', 'following', 'code', 'but', 'its', 'not', 'working', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'import', 'nltk', 'xa', 'with', 'open', 'quot', 'c', 'users', 'user', 'desktop', 'datascience', 'moviesr', 'movies', 'txt', 'quot', 'r', 'as', 'm', 'xa', 'mov_rev', 'm', 'read', 'xa', 'mov_review', 'nltk', 'word_tokenize', 'mov_rev', 'xa', 'bon', 'quot', 'crap', 'aweful', 'horrible', 'terrible', 'bad', 'bland', 'trite', 'sucks', 'unpleasant', 'boring', 'dull', 'moronic', 'dreadful', 'disgusting', 'distasteful', 'flawed', 'ordinary', 'slow', 'senseless', 'unoriginal', 'weak', 'wacky', 'uninteresting', 'unpretentious', 'quot', 'xa', 'bag_of_negative_words', 'nltk', 'word_tokenize', 'bon', 'xa', 'bop', 'quot', 'absorbing', 'big', 'budget', 'brilliant', 'brutal', 'charismatic', 'charming', 'clever', 'comical', 'dazzling', 'dramatic', 'enjoyable', 'entertaining', 'excellent', 'exciting', 'expensive', 'fascinating', 'fast', 'moving', 'first', 'rate', 'funny', 'highly', 'charged', 'hilarious', 'imaginative', 'insightful', 'inspirational', 'intriguing', 'juvenile', 'lasting', 'legendary', 'pleasant', 'powerful', 'ripping', 'riveting', 'romantic', 'sad', 'satirical', 'sensitive', 'sentimental', 'surprising', 'suspenseful', 'tender', 'thought', 'provoking', 'tragic', 'uplifting', 'uproarious', 'quot', 'xa', 'bop', 'lower', 'xa', 'bag_of_positive_words', 'nltk', 'word_tokenize', 'bop', 'xa', 'vec', 'xa', 'for', 'i', 'in', 'bag_of_negative_words', 'xa', 'if', 'i', 'in', 'mov_review', 'xa', 'vec', 'append', 'xa', 'else', 'xa', 'for', 'w', 'in', 'bag_of_positive_words', 'xa', 'if', 'w', 'in', 'moview_review', 'xa', 'vec', 'append', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'am', 'trying', 'to', 'check', 'whether', 'the', 'review', 'contains', 'a', 'positive', 'word', 'or', 'a', 'negative', 'word', 'if', 'it', 'contains', 'negative', 'word', 'then', 'a', 'value', 'will', 'be', 'assigned', 'to', 'the', 'vector', 'vec', 'else', 'a', 'value', 'of', 'will', 'be', 'assigned', 'xa', 'but', 'the', 'output', 'i', 'am', 'getting', 'is', 'an', 'empty', 'vector', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'please', 'help', 'xa', 'also', 'please', 'suggest', 'others', 'way', 'of', 'solving', 'this', 'problem', 'xa', 'thanks', 'in', 'advance', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'in', 'this', 'lt', 'a', 'href', 'quot', 'https', 'cwiki', 'apache', 'org', 'confluence', 'display', 'hive', 'languagemanual', 'udf', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'wiki', 'page', 'lt', 'a', 'gt', 'there', 'is', 'a', 'function', 'lt', 'code', 'gt', 'corr', 'lt', 'code', 'gt', 'that', 'calculates', 'the', 'pearson', 'coefficient', 'of', 'correlation', 'but', 'my', 'question', 'is', 'that', 'is', 'there', 'any', 'function', 'in', 'hive', 'that', 'enables', 'to', 'calculate', 'the', 'lt', 'a', 'href', 'quot', 'http', 'en', 'wikipedia', 'org', 'wiki', 'kendall', 's_w', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'kendall', 'coefficient', 'lt', 'a', 'gt', 'of', 'correlation', 'of', 'a', 'pair', 'of', 'a', 'numeric', 'columns', 'in', 'the', 'group', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'looking', 'for', 'api', 'suggestions', 'for', 'enriching', 'data', 'on', 'companies', 'currently', 'i', 'use', 'the', 'crunchbase', 'api', 'to', 'look', 'up', 'a', 'company', 's', 'name', 'or', 'domain', 'and', 'i', 'am', 'trying', 'to', 'gather', 'the', 'domain', 'name', 'if', 'i', 'don', 't', 'already', 'have', 'both', 'contact', 'email', 'this', 'one', 'is', 'a', 'long', 'shot', 'and', 'the', 'location', 'of', 'their', 'headquarters', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'this', 'works', 'incredibly', 'well', 'if', 'crunchbase', 'has', 'the', 'company', 'in', 'their', 'api', 'but', 'i', 'd', 'say', 'this', 'only', 'happens', 'about', 'of', 'the', 'time', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'd', 'love', 'to', 'get', 'some', 'suggestions', 'on', 'some', 'free', 'apis', 'that', 'i', 'could', 'use', 'along', 'with', 'crunchbase', 'i', 'd', 'also', 'love', 'to', 'see', 'if', 'anyone', 'has', 'had', 'positive', 'or', 'negative', 'experiences', 'with', 'paid', 'apis', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'have', 'some', 'question', 'regarding', 'to', 'the', 'choice', 'of', 'the', 'better', 'implementation', 'i', 'would', 'know', 'the', 'differences', 'and', 'advantages', 'of', 'lt', 'a', 'href', 'quot', 'https', 'mahout', 'apache', 'org', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'mahout', 'apache', 'lt', 'a', 'gt', 'java', 'implementation', 'versus', 'lt', 'a', 'href', 'quot', 'http', 'graphlab', 'com', 'index', 'html', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'graphlab', 'lt', 'a', 'gt', 'python', 'implementation', 'in', 'the', 'area', 'of', 'the', 'data', 'sciences', 'specially', 'in', 'the', 'area', 'of', 'recommenders', 'and', 'classifiers', 'can', 'anybody', 'here', 'get', 'some', 'qualified', 'feedback', 'about', 'both', 'possibilities', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'which', 'of', 'the', 'following', 'is', 'best', 'or', 'widely', 'used', 'for', 'calculating', 'item', 'item', 'similarity', 'measure', 'in', 'mahout', 'and', 'why', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'pearson', 'correlation', 'xa', 'spearman', 'correlation', 'xa', 'euclidean', 'distance', 'xa', 'tanimoto', 'coefficient', 'xa', 'loglikelihood', 'similarity', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'any', 'thumb', 'rule', 'to', 'chose', 'from', 'these', 'set', 'of', 'algorithm', 'also', 'how', 'to', 'differentiate', 'each', 'of', 'them', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'trying', 'to', 'match', 'new', 'product', 'description', 'with', 'the', 'existing', 'ones', 'product', 'description', 'looks', 'like', 'this', 'panasonic', 'dmc', 'fx', 'eb', 'digital', 'camera', 'silver', 'these', 'are', 'steps', 'to', 'be', 'performed', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ol', 'gt', 'xa', 'lt', 'li', 'gt', 'tokenize', 'description', 'and', 'recognize', 'attributes', 'panasonic', 'gt', 'brand', 'dmc', 'fx', 'eb', 'gt', 'model', 'etc', 'lt', 'br', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'get', 'few', 'candidates', 'with', 'similar', 'features', 'lt', 'br', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'get', 'the', 'best', 'candidate', 'lt', 'br', 'gt', 'lt', 'li', 'gt', 'xa', 'lt', 'ol', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'having', 'problem', 'with', 'the', 'first', 'step', 'in', 'order', 'to', 'get', 'panasonic', 'gt', 'brand', 'dmc', 'fx', 'eb', 'gt', 'model', 'silver', 'gt', 'color', 'i', 'need', 'to', 'have', 'index', 'where', 'each', 'token', 'of', 'the', 'product', 'description', 'correspond', 'to', 'certain', 'attribute', 'name', 'brand', 'model', 'color', 'etc', 'in', 'the', 'existing', 'database', 'the', 'problem', 'is', 'that', 'in', 'my', 'database', 'product', 'descriptions', 'are', 'presented', 'as', 'one', 'atomic', 'attribute', 'e', 'g', 'description', 'no', 'separated', 'product', 'attributes', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'basically', 'i', 'don', 't', 'have', 'training', 'data', 'so', 'i', 'am', 'trying', 'to', 'build', 'index', 'of', 'all', 'product', 'attributes', 'so', 'i', 'can', 'build', 'training', 'data', 'so', 'far', 'i', 'have', 'attributes', 'from', 'bestbuy', 'com', 'and', 'semantics', 'com', 'apis', 'but', 'both', 'sources', 'lack', 'most', 'of', 'attributes', 'or', 'contain', 'irrelevant', 'ones', 'any', 'suggestions', 'for', 'better', 'apis', 'to', 'get', 'product', 'attributes', 'better', 'approach', 'to', 'do', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'p', 's', 'for', 'every', 'product', 'there', 'is', 'a', 'matched', 'product', 'description', 'in', 'the', 'database', 'which', 'is', 'as', 'well', 'in', 'a', 'form', 'of', 'one', 'atomic', 'attribute', 'i', 'have', 'checked', 'this', 'lt', 'a', 'href', 'quot', 'https', 'stackoverflow', 'com', 'questions', 'how', 'to', 'parse', 'product', 'titles', 'unstructured', 'into', 'structured', 'data', 'quot', 'gt', 'question', 'on', 'so', 'lt', 'a', 'gt', 'it', 'helped', 'me', 'and', 'it', 'seems', 'we', 'have', 'same', 'approach', 'but', 'i', 'am', 'still', 'trying', 'to', 'get', 'training', 'data', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'currently', 'using', 'svm', 'and', 'scaling', 'my', 'training', 'features', 'to', 'the', 'range', 'of', 'xa', 'i', 'first', 'fit', 'transform', 'my', 'training', 'set', 'and', 'then', 'apply', 'the', 'lt', 'strong', 'gt', 'same', 'lt', 'strong', 'gt', 'transformation', 'to', 'my', 'testing', 'set', 'for', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'pre', 'gt', 'lt', 'code', 'gt', 'configure', 'transformation', 'and', 'apply', 'to', 'training', 'set', 'xa', 'min_max_scaler', 'minmaxscaler', 'feature_range', 'xa', 'x_train', 'min_max_scaler', 'fit_transform', 'x_train', 'xa', 'xa', 'perform', 'transformation', 'on', 'testing', 'set', 'xa', 'x_test', 'min_max_scaler', 'transform', 'x_test', 'xa', 'lt', 'code', 'gt', 'lt', 'pre', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'let', 'assume', 'that', 'a', 'given', 'feature', 'in', 'the', 'training', 'set', 'has', 'a', 'range', 'of', 'and', 'that', 'same', 'feature', 'in', 'the', 'testing', 'set', 'has', 'a', 'range', 'of', 'in', 'the', 'training', 'set', 'that', 'feature', 'will', 'be', 'scaled', 'appropriately', 'to', 'while', 'in', 'the', 'testing', 'set', 'that', 'feature', 'will', 'be', 'scaled', 'to', 'a', 'range', 'outside', 'of', 'that', 'first', 'specified', 'something', 'like', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'was', 'wondering', 'what', 'the', 'consequences', 'of', 'the', 'testing', 'set', 'features', 'being', 'out', 'of', 'range', 'of', 'those', 'being', 'used', 'to', 'train', 'the', 'model', 'is', 'this', 'a', 'problem', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'm', 'very', 'passionate', 'about', 'how', 'computers', 'can', 'be', 'made', 'able', 'to', 'think', 'intelligently', 'and', 'independently', 'in', 'our', 'favour', 'of', 'course', 'i', 'm', 'currently', 'studying', 'bachelors', 'science', 'of', 'information', 'technology', 'at', 'uts', 'university', 'of', 'technology', 'sydney', 'i', 'have', 'two', 'months', 'before', 'i', 'start', 'my', 'second', 'year', 'and', 'have', 'not', 'yet', 'been', 'able', 'to', 'decide', 'on', 'which', 'major', 'should', 'i', 'select', 'that', 'can', 'lead', 'myself', 'towards', 'dedicated', 'study', 'of', 'artificial', 'intelligence', 'which', 'i', 'love', 'with', 'my', 'life', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'have', 'the', 'following', 'majors', 'available', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'internetworking', 'and', 'applications', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'data', 'analytics', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'there', 'are', 'other', 'two', 'as', 'well', 'but', 'business', 'oriented', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'http', 'uts', 'edu', 'au', 'quot', 'rel', 'quot', 'nofollow', 'quot', 'gt', 'here', 'lt', 'a', 'gt', 'is', 'the', 'link', 'to', 'my', 'subjects', 'i', 'believe', 'that', 'being', 'able', 'to', 'play', 'with', 'data', 'is', 'a', 'sign', 'of', 'intelligence', 'i', 'may', 'be', 'wrong', 'too', 'will', 'one', 'of', 'these', 'subjects', 'form', 'me', 'a', 'good', 'foundation', 'for', 'my', 'further', 'study', 'in', 'a', 'i', 'or', 'should', 'i', 'jump', 'into', 'engineering', 'or', 'pure', 'science', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'is', 'there', 'a', 'method', 'class', 'available', 'in', 'apache', 'mahout', 'to', 'perform', 'n', 'fold', 'cross', 'validation', 'xa', 'if', 'yes', 'how', 'it', 'can', 'be', 'done', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'pose', 'a', 'question', 'about', 'how', 'to', 'treat', 'additional', 'holders', 'in', 'the', 'propensity', 'to', 'buy', 'models', 'of', 'banking', 'products', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'up', 'to', 'now', 'i', 'was', 'only', 'taking', 'into', 'considerations', 'the', 'clients', 'as', 'first', 'holders', 'xa', 'for', 'example', 'if', 'a', 'client', 'appears', 'as', 'the', 'first', 'holder', 'of', 'a', 'saving', 'account', 'a', 'with', 'a', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'of', 'and', 'as', 'an', 'additional', 'holder', 'of', 'a', 'saving', 'account', 'b', 'with', 'a', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'of', 'the', 'saving', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'for', 'the', 'client', 'is', 'considered', 'to', 'be', 'just', 'xa', 'moreover', 'if', 'a', 'client', 'only', 'appears', 'as', 'an', 'additional', 'holder', 'and', 'he', 'she', 'is', 'not', 'a', 'first', 'account', 'holder', 'of', 'any', 'product', 'he', 'she', 'is', 'dismissed', 'by', 'the', 'model', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'have', 'been', 'told', 'to', 'include', 'additional', 'holders', 'in', 'the', 'models', 'additional', 'holders', 'have', 'the', 'same', 'rights', 'of', 'the', 'first', 'holders', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'possibility', 'is', 'to', 'recalculate', 'all', 'the', 'variables', 'summing', 'up', 'the', 'position', 'as', 'first', 'and', 'additional', 'holder', 'in', 'the', 'previous', 'example', 'the', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'of', 'client', 'would', 'be', 'together', 'with', 'this', 'i', 'would', 'create', 'some', 'variable', 'that', 'represents', 'the', 'maximum', 'degree', 'of', 'intervention', 'of', 'the', 'client', 'in', 'the', 'account', 'ex', 'first', 'holder', 'second', 'holder', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'possibility', 'would', 'be', 'to', 'double', 'all', 'the', 'variables', 'considering', 'the', 'client', 'as', 'first', 'and', 'additional', 'holder', 'in', 'the', 'example', 'we', 'would', 'create', 'two', 'variables', 'the', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'as', 'fh', 'the', 'balance', 'at', 'the', 'end', 'of', 'the', 'month', 'as', 'ah', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'did', 'any', 'of', 'you', 'encounter', 'a', 'similar', 'problem', 'it', 'would', 'be', 'very', 'helpful', 'to', 'understand', 'how', 'you', 'solved', 'it', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'create', 'a', 'model', 'to', 'predict', 'the', 'propensity', 'to', 'buy', 'a', 'certain', 'product', 'as', 'my', 'proportion', 'of', 's', 'is', 'very', 'low', 'i', 'decided', 'to', 'apply', 'oversampling', 'to', 'get', 'a', 'of', 's', 'and', 'a', 'of', 's', 'now', 'i', 'want', 'to', 'discretize', 'some', 'of', 'the', 'variables', 'to', 'do', 'so', 'i', 'run', 'a', 'tree', 'for', 'each', 'variable', 'against', 'the', 'target', 'my', 'question', 'is', 'shall', 'i', 'define', 'the', 'prior', 'probabilities', 'when', 'i', 'do', 'this', 'run', 'the', 'trees', 'or', 'it', 'doesn', 't', 'matter', 'and', 'i', 'can', 'use', 'the', 'over', 'sampled', 'dataset', 'just', 'like', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'want', 'to', 'write', 'a', 'data', 'mining', 'service', 'in', 'lt', 'a', 'href', 'quot', 'http', 'golang', 'org', 'quot', 'gt', 'google', 'go', 'lt', 'a', 'gt', 'which', 'collects', 'data', 'through', 'scraping', 'and', 'apis', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'as', 'go', 'lacks', 'good', 'ml', 'support', 'i', 'would', 'like', 'to', 'do', 'the', 'ml', 'stuff', 'in', 'python', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'having', 'a', 'web', 'background', 'i', 'would', 'connect', 'both', 'services', 'with', 'something', 'like', 'rpc', 'but', 'as', 'i', 'believe', 'that', 'this', 'is', 'a', 'common', 'problem', 'in', 'data', 'science', 'i', 'think', 'that', 'there', 'is', 'some', 'better', 'solution', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'most', 'web', 'protocols', 'lack', 'at', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'ul', 'gt', 'xa', 'lt', 'li', 'gt', 'buffering', 'between', 'processes', 'lt', 'li', 'gt', 'xa', 'lt', 'li', 'gt', 'clustering', 'over', 'multiple', 'instances', 'lt', 'li', 'gt', 'xa', 'lt', 'ul', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'strong', 'gt', 'so', 'what', 'type', 'of', 'libraries', 'do', 'data', 'scientists', 'use', 'to', 'connect', 'different', 'languages', 'processes', 'lt', 'strong', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'bodo', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'been', 'toying', 'with', 'this', 'idea', 'for', 'a', 'while', 'i', 'think', 'there', 'is', 'probably', 'some', 'method', 'in', 'the', 'text', 'mining', 'literature', 'but', 'i', 'haven', 't', 'come', 'across', 'anything', 'just', 'right', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'what', 'is', 'are', 'some', 'methods', 'for', 'tackling', 'a', 'problem', 'where', 'the', 'number', 'of', 'variables', 'it', 'its', 'self', 'a', 'variable', 'this', 'is', 'not', 'a', 'missing', 'data', 'problem', 'but', 'one', 'where', 'the', 'nature', 'of', 'the', 'problem', 'fundamentally', 'changes', 'consider', 'the', 'following', 'example', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'suppose', 'i', 'want', 'to', 'predict', 'who', 'will', 'win', 'a', 'race', 'a', 'simple', 'multinomial', 'classification', 'problem', 'i', 'have', 'lots', 'of', 'past', 'data', 'on', 'races', 'plenty', 'to', 'train', 'on', 'lets', 'further', 'suppose', 'i', 'have', 'observed', 'each', 'contestant', 'run', 'multiple', 'races', 'the', 'problem', 'however', 'is', 'that', 'the', 'number', 'or', 'racers', 'is', 'variable', 'sometimes', 'there', 'are', 'only', 'racers', 'sometimes', 'there', 'are', 'as', 'many', 'as', 'racers', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'one', 'solution', 'might', 'be', 'to', 'train', 'a', 'separate', 'model', 'for', 'each', 'number', 'or', 'racers', 'resulting', 'in', 'models', 'in', 'this', 'case', 'using', 'any', 'method', 'i', 'choose', 'e', 'g', 'i', 'could', 'have', 'random', 'forests', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'another', 'solution', 'might', 'be', 'to', 'include', 'an', 'additional', 'variable', 'called', 'number_of_contestants', 'and', 'have', 'input', 'field', 'for', 'racers', 'and', 'simply', 'leave', 'them', 'blank', 'when', 'no', 'racer', 'is', 'present', 'intuitively', 'it', 'seems', 'that', 'this', 'method', 'would', 'have', 'difficulties', 'predicting', 'the', 'outcome', 'of', 'a', 'contestant', 'race', 'if', 'the', 'number', 'of', 'racers', 'follows', 'a', 'poisson', 'distribution', 'which', 'i', 'didn', 't', 'originally', 'specify', 'in', 'the', 'problem', 'but', 'i', 'am', 'saying', 'it', 'here', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thoughts', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'would', 'like', 'to', 'know', 'how', 'exactly', 'mahout', 'user', 'based', 'and', 'item', 'based', 'recommendation', 'differ', 'from', 'each', 'other', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'it', 'defines', 'that', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'mahout', 'apache', 'org', 'users', 'recommender', 'userbased', 'minutes', 'html', 'quot', 'gt', 'user', 'based', 'lt', 'a', 'gt', 'recommend', 'items', 'by', 'finding', 'similar', 'users', 'this', 'is', 'often', 'harder', 'to', 'scale', 'because', 'of', 'the', 'dynamic', 'nature', 'of', 'users', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'a', 'href', 'quot', 'https', 'mahout', 'apache', 'org', 'users', 'recommender', 'intro', 'itembased', 'hadoop', 'html', 'quot', 'gt', 'item', 'based', 'lt', 'a', 'gt', 'calculate', 'similarity', 'between', 'items', 'and', 'make', 'recommendations', 'items', 'usually', 'don', 't', 'change', 'much', 'so', 'this', 'often', 'can', 'be', 'computed', 'off', 'line', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'but', 'though', 'there', 'are', 'two', 'kind', 'of', 'recommendation', 'available', 'what', 'i', 'understand', 'is', 'that', 'both', 'these', 'will', 'take', 'some', 'data', 'model', 'say', 'or', 'as', 'item', 'item', 'value', 'or', 'user', 'user', 'value', 'where', 'value', 'is', 'not', 'mandatory', 'and', 'will', 'perform', 'all', 'calculation', 'as', 'the', 'similarity', 'measure', 'and', 'recommender', 'build', 'in', 'function', 'we', 'chose', 'and', 'we', 'can', 'run', 'both', 'user', 'item', 'based', 'recommendation', 'on', 'the', 'same', 'data', 'is', 'this', 'a', 'correct', 'assumption', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'so', 'i', 'would', 'like', 'to', 'know', 'how', 'exactly', 'and', 'in', 'which', 'all', 'aspects', 'these', 'two', 'type', 'of', 'algorithm', 'differ', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'am', 'interested', 'in', 'graph', 'problems', 'like', 'color', 'max', 'clique', 'stable', 'sets', 'etc', 'but', 'the', 'documentation', 'for', 'scipy', 'optimize', 'anneal', 'seems', 'to', 'be', 'for', 'ordinary', 'functions', 'how', 'would', 'one', 'apply', 'this', 'library', 'towards', 'graph', 'formulations', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'i', 'came', 'across', 'a', 'package', 'in', 'r', 'which', 'has', 'a', 'function', 'called', 'lt', 'code', 'gt', 'sann', 'lt', 'code', 'gt', 'for', 'simulated', 'annealing', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'code', 'gt', 'sann', 'lt', 'code', 'gt', 'uses', 'parameters', 'lt', 'code', 'gt', 'fn', 'lt', 'code', 'gt', 'and', 'lt', 'code', 'gt', 'gr', 'lt', 'code', 'gt', 'to', 'optimize', 'and', 'to', 'select', 'new', 'points', 'respectively', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'something', 'like', 'the', 'max', 'clique', 'or', 'max', 'stable', 'set', 'problems', 'lt', 'code', 'gt', 'fn', 'lt', 'code', 'gt', 'would', 'be', 'a', 'summing', 'function', 'but', 'it', 's', 'less', 'clear', 'how', 'one', 'would', 'formulate', 'lt', 'code', 'gt', 'gr', 'lt', 'code', 'gt', 'to', 'fix', 'these', 'graph', 'computations', 'in', 'these', 'cases', 'how', 'would', 'lt', 'code', 'gt', 'gr', 'lt', 'code', 'gt', 'quot', 'select', 'quot', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'my', 'data', 'looks', 'like', 'this', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'lt', 'img', 'src', 'quot', 'https', 'i', 'stack', 'imgur', 'com', 'lgwu', 'png', 'quot', 'alt', 'quot', 'enter', 'image', 'description', 'here', 'quot', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'why', 'is', 'this', 'error', 'showing', 'up', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'can', 'anyone', 'suggest', 'any', 'good', 'books', 'to', 'learn', 'hadoop', 'and', 'map', 'reduce', 'basics', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'also', 'something', 'for', 'spark', 'and', 'spark', 'streaming', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'thanks', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'although', 'i', 'have', 'seen', 'a', 'few', 'good', 'questions', 'asked', 'about', 'data', 'anonymization', 'i', 'was', 'wondering', 'if', 'there', 'were', 'answers', 'to', 'this', 'more', 'specific', 'variant', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'i', 'am', 'seeking', 'a', 'tool', 'or', 'to', 'design', 'one', 'that', 'will', 'anonymize', 'human', 'names', 'from', 'a', 'specific', 'country', 'particularly', 'first', 'names', 'in', 'unstructured', 'text', 'many', 'of', 'the', 'tools', 'that', 'i', 'have', 'seen', 'have', 'considered', 'the', 'wider', 'dimensions', 'of', 'data', 'anonymization', 'with', 'an', 'equal', 'focus', 'on', 'dates', 'of', 'birth', 'addresses', 'etc', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'an', 'imperative', 'aspect', 'is', 'that', 'it', 'needs', 'to', 'have', 'near', 'absolute', 'recall', 'the', 'major', 'pitfalls', 'as', 'far', 'as', 'i', 'can', 'see', 'are', 'diminutive', 'variants', 'quot', 'tommy', 'quot', 'instead', 'of', 'quot', 'thomas', 'quot', 'quot', 'ben', 'quot', 'instead', 'of', 'quot', 'benjamin', 'quot', 'etc', 'and', 'typos', 'these', 'two', 'factors', 'prevent', 'a', 'simple', 'regex', 'based', 'on', 'a', 'database', 'of', 'names', 'based', 'on', 'censuses', 'etc', 'lt', 'p', 'gt', 'xa']),\n",
       " Row(Body_arr=['lt', 'p', 'gt', 'few', 'things', 'in', 'life', 'give', 'me', 'pleasure', 'like', 'scraping', 'structured', 'and', 'unstructured', 'data', 'from', 'the', 'internet', 'and', 'making', 'use', 'of', 'it', 'in', 'my', 'models', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'instance', 'the', 'data', 'science', 'toolkit', 'or', 'lt', 'code', 'gt', 'rdstk', 'lt', 'code', 'gt', 'for', 'r', 'programmers', 'allows', 'me', 'to', 'pull', 'lots', 'of', 'good', 'location', 'based', 'data', 'using', 'ip', 's', 'or', 'addresses', 'and', 'the', 'lt', 'code', 'gt', 'tm', 'webmining', 'plugin', 'lt', 'code', 'gt', 'for', 'r', 's', 'lt', 'code', 'gt', 'tm', 'lt', 'code', 'gt', 'package', 'makes', 'scraping', 'financial', 'and', 'news', 'data', 'straightfoward', 'when', 'going', 'beyond', 'such', 'semi', 'structured', 'data', 'i', 'tend', 'to', 'use', 'lt', 'code', 'gt', 'xpath', 'lt', 'code', 'gt', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'however', 'i', 'm', 'constantly', 'getting', 'throttled', 'by', 'limits', 'on', 'the', 'number', 'of', 'queries', 'you', 're', 'allowed', 'to', 'make', 'i', 'think', 'google', 'limits', 'me', 'to', 'about', 'requests', 'per', 'hours', 'which', 'is', 'a', 'problem', 'for', 'big', 'data', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'from', 'a', 'lt', 'em', 'gt', 'technical', 'lt', 'em', 'gt', 'perspective', 'getting', 'around', 'these', 'limits', 'is', 'easy', 'just', 'switch', 'ip', 'addresses', 'and', 'purge', 'other', 'identifiers', 'from', 'your', 'environment', 'however', 'this', 'presents', 'both', 'ethical', 'and', 'financial', 'concerns', 'i', 'think', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'is', 'there', 'a', 'solution', 'that', 'i', 'm', 'overlooking', 'lt', 'p', 'gt', 'xa'])]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = tokenizer \\\n",
    "    .transform(samTekst) \\\n",
    "    .select('Body_arr') \n",
    "\n",
    "tok.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Body_arr: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35\n",
    "#https://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "add_stopwords = [\"http\",\"https\",\"the\", \"lt\", \"p\",\"gt\", \"#xa\", \"quot\",  \"href\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# z parametrem setStopWords usuwa słowa zdefiniowane w powyższej liście \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"Body_arr\", outputCol=\"filtered\").setStopWords(add_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'always', 'been', 'interested', 'in', 'machine', 'learning', 'but', 'i', 'can', 't', 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', 'quot', 'hello', 'world', 'quot', 'example', 'how', 'can', 'i', 'avoid', 'hard', 'coding', 'behavior', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'i', 'wanted', 'to', 'quot', 'teach', 'quot', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles', 'i', 'couldn', 't', 'just', 'use', 'relative', 'motion', 'because', 'the', 'obstacles', 'move', 'around', 'but', 'i', 'don', 't', 'want', 'to', 'hard', 'code', 'say', 'distance', 'because', 'that', 'ruins', 'the', 'whole', 'point', 'of', 'machine', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'obviously', 'randomly', 'generating', 'code', 'would', 'be', 'impractical', 'so', 'how', 'could', 'i', 'do', 'this', 'lt', 'p', 'gt', 'xa'], filtered=['i', 've', 'always', 'been', 'interested', 'in', 'machine', 'learning', 'but', 'i', 'can', 't', 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', 'hello', 'world', 'example', 'how', 'can', 'i', 'avoid', 'hard', 'coding', 'behavior', 'xa', 'xa', 'for', 'example', 'if', 'i', 'wanted', 'to', 'teach', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles', 'i', 'couldn', 't', 'just', 'use', 'relative', 'motion', 'because', 'obstacles', 'move', 'around', 'but', 'i', 'don', 't', 'want', 'to', 'hard', 'code', 'say', 'distance', 'because', 'that', 'ruins', 'whole', 'point', 'of', 'machine', 'learning', 'xa', 'xa', 'obviously', 'randomly', 'generating', 'code', 'would', 'be', 'impractical', 'so', 'how', 'could', 'i', 'do', 'this', 'xa'])]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok1=stopwordsRemover.transform(tok)\n",
    "tok1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bez parametru usuwa słowa, które ma zdefiniowane od kuchni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopwordsRemover2 = StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=['lt', 'p', 'gt', 'i', 've', 'always', 'been', 'interested', 'in', 'machine', 'learning', 'but', 'i', 'can', 't', 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', 'quot', 'hello', 'world', 'quot', 'example', 'how', 'can', 'i', 'avoid', 'hard', 'coding', 'behavior', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'for', 'example', 'if', 'i', 'wanted', 'to', 'quot', 'teach', 'quot', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles', 'i', 'couldn', 't', 'just', 'use', 'relative', 'motion', 'because', 'the', 'obstacles', 'move', 'around', 'but', 'i', 'don', 't', 'want', 'to', 'hard', 'code', 'say', 'distance', 'because', 'that', 'ruins', 'the', 'whole', 'point', 'of', 'machine', 'learning', 'lt', 'p', 'gt', 'xa', 'xa', 'lt', 'p', 'gt', 'obviously', 'randomly', 'generating', 'code', 'would', 'be', 'impractical', 'so', 'how', 'could', 'i', 'do', 'this', 'lt', 'p', 'gt', 'xa'], filtered=['i', 've', 'always', 'been', 'interested', 'in', 'machine', 'learning', 'but', 'i', 'can', 't', 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', 'hello', 'world', 'example', 'how', 'can', 'i', 'avoid', 'hard', 'coding', 'behavior', 'xa', 'xa', 'for', 'example', 'if', 'i', 'wanted', 'to', 'teach', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles', 'i', 'couldn', 't', 'just', 'use', 'relative', 'motion', 'because', 'obstacles', 'move', 'around', 'but', 'i', 'don', 't', 'want', 'to', 'hard', 'code', 'say', 'distance', 'because', 'that', 'ruins', 'whole', 'point', 'of', 'machine', 'learning', 'xa', 'xa', 'obviously', 'randomly', 'generating', 'code', 'would', 'be', 'impractical', 'so', 'how', 'could', 'i', 'do', 'this', 'xa'], filtered1=['ve', 'always', 'interested', 'machine', 'learning', 'figure', 'one', 'thing', 'starting', 'simple', 'hello', 'world', 'example', 'avoid', 'hard', 'coding', 'behavior', 'xa', 'xa', 'example', 'wanted', 'teach', 'bot', 'avoid', 'randomly', 'placed', 'obstacles', 'couldn', 'use', 'relative', 'motion', 'obstacles', 'move', 'around', 'want', 'hard', 'code', 'say', 'distance', 'ruins', 'whole', 'point', 'machine', 'learning', 'xa', 'xa', 'obviously', 'randomly', 'generating', 'code', 'impractical', 'xa'])]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2=stopwordsRemover2.transform(tok1)\n",
    "tok2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filtered1=['ve', 'always', 'interested', 'machine', 'learning', 'figure', 'one', 'thing', 'starting', 'simple', 'hello', 'world', 'example', 'avoid', 'hard-coding', 'behavior', 'example', 'wanted', 'teach', 'bot', 'avoid', 'randomly', 'placed', 'obstacles', 'couldn', 'use', 'relative', 'motion', 'obstacles', 'move', 'around', 'want', 'hard', 'code', 'say', 'distance', 'ruins', 'whole', 'point', 'machine', 'learning', 'obviously', 'randomly', 'generating', 'code', 'impractical'])]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2.select(\"filtered1\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=[\"&lt;p&gt;i've\", 'always', 'been', 'interested', 'in', 'machine', 'learning,', 'but', 'i', \"can't\", 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', '&quot;hello', 'world&quot;', 'example', '-', 'how', 'can', 'i', 'avoid', 'hard-coding', 'behavior?&lt;/p&gt;&#xa;&#xa;&lt;p&gt;for', 'example,', 'if', 'i', 'wanted', 'to', '&quot;teach&quot;', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles,', 'i', \"couldn't\", 'just', 'use', 'relative', 'motion,', 'because', 'the', 'obstacles', 'move', 'around,', 'but', 'i', \"don't\", 'want', 'to', 'hard', 'code,', 'say,', 'distance,', 'because', 'that', 'ruins', 'the', 'whole', 'point', 'of', 'machine', 'learning.&lt;/p&gt;&#xa;&#xa;&lt;p&gt;obviously,', 'randomly', 'generating', 'code', 'would', 'be', 'impractical,', 'so', 'how', 'could', 'i', 'do', 'this?&lt;/p&gt;&#xa;'], filtered=[\"&lt;p&gt;i've\", 'always', 'been', 'interested', 'in', 'machine', 'learning,', 'but', 'i', \"can't\", 'figure', 'out', 'one', 'thing', 'about', 'starting', 'out', 'with', 'a', 'simple', '&quot;hello', 'world&quot;', 'example', 'how', 'can', 'i', 'avoid', 'hard-coding', 'behavior?&lt;/p&gt;&#xa;&#xa;&lt;p&gt;for', 'example,', 'if', 'i', 'wanted', 'to', '&quot;teach&quot;', 'a', 'bot', 'how', 'to', 'avoid', 'randomly', 'placed', 'obstacles,', 'i', \"couldn't\", 'just', 'use', 'relative', 'motion,', 'because', 'obstacles', 'move', 'around,', 'but', 'i', \"don't\", 'want', 'to', 'hard', 'code,', 'say,', 'distance,', 'because', 'that', 'ruins', 'whole', 'point', 'of', 'machine', 'learning.&lt;/p&gt;&#xa;&#xa;&lt;p&gt;obviously,', 'randomly', 'generating', 'code', 'would', 'be', 'impractical,', 'so', 'how', 'could', 'i', 'do', 'this?&lt;/p&gt;&#xa;'], filtered1=[\"&lt;p&gt;i've\", 'always', 'interested', 'machine', 'learning,', 'figure', 'one', 'thing', 'starting', 'simple', '&quot;hello', 'world&quot;', 'example', 'avoid', 'hard-coding', 'behavior?&lt;/p&gt;&#xa;&#xa;&lt;p&gt;for', 'example,', 'wanted', '&quot;teach&quot;', 'bot', 'avoid', 'randomly', 'placed', 'obstacles,', 'use', 'relative', 'motion,', 'obstacles', 'move', 'around,', 'want', 'hard', 'code,', 'say,', 'distance,', 'ruins', 'whole', 'point', 'machine', 'learning.&lt;/p&gt;&#xa;&#xa;&lt;p&gt;obviously,', 'randomly', 'generating', 'code', 'impractical,', 'this?&lt;/p&gt;&#xa;'])]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok3 = tok2.select(\"filtered1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-ffeb64ac0232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtok3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \"\"\"\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         \u001b[0margs_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         args_command = \"\".join(\n\u001b[1;32m-> 1094\u001b[1;33m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         args_command = \"\".join(\n\u001b[1;32m-> 1094\u001b[1;33m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[1;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\";\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mcommand_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "tok3.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filtered1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|filtered1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[ve, always, interested, machine, learning, figure, one, thing, starting, simple, hello, world, example, avoid, hard-coding, behavior, example, wanted, teach, bot, avoid, randomly, placed, obstacles, couldn, use, relative, motion, obstacles, move, around, want, hard, code, say, distance, ruins, whole, point, machine, learning, obviously, randomly, generating, code, impractical]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[researcher, instructor, m, looking, open-source, books, similar, materials, provide, relatively, thorough, overview, data, science, applied, perspective, clear, m, especially, interested, thorough, overview, provides, material, suitable, college-level, course, particular, pieces, papers]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[sure, data, science, discussed, forum, several, synonyms, least, related, fields, large, data, analyzed, particular, question, regards, data, mining, took, graduate, class, data, mining, years, back, differences, data, science, data, mining, particular, need, look, become, proficient, data, mining]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[situations, one, system, preferred, relative, advantages, disadvantages, relational, databases, versus, non-relational, databases]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|[use, href=, www, csie, ntu, edu, tw, ~cjlin, libsvm, libsvm, train, data, predict, classification, strong, semantic, analysis, strong, problem, strong, performance, strong, issue, large-scale, data, semantic, analysis, concerns, strong, em, n-dimension, em, strong, problem, last, year, href=, www, csie, ntu, edu, tw, ~cjlin, liblinear, liblinear, release, solve, performance, bottleneck, cost, much, strong, memory, strong, strong, mapreduce, strong, way, solve, semantic, analysis, problem, big, data, methods, improve, memory, bottleneck, strong, liblinear, strong]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[lots, people, use, term, em, big, data, em, rather, em, commercial, em, way, means, indicating, large, datasets, involved, computation, therefore, potential, solutions, must, good, performance, course, em, big, data, em, always, carry, associated, terms, like, scalability, efficiency, exactly, defines, problem, em, big, data, em, problem, computation, related, set, specific, purposes, like, data, mining, information, retrieval, algorithm, general, graph, problems, labeled, em, big, data, em, dataset, em, big, enough, em, also, em, big, em, em, big, enough, em, possible, define]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[created, social, network, application, elearning, purposes, experimental, thing, researching, lab, used, case, studies, data, relational, dbms, sql, server, getting, big, gigabytes, tables, highly, connected, performance, still, fine, consider, options, matter, performance]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|[data, set, contains, number, numeric, attributes, one, categorical, say, code, numericattr, numericattr, numericattrn, categoricalattr, code, code, categoricalattr, code, takes, one, three, possible, values, code, categoricalattrvalue, code, code, categoricalattrvalue, code, code, categoricalattrvalue, code, m, using, default, k-means, clustering, algorithm, implementation, octave, href=, blog, west, uni-koblenz, de, a-working-k-means-code-for-octave, blog, west, uni-koblenz, de, a-working-k-means-code-for-octave, works, numeric, data, question, correct, split, categorical, attribute, code, categoricalattr, code, three, numeric, binary, variables, like, code, iscategoricalattrvalue, iscategoricalattrvalue, iscategoricalattrvalue, code]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[bunch, customer, profiles, stored, href=, questions, tagged, elasticsearch, class=, post-tag, title=, show, questions, tagged, amp, #, elasticsearch, amp, #, rel=, tag, elasticsearch, cluster, profiles, used, creation, target, groups, email, subscriptions, target, groups, formed, manually, using, elasticsearch, faceted, search, capabilities, like, get, male, customers, age, one, car, children, search, interesting, groups, strong, automatically, strong, using, data, science, machine, learning, clustering, something, else, href=, questions, tagged, r, class=, post-tag, title=, show, questions, tagged, amp, #, r, amp, #, rel=, tag, r, programming, language, seems, good, tool, task, form, methodology, group, search, one, solution, somehow, find, largest, clusters, customers, use, target, groups, question, strong, automatically, choose, largest, clusters, similar, customers, similar, parameters, know, moment, strong, example, program, connect, elasticsearch, offload, customer, data, csv, using, r, language, script, find, large, portion, customers, male, children, another, large, portion, customers, car, eye, color, brown]                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[working, exploratory, data, analysis, developing, algorithms, find, time, spent, cycle, visualize, write, code, run, small, dataset, repeat, data, tends, computer, vision, sensor, fusion, type, stuff, algorithms, vision-heavy, example, object, detection, tracking, etc, shelf, algorithms, work, context, find, takes, lot, iterations, example, dial, type, algorithm, tune, parameters, algorithm, get, visualization, right, also, run, times, even, small, dataset, quite, long, together, takes, algorithm, development, sped, made, scalable, specific, challenges, number, iterations, reduced, esp, kind, algorithm, let, alone, specifics, seem, easily, foreseeable, without, trying, different, versions, examining, behavior, run, bigger, datasets, development, often, going, small, large, dataset, bunch, new, behavior, new, issues, seen, algorithm, parameters, tuned, faster, apply, machine, learning, type, tools, algorithm, development, example, instead, writing, algorithm, hand, write, simple, building, blocks, combine, way, learned, problem, etc]                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[heard, many, tools, frameworks, helping, people, process, data, big, data, environment, one, called, hadoop, nosql, concept, difference, point, processing, complementary]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[r, many, libraries, aimed, data, analysis, e, g, jags, bugs, arules, etc, mentioned, popular, textbooks, j, krusche, bayesian, data, analysis, b, lantz, machine, learning, r, ve, seen, guideline, tb, dataset, considered, big, data, question, r, suitable, amount, data, typically, seen, big, data, problems, strategies, employed, using, r, size, dataset]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[r, script, generates, report, based, current, contents, database, database, constantly, flux, records, added, deleted, many, times, day, ask, computer, run, every, night, date, report, waiting, morning, perhaps, want, re-run, certain, number, new, records, added, database, might, go, automating, mention, m, windows, easily, put, script, linux, machine, simplify, process]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|[limited, dabbling, data, science, using, r, realized, cleaning, bad, data, important, part, preparing, data, analysis, best, practices, processes, cleaning, data, processing, automated, semi-automated, tools, implement, best, practices]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|[reviewing, “, href=, rads, stackoverflow, com, amzn, click, rel=, nofollow, applied, predictive, modeling, href=, www, information-management, com, blogs, applied-predictive-modeling-, html, rel=, nofollow, reviewer, states, blockquote, one, critique, statistical, learning, sl, pedagogy, absence, computation, performance, considerations, evaluation, different, modeling, techniques, emphases, bootstrapping, cross-validation, tune, test, models, sl, quite, compute-intensive, add, re-sampling, embedded, techniques, like, bagging, boosting, specter, computation, hell, supervised, learning, large, data, sets, strong, fact, r, memory, constraints, impose, pretty, severe, limits, size, models, fit, top-performing, methods, like, random, forests, strong, though, sl, good, job, calibrating, model, performance, small, data, sets, d, sure, nice, understand, performance, versus, computational, cost, larger, data, blockquote, r, memory, constraints, impose, severe, limits, size, models, fit, top-performing, methods, like, href=, en, wikipedia, org, wiki, random_forest, rel=, nofollow, random, forests]                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[logic, often, states, overfitting, model, capacity, generalize, limited, though, might, mean, overfitting, stops, model, improving, certain, complexity, overfitting, cause, models, become, worse, regardless, complexity, data, case, hr, strong, related, strong, followup, question, href=, datascience, stackexchange, com, questions, when-is-a-model-underfitted, model, underfitted]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|[first, think, worth, stating, mean, replication, amp, amp, reproducibility, ul, li, replication, analysis, results, exact, copy, inputs, processes, supply, result, incidental, outputs, analysis, b, li, li, reproducibility, analysis, results, inputs, processes, outputs, semantically, incidental, analysis, without, access, exact, inputs, processes, li, ul, putting, aside, easy, might, replicate, given, build, especially, ad-hoc, one, replication, always, possible, planned, worth, said, unclear, execute, data, science, workflow, allows, reproducibility, closet, comparison, m, able, think, href=, en, wikipedia, org, wiki, documentation_generator, rel=, nofollow, noreferrer, documentation, generators, generates, software, documentation, intended, programmers, though, main, difference, see, theory, two, sets, analysis, ran, reproducibility, documentation, generators, documentation, match, another, issue, get, concept, reproducibility, documentation, hard, time, imagining, look, like, usable, form, without, guide, replicating, analysis, lastly, whole, intent, understand, possible, bake-in, reproducibility, documentation, build, stack, stack, built, possible, automate, generating, reproducibility, documentation, look, like, hr, strong, em, update, strong, please, note, second, draft, question, href=, datascience, stackexchange, com, users, christopher-louden, christopher, louden, kind, enough, let, edit, question, realized, likely, first, draft, unclear, thanks!, em]|\n",
      "|[data, conditions, watch, p-values, may, best, way, deciding, statistical, significance, specific, problem, types, fall, category]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[small, p-values, plentiful, big, data, comparable, replacement, p-values, data, million, samples]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[em, note, pulled, question, href=, area, stackexchange, com, proposals, data-science, #, list, questions, area, believe, question, self, explanatory, said, believe, get, general, intent, question, result, likely, able, field, questions, question, might, pop-up, em, strong, big, data, technology, stack, suitable, processing, tweets, extracting, expanding, urls, pushing, new, links, rd, party, system, strong]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# teraz robimy df, w którym w pierszej kolumnie będzie klucz, a w drugiej odfiltrowane słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_withkey = tok3.withColumn(\"key\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           filtered1|key|\n",
      "+--------------------+---+\n",
      "|[always, interest...|  1|\n",
      "|[researcher, inst...|  1|\n",
      "+--------------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_withkey.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1_withkey = df1_withkey[\"key\", \"filtered1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|           filtered1|\n",
      "+---+--------------------+\n",
      "|  1|[always, interest...|\n",
      "|  1|[researcher, inst...|\n",
      "|  1|[sure, data, scie...|\n",
      "|  1|[situations, one,...|\n",
      "|  1|[use, href=, http...|\n",
      "|  1|[lots, people, us...|\n",
      "|  1|[created, social,...|\n",
      "|  1|[data, set, conta...|\n",
      "|  1|[bunch, customer,...|\n",
      "|  1|[working, explora...|\n",
      "|  1|[heard, many, too...|\n",
      "|  1|[r, many, librari...|\n",
      "|  1|[r, script, gener...|\n",
      "|  1|[limited, dabblin...|\n",
      "|  1|[reviewing, “, hr...|\n",
      "|  1|[logic, often, st...|\n",
      "|  1|[first, think, wo...|\n",
      "|  1|[data, conditions...|\n",
      "|  1|[small, values, p...|\n",
      "|  1|[em, (note:, pull...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_withkey.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- filtered1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_withkey.printSchema() # czemu jest ten string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"element\" among (key, filtered1);'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o197.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"element\" among (key, filtered1);\r\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:212)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:212)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:211)\r\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1102)\r\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1072)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-206a1ac0f59a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1_withkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"element\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"\"\"\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"element\" among (key, filtered1);'"
     ]
    }
   ],
   "source": [
    "df1_withkey[\"element\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ta sama procedura dla drugiej paczki postów z innego forum, przydzielamy inny klucz (lepiej byłoby utworzyć kolumnę z kluczem\n",
    "# na wcześniejszym etapie i wszystkie czysczenia robić na połączonej tabeli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"file:///Users/Spal/Desktop/Praca dyplomowa/Dane1/Posts.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleared=cleaningRDD(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark/python\\pyspark\\sql\\session.py:356: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samTekst=df.\\\n",
    "select('Body').\\\n",
    "where(df.PostTypeId.like('%1%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=['lt', 'p', 'gt', 'if', 'a', 'currency', 'can', 'be', 'worth', 'too', 'little', 'e', 'g', 'needing', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'and', 'worth', 'too', 'much', 'e', 'g', 'being', 'able', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'for', 'why', 'isn', 't', 'there', 'an', 'quot', 'ideal', 'value', 'quot', 'a', 'point', 'range', 'or', 'a', 'shifting', 'set', 'of', 'priorities', 'based', 'on', 'market', 'conditions', 'that', 'a', 'currency', 'could', 'be', 'fixed', 'or', 'drawn', 'toward', 'lt', 'p', 'gt', '#xa', '#xa', 'lt', 'p', 'gt', 'many', 'everyday', 'transactions', 'are', 'still', 'done', 'in', 'cash', 'requiring', 'mental', 'calculations', 'from', 'the', 'transacting', 'parties', 'too', 'large', 'numbers', 'and', 'too', 'small', 'numbers', 'create', 'transaction', 'costs', 'that', 'at', 'least', 'for', 'the', 'extremes', 'cannot', 'be', 'ignored', 'so', 'conceivably', 'some', 'quot', 'middle', 'quot', 'point', 'could', 'be', 'considered', 'quot', 'ideal', 'quot', 'as', 'regards', 'the', 'lt', 'em', 'gt', 'efficiency', 'of', 'money', 'as', 'medium', 'of', 'transactions', 'lt', 'em', 'gt', 'lt', 'p', 'gt', '#xa', '#xa', 'lt', 'p', 'gt', 'what', 'forces', 'may', 'prevent', 'such', 'an', 'quot', 'ideal', 'value', 'quot', 'for', 'a', 'currency', 'from', 'being', 'established', 'lt', 'p', 'gt', '#xa'])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = tokenizer \\\n",
    "    .transform(samTekst) \\\n",
    "    .select('Body_arr') \n",
    "\n",
    "tok.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            Body_arr|            filtered|\n",
      "+--------------------+--------------------+\n",
      "|[lt, p, gt, if, a...|[if, a, currency,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok1=stopwordsRemover.transform(tok)\n",
    "tok1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            Body_arr|            filtered|           filtered1|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[lt, p, gt, if, a...|[if, a, currency,...|[currency, worth,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok2=stopwordsRemover2.transform(tok1)\n",
    "tok2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body_arr=['lt', 'p', 'gt', 'if', 'a', 'currency', 'can', 'be', 'worth', 'too', 'little', 'e', 'g', 'needing', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'and', 'worth', 'too', 'much', 'e', 'g', 'being', 'able', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'for', 'why', 'isn', 't', 'there', 'an', 'quot', 'ideal', 'value', 'quot', 'a', 'point', 'range', 'or', 'a', 'shifting', 'set', 'of', 'priorities', 'based', 'on', 'market', 'conditions', 'that', 'a', 'currency', 'could', 'be', 'fixed', 'or', 'drawn', 'toward', 'lt', 'p', 'gt', '#xa', '#xa', 'lt', 'p', 'gt', 'many', 'everyday', 'transactions', 'are', 'still', 'done', 'in', 'cash', 'requiring', 'mental', 'calculations', 'from', 'the', 'transacting', 'parties', 'too', 'large', 'numbers', 'and', 'too', 'small', 'numbers', 'create', 'transaction', 'costs', 'that', 'at', 'least', 'for', 'the', 'extremes', 'cannot', 'be', 'ignored', 'so', 'conceivably', 'some', 'quot', 'middle', 'quot', 'point', 'could', 'be', 'considered', 'quot', 'ideal', 'quot', 'as', 'regards', 'the', 'lt', 'em', 'gt', 'efficiency', 'of', 'money', 'as', 'medium', 'of', 'transactions', 'lt', 'em', 'gt', 'lt', 'p', 'gt', '#xa', '#xa', 'lt', 'p', 'gt', 'what', 'forces', 'may', 'prevent', 'such', 'an', 'quot', 'ideal', 'value', 'quot', 'for', 'a', 'currency', 'from', 'being', 'established', 'lt', 'p', 'gt', '#xa'], filtered=['if', 'a', 'currency', 'can', 'be', 'worth', 'too', 'little', 'e', 'g', 'needing', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'and', 'worth', 'too', 'much', 'e', 'g', 'being', 'able', 'to', 'buy', 'a', 'loaf', 'of', 'bread', 'for', 'why', 'isn', 't', 'there', 'an', 'ideal', 'value', 'a', 'point', 'range', 'or', 'a', 'shifting', 'set', 'of', 'priorities', 'based', 'on', 'market', 'conditions', 'that', 'a', 'currency', 'could', 'be', 'fixed', 'or', 'drawn', 'toward', 'many', 'everyday', 'transactions', 'are', 'still', 'done', 'in', 'cash', 'requiring', 'mental', 'calculations', 'from', 'transacting', 'parties', 'too', 'large', 'numbers', 'and', 'too', 'small', 'numbers', 'create', 'transaction', 'costs', 'that', 'at', 'least', 'for', 'extremes', 'cannot', 'be', 'ignored', 'so', 'conceivably', 'some', 'middle', 'point', 'could', 'be', 'considered', 'ideal', 'as', 'regards', 'em', 'efficiency', 'of', 'money', 'as', 'medium', 'of', 'transactions', 'em', 'what', 'forces', 'may', 'prevent', 'such', 'an', 'ideal', 'value', 'for', 'a', 'currency', 'from', 'being', 'established'], filtered1=['currency', 'worth', 'little', 'e', 'g', 'needing', 'buy', 'loaf', 'bread', 'worth', 'much', 'e', 'g', 'able', 'buy', 'loaf', 'bread', 'isn', 'ideal', 'value', 'point', 'range', 'shifting', 'set', 'priorities', 'based', 'market', 'conditions', 'currency', 'fixed', 'drawn', 'toward', 'many', 'everyday', 'transactions', 'still', 'done', 'cash', 'requiring', 'mental', 'calculations', 'transacting', 'parties', 'large', 'numbers', 'small', 'numbers', 'create', 'transaction', 'costs', 'least', 'extremes', 'ignored', 'conceivably', 'middle', 'point', 'considered', 'ideal', 'regards', 'em', 'efficiency', 'money', 'medium', 'transactions', 'em', 'forces', 'may', 'prevent', 'ideal', 'value', 'currency', 'established'])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok3 = tok2.select(\"filtered1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_withkey = tok3.withColumn(\"key\", lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_withkey = df2_withkey[\"key\", \"filtered1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|           filtered1|\n",
      "+---+--------------------+\n",
      "|  2|[currency, worth,...|\n",
      "+---+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_withkey.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6041"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_withkey.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- filtered1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_withkey.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i łączymy dwa data framy w jeden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin_df = df1_withkey.union(df2_withkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14653"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key|filtered1                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |[always, interested, machine, learning, figure, one, thing, starting, simple, hello, world, example, avoid, hard, coding, behavior, example, wanted, teach, bot, avoid, randomly, placed, obstacles, use, relative, motion, obstacles, move, around, want, hard, code, say, distance, ruins, whole, point, machine, learning, obviously, randomly, generating, code, impractical]|\n",
      "|1  |[researcher, instructor, looking, open, source, books, (or, similar, materials), provide, relatively, thorough, overview, data, science, applied, perspective, clear, especially, interested, thorough, overview, provides, material, suitable, college, level, course, particular, pieces, papers]                                                                              |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fin_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- filtered1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fin_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# teraz rozbudowujemy nasz df o kolumny tf i  idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF as MLHashingTF\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "from pyspark.sql.types import DoubleType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "htf = MLHashingTF(inputCol = \"filtered1\", outputCol = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = htf.transform(fin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key|filtered1                                                                                                                                                                                                                                                                                                                                                                        |tf                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |[always, interested, machine, learning, figure, one, thing, starting, simple, hello, world, example, avoid, hard, coding, behavior, example, wanted, teach, bot, avoid, randomly, placed, obstacles, use, relative, motion, obstacles, move, around, want, hard, code, say, distance, ruins, whole, point, machine, learning, obviously, randomly, generating, code, impractical]|(262144,[1836,2437,14072,24610,29238,43223,45245,53778,65212,78329,93206,93284,96550,107499,116873,118651,124189,125372,155074,159066,163984,165160,169195,175098,181726,190256,190355,192823,204376,217228,230591,235528,236821,242532,255376,256803,258463],[2.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])|\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key=1, filtered1=['always', 'interested', 'machine', 'learning', 'figure', 'one', 'thing', 'starting', 'simple', 'hello', 'world', 'example', 'avoid', 'hard', 'coding', 'behavior', 'example', 'wanted', 'teach', 'bot', 'avoid', 'randomly', 'placed', 'obstacles', 'use', 'relative', 'motion', 'obstacles', 'move', 'around', 'want', 'hard', 'code', 'say', 'distance', 'ruins', 'whole', 'point', 'machine', 'learning', 'obviously', 'randomly', 'generating', 'code', 'impractical'], tf=SparseVector(262144, {1836: 2.0, 2437: 2.0, 14072: 1.0, 24610: 2.0, 29238: 1.0, 43223: 1.0, 45245: 2.0, 53778: 1.0, 65212: 1.0, 78329: 1.0, 93206: 1.0, 93284: 2.0, 96550: 1.0, 107499: 1.0, 116873: 1.0, 118651: 2.0, 124189: 1.0, 125372: 1.0, 155074: 1.0, 159066: 1.0, 163984: 2.0, 165160: 1.0, 169195: 1.0, 175098: 1.0, 181726: 1.0, 190256: 1.0, 190355: 1.0, 192823: 1.0, 204376: 1.0, 217228: 1.0, 230591: 1.0, 235528: 1.0, 236821: 1.0, 242532: 1.0, 255376: 2.0, 256803: 1.0, 258463: 1.0}))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: int, filtered1: array<string>, tf: vector]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- filtered1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o918.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\r\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)\r\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:651)\r\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42)\r\n\tat org.apache.spark.ml.param.Params$class.$(params.scala:658)\r\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42)\r\n\tat org.apache.spark.ml.feature.IDFBase$class.validateAndTransformSchema(IDF.scala:59)\r\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:68)\r\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:98)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:88)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-8717fc61a833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0midf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o918.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\r\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)\r\n\tat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:651)\r\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42)\r\n\tat org.apache.spark.ml.param.Params$class.$(params.scala:658)\r\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42)\r\n\tat org.apache.spark.ml.feature.IDFBase$class.validateAndTransformSchema(IDF.scala:59)\r\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:68)\r\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:98)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:88)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    " idf = MLIDF().fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o778.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 65, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:92)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-168933c55840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tu boli obliczeniowo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o778.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 65, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:92)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
